[{"paperId": "d3712c115fd077ef0789f33ba80c6e4904ca85d5", "url": "https://www.semanticscholar.org/paper/d3712c115fd077ef0789f33ba80c6e4904ca85d5", "title": "A cooperative crowdsourcing framework for knowledge extraction in digital humanities - cases on Tang poetry", "abstract": "The purpose of this paper is to propose a knowledge extraction framework to extract knowledge, including entities and relationships between them, from unstructured texts in digital humanities (DH).,The proposed cooperative crowdsourcing framework (CCF) uses both human\u2013computer cooperation and crowdsourcing to achieve high-quality and scalable knowledge extraction. CCF integrates active learning with a novel category-based crowdsourcing mechanism to facilitate domain experts labeling and verifying extracted knowledge.,The case study shows that CCF can effectively and efficiently extract knowledge from multi-sourced heterogeneous data in the field of Tang poetry. Specifically, CCF achieves higher accuracy of knowledge extraction than the state-of-the-art methods, the contribution of feedbacks to the training model can be maximized by the active learning mechanism and the proposed category-based crowdsourcing mechanism can scale up the effective human\u2013computer collaboration by considering the specialization of workers in different categories of tasks.,This research proposes CCF to enable high-quality and scalable knowledge extraction in the field of Tang poetry. CCF can be generalized to other fields of DH by introducing domain knowledge and experts.,The extracted knowledge is machine-understandable and can support the research of Tang poetry and knowledge-driven intelligent applications in DH.,CCF is the first human-in-the-loop knowledge extraction framework that integrates active learning and crowdsourcing mechanisms; he human\u2013computer cooperation method uses the feedback of domain experts through the active learning mechanism; the category-based crowdsourcing mechanism considers the matching of categories of DH data and especially of domain experts.", "venue": "Aslib Journal of Information Management", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 10}, "nlp_mention_counts": {"ke": 10}, "ld_mention_counts": {"ke": 10}, "relevance_score": 0.9996646498695336}, {"paperId": "192d084cd6d0b63dbbe56c3813b08e83cdaec5d9", "url": "https://www.semanticscholar.org/paper/192d084cd6d0b63dbbe56c3813b08e83cdaec5d9", "title": "Using Spreading Activation to Evaluate and Improve Ontologies", "abstract": "In this paper, we explore the relationship between the human-encoded semantics of ontologies and their application to natural language processing (NLP) tasks, such as word-sense disambiguation (WSD), for which such ontologies may not have been originally designed. We present a method for assessing the semantic content of an ontology with respect to a target domain, by spreading activation over a graph that represents instances of ontology concepts and relationships, in domain text. Our proposed method has several advantages beyond existing ontology metrics. By identifying bias or imbalance in the ontology, we can suggest target areas for improvement, and simultaneously facilitate the automated optimisation of the graph for use in the chosen NLP task. On applying this method to the Unified Medical Language System (UMLS) ontology, we significantly outperformed existing graph-based methods for WSD in biomedical NLP (0.82 accuracy). The subsequent introduction of a fall-back mechanism, using word-sense probability, achieved state of the art for unsupervised biomedical WSD (0.89 accuracy).", "venue": "International Conference on Computational Linguistics", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "wsd", "onto", "nlp", "onto", "nlp", "wsd", "onto", "nlp", "onto", "onto", "nlp", "wsd", "onto", "wsd"], "mention_counts": {"nlp": 4, "wsd": 4, "onto": 8}, "nlp_mention_counts": {"nlp": 4, "wsd": 4}, "ld_mention_counts": {"onto": 8}, "relevance_score": 0.9975273768433653}, {"paperId": "69aa5126098f39ada0ac4855d0f186b741bc7ed5", "url": "https://www.semanticscholar.org/paper/69aa5126098f39ada0ac4855d0f186b741bc7ed5", "title": "Towards the relationship between Semantic Web and NLP", "abstract": "With the development of Semantic the Web technology, the NLP technology has much broader prospects. This article analyses the fusion degrees between the two technologies based on the survey of relations of them. We explain the relationship between Semantic Web and NLP in two aspects. One is NLP how to support Semantic Web development in Ontology Learning, Ontology Query and Multilingual Ontology Mapping. The other is Semantic Web technologies how to improve NLP results in Information Extraction and Word Sense Disambiguation. We also propose some research challenges for the cooperation between Semantic Web and NLP.", "venue": "International Conference on Natural Language Processing and Knowledge Engineering", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "ie", "onto", "sw", "onto", "onto", "sw", "nlp", "nlp", "sw", "sw", "nlp", "nlp", "nlp", "wsd"], "mention_counts": {"onto": 3, "nlp": 6, "sw": 5, "wsd": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 6, "wsd": 1, "ie": 1}, "ld_mention_counts": {"sw": 5, "onto": 3}, "relevance_score": 0.9975273768433653}, {"paperId": "545322f1eef5a3e467a160f2d4037ea139165b4b", "url": "https://www.semanticscholar.org/paper/545322f1eef5a3e467a160f2d4037ea139165b4b", "title": "Towards Learning from User Feedback for Ontology-based Information Extraction", "abstract": "Many engineering projects involve the integration of various hardware parts from different suppliers. In preparation, parts that are best suited for the project requirements have to be selected. Information on these parts\u2019 characteristics is published in so called data sheets usually only available in textual form, e.g. as PDF files. To realize the automated processing, these characteristics have to be extracted into a machine-interpretable format. Such a process requires a lot of manual intervention and is prone to errors. Domain ontologies, among other approaches, can be used to implement the automated information extraction from the data sheets. However, ontologies rely solely on the experiences and perspectives of their creators at the time of creation. To automate the evolution of ontologies, we developed ConTrOn Continuously Trained Ontology that automatically extracts information from data sheets to augment an ontology created by domain experts. The evaluation results of ConTrOn show that the enriched ontology can help improve the information extraction from technical documents. Nonetheless, the extracted information should be reviewed by experts before using it in the integration process. We want to provide an intuitive way of reviewing, in which the extracted information will be highlighted on the data sheets. The experts will be able to accept, reject, or correct the extracted data via a graphical interface. This process of revision and correction can be leveraged by the system to improve itself: learning from its own mistakes and identifying common patterns to adapt in the next extraction iteration. This paper presents ideas how to use machine learning based on user feedback to improve the information extraction process.", "venue": "DI2KG@KDD", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "ie", "ie", "onto", "onto", "ie", "ie", "ie", "onto", "onto", "onto"], "mention_counts": {"onto": 7, "ie": 7}, "nlp_mention_counts": {"ie": 7}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.9933071490757153}, {"paperId": "1dcee66494852538a554091f41e87740a6fd32d8", "url": "https://www.semanticscholar.org/paper/1dcee66494852538a554091f41e87740a6fd32d8", "title": "Language Independent and Practical Ontology in Korean-Japanese Machine Translation Systems", "abstract": "This paper presents a semi-automatic ontology construction method using various resources, and an ontology-based word sense disambiguation method in machine translation. To acquire a reasonably practical ontology in limited time and with less manpower, we extend the Kadokawa thesaurus by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. The former can be obtained by converting valency information and case frames from previously built computational dictionaries used in machine translation. The latter can be acquired from concept co-occurrence information, which is extracted automatically from large corpora. The ontology stores rich semantic constraints among 1110 concepts, and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology. In practical machine translation systems, our word sense disambiguation method achieved a 6.0 per cent and 7.9 per cent improvement over methods that do not use an ontology for each Japanese and Korean translation.", "venue": "Lit. Linguistic Comput.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "wsd", "onto", "nlp", "onto", "mt", "onto", "wsd", "onto", "onto", "mt", "onto", "mt"], "mention_counts": {"nlp": 1, "wsd": 2, "onto": 7, "mt": 4}, "nlp_mention_counts": {"nlp": 1, "wsd": 2, "mt": 4}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.9933071490757153}, {"paperId": "a6851635364a0ea708958601def8d8a4d562375c", "url": "https://www.semanticscholar.org/paper/a6851635364a0ea708958601def8d8a4d562375c", "title": "RDF2PT: Generating Brazilian Portuguese Texts from RDF Data", "abstract": "The generation of natural language from Resource Description Framework (RDF) data has recently gained significant attention due to the continuous growth of Linked Data. A number of these approaches generate natural language in languages other than English, however, no work has been proposed to generate Brazilian Portuguese texts out of RDF. We address this research gap by presenting RDF2PT, an approach that verbalizes RDF data to Brazilian Portuguese language. We evaluated RDF2PT in an open questionnaire with 44 native speakers divided into experts and non-experts. Our results suggest that RDF2PT is able to generate text which is similar to that generated by humans and can hence be easily understood.", "venue": "LREC", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlg", "rdf", "rdf", "nle", "nlg", "rdf", "nll", "ld", "rdf", "rdf", "nlu"], "mention_counts": {"ld": 1, "nll": 1, "nlu": 1, "nlp": 1, "nlg": 2, "nle": 1, "rdf": 5}, "nlp_mention_counts": {"nlu": 1, "nll": 1, "nlp": 1, "nlg": 2, "nle": 1}, "ld_mention_counts": {"ld": 1, "rdf": 5}, "relevance_score": 0.9820137900379085}, {"paperId": "a36bf3c74af0fa2393c31db45f09c03fc2cee004", "url": "https://www.semanticscholar.org/paper/a36bf3c74af0fa2393c31db45f09c03fc2cee004", "title": "Knowledge extraction from time series and its application to surface roughness simulation", "abstract": "Knowledge extracted from time series data influences decision-making in business, medicine, manufacturing, science and in other fields. Various knowledge extraction methods have so far been proposed wherein it is typically assumed that a piece of time series data possesses a set of trends that deterministically or stochastically repeat in time. However, for noisy time series data (data having no trend) the delay maps (return maps) (x(t) ,x (t + \u03b4)) ,t =0 ,1 ,... , \u03b4 =1 ,2 ,... , N(N is a small integer), are more informative than the time series itself. This paper shows a knowledge extraction method that extracts a small set of \"if ... then ... \" rules from the return maps of a given set of time series data. A JAVA TM based tool is developed to automate the rule extraction process. This tool is also able to use the extracted rules recursively to simulate the qualitatively similar time series. The performance of the proposed knowledge extraction method (as well as the tool) is demonstrated by using an example time series (surface roughness profile of a machined surface). This exemplification demonstrates that the proposed knowledge extraction method can be used to enhance the performance of computer integrated manufacturing systems by giving those systems a means to exchange the information of nonlinear behaviors among the subsystems (process planning, quality control, and so on).", "venue": "Information, Knowledge, Systems Management", "citationCount": 20, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 6}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"ke": 6}, "relevance_score": 0.9820137900379085}, {"paperId": "513331cf8380815acc931e29346fe60e2c437c75", "url": "https://www.semanticscholar.org/paper/513331cf8380815acc931e29346fe60e2c437c75", "title": "Ontology-based Information Extraction with SOBA", "abstract": "In this paper we describe SOBA, a sub-component of the SmartWeb multi-modal dialog system. SOBA is a component for ontologybased information extraction from soccer web pages for automatic population of a knowledge base that can be used for domainspecific question answering. SOBA realizes a tight connection between the ontology, knowledge base and the information extraction component. The originality of SOBA is in the fact that it extracts information from heterogeneous sources such as tabular structures, text and image captions in a semantically integrated way. In particular, it stores extracted information in a knowledge base, and in turn uses the knowledge base to interpret and link newly extracted information with respect to already existing entities.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 99, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "kg", "ie", "ie", "ie", "kg", "ie", "kg", "kg", "ie", "ie"], "mention_counts": {"kg": 4, "onto": 2, "ie": 6}, "nlp_mention_counts": {"ie": 6}, "ld_mention_counts": {"kg": 4, "onto": 2}, "relevance_score": 0.9820137900379085}, {"paperId": "f2786451bc0a02835e6890a76f71e04e75ce55e8", "url": "https://www.semanticscholar.org/paper/f2786451bc0a02835e6890a76f71e04e75ce55e8", "title": "Ontology-based information extraction: An introduction and a survey of current approaches", "abstract": "Information extraction (IE) aims to retrieve certain types of information from natural language text by processing them automatically. For example, an IE system might retrieve information about geopolitical indicators of countries from a set of web pages while ignoring other types of information. Ontology-based information extraction (OBIE) has recently emerged as a subfield of information extraction. Here, ontologies - which provide formal and explicit specifications of conceptualizations - play a crucial role in the IE process. Because of the use of ontologies, this field is related to knowledge representation and has the potential to assist the development of the Semantic Web. In this paper, we provide an introduction to ontology-based information extraction and review the details of different OBIE systems developed so far. We attempt to identify a common architecture among these systems and classify them based on different factors, which leads to a better understanding on their operation. We also discuss the implementation details of these systems including the tools used by them and the metrics used to measure their performance. In addition, we attempt to identify the possible future directions for this field.", "venue": "Journal of information science", "citationCount": 426, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto", "tp", "ie", "ie", "sw", "onto", "onto", "ie", "onto"], "mention_counts": {"sw": 1, "onto": 5, "tp": 1, "ie": 5}, "nlp_mention_counts": {"tp": 1, "ie": 5}, "ld_mention_counts": {"sw": 1, "onto": 5}, "relevance_score": 0.9820137900379085}, {"paperId": "fd1cb28fdde85869e1e9d6c5eee5488356ee3a2b", "url": "https://www.semanticscholar.org/paper/fd1cb28fdde85869e1e9d6c5eee5488356ee3a2b", "title": "Extending the Finnish Linked Data Infrastructure with Natural Language Processing Services in FIN-CLARIAH", "abstract": "The DARIAH-EU infrastructure for Digital Humanities (DH) is often focusing on using structured data for quantitative studies, while the EU-CLARIN infrastructure deals primarily with unstructured natural language texts. However, in DH research both texts and structured data are often needed. It therefore makes sense to develop and use both infrastructures together, as suggested in the Dutch CLARIAH pro-gramme and the corresponding FIN-CLARIAH initiative in Finland, a new part of the Finnish research infrastructure road map of the Academy of Finland. This poster paper introduces work in FIN-CLARIAH relating to the idea of integrating natural language processing (NLP) tools with the Linked Open Data (LOD) Infrastructure for Digital Humanities in Finland (LODI4DH). We present a plan for NLP services to be opened as part of the Linked Data Finland (LDF.fi) platform. The new services are used on the other hand for knowledge extraction from Finnish texts for weaving LOD, and on the other hand for language DH data analyses of the published datasets in applications in many domains, such as political culture. The extended LDF.fi platform will provide users with documented APIs for NLP services using unified output formats as well as software delivery as Docker containers, to lower the bar for deployment.", "venue": "DHNB", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "lod", "ld", "lod", "ld", "nlp", "lod", "ke", "nlp", "nlp", "nlp"], "mention_counts": {"ld": 2, "nlp": 5, "lod": 3, "ke": 1}, "nlp_mention_counts": {"nlp": 5, "ke": 1}, "ld_mention_counts": {"ld": 2, "ke": 1, "lod": 3}, "relevance_score": 0.9820137900379085}, {"paperId": "4ee4e49c48d592269284aeff7eaf4f83692f7b06", "url": "https://www.semanticscholar.org/paper/4ee4e49c48d592269284aeff7eaf4f83692f7b06", "title": "Text to Insight: Accelerating Organic Materials Knowledge Extraction via Deep Learning", "abstract": "Scientific literature is one of the most significant resources for sharing knowledge. Researchers turn to scientific literature as a first step in designing an experiment. Given the extensive and growing volume of literature, the common approach of reading and manually extracting knowledge is too time consuming, creating a bottleneck in the research cycle. This challenge spans nearly every scientific domain. For the materials science, experimental data distributed across millions of publications are extremely helpful for predicting materials properties and the design of novel materials. However, only recently researchers have explored computational approaches for knowledge extraction primarily for inorganic materials. This study aims to explore knowledge extraction for organic materials. We built a research dataset composed of 855 annotated and 708,376 unannotated sentences drawn from 92,667 abstracts. We used named\u2010entity\u2010recognition (NER) with BiLSTM\u2010CNN\u2010CRF deep learning model to automatically extract key knowledge from literature. Early\u2010phase results show a high potential for automated knowledge extraction. The paper presents our findings and a framework for supervised knowledge extraction that can be adapted to other scientific domains.", "venue": "Proceedings of the Association for Information Science and Technology", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 6}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"ke": 6}, "relevance_score": 0.9820137900379085}, {"paperId": "8ddcbb5084b73097b2d605fdc23a0d38dba4a942", "url": "https://www.semanticscholar.org/paper/8ddcbb5084b73097b2d605fdc23a0d38dba4a942", "title": "Knowledge Discovery Web Service for Spatial Data Infrastructures", "abstract": "The size, volume, variety, and velocity of geospatial data collected by geo-sensors, people, and organizations are increasing rapidly. Spatial Data Infrastructures (SDIs) are ongoing to facilitate the sharing of stored data in a distributed and homogeneous environment. Extracting high-level information and knowledge from such datasets to support decision making undoubtedly requires a relatively sophisticated methodology to achieve the desired results. A variety of spatial data mining techniques have been developed to extract knowledge from spatial data, which work well on centralized systems. However, applying them to distributed data in SDI to extract knowledge has remained a challenge. This paper proposes a creative solution, based on distributed computing and geospatial web service technologies for knowledge extraction in an SDI environment. The proposed approach is called Knowledge Discovery Web Service (KDWS), which can be used as a layer on top of SDIs to provide spatial data users and decision makers with the possibility of extracting knowledge from massive heterogeneous spatial data in SDIs. By proposing and testing a system architecture for KDWS, this study contributes to perform spatial data mining techniques as a service-oriented framework on top of SDIs for knowledge discovery. We implemented and tested spatial clustering, classification, and association rule mining in an interoperable environment. In addition to interface implementation, a prototype web-based system was designed for extracting knowledge from real geodemographic data in the city of Tehran. The proposed solution allows a dynamic, easier, and much faster procedure to extract knowledge from spatial data.", "venue": "ISPRS Int. J. Geo Inf.", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 6}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"ke": 6}, "relevance_score": 0.9820137900379085}, {"paperId": "13c93d79c11df684a717f7ad046167dba2a0c34d", "url": "https://www.semanticscholar.org/paper/13c93d79c11df684a717f7ad046167dba2a0c34d", "title": "Construction of biological networks from unstructured information based on a semi-automated curation workflow", "abstract": "Abstract Capture and representation of scientific knowledge in a structured format are essential to improve the understanding of biological mechanisms involved in complex diseases. Biological knowledge and knowledge about standardized terminologies are difficult to capture from literature in a usable form. A semi-automated knowledge extraction workflow is presented that was developed to allow users to extract causal and correlative relationships from scientific literature and to transcribe them into the computable and human readable Biological Expression Language (BEL). The workflow combines state-of-the-art linguistic tools for recognition of various entities and extraction of knowledge from literature sources. Unlike most other approaches, the workflow outputs the results to a curation interface for manual curation and converts them into BEL documents that can be compiled to form biological networks. We developed a new semi-automated knowledge extraction workflow that was designed to capture and organize scientific knowledge and reduce the required curation skills and effort for this task. The workflow was used to build a network that represents the cellular and molecular mechanisms implicated in atherosclerotic plaque destabilization in an apolipoprotein-E-deficient (ApoE \u2212/\u2212 ) mouse model. The network was generated using knowledge extracted from the primary literature. The resultant atherosclerotic plaque destabilization network contains 304 nodes and 743 edges supported by 33 PubMed referenced articles. A comparison between the semi-automated and conventional curation processes showed similar results, but significantly reduced curation effort for the semi-automated process. Creating structured knowledge from unstructured text is an important step for the mechanistic interpretation and reusability of knowledge. Our new semi-automated knowledge extraction workflow reduced the curation skills and effort required to capture and organize scientific knowledge. The atherosclerotic plaque destabilization network that was generated is a causal network model for vascular disease demonstrating the usefulness of the workflow for knowledge extraction and construction of mechanistically meaningful biological networks.", "venue": "Database J. Biol. Databases Curation", "citationCount": 28, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 6}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"ke": 6}, "relevance_score": 0.9820137900379085}, {"paperId": "4c3a789655282c0d4a6e3a9f5a82f79d69a51f54", "url": "https://www.semanticscholar.org/paper/4c3a789655282c0d4a6e3a9f5a82f79d69a51f54", "title": "Template construction and Tibetan knowledge extraction", "abstract": "Entity knowledge extraction is the foundation of the Tibetan knowledge graph construction, which provides support for Tibetan question answering system, information retrieval, information extraction and other researches, and promotes national unity and social stability. This paper proposes a SVM and template-based approach to Tibetan person knowledge extraction. Through constructing the training corpus, we build the templates based the shallow parsing analysis of Tibetan syntactic, semantic features and verbs. Using the training corpus, we design a hierarchical SVM classifier to realize the entity knowledge extraction. Finally, experimental results prove the method has greater improvement in Tibetan person knowledge extraction.", "venue": "2018 International Conference on Artificial Intelligence and Big Data (ICAIBD)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "ke", "ie", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 5, "ie": 1}, "nlp_mention_counts": {"ke": 5, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 5}, "relevance_score": 0.9820137900379085}, {"paperId": "07be3fd820419251590de8e8357902d1073d06fb", "url": "https://www.semanticscholar.org/paper/07be3fd820419251590de8e8357902d1073d06fb", "title": "Method of Tibetan Person Knowledge Extraction", "abstract": "Person knowledge extraction is the foundation of the Tibetan knowledge graph construction, which provides support for Tibetan question answering system, information retrieval, information extraction and other researches, and promotes national unity and social stability. This paper proposes a SVM and template based approach to Tibetan person knowledge extraction. Through constructing the training corpus, we build the templates based the shallow parsing analysis of Tibetan syntactic, semantic features and verbs. Using the training corpus, we design a hierarchical SVM classifier to realize the entity knowledge extraction. Finally, experimental results prove the method has greater improvement in Tibetan person knowledge extraction.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke", "ke", "kg", "ke", "ke"], "mention_counts": {"ke": 5, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 5, "ie": 1}, "ld_mention_counts": {"ke": 5, "kg": 1}, "relevance_score": 0.9820137900379085}, {"paperId": "b75b467ba586e6c5c3206576ea54fc06344688ed", "url": "https://www.semanticscholar.org/paper/b75b467ba586e6c5c3206576ea54fc06344688ed", "title": "Implementation of the Multiagent-based Product Knowledge Extraction System", "abstract": "Building upon an analysis of traditional product knowledge extraction methods, a new prototype of Product Knowledge Extraction System (PKES) based on multiagent theory is developed. In this system, a Client/Server(C/S) architecture based on a local computer network is adopted for knowledge extraction. The knowledge extraction task is divided into different parts of sub-tasks and distributed to different clients. By making full use of the capability of client computers and coordination of the clients and the server, the product knowledge extraction task could be implemented efficiently. To achieve better use of the extracted product knowledge, the knowledge management functions are also integrated with the PKES system. Finally, a demonstration of the effectiveness and viability of the system is given by a simulation of the PKES.", "venue": "2009 WRI Global Congress on Intelligent Systems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 6}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"ke": 6}, "relevance_score": 0.9820137900379085}, {"paperId": "690559ba750a109252522b4150465904f97844d3", "url": "https://www.semanticscholar.org/paper/690559ba750a109252522b4150465904f97844d3", "title": "Natural language based constraints for web services", "abstract": "Natural Language Processing (NLP) is an important and beneficial field of knowledge in the modern age of software development and playing an essential role in problem solving, researchers used it to evaluate and interpret problems. Constraints perform significant role in Natural Language Processing (NLP), these have been already used in natural language generation to solve generation and ambiguity issues. OWL Syntax is challenging, writing constraints in OWL is time consuming process and understanding of the domain is also required, specifically to new users. The proposed methodologies will affluence the application process by operating natural language constraints over web to ease the commercial business. In this paper our basic aim is to write NL constraints in simple English language and purposed model automatically convert them to OWL. The purposed model contains simple NL constraints which are analyzed using NLP techniques and afterward by using RDF and RDFS, OWL is produced. The design system produces profligate and rapid NL constraints, instead of OWL in which writing constraints is very problematic and elucidate many complex complications of web application.", "venue": "2016 Sixth International Conference on Innovative Computing Technology (INTECH)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "rdf", "onto", "nlp", "onto", "nlp", "onto", "onto", "onto", "nlg"], "mention_counts": {"nlp": 5, "nlg": 1, "onto": 5, "rdf": 1}, "nlp_mention_counts": {"nlp": 5, "nlg": 1}, "ld_mention_counts": {"onto": 5, "rdf": 1}, "relevance_score": 0.9820137900379085}, {"paperId": "8c300bf6090c427631d772b875fdabf57af4257a", "url": "https://www.semanticscholar.org/paper/8c300bf6090c427631d772b875fdabf57af4257a", "title": "Semantic Similarity from Natural Language and Ontology Analysis", "abstract": "Automatic text understanding requires knowledge and, so far, machines know only what we give or teach them. As a consequence, most natural language processing (NLP) tasks crucially rely on the existence of linguistic resources that encode information about language, be it of morphological, syntactic, or semantic nature. Such resources are typically acquired via two main approaches: the knowledge-based approach, or top\u2013down, where information is manually curated by humans, and the corpus-based approach, or bottom\u2013up, where information is automatically learned from corpora. Although the latter has gained ground during the last decade\u2014benefiting from the availability of large amounts of text and from increased computing capacities\u2014the former remains fundamental for it allows us to collect reliable, fine-grained, and explicit information. Lexical knowledge bases (LKBs), also known as lexico-semantic resources, provide information about words and potentially entities, and are at the core of knowledgebased approaches. They are widely used in a variety of NLP tasks (e.g., word sense disambiguation, information retrieval, and question answering), all the more so since their traditional limitations (i.e., lack of language and domain-specific coverage) have recently started to fall. Indeed, beside the long-established process of expert-based resource creation (e.g., WordNet), Web technologies have enabled the collaborative, crowd-based construction of resources (e.g., Wikipedia and Wiktionary). This contributed to significantly widen the scope of the available machine-readable knowledge and, in the context of an already diverse landscape of LKBs, it encouraged and motivated even more the need to integrate different resources so as to make the best of them all. This book introduces linked lexical knowledge bases by giving an account of their foundations and presenting their main applications. Its target audience includes NLP practitioners or students who wish to better understand linked lexical knowledge bases, how they are built, and their typical usages and added value. The book is organized into eight chapters plus a preface, and is additionally put into perspective by a foreword (by Ido Dagan) that considers the history, recent evolution, and probable future directions of knowledge acquisition.", "venue": "Computational Linguistics", "citationCount": 39, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "nlp", "onto", "kg", "kg", "nlp", "nlp", "nlp", "wsd"], "mention_counts": {"nlp": 4, "kg": 4, "wsd": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 4, "wsd": 1}, "ld_mention_counts": {"kg": 4, "onto": 1}, "relevance_score": 0.9525741268224334}, {"paperId": "82a663d3b21a85d2b4632ffa9600662f0f147b81", "url": "https://www.semanticscholar.org/paper/82a663d3b21a85d2b4632ffa9600662f0f147b81", "title": "The MEANING Project", "abstract": "Resumen: A pesar del progreso que se realiza en el Procesamiento del Lenguaje Natural (PLN) a un estamos lejos de la Comprensii on del Lenguaje Natural. Un paso importante hacia este objetivo es el desarrollo de t ecnicas y recursos que traten conceptos en lugar de palabras. Sin embargo, si queremos construir la prr oxima generacii on de sistemas inteligentes que traten Tecnolog a de Lenguaje Humano en dominios abiertos necesitamos resolver dos tareas intermedias y complementarias: Resolucii on de la ambiguedad l exica de las palabras y enriquecimeinto automm atico y a gran escala de bases de conocimiento l exico Abstract: Progress is being made in Natural Language Processing (NLP) but there is still a long way towards Natural Language Understanding. An important step towards this goal is the development of technologies and resources that deal with concepts rather than words. However, to be able to build the next generation of intelligent open domain Human Language Technology (HLT) application systems we need to solve two complementary and intermediate tasks: Word Sense Disambi-guation (WSD) and automatic large-scale enrichment of Lexical Knowledge Bases. Knowledge Technologies aim to provide meaning to petabytes of information content our societies will generate in a near future. Information and knowledge management systems need to evolve accordingly, to help release next generation of intelligent open domain HLT to deal with the growing potential of the knowledge-rich and multilingual society. To develop a trustable semantic web infrastructure and a multilingual ontology framework to support knowledge management it is required a wide range of techniques to progressively automate the knowledge lifecycle. In particular this include extracting high level meaning from the large collections of content data and its representation and management in a common knowledge bases. Even now, building large and rich knowledge bases takes a great deal of expensive manual eeort; this has severely hampered HLT application development. For example, dozens of person-years have been invest into the development of wordnets, but the data in these resources is still not suuciently rich to support advanced concept-based HLT applications directly. Furthermore, resources produced by introspection usually fail to register what really occurs in texts. Applications will not scale up to working in the open domain without more detailed and rich general-purpose and also domain-speciic linguistic knowledge. To be able to build the next generation of intelligent open domain HLT application systems we need to solve two complementary intermediate tasks: \u2026", "venue": "Proces. del Leng. Natural", "citationCount": 6, "fieldsOfStudy": ["Sociology", "Computer Science"], "mentions": ["wsd", "kg", "sw", "kg", "nlp", "onto", "nlu", "hlt", "kg", "nlp"], "mention_counts": {"onto": 1, "nlu": 1, "nlp": 2, "sw": 1, "kg": 3, "wsd": 1, "hlt": 1}, "nlp_mention_counts": {"nlp": 2, "wsd": 1, "hlt": 1, "nlu": 1}, "ld_mention_counts": {"kg": 3, "sw": 1, "onto": 1}, "relevance_score": 0.9525741268224334}, {"paperId": "67c409686c11782e0d2385c84b67c0f00915814b", "url": "https://www.semanticscholar.org/paper/67c409686c11782e0d2385c84b67c0f00915814b", "title": "Auto Generation of Gold Standard, Class Labeled Data Set and Ontology Extension Tool [QuadW]", "abstract": "Automatic Knowledge Extraction (AKE) from domain independent, unstructured text sources is a challenging task in Natural Language Processing and Text analytics. Though, supervised learning mechanisms are very much result promising, application is painful due to the mandatory requirement of a class labeled training data set, as it involves expensive manual effort which is more time consuming. As a solution for this problem, this paper introduces a novel mechanism to build a self-learned classifier model that can automatically generate class labeled training data set for Knowledge/Information Extraction from domain independent unstructured text. Sri Lankan English newspapers (which comprise unstructured text in unconstrained domains) are the main data source for this study and a prototype was built to Professional Information Extraction with the semantic pattern Who holds/held What position, Where and When (Four words start with \u2018W\u2019, hence named \u2018QuadW\u2019). Methodology uses advanced machine learning techniques such as, a Random Forest with Adaboost ensemble algorithm to build a composite classification model. This classifier is called as self-learned since, it generates its own training data set automatically. This composite model has improved accuracy and avoided over fitting to data as well. The rule-based feature extraction algorithm and the hand-craft ontology developed, can also be considered as novel components of this study. Self-learned classifier has been extensively improved and tested to show higher accuracy with precision and recall close to one. Therefore, the classified output from the self-learned classifier can be used as a gold-standard data set for future research in Professional Information Extraction. The constructed ontology with approximately 400 facts, also can be effectively used in future researches. Further, introduced classifier can be used as a tool to extend the existing ontology as well. A novel usage of machine learning algorithms to text classification demonstrates that, this study goes with the state-of-the-art technologies.", "venue": "2019 Second International Conference on Advanced Computational and Communication Paradigms (ICACCP)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "onto", "onto", "ie", "ie", "ie", "onto", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "onto": 4, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 3}, "ld_mention_counts": {"ke": 1, "onto": 4}, "relevance_score": 0.9525741268224334}, {"paperId": "f6a36a324ee1c98b40bd430436a58e66c44cab18", "url": "https://www.semanticscholar.org/paper/f6a36a324ee1c98b40bd430436a58e66c44cab18", "title": "Toponym Disambiguation in an English-Lithuanian SMT System with Spatial Knowledge", "abstract": "This paper presents an innovative research resulting in the English-Lithuanian statistical factored phrase-based machine translation system with a spatial ontology. The system is based on the Moses toolkit and is enriched with semantic knowledge inferred from the spatial ontology. The ontology was developed on the basis of the GeoNames database (more than 15 000 toponyms), implemented in the web ontology language (OWL), and integrated into the machine translation process. Spatial knowledge was added as an additional factor in the statistical translation model and used for toponym disambiguation during machine translation. The implemented machine translation approach was evaluated against the baseline system without spatial knowledge. A multifaceted evaluation strategy including automatic metrics, human evaluation and linguistic analysis, was implemented to perform evaluation experiments. The results of the evaluation have shown a slight improvement in the output quality of machine translation with spatial knowledge.", "venue": "NODALIDA", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "onto", "onto", "onto", "mt", "mt", "mt", "mt", "onto"], "mention_counts": {"onto": 5, "mt": 5}, "nlp_mention_counts": {"mt": 5}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "71d51bcdc0a20d3142a6184990d31e8406b71e9e", "url": "https://www.semanticscholar.org/paper/71d51bcdc0a20d3142a6184990d31e8406b71e9e", "title": "Semantic Web for Machine Translation: Challenges and Directions", "abstract": "A large number of machine translation approaches have recently been developed to facilitate the fluid migration of content across languages. However, the literature suggests that many obstacles must still be dealt with to achieve better automatic translations. One of these obstacles is lexical and syntactic ambiguity. A promising way of overcoming this problem is using Semantic Web technologies. This article is an extended abstract of our systematic review on machine translation approaches that rely on Semantic Web technologies for improving the translation of texts. Overall, we present the challenges and opportunities in the use of Semantic Web technologies in Machine Translation. Moreover, our research suggests that while Semantic Web technologies can enhance the quality of machine translation outputs for various problems, the combination of both is still in its infancy.", "venue": "JT@ISWC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "sw", "sw", "mt", "sw", "sw", "mt", "mt", "mt", "sw"], "mention_counts": {"sw": 5, "mt": 5}, "nlp_mention_counts": {"mt": 5}, "ld_mention_counts": {"sw": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "35eb714fe2f21662b1446544e3262b2ae376dc6f", "url": "https://www.semanticscholar.org/paper/35eb714fe2f21662b1446544e3262b2ae376dc6f", "title": "Embedding Methods for Natural Language Processing", "abstract": "Embedding-based models are popular tools in Natural Language Processing these days. In this tutorial, our goal is to provide an overview of the main advances in this domain. These methods learn latent representations of words, as well as database entries that can then be used to do semantic search, automatic knowledge base construction, natural language understanding, etc. Our current plan is to split the tutorial into 2 sessions of 90 minutes, with a 30 minutes coffee break in the middle, so that we can cover in a first session the basics of learning embeddings and advanced models in the second session. This is detailed in the following.Part 1: Unsupervised and Supervised EmbeddingsWe introduce models that embed tokens (words, database entries) by representing them as low dimensional embedding vectors. Unsupervised and supervised methods will be discussed, including SVD, Word2Vec, Paragraph Vectors, SSI, Wsabie and others. A comparison between methods will be made in terms of applicability, type of loss function (ranking loss, reconstruction loss, classification loss), regularization, etc. The use of these models in several NLP tasks will be discussed, including question answering, frame identification, knowledge extraction and document retrieval.Part 2: Embeddings for Multi-relational DataThis second part will focus mostly on the construction of embeddings for multi-relational data, that is when tokens can be interconnected in different ways in the data such as in knowledge bases for instance. Several methods based on tensor factorization, collective matrix factorization, stochastic block models or energy-based learning will be presented. The task of link prediction in a knowledge base will be used as an application example. Multiple empirical results on the use of embedding models to align textual information to knowledge bases will also be presented, together with some demos if time permits.", "venue": "EMNLP 2014", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "nlp", "kg", "ke", "kg", "nlu", "nlp", "nlp"], "mention_counts": {"nlp": 3, "kg": 4, "ke": 1, "nlu": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1, "nlu": 1}, "ld_mention_counts": {"kg": 4, "ke": 1}, "relevance_score": 0.9525741268224334}, {"paperId": "637d726173933fdfb0729dca0e75064b005c791a", "url": "https://www.semanticscholar.org/paper/637d726173933fdfb0729dca0e75064b005c791a", "title": "Text Information Extraction Based on OWL Ontologies", "abstract": "This paper presents an information extraction system which can precisely extract semantic elements from texts by using OWL-based domain ontologies. In the system, each inputted text is viewed as a non-formalized instance of a domain ontology. And the outputted semantic elements extracted from the text are formalized elements composed of classes, individuals and properties. To perform the extraction, two extracting algorithms are proposed and used in the system. They are \"semantic information extracting algorithm\" and \"semantic information re-recognizing algorithmrdquo. When the extraction begins, ldquosemantic information extracting algorithm\" will extract classes, individuals and explicit properties from the text and store them in the database. After that, the text will be scanned by the \"semantic information re-recognizing algorithm\" and some useful information that is not found by \"semantic information extracting algorithm\" will be extracted. Experiments show that the results are accurate with two algorithms working cooperatively.", "venue": "2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "onto", "ie", "ie", "ie", "ie", "ie"], "mention_counts": {"onto": 5, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "ce718d281f21a25da6fe878dd8af5c2921271f9f", "url": "https://www.semanticscholar.org/paper/ce718d281f21a25da6fe878dd8af5c2921271f9f", "title": "Lexical Units in Ontological Semantics", "abstract": "In this paper we try to focus on the ontological semantic lexicons, one of the static knowledge sources that are used in Ontological Semantic theory for natural language processing. Ontological semantic (OS) is an approach to developing an exhaustive and detailed linguistic theory of meaning that is sufficient for NLP (natural language processing) by computers. It is interested in developing all the necessary processing and knowledge modules and combining them in a comprehensive system for a class of real-life NLP applications, such as MT, information extraction, text summarization, question answering, etc. It is a knowledge based system that required a vast amount of information regarding the world around a specific domain of application. Although we review the fundamentals of the approach, the focus is on the knowledge sources structures especially the lexicon. The paper Concentrate on some specific aspects that are key to the development of this OS approach, such as the acquisition of lexical units information, and the structure of the lexicon which is one of the central resources used in it. And some modern issues like the possibility of automation of static knowledge acquisition.", "venue": "2007 International Symposium on Computational Intelligence and Intelligent Informatics", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "nlp", "ie", "nlp", "nlp", "nlp", "kg"], "mention_counts": {"nlp": 4, "onto": 4, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 4, "ie": 1}, "ld_mention_counts": {"kg": 1, "onto": 4}, "relevance_score": 0.9525741268224334}, {"paperId": "38e399ab2ee9043bd7fa2b161f7a73c512f72f6b", "url": "https://www.semanticscholar.org/paper/38e399ab2ee9043bd7fa2b161f7a73c512f72f6b", "title": "Knowledge extraction by Internet monitoring to enhance crisis management", "abstract": "This paper presents our work on developing a system for Internet monitoring and knowledge extraction from different web documents which contain information about disasters. This system is based on ontology of the disasters domain for the knowledge extraction and it presents all the information extracted according to the kind of the disaster defined in the ontology. The system disseminates the information extracted (as a synthesis of the web documents) to the users after a filtering based on their profiles. The profile of a user is updated automatically by interactively taking into account his feedback.", "venue": "ISCRAM", "citationCount": 2, "fieldsOfStudy": ["Engineering", "Computer Science"], "mentions": ["ke", "ie", "onto", "ke", "onto", "ie", "ke"], "mention_counts": {"ke": 3, "onto": 2, "ie": 2}, "nlp_mention_counts": {"ke": 3, "ie": 2}, "ld_mention_counts": {"ke": 3, "onto": 2}, "relevance_score": 0.9525741268224334}, {"paperId": "6ff7021ad6a66f5a9f4f3e755f5d6d367e656ff7", "url": "https://www.semanticscholar.org/paper/6ff7021ad6a66f5a9f4f3e755f5d6d367e656ff7", "title": "KEEP: An Industrial Pre-Training Framework for Online Recommendation via Knowledge Extraction and Plugging", "abstract": "An industrial recommender system generally presents a hybrid list that contains results from multiple subsystems. In practice, each subsystem is optimized with its own feedback data to avoid the disturbance among different subsystems. However, we argue that such data usage may lead to sub-optimal online performance because of thedata sparsity. To alleviate this issue, we propose to extract knowledge from thesuper-domain that contains web-scale and long-time impression data, and further assist the online recommendation task (downstream task). To this end, we propose a novel industrial KnowlEdge Extraction and Plugging (KEEP) framework, which is a two-stage framework that consists of 1) a supervised pre-training knowledge extraction module on super-domain, and 2) a plug-in network that incorporates the extracted knowledge into the downstream model. This makes it friendly for incremental training of online recommendation. Moreover, we design an efficient empirical approach for KEEP and introduce our hands-on experience during the implementation of KEEP in a large-scale industrial system. Experiments conducted on two real-world datasets demonstrate that KEEP can achieve promising results. It is notable that KEEP has also been deployed on the display advertising system in Alibaba, bringing a lift of +5.4% CTR and +4.7% RPM.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "6b25e437556bc8e810c81bb8d90896a5a051c18c", "url": "https://www.semanticscholar.org/paper/6b25e437556bc8e810c81bb8d90896a5a051c18c", "title": "Multidirectional knowledge extraction process for creating behavioral personas", "abstract": "The consideration of user aspects in the design of computer systems is vital to maintaining the quality of these products. The search for user information as to their interests, needs, and behaviors, requires actions that can be costly. However, the use of such information in an inefficient, inaccurate manner, without the formation of knowledge discourages the process of user-centered development. Interface designers need to collect information and consume it efficiently. One option for documenting and efficiently consuming the information from user research is to apply the technique of personas. The extraction of knowledge in databases is the selection and processing of data with the purpose of identifying new patterns, provide greater accuracy on known patterns, and model the real world. Thus, this work presents a multidirectional process of extracting knowledge for creating behavioral personas. The processes of data mining are performed and compared with variations allowing a real knowledge extraction in the creation of personas. The process is multidirectional being able to be applied in both directions for the composition of any kind of personas. The results of the work demonstrate the extraction of knowledge originating from demographic information to the construction of personas focusing on behavioral aspects.", "venue": "IHC+CLIHC", "citationCount": 25, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "d21301e58137ee396be1ead8c9eb97784ba1083f", "url": "https://www.semanticscholar.org/paper/d21301e58137ee396be1ead8c9eb97784ba1083f", "title": "Converting unstructured and semi-structured data into knowledge", "abstract": "With the rapid growth in number and dimension of databases and database applications in business, administrative, industrial and other fields, it is necessary to examine the automatic extraction of knowledge from these large databases. Due to knowledge extraction from databases, these have become rich and safe sources for generating and verification of knowledge, and the knowledge discovery can be applied in software management, querying process, making decisions, process control and many other fields of interest. At the same time, there is a challenge in managing unstructured data. Among organizations with large concentration of unstructured information, there is a greater tendency to devote more resources to this kind of data. The acquisition of knowledge from unstructured data is often difficult and expensive. Some possible solutions on extracting useful information (knowledge) from unstructured data are provided. Knowledge extraction is the process of creation of knowledge from structured, unstructured and semi-structured data. The objective of this paper is to present the possibilities of extracting knowledge from unstructured and semi-structured data particularly. The theories and tools for knowledge extraction are the subject of the emerging field of knowledge discovery in databases (KDD). Definitions of KDD are provided and the general multistep KDD process is outlined. A brief summary of recent KDD real-world applications is also provided. Finally, the article enumerates challenges for future research and development in KDD systems.", "venue": "2013 11th RoEduNet International Conference", "citationCount": 38, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "a9b4b1ec648c6fb41855c1cfc53df2b691dbfa59", "url": "https://www.semanticscholar.org/paper/a9b4b1ec648c6fb41855c1cfc53df2b691dbfa59", "title": "Knowledge extraction and the integration by artificial life approach", "abstract": "Artificial life (A-Life) is a new paradigm to realize a phenomena of life and to extract the hidden principles. One of the most attractive features in the A-Life approach is emergence: simple elements interact with each other based on lower level rules, and then the higher level complex phenomena can emerge by interaction. The paper proposes a method for knowledge extraction and integration based on an A-Life approach. The proposed system has two parts: the knowledge extraction network and the A-Life environment. The simple elements interact in the A-Life environment and the data is transferred to the knowledge extraction network. The knowledge is extracted in the form of rules in the rule layer and then they are fed back to the simple elements in the A-Life environment. We dealt with a path planning problem as an example of A-Life environment. In the simulation, we assumed a severe condition: the position of the goal was unknown to the robots. Since the robots did not know the goal in the initial condition, the trajectory by the first robot that reached the goal is very complicated. The trajectory data which the robots had taken were inputted to the knowledge extraction network to extract rules. The trajectories become smooth step by step because of the extracted rules. We extracted various kinds of the rules using several different simple environments. By using the rules extracted from the simpler environments, the robot could reach the goal in a more complex environment.", "venue": "SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "1e34306c8f812fd3568fe0bea09449d200d438aa", "url": "https://www.semanticscholar.org/paper/1e34306c8f812fd3568fe0bea09449d200d438aa", "title": "Online Knowledge Extraction and Preference Guided Multi-Objective Optimization in Manufacturing", "abstract": "The integration of simulation-based optimization and data mining is an emerging approach to support decision-making in the design and improvement of manufacturing systems. In such an approach, knowledge extracted from the optimal solutions generated by the simulation-based optimization process can provide important information to decision makers, such as the importance of the decision variables and their influence on the design objectives, which cannot easily be obtained by other means. However, can the extracted knowledge be directly used during the optimization process to further enhance the quality of the solutions? This paper proposes such an online knowledge extraction approach that is used together with a preference-guided multi-objective optimization algorithm on simulation models of manufacturing systems. Specifically, it introduces a combination of the multi-objective evolutionary optimization algorithm, NSGA-II, and a customized data mining algorithm, called Flexible Pattern Mining (FPM), which can extract knowledge in the form of rules in an online and automatic manner, in order to guide the optimization to converge towards a decision maker\u2019s preferred region in the objective space. Through a set of application problems, this paper demonstrates how the proposed FPM-NSGA-II can be used to support higher quality decision-making in manufacturing.", "venue": "IEEE Access", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "42f239276eb4ed42f393ca58b207676cad107d70", "url": "https://www.semanticscholar.org/paper/42f239276eb4ed42f393ca58b207676cad107d70", "title": "Intelligent Methods to Extract Knowledge from Process Data in the Industrial Applications", "abstract": "Data are an expression of the language or numerical values that show some features. And the information is extracted from data for the specific purposes. The knowledge is utilized as information to construct rules that recognize patterns or make a decision. Today, knowledge extraction and application of that are broadly accomplished for the easy comprehension and the performance improvement of systems in the several industrial fields. The knowledge extraction can be achieved by some steps that include the knowledge acquisition, expression, and implementation. Such extracted knowledge is drawn by rules with data mining techniques. Clustering (CL), input space partition (ISP), neuro-fuzzy (NF), neural network (NN), extension matrix (EM), etc. are employed for the knowledge expression based upon rules. In this paper, the various approaches of the knowledge extraction are surveyed and categorized by methodologies and applied industrial fields. Also, the trend and examples of each approaches are shown in the tables and graphes using the categories such as CL, ISP, NF, NN, EM, and so on.", "venue": "Int. J. Fuzzy Log. Intell. Syst.", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "95d2ceca2e4303e73d62e5d22a7cd8e669e78a8a", "url": "https://www.semanticscholar.org/paper/95d2ceca2e4303e73d62e5d22a7cd8e669e78a8a", "title": "Knowledge Extraction for Identification of Chinese Organization Names", "abstract": "In this paper, a knowledge extraction process was proposed to extract the knowledge for identifying Chinese organization names. The knowledge extraction process utilizes the structure property, statistical property as well as partial linguistic knowledge of the organization names to extract new organizations from domain texts. The knowledge extraction processes were experimented on large amount of texts retrieved from WWW. With high standard of threshold values, new organization names can be identified with very high precision. Therefore the knowledge extraction processes can be carried out automatically to self improve the performance in the future.", "venue": "ACL 2000", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "c5f238b3f1aa123a04036373fe41e1759b704d2d", "url": "https://www.semanticscholar.org/paper/c5f238b3f1aa123a04036373fe41e1759b704d2d", "title": "Possibilistic Pertinence Feedback and Semantic Networks for Goal's Extraction", "abstract": "Pertinence Feedback is a technique that enables a u ser to interactively express his information requir ement by modifying his original query formulation with furth er information. This information is provided by exp licitly confirming the pertinent of some indicating objects and/or goals extracted by the system. Obviously th e user cannot mark objects and/or goals as pertinent until some a re extracted, so the first search has to be initiat ed by a query and the initial query specification has to be good enou gh to pick out some pertinent objects and/or goals from the Semantic Network. In this paper we present a short survey of fuzzy an d Semantic approaches to Knowledge Extraction. The goal of such approaches is to defin e flexible Knowledge Extraction Systems able to dea l with the inherent vagueness and uncertainty of the Extractio n process. It has long been recognised that interac tivity improves the effectiveness of Knowledge Extraction systems. Novice user\u2019s queries is the most natural and inter active medium of communication and recent progress in recognitio n is making it possible to build systems that inter act with the user. However, given the typical novice user\u2019s queries su bmitted to Knowledge Extraction Systems, it is easy to imagine that the effects of goal recognition errors in novi ce user\u2019s queries must be severely destructive on t he system\u2019s effectiveness. The experimental work reported in th is paper shows that the use of possibility theory i n classical Knowledge Extraction techniques for novice user\u2019s q uery processing is more robust than the use of the probability theory. Moreover, both possibilistic and probabilis tic pertinence feedback can be effectively employed to improve the effectiveness of novice user\u2019s query processing.", "venue": "ArXiv", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "24d6ee78d957192d38793b97326f9cfc4cfd73d9", "url": "https://www.semanticscholar.org/paper/24d6ee78d957192d38793b97326f9cfc4cfd73d9", "title": "On the Design of PSyKE: A Platform for Symbolic Knowledge Extraction", "abstract": "A common practice in modern explainable AI is to post-hoc explain black-box machine learning (ML) predictors \u2013 such as neural networks \u2013 by extracting symbolic knowledge out of them, in the form of either rule lists or decision trees. By acting as a surrogate model, the extracted knowledge aims at revealing the inner working of the black box, thus enabling its inspection, representation, and explanation. Various knowledge-extraction algorithms have been presented in the literature so far. Unfortunately, running implementations of most of them are currently either proof of concepts or unavailable. In any case, a unified, coherent software framework supporting them all \u2013 as well as their interchange, comparison, and exploitation in arbitrary ML workflows \u2013 is currently missing. Accordingly, in this paper we present the design of PSyKE, a platform providing general-purpose support to symbolic knowledge extraction from different sorts of black-box predictors via many extraction algorithms. Notably, PSyKE targets the extraction of symbolic knowledge in logic form , making it possible to extract first-order logic clauses as output. The extracted knowledge is thus both machine- and human-interpretable, and it can be used as a starting point for further symbolic processing\u2014e.g. automated reasoning.", "venue": "WOA", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "bb7393b8423bc49b3f63463a69ce8f9858075652", "url": "https://www.semanticscholar.org/paper/bb7393b8423bc49b3f63463a69ce8f9858075652", "title": "Workflow for Knowledge Extraction from Neural Network Classifiers", "abstract": "Artificial neural network classifiers are widespread models used by many machine learning engineers. Although due to fact they are black box models, in mission critical areas (like healthcare, finance, atomic power) when explainability is required they cannot be used even when they show higher classification performance in comparison to explainable models like decision trees. To mitigate this problem knowledge extraction algorithms have been proposed allowing to extract knowledge in different forms. Current paper gives a review of three knowledge extraction algorithms, presents their strengths and weaknesses. Finally knowledge extraction workflow utilizing abovementioned algorithms is described.", "venue": "2018 59th International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "e40c16249946936a3a68cfd705c24957a53d9751", "url": "https://www.semanticscholar.org/paper/e40c16249946936a3a68cfd705c24957a53d9751", "title": "Symbolic knowledge extraction from opaque ML predictors in PSyKE: Platform design & experiments", "abstract": "A common practice in modern explainable AI is to post-hoc explain black-box machine learning (ML) predictors \u2013 such as neural networks \u2013 by extracting symbolic knowledge out of them, in the form of either rule lists or decision trees. By acting as a surrogate model, the extracted knowledge aims at revealing the inner working of the black box, thus enabling its inspection, representation, and explanation. Various knowledge-extraction algorithms have been presented in the literature so far. Unfortunately, running implementations of most of them are currently either proofs of concept or unavailable. In any case, a unified, coherent software framework supporting them all \u2013 as well as their interchange, comparison, and exploitation in arbitrary ML workflows \u2013 is currently missing. Accordingly, in this paper we discuss the design of PSyKE, a platform providing general-purpose support to symbolic knowledge extraction from different sorts of black-box predictors via many extraction algorithms. Notably, PSyKE targets symbolic knowledge in logic form, allowing the extraction of first-order logic clauses. The extracted knowledge is thus both machine- and human-interpretable, and can be used as a starting point for further symbolic processing\u2014e.g. automated reasoning.", "venue": "Intelligenza Artificiale", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "79fe6fe1810f7dec043fc5ebb6b3ce59da90c3cf", "url": "https://www.semanticscholar.org/paper/79fe6fe1810f7dec043fc5ebb6b3ce59da90c3cf", "title": "Learning by Reading by Learning to Read", "abstract": "Knowledge-based natural language processing systems learn by reading, i.e., they process texts to extract knowledge. The performance of these systems crucially depends on knowledge about the domain of language itself, such as lexicons and ontologies to ground the semantics of the texts. In this paper we describe the architecture of the GIBRALTAR system, which is based on the OntoSem semantic analyzer, which learns by reading by learning to read. That is, while processing texts GIBRALTAR extracts both knowledge about the topics of the texts and knowledge about language (e.g., new ontological concepts and semantic mappings from previously unknown words to ontological concepts) that enables improved text processing. We present the results of initial experiments with GIBRALTAR and directions for future research.", "venue": "International Conference on Semantic Computing (ICSC 2007)", "citationCount": 26, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "tp", "tp", "nlp", "onto", "onto", "tp", "ke"], "mention_counts": {"onto": 3, "nlp": 1, "ke": 1, "kg": 1, "tp": 3}, "nlp_mention_counts": {"nlp": 1, "tp": 3, "ke": 1}, "ld_mention_counts": {"kg": 1, "onto": 3, "ke": 1}, "relevance_score": 0.9525741268224334}, {"paperId": "65f80e9ab05e251c8bf57d0a619f14fef1cd63db", "url": "https://www.semanticscholar.org/paper/65f80e9ab05e251c8bf57d0a619f14fef1cd63db", "title": "Concept recognition as a machine translation problem", "abstract": "Automated assignment of specific ontology concepts to mentions in text is a critical task in biomedical natural language processing, and the subject of many open shared tasks. Although the current state of the art involves the use of neural network language models as a post-processing step, the very large number of ontology classes to be recognized and the limited amount of gold-standard training data has impeded the creation of end-to-end systems based entirely on machine learning. Recently, Hailu et al. recast the concept recognition problem as a type of machine translation and demonstrated that sequence-to-sequence machine learning models have the potential to outperform multi-class classification approaches. We systematically characterize the factors that contribute to the accuracy and efficiency of several approaches to sequence-to-sequence machine learning through extensive studies of alternative methods and hyperparameter selections. We not only identify the best-performing systems and parameters across a wide variety of ontologies but also provide insights into the widely varying resource requirements and hyperparameter robustness of alternative approaches. Analysis of the strengths and weaknesses of such systems suggest promising avenues for future improvements as well as design choices that can increase computational efficiency with small costs in performance. Bidirectional encoder representations from transformers for biomedical text mining (BioBERT) for span detection along with the open-source toolkit for neural machine translation (OpenNMT) for concept normalization achieve state-of-the-art performance for most ontologies annotated in the CRAFT Corpus. This approach uses substantially fewer computational resources, including hardware, memory, and time than several alternative approaches. Machine translation is a promising avenue for fully machine-learning-based concept recognition that achieves state-of-the-art results on the CRAFT Corpus, evaluated via a direct comparison to previous results from the 2019 CRAFT shared task. Experiments illuminating the reasons for the surprisingly good performance of sequence-to-sequence methods targeting ontology identifiers suggest that further progress may be possible by mapping to alternative target concept representations. All code and models can be found at: https://github.com/UCDenver-ccp/Concept-Recognition-as-Translation.", "venue": "BMC Bioinformatics", "citationCount": 6, "fieldsOfStudy": ["Computer Science", "Medicine", "Biology"], "mentions": ["nlp", "onto", "mt", "mt", "onto", "onto", "mt", "mt", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 5, "mt": 4}, "nlp_mention_counts": {"nlp": 1, "mt": 4}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "51910432af30042f69afa60542a87c056b129c52", "url": "https://www.semanticscholar.org/paper/51910432af30042f69afa60542a87c056b129c52", "title": "Using multiple ontologies in information extraction", "abstract": "Ontology-Based Information Extraction (OBIE) has recently emerged as a subfield of Information Extraction (IE). Here, ontologies - which provide formal and explicit specifications of conceptualizations - play a crucial role in the information extraction process. Several OBIE systems have been implemented previously but all of them use a single ontology although multiple ontologies have been designed for many domains. We have studied the theoretical basis for using multiple ontologies in information extraction and have developed information extraction systems that use them. These systems investigate the two major scenarios for having multiple ontologies for the same domain: specializing in sub-domains and providing different perspectives. The domain of universities has been used for the former scenario through a corpus collected from university websites. For the latter, the domain of terrorist attacks and a corpus used by a previous Message Understanding Conference (MUC) have been used. The results from these two case studies indicate that using multiple ontologies in information extraction has led to a clear improvement in performance measures.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 36, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "ie", "onto", "ie", "onto", "onto", "onto", "ie", "onto", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 8, "ie": 7}, "nlp_mention_counts": {"ie": 7}, "ld_mention_counts": {"onto": 8}, "relevance_score": 0.9295345381318304}, {"paperId": "48d553d84f8844b6d86dbff55567f5fd7e9303d5", "url": "https://www.semanticscholar.org/paper/48d553d84f8844b6d86dbff55567f5fd7e9303d5", "title": "An approach for natural language understanding in GIS based on ontology", "abstract": "A natural language interface can make a geographic information system (GIS) easy to use. It allows one to use the natural language quickly and conveniently to operate in such as digital city management system or traffic guidance system. This paper discusses the method of nature language understanding in GIS based on ontology. Natural language understanding is general apply in computer or artificial intelligence research area, yet in GIS the natural language understanding is mainly concerned about spatial information. In order to implement the natural language understanding for spatial information perfectly we use the ontology model. First we put forward a generally process of natural language understanding in GIS, defined the conception of the ontology, next set up the ontology structure, ontology-based understanding model, also indicate the mechanism of natural language understanding based on ontology. Finally are a case study and a prototype, a discussion about the research deficiency and the development forecast of my research.", "venue": "Geoinformatics", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Engineering"], "mentions": ["nlu", "nlu", "nlu", "nlu", "nlp", "nlu", "onto", "onto", "onto", "nlu", "onto", "onto", "onto", "onto", "nlu"], "mention_counts": {"nlp": 1, "onto": 7, "nlu": 7}, "nlp_mention_counts": {"nlp": 1, "nlu": 7}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.9295345381318304}, {"paperId": "179957fd648e0af875bc8f3797904b7cf7955199", "url": "https://www.semanticscholar.org/paper/179957fd648e0af875bc8f3797904b7cf7955199", "title": "Word Sense Disambiguation for Ontology Learning", "abstract": "Ontology learning aims to automatically extract ontological concepts and relationships from related text repositories and is expected to be more efficient and scalable than manual ontology development. One of the challenging issues associated with ontology learning is word sense disambiguation (WSD). Most WSD research employs resources such as WordNet, text corpora, or a hybrid approach. Motivated by the large volume and richness of user-generated content in social media, this research explores the role of social media in ontology learning. Specifically, our approach exploits social media as a dynamic context rich data source for WSD. This paper presents a method and preliminary evidence for the efficacy of our proposed method for WSD. The research is in progress toward conducting a formal evaluation of the social media based method for WSD, and plans to incorporate the WSD routine into an ontology learning system in the future.", "venue": "Americas Conference on Information Systems", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "wsd", "wsd", "onto", "wsd", "wsd", "wsd", "onto", "onto", "onto", "onto", "wsd", "wsd", "wsd"], "mention_counts": {"wsd": 8, "onto": 7}, "nlp_mention_counts": {"wsd": 8}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.9295345381318304}, {"paperId": "d9ce9e2615601c0e3dbcaa1813f7020db0615226", "url": "https://www.semanticscholar.org/paper/d9ce9e2615601c0e3dbcaa1813f7020db0615226", "title": "Ontology-Based Drug Product Information Extraction System", "abstract": "Along with Internet and Infobahn is springing up, it has become an important problem that how to catch the interesting information exactly and rapidly in drug ocean. Information Extraction (IE) is a new information management technology: basing on pre-definition template, it extracts special information from semi-structured data and unstructured data. A new approach to extracting drug product information from normal document based on application ontology is presented in this paper. In the paper, we analyze the system architecture, the taxonomy of information extraction, the key technology and weighing measure of information extraction, and introduce the main frame of the system and describe how to design and implement main modules. The results of IE experiment indicate ontology-based drug information extraction system can improve the F-measure which is reflection of recall and precision. Keywords-information extraction; ontology; semantic; inquiry; domain ontology I. INFORMATION EXTRACTION TECHNOLOGY Information Extraction (IE) [1] is a new information processing technology, whose purpose is extracting specific information from semi-structured text and unstructured text according to pre-definition template, and the main function of information extraction system [2] is extracting specific factual information from text. Usually, the information extracted is described in the structured form and can be stored in database directly for being queried and further analysis. Information extraction system can not only help people find information needed easily, but also help do further information processing (e.g. data-mining, text generation) after gaining the interest information efficiently from the information content of reasonable analysis and organization. Compared with traditional information retrieval technology, information extraction has obvious advantages [3]. Information retrieval gains related documents only by match retrieval, and it can\u2019t understand the factual information of documents but equates them with insignificant materials packed with vocabulary. However, information extraction extracts effective information content by the process of text analysis, semantic analysis, structured generation and etc. Information extraction is the further step of information retrieval, because it can not only search information but also understand information for users and output information via the form appointed by users, and it is more advanced than information retrieval. II. ONTOLOGY-BASED EXTRACTION METHOD Ontology-based drug information extraction is an information extraction system which is based on ontology and can process semantic [4]. The ontology technique can make the information extracted accurate and comprehensive. The system extracts specific information from the body of the texts according to the relations, concepts and keys in ontology, and writes the extraction results into intermediate result file. The system improves the information extraction to a new level by the application of ontology in information extraction. The method does not depend on the structure of documents because it is based on ontology. In theory, if the domain ontology is powerful enough, the method can achieve high precision and recall rate in the information extraction of the domain. Because of the distributed nature of ontology, the new domain ontology can be used in different places and be created by experts in different domain. When new domain ontology has be created, if the URI of the system has been given, the information extraction based on new ontology can be done to implement the extension in new domain in further. Generally, ontology\u2013based information extraction is carried out by both knowledge engineering method and automatic training method [5]. The knowledge engineering method is that rules and templates are made artificially via analyzing and adjusting the ontology knowledge base by experts. The automatic training method is that templates and their automatic stuffed knowledge base and rules are derived via machine learning based on the given tagged example document set or the statistical method. 978-1-4244-4134-1/09/$25.00 \u00a92009 IEEE Figure 1. The general structure of information extraction system III. OVERALL DESIGN OF THE EXTRACTION SYSTEM The whole information extraction system is divided into Application Ontology, Ontology Parsing, Syntax Analysis, Dictionary Editing, Rule Generation, Information Extraction, Database, Inquiry and Statistics, and etc, showed in Fig. 1. The introductions of every module are as follows. 1) Application Ontology. Ontology is the foundation of the information extraction system and includes concepts and relations of described domain. In addition, ontology also includes the restriction of concepts and relations, domain and range of relations, hierarchical structure between concepts, and etc. Ontology is a description of domain. Information extraction based on ontology is information extraction of domain which is described by ontology. 2) Ontology Parsing. The all concepts, relations and hierarchical structure which have been described in ontology are parsed in this process. These concepts and relations are stored into database in this system. The relations between records in database reflect the hierarchical relations between concepts in ontology. 3) Dictionary Editing. In the part of Ontology Parsing, the system only records concepts and relations, but the keys which represent these concepts and relations are not recorded. However, the specific vocabulary must be extracted finally, and Dictionary Editing is assigned to manage the keys of these concepts and relations, with which the users can add, modify, and delete the keys. And so do the keys\u2019 domain and range. That is, the user can manage the domain and range of relations, and store the results into database. 4) Syntax Analysis. Syntax Analysis is a module of pretreatment. Generally, the document to be processed is document of natural language. In this system, Syntax Analysis is mainly deleting the useless components (e.g. adverb, quantifier, adjectives, and etc.) in complex sentence and reserving the available noun structures and verb structures. In this way, the precision of information extraction can be improved. 5) Rule Generation. After parsing Domain Ontology and Dictionary Editing, the Rule Generation module generates the information extraction rule according to the parsing results and Dictionary Editing results in database. The system can do information extraction with the input documents according to the information extraction rule. 6) Information Extraction. The main functions of this module are information extraction using the rules generated by Rule Generation and the output text after pretreatment of Syntax Analysis, and storing the results into database. 7) Inquiry and Statistics. The main functions of this module are inquiring the results extracted from data according to conditions users appointed, and making statistical analysis on the results. IV. BASIC PROCESS OF THE EXTRACTION SYSTEM 1) Building Domain Ontology. Generally, the ontology is built by domain expert, and it includes the concepts, relations, restrictions, and other information about domain. 2) Ontology Parser parses Domain Ontology. The domain information is extracted and in the system proposed this paper it is stored into database. 3) Running the module of Dictionary Editing. It manages the keys of ontology\u2019s concepts and relations, and the domain In qu iry a nd S ta tis tic s Resource Database Dictionary Editing Syntax Analysis Ontology Parsing Untreated Document Ontology", "venue": "International Conference on BioMedical Engineering and Informatics", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "ie", "onto", "onto", "ie", "onto", "onto", "ie", "onto", "ie", "ie", "onto", "onto", "onto", "ie", "onto", "ie", "onto", "ie", "onto", "ie", "ie", "onto", "onto", "onto", "onto", "onto", "onto", "onto", "onto", "ie", "ie", "ie", "ie", "onto", "onto", "ie", "ie", "onto", "ie", "onto", "ie", "onto", "onto", "kg", "ie", "ie", "onto", "ie", "ie", "ie", "onto", "ie", "onto", "tp", "ie", "ie", "onto", "onto", "ke", "ie", "ie", "onto", "onto", "onto", "onto", "ie", "onto", "onto", "kg", "ie", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 39, "ke": 1, "kg": 2, "tp": 1, "ie": 34}, "nlp_mention_counts": {"ke": 1, "tp": 1, "ie": 34}, "ld_mention_counts": {"kg": 2, "onto": 39, "ke": 1}, "relevance_score": 0.9230769230769231}, {"paperId": "7632bb1b34f5338cc6ddc677cfb37806a428b9ba", "url": "https://www.semanticscholar.org/paper/7632bb1b34f5338cc6ddc677cfb37806a428b9ba", "title": "The Rationale for Building an Ontology Expressly for NLP", "abstract": "In this paper we argue for the need of NLP-specific resources to support truly high level, semantically oriented applications. We describe what, in our experience, constitutes useful knowledge for such applications and why most extant resources are not sufficient for this purpose, leading our Ontological Semantics group to build its own. We suggest that extensive time and energy are being spent on resources for NLP, though not on developing ones of higher utility but, rather, on trying to discover ways of using less than ideal ones. We believe that a more useful long-term approach to the problem of knowledge acquisition for NLP would be to acquire what is needed from the outset, since it is likely that in the end such work will prove necessary anyway. Introduction. A frequent question asked of our Ontological Semantics (OntoSem) group is, what available knowledge resources do you use? WordNet? FrameNet? XTAG?, etc. The question is valid: a number of research groups are building resources that are claimed to have if not primary then secondary applicability to natural language processing (NLP). So, if one were to assume that knowledge is knowledge \u2013 with the implication that any and all knowledge is valuable \u2013 then one would expect the developers of a knowledge-based system like OntoSem to voraciously incorporate everything available. We, however, do not do this because past attempts to incorporate resources that were not built explicitly to support semantic-rich text processing were less time efficient than starting from scratch; and, in a practical, application-oriented environment like OntoSem, the potential theoretical insights from experiments in resource merging become secondary to the practical necessity of building systems. Thus, we have been developing a suite of interconnected static resources and processors that are specifically targeted at high-end applications. In this paper we present a brief overview of OntoSem, describe why a number of the most widely reported resources are less applicable to NLP than is widely believed and hoped, and present the opinion that, as a field, we should develop resources that are truly sufficient for high-end NLP rather than spend the same significant amount of time and effort attempting to utilize resources borrowed from other fields or developed for other purposes, with inevitably inferior results. A Snapshot of Ontological Semantics. OntoSem is a text processing environment that takes as input unrestricted raw text and carries out its tokenization, morphological analysis, syntactic analysis, and semantic analysis to yield formal text-meaning representations (TMRs). Text analysis relies on: \u2022 the OntoSem language-independent ontology, which is represented using its own metalanguage and currently contains around 5,500 concepts, each described by an average of 16 properties (\u201cfeatures\u201d), selected from the hundreds of properties defined in the ontology; the number of concepts is intentionally restricted, so that mappings from lexicons are many-to-one; \u2022 an OntoSem lexicon for each language processed, whose entries contain (among other information) syntactic and semantic zones (linked through special variables) as well as procedural-semantic attachments that we call \u201cmeaning procedures;\u201d the semantic zone most frequently invokes ontological concepts, either directly or with modifications, but can also describe word meaning extraontologically, for example, in terms of parameterized values of modality, aspect, time, etc., or combinations thereof; \u2022 a fact repository, which contains real-world facts represented as numbered \u201cremembered instances\u201d of ontological concepts (e.g., SPEECH-ACT-3186 is the 3186 instantiation of the concept SPEECH-ACT in the world model constructed during text processing as the embodiment of text meaning); \u2022 the OntoSem text analyzers, covering everything from tokenization to TMR creation; \u2022 the TMR language, which is the metalanguage for representing text meaning, compatible with the metalanguage of the ontology and the fact repository. Details of this approach to text processing can be found, e.g., in Nirenburg and Raskin forthcoming and Nirenburg et al. 2003. The ontology itself, a brief ontology tutorial, and an extensive lexicon tutorial can be viewed at http://ilit.umbc.edu. TMRs represent, to our knowledge, the most semantically rich, automatically generated expressions of text meaning of any extant system. They require detailed lexical and world knowledge, most of which must be manually acquired. Many believe that manual knowledge acquisition is too expensive to be feasible, so they work on circumventing this problem: some groups attempt to maximize the use of noisy knowledge in NLP applications, e.g., Krymolowski and Roth (1998); and numerous groups attempt to adapt WordNet for use in NLP (especially with respect to problems of ambiguity): e.g., Mihalcea and Moldovan (2001) automatically generate a more coarse-", "venue": "LREC", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "tp", "nlp", "nlp", "kg", "nlp", "nlp", "onto", "tp", "nlp", "onto", "onto", "nlp", "onto", "onto", "onto", "onto", "tp", "nlp", "tp", "onto", "onto", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 10, "kg": 1, "onto": 11, "tp": 4}, "nlp_mention_counts": {"nlp": 10, "tp": 4}, "ld_mention_counts": {"kg": 1, "onto": 11}, "relevance_score": 0.9230615063798328}, {"paperId": "80e3014094b189e06c05be159d7c0f76fce0fba8", "url": "https://www.semanticscholar.org/paper/80e3014094b189e06c05be159d7c0f76fce0fba8", "title": "A 2-phase frame-based knowledge extraction framework", "abstract": "We present an approach for extracting knowledge from natural language English texts where processing is decoupled in two phases. The first phase comprises several standard NLP tasks whose results are integrated in a single RDF graph of mentions. The second phase processes the mention graph with SPARQL-like mapping rules to produce a knowledge graph organized around semantic frames (i.e., prototypical descriptions of events and situations). The decoupling allows: (i) choosing different tools for the NLP tasks without affecting the remaining computation; (ii) combining the outputs of different NLP tasks in non-trivial ways, leveraging their integrated and coherent representation in a mention graph; and (iii) relating each piece of extracted knowledge to the mention(s) it comes from, leveraging the single RDF representation. We evaluate precision and recall of our approach on a gold standard, showing its competitiveness w.r.t. the state of the art. We also evaluate execution times and (sampled) accuracy on a corpus of 110K Wikipedia pages, showing the applicability of the approach on large corpora.", "venue": "ACM Symposium on Applied Computing", "citationCount": 30, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "kg", "nlp", "nlp", "ke", "kg", "ke", "rdf", "rdf"], "mention_counts": {"nlp": 3, "ke": 3, "kg": 2, "rdf": 2}, "nlp_mention_counts": {"nlp": 3, "ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 2, "rdf": 2}, "relevance_score": 0.9129351298794525}, {"paperId": "c9fce4a2ac8388ba601c6a664da7b5326d3cdf24", "url": "https://www.semanticscholar.org/paper/c9fce4a2ac8388ba601c6a664da7b5326d3cdf24", "title": "Knowledge Extraction from Texts: a method for extracting predicate-argument structures from texts", "abstract": "1. A i m s o f t h e p r o j e c t The general aim of our project is to improve the quality of existing systems extracting knowledge from texts by introducing refined lexical semantics data. The conlribution of lexical ~mantics to knowledge extraction is not new and has already been demonstrated in a few systems. Our more precise aims are to: propose and show feasability of more radical semantic classifications which facilitate lexical descriptions by factoring out as much information as possible, enhancing re-usability of linguistic ressources. We show how the different linguistic ressources can be org~mized and how they interact, investigate different levels of granularity in the semantic descriptions and their impact on the quality of the extracted knowledge. In our system, granularity is considered at two levels: (1) linguistic: linguistic knowledge representations may be more or less precise, (2) functional: most modules of our system can work independently and thus can be used ~pamtely, evaluate different algorithms for extracting knowledge, taking into account efficiency aspects, evaluate the costs of extending our system to larger sets of texts anti to differeut application domains. Our prqiect is applied to research projects descriptions (noted hereafter as RPD) where the annual work of researchers at the DER of EDF (Direction des Etudes et des Rechcrches, Electricit6 de France) is described in terms of research actions. The extracted knowledge must be sufficiently accurate to allow for the realization of the following Imrposes: (1) evaluation of the importance of the use of techniques, procedures anti equipments, (2) automatic distribution of documents in different services, (3) interrogation, e.g. who does what anti what kind of results are available, (4) identification of relations of various types between projects, (5) construction of synthesis of research activities on precise topics, and (6) creation of the 'history' of a project. About 2.000 RPD are produced each year, each of about 200 words hmg. The total vocabulary is about 50.000 different words. Texts include fairly complex linguistic constructs. We also use the EDF thesaurus (encoding for nouns: taxonomies, associative relations, and synonyms, in a broad sense). In this document, we first introduce the linguistic organization of our project, present the general form of texts and identify the type of information which mnst be extracted out of them. Next, we present a semantic representation for the extracted knowlexlge, and study in more depth the extraction of information under the form of predicate-argument and predicate-modifier structures (Jackendoff 87a, Ka~ and Fodor 63).", "venue": "COLING", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke", "ie"], "mention_counts": {"ke": 6, "ie": 1}, "nlp_mention_counts": {"ke": 6, "ie": 1}, "ld_mention_counts": {"ke": 6}, "relevance_score": 0.9129351298794525}, {"paperId": "e594246abe604262265597c547225bc89cd57413", "url": "https://www.semanticscholar.org/paper/e594246abe604262265597c547225bc89cd57413", "title": "Knowledge Extraction Based on Sentence Matching and Analyzing", "abstract": "Knowledge extraction extracts knowledge from documents by analyzing document contents, then markups knowledge attribute and stores them into knowledge base. This paper discusses definition of knowledge extraction at first, then presents goal, key technique, application and system architecture of knowledge extraction based on sentence matching and analyzing. The system identifies new sentences in a paper, and makes an analysis of the sentences from internal structure and subject semantics. Then justify knowledge metadata by analyzing association among sentences and pragmatic of the whole paper. At last, allocates knowledge attribute tag for extracted knowledge, such as definition, historic development, characteristics, key technique, classification, application and development trend in the future. Knowledge extraction based on sentence matching and analyzing can not only justify the academic copying or scientific citation automatically, as well as make a document reviewed automatically, but also achieve a conversion on analysis from paper and chapter to sentence and paragraph, which can induce a revolution in knowledge organization and management. Therefore, the study has far-reaching theoretical significance and wide application.", "venue": "2008 International Symposium on Knowledge Acquisition and Modeling", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 6}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"kg": 1, "ke": 6}, "relevance_score": 0.9129351298794525}, {"paperId": "196a6ac3065c6a01db89395ffea501cd8042bfd6", "url": "https://www.semanticscholar.org/paper/196a6ac3065c6a01db89395ffea501cd8042bfd6", "title": "Ontology-based word sense disambiguation using semi-automatically constructed ontology", "abstract": "This paper describes a method for disambiguating word senses by using semi-automatically constructed ontology. The ontology stores rich semantic constraints among 1,110 concepts, and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology. In order to acquire a reasonably practical ontology in limited time and with less manpower, we extend the existing Kadokawa thesaurus by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. The former can be obtained by converting valency information and case frames from previously-built electronic dictionaries used in machine translation. The latter can be acquired from concept co-occurrence information, which is extracted automatically from large corpora. In our practical machine translation system, our word sense disambiguation method achieved a 9.2% improvement over methods which do not use an ontology for Korean translation.", "venue": "MTSUMMIT", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "mt", "wsd", "onto", "wsd", "onto", "onto", "nlp", "onto", "mt", "onto", "wsd"], "mention_counts": {"nlp": 1, "wsd": 3, "onto": 7, "mt": 2}, "nlp_mention_counts": {"nlp": 1, "wsd": 3, "mt": 2}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.9129351298794525}, {"paperId": "56ea86bc7fe656e0b512f9e3efd165ba2f78d3f3", "url": "https://www.semanticscholar.org/paper/56ea86bc7fe656e0b512f9e3efd165ba2f78d3f3", "title": "Semantic-based lightweight ontology learning framework: a case study of intrusion detection ontology", "abstract": "Building ontology for wireless network intrusion detection is an emerging method for the purpose of achieving high accuracy, comprehensive coverage, self-organization and flexibility for network security. In this paper, we leverage the power of Natural Language Processing (NLP) and Crowdsourcing for this purpose by constructing lightweight semi-automatic ontology learning framework which aims at developing a semantic-based solution-oriented intrusion detection knowledge map using documents from Scopus. Our proposed framework uses NLP as its automatic component and Crowdsourcing is applied for the semi part. The main intention of applying both NLP and Crowdsourcing is to develop a semi-automatic ontology learning method in which NLP is used to extract and connect useful concepts while in uncertain cases human power is leveraged for verification. This heuristic method shows a theoretical contribution in terms of lightweight and timesaving ontology learning model as well as practical value by providing solutions for detecting different types of intrusions.", "venue": "WI", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto", "nlp", "onto", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 5, "onto": 6}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.8824434265896761}, {"paperId": "5972b34e313a03dcdf3bd81589cd54069d8f439d", "url": "https://www.semanticscholar.org/paper/5972b34e313a03dcdf3bd81589cd54069d8f439d", "title": "Using Background Knowledge to Support Coreference Resolution", "abstract": "Systems based on statistical and machine learning methods have been shown to be extremely effective and scalable for the analysis of large amount of textual data. However, in the recent years, it becomes evident that one of the most important direction of improvement in natural language processing (NLP) tasks, like word sense disambiguation, coreference resolution, relation extraction, and other tasks related to knowledge extraction, is by exploiting semantics. While in the past, the unavailability of rich and complete semantic descriptions constituted a serious limitation of their applicability, nowadays, the Semantic Web made available a large amount of logically encoded information (e.g. ontologies, RDF(S)-data, linked data, etc.), which constitute a valuable source of semantics. However, web semantics cannot be easily plugged into machine learning systems. Therefore the objective of this paper is to define a reference methodology for combining semantics information available in the web under the form of logical theories, with statistical methods for NLP. The major problems that we have to solve to implement our methodology concern (i) the selection of the correct and minimal knowledge among the large amount available in the web, (ii) the representation of uncertain knowledge, and (iii) the resolution and the encoding of the rules that combine knowledge retrieved from Semantic Web sources with semantics in the text. In order to evaluate the appropriateness of our approach, we present an application of the methodology to the problem of intra-document coreference resolution, and we show by means of some experiments on the ACE 2005 dataset, how the injection of knowledge is correlated to the improvement of the performance of our approach on this tasks.", "venue": "European Conference on Artificial Intelligence", "citationCount": 34, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ke", "onto", "wsd", "sw", "ld", "nlp", "nlp", "nlp", "sw"], "mention_counts": {"onto": 1, "ld": 1, "sw": 3, "ke": 1, "nlp": 3, "wsd": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1, "wsd": 1}, "ld_mention_counts": {"ld": 1, "sw": 3, "onto": 1, "ke": 1}, "relevance_score": 0.8824434265896761}, {"paperId": "08e036080bac77c2a5f57f9cec78ce4eb4674d9b", "url": "https://www.semanticscholar.org/paper/08e036080bac77c2a5f57f9cec78ce4eb4674d9b", "title": "Knowledge Extraction by Using an Ontology Based Annotation Tool", "abstract": "This paper describes a Semantic Annotation Tool for extraction of knowledge structures from web pages through the use of simple user-defined knowledge extraction patterns. The semantic annotation tool contains: an ontology-based mark-up component which allows the user to browse and to mark-up relevant pieces of information; a learning component (Crystal from the University of Massachusetts at Amherst) which learns rules from examples and an information extraction component which extracts the objects and relation between these objects. Our final aim is to provide support for ontology population by using the information extraction component. Our system uses as domain of study \u201cKMi Planet\u201d, a Webbased news server that helps to communicate relevant information between members in our institute.", "venue": "Semannot@K-CAP 2001", "citationCount": 118, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ke", "ie", "ie", "onto", "ke", "ke"], "mention_counts": {"ke": 3, "onto": 3, "ie": 2}, "nlp_mention_counts": {"ke": 3, "ie": 2}, "ld_mention_counts": {"ke": 3, "onto": 3}, "relevance_score": 0.8824434265896761}, {"paperId": "3a61020c61a81af745da72e423d3ec9429f9e196", "url": "https://www.semanticscholar.org/paper/3a61020c61a81af745da72e423d3ec9429f9e196", "title": "Research on Ontology Construction and Information Extraction Technology Based on WordNet", "abstract": "Introducing ontology in the field of information extraction can effectively improve the performance of information extraction. Ontology of college teachers\u2019 resumes are constructed in the form three layers of ontology framework structure on the basis of exploring the related technologies and criterion for building ontology, the Racer inference engine is used for realizing the consistency and accuracy detection. WordNet similarity calculation method is improved on the basis of this, so that manual collection method can be integrated into WordNet semantic similarity, thus substantially improving this method. The test shows that the result precisions based on WordNet ontology construction and information extraction both see significant improvement, which sufficiently shows the feasibility and validity of the method. Subject Categories and Descriptors: I.2.10 [Vision and Scene Understanding]: Information Extraction; I.4.10 [Image Representation] General Terms: Ontotology Construction, Information Processing", "venue": "J. Digit. Inf. Manag.", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "onto", "onto", "onto", "ie", "ie", "ie", "ie"], "mention_counts": {"onto": 6, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.8824434265896761}, {"paperId": "6ddbf0dbb5e24fcf2d730440f974653bdaf0ee65", "url": "https://www.semanticscholar.org/paper/6ddbf0dbb5e24fcf2d730440f974653bdaf0ee65", "title": "Open domain knowledge extraction: inference on a web scale", "abstract": "Though ontologies are considered central to foster the Semantic Web effort, their practical application on a Web scale is limited by the difficulty of building and maintaining high-coverage ontologies, for any of the almost unbounded number of social and technical domains mirrored on the Web. In this paper we present Open Knowledge Extraction (Open KE), a novel paradigm that creates a bridge between information retrieval, taxonomy learning and automated reasoning. Open KE builds on recently published algorithms for Open Information Extraction (Open IE) and automated taxonomy learning, which were shown able to extract information on a Web scale basis in an unsupervised manner. The key idea of Open KE is to generalize Open IE's lexicalized extractions with an automatically learned taxonomy. Lexicalized extractions are transformed in logic predicates, and used to populate a Semantic Model. An inference engine is then used to perform inductions and deductions. In this paper we describe the knowledge extraction workflow in its entirety, and apply it to a large-scale experiment in the domains of Artificial Intelligence and Virology.", "venue": "Web Intelligence, Mining and Semantics", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ke", "ke", "onto", "ke", "sw", "ie", "ie"], "mention_counts": {"ke": 3, "sw": 1, "onto": 2, "ie": 2}, "nlp_mention_counts": {"ke": 3, "ie": 2}, "ld_mention_counts": {"ke": 3, "sw": 1, "onto": 2}, "relevance_score": 0.8824434265896761}, {"paperId": "5446dec6e0ae8283a333915d0da23f6b8a900291", "url": "https://www.semanticscholar.org/paper/5446dec6e0ae8283a333915d0da23f6b8a900291", "title": "Automatic Production of an Ontology with NLP: Comparison between a Prolog Based Approach and a Cloud Approach Based on Bluemix Watson Service", "abstract": "Nowadays, most of the information available on the web is in Natural Language. Extracting such knowledge from Natural Language text is an essential work and a very remarkable research topic in the Semantic Web field. The logic programming language Prolog, based on the definite-clause formalism, is a useful tool for implementing a Natural Language Processing (NLP) systems. However, web-based services for NLP have also been developed recently, and they represent an important alternative to be considered. In this paper we present the comparison between two different approaches in NLP, for the automatic creation of an OWL ontology supporting the semantic annotation of text. The first one is a pure Prolog approach, based on grammar and logic analysis rules. The second one is based on Watson Relationship Extraction service of IBM Cloud platform Bluemix. We evaluate the two approaches in terms of performance, the quality of NLP result, OWL completeness and richness.", "venue": "2016 10th International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "sw", "onto", "nlp", "onto", "nlp", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 6, "sw": 1, "onto": 4}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.8824434265896761}, {"paperId": "fc7df3544c4a2d2df3b76bac5f66865760316051", "url": "https://www.semanticscholar.org/paper/fc7df3544c4a2d2df3b76bac5f66865760316051", "title": "Legal Knowledge Extraction for Knowledge Graph Based Question-Answering", "abstract": "This paper presents the Open Knowledge Extraction (OKE) tools combined with natural language analysis of the sentence in order to enrich the semantic of the legal knowledge extracted from legal text. In particular the use case is on international private law with specific regard to the Rome I Regulation EC 593/2008, Rome II Regulation EC 864/2007, and Brussels I bis Regulation EU 1215/2012. A Knowledge Graph (KG) is built using OKE and Natural Language Processing (NLP) methods jointly with the main ontology design patterns defined for the legal domain (e.g., event, time, role, agent, right, obligations, jurisdiction). Using critical questions, underlined by legal experts in the domain, we have built a question answering tool capable to support the information retrieval and to answer to these queries. The system should help the legal expert to retrieve the relevant legal information connected with topics, concepts, entities, normative references in order to integrate his/her searching activities.", "venue": "International Conference on Legal Knowledge and Information Systems", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "kg", "ke", "kg", "ke", "nlp", "onto"], "mention_counts": {"nlp": 2, "kg": 2, "onto": 1, "ke": 3}, "nlp_mention_counts": {"nlp": 2, "ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 1, "kg": 2}, "relevance_score": 0.8824434265896761}, {"paperId": "ae2506d664044faf92d9b953db8a60b9c1917bc1", "url": "https://www.semanticscholar.org/paper/ae2506d664044faf92d9b953db8a60b9c1917bc1", "title": "A knowledge extraction process specification for today's non-semantic Web", "abstract": "The semantic Web shall enable Web agents an efficient, precise, and comprehensive extraction of knowledge. Nevertheless, this new Web is not likely to be adopted in the immediate future. We present a specification of a new framework in order to extract knowledge from today's dynamics nonsemantic Web. Our proposal is novel in that it associates semantics with the information extracted, which improves agent interoperability; it can also deal with changes to the structure of a Web page, which improves adaptability; furthermore, it achieves to delegate the knowledge extraction procedure to specialist agents, easing software development and promoting software reuse and maintainability.", "venue": "Proceedings IEEE/WIC International Conference on Web Intelligence (WI 2003)", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ke", "ke", "sw", "ke", "ie", "ke"], "mention_counts": {"sw": 2, "ke": 4, "ie": 1}, "nlp_mention_counts": {"ke": 4, "ie": 1}, "ld_mention_counts": {"sw": 2, "ke": 4}, "relevance_score": 0.8824434265896761}, {"paperId": "49bfdcee046bde9e981261874d82ecc224a4e8e1", "url": "https://www.semanticscholar.org/paper/49bfdcee046bde9e981261874d82ecc224a4e8e1", "title": "Arabic Ontology Learning from Un-structured Text", "abstract": "Ontology Learning (OL) from a text is a process that consists of text processing, knowledge extraction, and ontology construction. For Arabic language, text processing, and knowledge extraction tasks are not mature as for Latin languages. They have not been integrated into the full Arabic OL pipeline. Currently, there is very little automated support for using knowledge from Arabic literature in semantically-enabled systems. This paper demonstrates the feasibility of using some existing OL methods for Arabic text and elicits proposals for further work toward building open domain OL systems for Arabic. This is done by building an OL system based on some available NLP tools for Arabic text utilizing GATE text analysis system for corpus and annotation management. The prototype is evaluated similarly to other OL systems and its performance is promising and recommended to enable more effective research and application of Arabic ontology learning.", "venue": "2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "tp", "onto", "nlp", "tp", "ke", "onto", "onto", "onto"], "mention_counts": {"nlp": 1, "ke": 2, "onto": 4, "tp": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2, "tp": 2}, "ld_mention_counts": {"ke": 2, "onto": 4}, "relevance_score": 0.8824434265896761}, {"paperId": "a737ec98d397a146e5f694cd2f3531187ff2517d", "url": "https://www.semanticscholar.org/paper/a737ec98d397a146e5f694cd2f3531187ff2517d", "title": "A spatio-temporal emotional framework for knowledge extraction and mining in digital humanities", "abstract": "PurposeThis paper aims to construct a spatio-temporal emotional framework (STEF) for digital humanities from a quantitative perspective, applying knowledge extraction and mining technology to promote innovation of humanities research paradigm and method.Design/methodology/approachThe proposed STEF uses methods of information extraction, sentiment analysis and geographic information system to achieve knowledge extraction and mining. STEF integrates time, space and emotional elements to visualize the spatial and temporal evolution of emotions, which thus enriches the analytical paradigm in digital humanities.FindingsThe case study shows that STEF can effectively extract knowledge from unstructured texts in the field of Chinese Qing Dynasty novels. First, STEF introduces the knowledge extraction tools \u2013 MARKUS and DocuSky \u2013 to profile character entities and perform plots extraction. Second, STEF extracts the characters' emotional evolutionary trajectory from the temporal and spatial perspective. Finally, the study draws a spatio-temporal emotional path figure of the leading characters and integrates the corresponding plots to analyze the causes of emotion fluctuations.Originality/valueThe STEF is constructed based on the \u201cspatio-temporal narrative theory\u201d and \u201cemotional narrative theory\u201d. It is the first framework to integrate elements of time, space and emotion to analyze the emotional evolution trajectories of characters in novels. The execuability and operability of the framework is also verified with a case novel to suggest a new path for quantitative analysis of other novels.", "venue": "Aslib J. Inf. Manag.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ie", "ke"], "mention_counts": {"ke": 5, "ie": 1}, "nlp_mention_counts": {"ke": 5, "ie": 1}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.8824434265896761}, {"paperId": "1f8dcd54faaf7863ce300add1c373cca8f8df795", "url": "https://www.semanticscholar.org/paper/1f8dcd54faaf7863ce300add1c373cca8f8df795", "title": "Comparison of Natural Language Processing Tools for Automatic Gene Ontology Annotation of Scientific Literature", "abstract": "Manual curation of scientific literature for ontology-based knowledge representation has proven infeasible and unscalable to the large and growing volume of scientific literature. Automated annotation solutions that leverage text mining and Natural Language Processing (NLP) have been developed to ameliorate the problem of literature curation. These NLP approaches use parsing, syntactical, and lexical analysis of text to recognize and annotate pieces of text with ontology concepts. Here, we conduct a comparison of four state of the art NLP tools at the task of recognizing Gene Ontology concepts from biomedical literature using the Colorado Richly Annotated Full-Text (CRAFT) corpus as a gold standard reference. We demonstrate the use of semantic similarity metrics to compare NLP tool annotations to the gold standard.", "venue": "International Conference on Biomedical Ontology", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "onto", "onto", "nlp", "kg", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 6, "onto": 4, "kg": 1}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"kg": 1, "onto": 4}, "relevance_score": 0.8824434265896761}, {"paperId": "84a8f09c5d2c98655323df0aba242c7811e1debe", "url": "https://www.semanticscholar.org/paper/84a8f09c5d2c98655323df0aba242c7811e1debe", "title": "The Ontology of Natural Language Processing", "abstract": "In this paper, we describe our proposed methodology for constructing an ontology of natural language processing (NLP). We use a semi-automatic method; a combination of rule-based and machine learning techniques; to construct and populate an ontology with bilingual (English-Persian) concept labels (lexicon) and evaluate it manually. This methodology results in a complete ontology in the natural language processing domain with 887 concepts, 88 relations, and 71 features. The built ontology is populated with near 36000 NLP related papers and 32000 authors, and about 201000 \"is_Related_to\", 83500 \"is_Author_of\", and 29000 \"Presented_in\" relations. The instantiation is done to enable applications find experts, publications and institutions related to various topics in NLP field.", "venue": "2019 5th International Conference on Web Research (ICWR)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "nlp", "onto", "onto", "nlp", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 6, "onto": 5}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.8824434265896761}, {"paperId": "80ee6d1d28ca1f3f67a8f2a1889c14afda27623c", "url": "https://www.semanticscholar.org/paper/80ee6d1d28ca1f3f67a8f2a1889c14afda27623c", "title": "Study on Word Sense Disambiguation Knowledge Base Based on Multi-Sources", "abstract": "Although there are many resources used in Natural Language Process, a specialized knowledge base used in word sense disambiguation(WSD) is still a shortage. By extracting knowledge from different resources and using them, we can improve the accuracy rate of word sense disambiguation. In this article, we use different methods existed to extract properties from The Grammatical Knowledge-base of Contemporary Chinese(GKB), HowNet, The Word-Sense Tagging Corpus (STC) and The Semantic Knowledge-base of Contemporary Chinese(SKCC) to build a specific knowledge base, which can help us with Chinese word sense disambiguation.", "venue": "2011 3rd International Workshop on Intelligent Systems and Applications", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "kg", "ke", "wsd", "wsd", "kg", "nlp", "kg", "kg", "kg"], "mention_counts": {"nlp": 1, "kg": 5, "wsd": 3, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "wsd": 3}, "ld_mention_counts": {"kg": 5, "ke": 1}, "relevance_score": 0.8824434265896761}, {"paperId": "b8167b9aca9a8263d9ddd54843467d2c616be1ec", "url": "https://www.semanticscholar.org/paper/b8167b9aca9a8263d9ddd54843467d2c616be1ec", "title": "Emotion Detection for Social Robots Based on NLP Transformers and an Emotion Ontology", "abstract": "For social robots, knowledge regarding human emotional states is an essential part of adapting their behavior or associating emotions to other entities. Robots gather the information from which emotion detection is processed via different media, such as text, speech, images, or videos. The multimedia content is then properly processed to recognize emotions/sentiments, for example, by analyzing faces and postures in images/videos based on machine learning techniques or by converting speech into text to perform emotion detection with natural language processing (NLP) techniques. Keeping this information in semantic repositories offers a wide range of possibilities for implementing smart applications. We propose a framework to allow social robots to detect emotions and to store this information in a semantic repository, based on EMONTO (an EMotion ONTOlogy), and in the first figure or table caption. Please define if appropriate. an ontology to represent emotions. As a proof-of-concept, we develop a first version of this framework focused on emotion detection in text, which can be obtained directly as text or by converting speech to text. We tested the implementation with a case study of tour-guide robots for museums that rely on a speech-to-text converter based on the Google Application Programming Interface (API) and a Python library, a neural network to label the emotions in texts based on NLP transformers, and EMONTO integrated with an ontology for museums; thus, it is possible to register the emotions that artworks produce in visitors. We evaluate the classification model, obtaining equivalent results compared with a state-of-the-art transformer-based model and with a clear roadmap for improvement.", "venue": "Italian National Conference on Sensors", "citationCount": 20, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlp", "nlp", "onto", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 4}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "a3379e7827331de5b182674e49e62f675bfc0a55", "url": "https://www.semanticscholar.org/paper/a3379e7827331de5b182674e49e62f675bfc0a55", "title": "Identification and classification of relations for Indian languages using machine learning approaches for developing a domain specific ontology", "abstract": "Information extraction and classification using Natural Language Processing techniques of layered architecture such as pre-processing task, processing of semantic analysis etc., helps in implementing further deeper evaluation techniques for the accuracy of natural language based electronic database. This paper explores relational information extraction of multilingual IndoWordNet database matching with domain specific terms. Further, extracted information are processed through conventional statistical methods, Normalized Web Distance (NWD) similarity method and two other machine learning evaluation techniques such as Support Vector Machine (SVM), Neural Network (NN) to compare with their accuracy. Results of machine leaning based techniques outperform with significant improved accuracy over conventional methods. The objective of using these techniques along with semantic web technology is to initiate a proof of concept for ontology generation by identification and classification of extracted relational information from IndoWordNet. This paper also highlights domain specific challenges and issues in developing relational model of ontology.", "venue": "International Conference on Computational Techniques in Information and Communication Technologies", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "sw", "nlp", "ie", "onto", "ie", "onto", "onto"], "mention_counts": {"nlp": 1, "sw": 1, "onto": 3, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ie": 3}, "ld_mention_counts": {"sw": 1, "onto": 3}, "relevance_score": 0.8807970779778823}, {"paperId": "17034f59ed7224f923d3808c04403f4de92f430b", "url": "https://www.semanticscholar.org/paper/17034f59ed7224f923d3808c04403f4de92f430b", "title": "[Research on information extraction of electronic medical records in Chinese].", "abstract": "This is a research to enhance the application of natural language understanding and ontology in the Chinese medical text semantic annotation and content analysis, and so to provide technology support for the computer-readable electronic medical records (EMR). The Chinese EMR information extraction and statistical analysis of related subjects in accordance to the user's demands were performed through building the named entity rules, the classified word list and field ontology by using GATE platform on the basis of EMR text set's construction and pre-processing. The automatic and artificial semantic annotation of EMR text set was implemented. The situation of drugs used in medicinal treatment and the distribution of patients' age and sex were obtained. The ontology-based semantic information extraction can improve the function of computer for text understanding, and the discovery of knowledge in EMR through field ontology is feasible.", "venue": "Sheng wu yi xue gong cheng xue za zhi = Journal of biomedical engineering = Shengwu yixue gongchengxue zazhi", "citationCount": 5, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ie", "onto", "ie", "ie", "onto", "nlu", "onto", "onto"], "mention_counts": {"onto": 4, "nlu": 1, "ie": 3}, "nlp_mention_counts": {"nlu": 1, "ie": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "27664ed94886d6d9d6e3f129ff6aa0dbb203182b", "url": "https://www.semanticscholar.org/paper/27664ed94886d6d9d6e3f129ff6aa0dbb203182b", "title": "Extracting Occupational Therapy Concepts to Develop Domain Ontology", "abstract": "Recently, unstructured data on the World Wide Web has generated significant interest in the extraction of text, emails, web pages, reports and research papers in their raw form. Far more interestingly, extracting information from a specific domain using distributed corpora from the World Wide Web is a vital step towards creating corpus annotation. This paper describes a method of annotation, based on Occupational Therapy (OT) concepts, to build domain ontology using Natural Language Programming (NLP) technology. We used Java Annotation Patterns Engine (JAPE) grammar to support regular expression matching and thus annotate OT concepts using a GATE developer tool. This speeds up the time-consuming development of the ontology, which is important for experts in the domain facing time constraints and high workloads. The rules provide significant results: the pattern matching of OT concepts based on the lookup list produced 403 correct concepts and the accuracy was generally higher. Using NLP technique is a good approach to reducing the domain expert's work, and the results can be evaluated . Keywords-Ontology; Information extracting; Regular expression; Natural Language Programming.", "venue": "ICDS 2013", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 4, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "850f1698dd11cc6970ff1fdbe0241ca23b15e557", "url": "https://www.semanticscholar.org/paper/850f1698dd11cc6970ff1fdbe0241ca23b15e557", "title": "Web Information Extraction for the Creation of Metadata in Semantic Web", "abstract": "In this paper, we develop an automatic metadata creation system using the information extraction technology for the Semantic Web. The information extraction system consists of preparation part that takes written text as the input and produces the POS tags for the words in the sentences. Then we employ finite state machine technology to extract the units from the tagged sequences, including complex words, basic phrases and domain events. We use the components of an NLP software architecture, GATE, as the processing engine and support all required language resources for the engine. We have carried out an experiment on Chinese financial news. It shows promising precision rate while it need further investigation on the recall part. We describe the implementation of storing the extracted result in RDF to an RDF server and show the service interface for accessing the content.", "venue": "ROCLING/IJCLCLP", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "nlp", "sw", "ie", "sw", "rdf", "ie", "ie"], "mention_counts": {"nlp": 1, "sw": 2, "rdf": 2, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ie": 3}, "ld_mention_counts": {"sw": 2, "rdf": 2}, "relevance_score": 0.8807970779778823}, {"paperId": "049b334a318777610e22baa6536437a9100c1adc", "url": "https://www.semanticscholar.org/paper/049b334a318777610e22baa6536437a9100c1adc", "title": "frances: A Deep Learning NLP and Text Mining Web Tool to Unlock Historical Digital Collections: A Case Study on the Encyclopaedia Britannica", "abstract": "This work presents frances, an integrated text mining tool that combines information extraction, knowledge graphs, NLP, deep learning, parallel processing and Semantic Web techniques to unlock the full value of historical digital textual collections, offering new capabilities for researchers to use powerful analysis methods without being distracted by the technology and middleware details. To demonstrate these capabilities, we use the first eight editions of the Encyclopaedia Britannica offered by the National Library of Scotland (NLS) as an example digital collection to mine and analyse. We have developed novel parallel heuristics to extract terms from the original collection (alongside metadata), which provides a mix of unstructured and semi-structured input data, and populated a new knowledge graph with this information. Our Natural Language Processing models enable frances to perform advanced analyses that go significantly beyond simple search using the information stored in the knowledge graph. Furthermore, frances also allows for creating and running complex text mining analyses at scale. Our results show that the novel computational techniques developed within frances provide a vehicle for researchers to formalize and connect findings and insights derived from the analysis of large-scale digital corpora such as the Encyclopaedia Britannica.", "venue": "IEEE International Conference on e-Science", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "ie", "nlp", "kg", "kg", "kg", "sw"], "mention_counts": {"nlp": 3, "sw": 1, "kg": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "ie": 1}, "ld_mention_counts": {"kg": 3, "sw": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "b953c0e848cbf25ccef65e89566e8c04c464efb0", "url": "https://www.semanticscholar.org/paper/b953c0e848cbf25ccef65e89566e8c04c464efb0", "title": "Ontology-Based Interactive Information Extraction", "abstract": "Interactive Information Extraction brings together search and \ninformation extraction to provide fast, interactive text mining over \nlarge volumes of text such as Medline abstracts, full text scientific \narticles, patents etc. As well as covering the two ends of the spectrum: \nkeyword search over documents, and detailed linguistic patterns within \nsentences, the Interactive Information Extraction System, I2E, also \ncovers the points in between such as keywords within the same sentence, \nor co-occurrence of biological entities within sentences or documents. \nThis talk briefly introduces the idea of Interactive Information \nExtraction, and describes how terminologies/ontologies are incorporated. \nWe also show how I2E can be used to augment ontologies by finding \npotential synonyms or members of classes from the literature using \nlinguistic patterns. Finally we discuss issues concerning how best to \nuse ontologies for text mining.", "venue": "Ontologies and Text Mining for Life Sciences", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie", "onto", "ie", "ie", "onto"], "mention_counts": {"onto": 4, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "d7be0d0d30c8d2bdf08e755ab89dd9bc7c3d7b41", "url": "https://www.semanticscholar.org/paper/d7be0d0d30c8d2bdf08e755ab89dd9bc7c3d7b41", "title": "A semantic framework for extracting taxonomic relations from text corpus", "abstract": "Nowadays, ontologies have been exploited in many current applications due to the abilities in representing knowledge and inferring new knowledge. However, the manual construction of ontologies is tedious and time-consuming. Therefore, the automated ontology construction from text has been investigated. The extraction of taxonomic relations between concepts is a crucial step in constructing domain ontologies. To obtain taxonomic relations from a text corpus, especially when the data is deficient, the approach of using the web as a source of collective knowledge (a.k.a web-based approach) is usually applied. The important challenge of this approach is how to collect relevant knowledge from a large amount of web pages. To overcome this issue, we propose a framework that combines Word Sense Disambiguation (WSD) and web approach to extract taxonomic relations from a domain-text corpus. This framework consists of two main stages: concept extraction and taxonomic-relation extraction. Concepts acquired from the concept-extraction stage are disambiguated through WSD module and passed to stage of extraction taxonomic relations afterward. To evaluate the efficiency of the proposed framework, we conduct experiments on datasets about two domains of tourism and sport. The obtained results show that the proposed method is efficient in corpora which are insufficient or have no training data. Besides, the proposed method outperforms the state of the art method in corpora having high WSD results.", "venue": "\u02dcThe \u0153international Arab journal of information technology", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "onto", "wsd", "wsd", "onto", "onto", "wsd", "onto"], "mention_counts": {"wsd": 4, "onto": 4}, "nlp_mention_counts": {"wsd": 4}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "092312cb2a59502a2db7ffea1e31c19ad2eecc7c", "url": "https://www.semanticscholar.org/paper/092312cb2a59502a2db7ffea1e31c19ad2eecc7c", "title": "H2-Golden-Retriever: Methodology and Tool for an Evidence-Based Hydrogen Research Grantsmanship", "abstract": "Background of Study: Hydrogen is poised to play a major role in decarbonizing the economy. The need to discover, develop, and understand low-cost, high-performance, durable materials that can help maximize the cost of electrolysis as well as the need for an intelligent tool to make evidence-based Hydrogen research funding decisions relatively easier warranted this study. Aim: In this work, we developed H2 Golden Retriever (H2GR) system for Hydrogen knowledge discovery and representation using Natural Language Processing (NLP), Knowledge Graph and Decision Intelligence. This system represents a novel methodology encapsulating state-of-the-art technique for evidence-based research grantmanship. Methods: Relevant Hydrogen papers were scraped and indexed from the web and preprocessing was done using noise and stop-words removal, language and spell check, stemming and lemmatization. The NLP tasks included Named Entity Recognition using Stanford and Spacy NER, topic modeling using Latent Dirichlet Allocation and TF-IDF. The Knowledge Graph module was used for the generation of meaningful entities and their relationships, trends and patterns in relevant H2 papers, thanks to an ontology of the hydrogen production domain. The Decision Intelligence component provides stakeholders with a simulation environment for cost and quantity dependencies. the abstract. Technical articles of interest are then read to find relevant information and further insights to orient their keyword searches in a more promising direction. This makes keeping up with the torrential pace of publications very difficult, cumbersome, time-consuming and labor-intensive. Since Hydrogen production cost depends on multiple assumptions including materials, electrolyzer design, manufacturing practices, and soft costs, it is difficult to trace back the contribution of the price of a component to the overall system cost. This makes cost of a component to vary from one technical report to another; for instance, a study[12] reported that bipolar plates is the third contributor behind the Catalyst Coated Membrane (CCM) and Porous Transport Layer (PTL) to the stack cost while another article[13] reported it as the highest contributor. It is therefore hard to crystallize on which sub-component deserves the highest focus. Lastly, experts are on a race against time. Understanding how one component affects the overall cost is one thing but how much money must be invested in versus the time it will take to reduce the overall system cost must drive the decisions of the experts. In this work, we addressed the missing link between the slew of information available in literature and the decision to fund research and, by implication, propose an AI-augmented decision tool for knowledge acquisition, knowledge extraction and an evidence-based research funding decision support tool. techniques in include word tokenization, word stemming and lemmatization, topical modeling, named-entity recognition, summarization, word cloud and keyword extraction. These tasks use both linguistics and mathematics to connect the language of with the language of computers.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "nlp", "kg", "ke", "nlp", "onto"], "mention_counts": {"nlp": 3, "kg": 2, "onto": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1}, "ld_mention_counts": {"kg": 2, "onto": 1, "ke": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "1d118d42b61e11a230ee6ae5f5a50db585ff3ac8", "url": "https://www.semanticscholar.org/paper/1d118d42b61e11a230ee6ae5f5a50db585ff3ac8", "title": "Knowledge Extraction from Question and Answer Platforms on the Semantic Web: A Systematic Review of Technologies Available for Information Extraction", "abstract": "Knowledge Extraction is the process of getting structured data from unstructured or semi-structured sources. Much research has been conducted in this field and applying these technologies to the web has become a key effort in the past few years. This is due to changes from web 1.0 where the web was simply a set of static pages where user interaction was minimal. With the rise of web 2.0, the internet is no longer a medium to access static information. Users can now share their own thoughts easily thus increasing the amount of user generated content. This has made the web ripe with knowledge, however not all information can be easily accessed. This paper aims to bridge the gap between knowledge available and the knowledge accessed using knowledge extraction.", "venue": "2018 8th International Conference on Intelligent Systems, Modelling and Simulation (ISMS)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "ke", "ke", "sw"], "mention_counts": {"ke": 3, "sw": 1, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3, "sw": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "efe99ed29946cafdea6e328dc80538561fb35c12", "url": "https://www.semanticscholar.org/paper/efe99ed29946cafdea6e328dc80538561fb35c12", "title": "Knowledge Extraction: A Comparison between Symbolic and Connectionist Methods", "abstract": "The use of a linguistic representation for expressing knowledge acquired by learning systems is an important issue as regards to user understanding. Under this assumption, and to make sure that these systems will be welcome and used, several techniques have been developed by the artificial intelligence community, under both the symbolic and the connectionist approaches. This work discusses and investigates three knowledge extraction techniques based on these approaches. The first two techniques, the C4.5 and CN2 symbolic learning algorithms, extract knowledge directly from the data set. The last technique, the TREPAN algorithm extracts knowledge from a previously trained neural network. The CN2 algorithm induces if...then rules from a given data set. The C4.5 algorithm extracts decision trees, although it can also extract ordered rules, from the data set. Decision trees are also the knowledge representation used by the TREPAN algorithm.", "venue": "Int. J. Neural Syst.", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "ffdcc01ac23c1511060882d901ab78ae92e52e0a", "url": "https://www.semanticscholar.org/paper/ffdcc01ac23c1511060882d901ab78ae92e52e0a", "title": "Endless and Scalable Knowledge Table Extraction from Semi-structured Websites", "abstract": "The problem of scalable knowledge extraction from the Web has attracted much attention in the past decade. However, it is under explored how to extract the structured knowledge from semi-structured Websites in a fully automatic and scalable way. In this work, we define the table-formatted structured data with clear schema as Knowledge Tables and propose a scalable learning system, which is named as Kable to extract knowledge from semi-structured Websites automatically in a never ending and scalable way. Kable consists of two major components, which are auto wrapper induction and schema matching respectively. In contrast to the state of the art auto wrappers for semi-structured Web sites, our adopted approach can run around 1'000 times faster, which makes the Web scale knowledge extraction possible. On the other hand, we propose a novel schema matching solution which can work effectively on the auto-extracted structured data. With 3 months' continuous run using ten Web servers, we successfully extracted 427,105,009 knowledge facts. The manual labeling over sampled knowledge extracted show the up to 87% precision for supporting various Web applications.", "venue": "2012 IEEE 12th International Conference on Data Mining Workshops", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "dadeefebaba33607d90572c36b7b427b309fab6d", "url": "https://www.semanticscholar.org/paper/dadeefebaba33607d90572c36b7b427b309fab6d", "title": "Mining the disaster hotspots - situation-adaptive crowd knowledge extraction for crisis management", "abstract": "When disaster strikes, emergency professionals rapidly need to gain Situation Awareness (SAW) on the unfolding crisis situation, thus need to determine what has happened and where help and resources are needed. Nowadays, platforms like Twitter are used as real-time communication hub for sharing such information, like humans' on-site observations, advice and requests, and thus can serve as a network of \u201chuman sensors\u201d for retrieving information on crisis situations. Recently, so-called crowd-sensing systems for crisis management have started to utilize these networks for harvesting crisis-related social media content. However, up to now these mainly support their human operators in the visual analysis of retrieved messages only and do not aim at the automated extraction and fusion of semantically-grounded descriptions of the underlying real-world crisis events from these textual contents, such as providing structured descriptions of the types and locations of reported damage. This hampers further computational situation assessment, such as providing overall description of the on-going crisis situation, its associated consequences and required response actions. Consequently, this lack of semantically-grounded situational context does not allow to fully implement situation-adaptive crowd knowledge extraction, meaning the system can utilize already established (crowd) knowledge to correspondingly adapt its crowd-sensing and knowledge extraction process alongside the monitored situation, to keep pace with the underlying real-world incidents. In the light of this, in the present paper, we illustrate the realization of a situation-adaptive crowd-sensing and knowledge extraction system by introducing our crowdSA prototype, and examine its potential in a case study on a real-world Twitter crisis data set.", "venue": "Conference on Cognitive and Computational Aspects of Situation Management", "citationCount": 16, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "2e09d95fccfb870c5d92c2f27e135f327f93df30", "url": "https://www.semanticscholar.org/paper/2e09d95fccfb870c5d92c2f27e135f327f93df30", "title": "The Need for Knowledge Extraction: Understanding Harmful Gambling Behavior with Neural Networks", "abstract": "Responsible gambling is a field of study that involves supporting gamblers so as to reduce the harm that their gambling activity might cause. Recently in the literature, machine learning algorithms have been introduced as a way to predict potentially harmful gambling based on patterns of gambling behavior, such as trends in amounts wagered and the time spent gambling. In this paper, neural network models are analyzed to help predict the outcome of a partial proxy for harmful gambling behavior: when a gambler \u201cself-excludes\u201d, requesting a gambling operator to prevent them from accessing gambling opportunities. Drawing on survey and interview insights from industry and public officials as to the importance of interpretability, a variant of the knowledge extraction algorithm TREPAN is proposed which can produce compact, human-readable logic rules efficiently, given a neural network trained on gambling data. To the best of our knowledge, this paper reports the first industrial-strength application of knowledge extraction from neural networks, which otherwise are black-boxes unable to provide the explanatory insights which are crucially required in this area of application. We show that through knowledge extraction one can explore and validate the kinds of behavioral and demographic profiles that best predict self-exclusion, while developing a machine learning approach with greater potential for adoption by industry and treatment providers. Experimental results reported in this paper indicate that the rules extracted can achieve high fidelity to the trained neural network while maintaining competitive accuracy and providing useful insight to domain experts in responsible gambling.", "venue": "ECAI", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "77c6fb4681da3bf903167483710ae9dd46c76dc0", "url": "https://www.semanticscholar.org/paper/77c6fb4681da3bf903167483710ae9dd46c76dc0", "title": "Knowledge extraction from big data using MapReduce-based Parallel-Reduct algorithm", "abstract": "Extraction of knowledge and predictive analysis are the new challenges for the rapidly growing large volume of data to make the right decision at right time. It is difficult to store, analyze and visualize such large data volume with its diversities with standard data mining tools. Hence, in this paper, we develop a MapReduce approach of a Parallel-Reduct algorithm based on the rough set theory for knowledge extraction. It performs data and task parallelism with the help of Map and Reduce functions to find minimum reduct. An extensive experimental evaluation shows that the proposed MapReduce-based parallel approach effectively processes big data on the Hadoop platform and it is more efficient than the sequential approach to extract knowledge from large data sets under different coarseness.", "venue": "International Conference on Computer Science and Network Technology", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "5e87fc714e8da51415b9d1ae773076451f9c586a", "url": "https://www.semanticscholar.org/paper/5e87fc714e8da51415b9d1ae773076451f9c586a", "title": "Comparing Data-Driven Methods for Extracting Knowledge from User Generated Content", "abstract": "This study aimed to compare two techniques of business knowledge extraction for the identification of insights related to the improvement of digital marketing strategies on a sample of 15,731 tweets. The sample was extracted from user generated content (UGC) from Twitter using two methods based on knowledge extraction techniques for business. In Method 1, an algorithm to detect communities in complex networks was applied; this algorithm, in which we applied data visualization techniques for complex networks analysis, used the modularity of nodes to discover topics. In Method 2, a three-phase process was developed for knowledge extraction that included the application of a latent Dirichlet allocation (LDA) model, a sentiment analysis (SA) that works with machine learning, and a data text mining (DTM) analysis technique. Finally, we compared the results of each of the two techniques to see whether or not the results yielded by these two methods regarding the analysis of companies\u2019 digital marketing strategies were mutually complementary.", "venue": "Journal of Open Innovation: Technology, Market and Complexity", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "41a6bdb5eac4187707f4bfcd14ec6e7421a4f527", "url": "https://www.semanticscholar.org/paper/41a6bdb5eac4187707f4bfcd14ec6e7421a4f527", "title": "Entity Pair Recognition using Semantic Enrichment and Adversarial Training for Chinese Drug Knowledge Extraction", "abstract": "Existing knowledge extraction methods in pharmacy often use natural language processing tools and deep learning model to identify drug entities and extract their relationships from drug instructions, thus obtaining drug-drug or drug-disease knowledge. However, sentences in drug instructions may contain multiple drug-related entities, and existing methods lack the capability of identifying valid the \"drug-drug\" or \"drug-disease\" entity pairs. This will introduce significant noise data in the subsequent tasks such as entity relationship extraction and knowledge graph construction. Meanwhile, some mentions in the sentence can have hierarchical relations even if they do not form valid entity pairs, such information is also crucial to knowledge extraction. To solve these two problems, this paper proposes an entity pair verification model based on entity semantic enhancement and adversarial training. Through the experiment on more than 2000 kinds of drug instructions data, the experimental results show that the F1 value of the model for entity pair verification is up to 98.65%, which is up to 9.37% compared with the existing methods.", "venue": "International Symposium on Artificial Intelligence in Medical Sciences", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "ke", "ke", "kg"], "mention_counts": {"nlp": 1, "kg": 1, "ke": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "27b09d410aa1d911d95867d29a8c44aebf376b0e", "url": "https://www.semanticscholar.org/paper/27b09d410aa1d911d95867d29a8c44aebf376b0e", "title": "Knowledge extraction from Copernicus satellite data", "abstract": "We describe two alternative approaches of how to extract knowledge from high- and medium-resolution Synthetic Aperture Radar (SAR) images of the European Sentinel-1 satellites. To this end, we selected two basic types of images, namely images depicting arctic shipping routes with icebergs, and - in contrast - coastal areas with various types of land use and human-made facilities. In both cases, the extracted knowledge is delivered as (semantic) categories (i.e., local content labels) of adjacent image patches from big SAR images. Then, machine learning strategies helped us design and validate two automated knowledge extraction systems that can be extended for the understanding of multispectral satellite images.", "venue": "IOP Conference Series: Earth and Environmental Science", "citationCount": 0, "fieldsOfStudy": ["Physics", "Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "457d96d2e4570d720937f264d94179b3695d2fb3", "url": "https://www.semanticscholar.org/paper/457d96d2e4570d720937f264d94179b3695d2fb3", "title": "Scientific and Technological Text Knowledge Extraction Method of based on Word Mixing and GRU", "abstract": "The knowledge extraction task is to extract triple relations (head entity-relation-tail entity) from unstructured text data. The existing knowledge extraction methods are divided into \"pipeline\" method and joint extraction method. The \"pipeline\" method is to separate named entity recognition and entity relationship extraction and use their own modules to extract them. Although this method has better flexibility, the training speed is slow. The learning model of joint extraction is an end-to-end model implemented by neural network to realize entity recognition and relationship extraction at the same time, which can well preserve the association between entities and relationships, and convert the joint extraction of entities and relationships into a sequence annotation problem. In this paper, we propose a knowledge extraction method for scientific and technological resources based on word mixture and GRU, combined with word mixture vector mapping method and self-attention mechanism, to effectively improve the effect of text relationship extraction for Chinese scientific and technological resources.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "66eb4dab46a1557262c8f2a94075470a9390c03f", "url": "https://www.semanticscholar.org/paper/66eb4dab46a1557262c8f2a94075470a9390c03f", "title": "Harnessing spontaneous participation on social media: implementing the knowledge extraction component", "abstract": "Recent efforts to mainstream social media-based and citizen-led political deliberations to complement traditional government-led e-Participation, must among others address a number of technical challenges. One of these challenges is how to automatically filter and extract meaningful information or knowledge from streams of social media data contributed by citizens to inform agenda setting or serve as feedback for specific policy issues. A central resource in developing an effective information extraction system is the Lexicon of terms and relationship among terms in the domain of interest. This poster summarizes our work in developing a lexicon of public services names as the kernel of the knowledge extraction component of our Social Software Infrastructure for spontaneous participation. Our lexicon-based knowledge extraction process is scalable and could be easily extended to capture other information of interest from social media or related platforms.", "venue": "Digital Government Research", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "ie", "ke"], "mention_counts": {"kg": 1, "ke": 3, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 3}, "relevance_score": 0.8807970779778823}, {"paperId": "5643e2e1ad72f4f0b020b712b968adf2c11fe7ac", "url": "https://www.semanticscholar.org/paper/5643e2e1ad72f4f0b020b712b968adf2c11fe7ac", "title": "Model and Algorithms for Optimizing a Human Computing System Oriented to Knowledge Extraction by Use of Crowdsourcing", "abstract": "The paper is addressing the actual context of Data Deluge, where the need and also premises to extract more knowledge are increasing, along with the increase of our expectations about performances. Besides, improving artificial intelligence (AI), by machine learning (ML), deep learning (DL) or cognitive learning (CL) performance/potential, when adding human contributions where necessary, is an important and promising research area. Consequently, our model, algorithms (ALG1; ALG2) and soft programs provide useful new instruments for implementing and optimizing the workflow based on crowdsourcing, when using human potential in a human computing system. We aim to increase AI quality adding multiple human outputs for every AI task and leveraging learning rules to be then extended to larger sets of tasks. This way, such hybrid system could be oriented to more knowledge extraction, by the generalization of images/ captions/labels toward more complex tasks, like providing content essential or question answering. Our instruments include features of ranking workers and tasks profiles, which will support the main original process of knowledge extraction, but also the inference elements, by small amounts of learning data (regarding the workers skills and tasks efficiency) to be transferred to AI/ML/DL/CL, which then could be used for processing larger volumes of similar data. Among the results conclusions is that using progressive optimization, structuring the data/tasks in variable (progressive) sets and potential (skill/number) of workers, is both efficacious and efficient, allowing a flexible control of the system and workflow for matching a diversity of tasks complexity/ difficulty/volume and leveraging knowledge extraction.", "venue": "2020 13th International Conference on Communications (COMM)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "8cf257d8ed64feec890cc7b526ccbe70235f0108", "url": "https://www.semanticscholar.org/paper/8cf257d8ed64feec890cc7b526ccbe70235f0108", "title": "Generating Domain-Specific Knowledge Graphs: Challenges with Open Information Extraction", "abstract": "Knowledge Graphs (KGs) are a popular way to structure and represent knowledge in a machine-readable way. While KGs serve as the foundation for many applications, the automatic construction of these KGs from texts is a challenging task where Open Information Extraction techniques are prominently leveraged. In this paper, we focus on generating a domain-specific knowledge graph based on art-historic texts from a digitized text collection. We describe the combined use and adaption of existing open information extraction methods to build an art-historic KG that can facilitate data exploration for domain experts. We discuss the challenges that were faced at each step and present detailed error analysis to identify the limitations of existing methods when working with domain-specific corpora.", "venue": "TEXT2KG/MK@ESWC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "kg", "ie", "ie", "ke", "ie"], "mention_counts": {"kg": 3, "ke": 1, "ie": 3}, "nlp_mention_counts": {"ke": 1, "ie": 3}, "ld_mention_counts": {"kg": 3, "ke": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "dc63187a3742465f014a5ce8a46fce35d4d14e7a", "url": "https://www.semanticscholar.org/paper/dc63187a3742465f014a5ce8a46fce35d4d14e7a", "title": "A Proposal for a Two-Way Journey on Validating Locations in Unstructured and Structured Data", "abstract": "The Web of Data has grown explosively over the past few years, and as with any dataset, there are bound to be invalid statements in the data, as well as gaps. Natural Language Processing (NLP) is gaining interest to fill gaps in data by transforming (unstructured) text into structured data. However, there is currently a fundamental mismatch in approaches between Linked Data and NLP as the latter is often based on statistical methods, and the former on explicitly modelling knowledge. However, these fields can strengthen each other by joining forces. In this position paper, we argue that using linked data to validate the output of an NLP system, and using textual data to validate Linked Open Data (LOD) cloud statements is a promising research avenue. We illustrate our proposal with a proof of concept on a corpus of historical travel stories.", "venue": "LDK", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ld", "nlp", "lod", "nlp", "lod", "ld", "nlp"], "mention_counts": {"ld": 2, "nlp": 4, "lod": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"ld": 2, "lod": 2}, "relevance_score": 0.8807970779778823}, {"paperId": "a85432684ff8f1f945e92744ee9636ed66c78eef", "url": "https://www.semanticscholar.org/paper/a85432684ff8f1f945e92744ee9636ed66c78eef", "title": "Segmentation-based Knowledge Extraction from Chest X-ray Images", "abstract": "Computer-aided detection applications have been extensively used to assist physicians in clinical diagnoses. Extracted information from X-ray, positron emission tomography, and magnetic resonance images enables radiologists and other physicians to identify pathologies, correlate findings with the symptoms, and determine the treatment steps. In this study, we proposed an automatic knowledge extraction methodology from chest X-ray images. The extracted knowledge is obtained from the segmented sections of the images that include pathological findings. We evaluated these segmented images with a) classical machine learning and b) pretrained convolutional neural network (CNN) models. Evaluations were based on areas under the receiver operating characteristic (AUROC) with segmented images using the pretrained CNN and the traditional method models, and they produced the average AUROC scores of 0.96 and 0.52, respectively. Traditional methods yielded lower AUROC scores compared with pretrained CNN methods. However, traditional methods may still be considered as appropriate solutions for disease diagnoses primarily based on their advantages regarding running time and flexibility.", "venue": "2019 4th Asia-Pacific Conference on Intelligent Robot Systems (ACIRS)", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke", "kg", "ke"], "mention_counts": {"ke": 3, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "af54be9932acafcb39be1755caaa3ad67f3eeb84", "url": "https://www.semanticscholar.org/paper/af54be9932acafcb39be1755caaa3ad67f3eeb84", "title": "A new automatic knowledge extraction method for course documents applied in the web-based teaching system", "abstract": "With the development of web-based teaching system, automatic knowledge extraction from course documents becomes more and more important. This paper gives a new automatic knowledge extraction method for course documents. Based on TF strategy, this method uses frequency and location to measure the credit value of knowledge. Moreover, the penalty factor is defined to adjust the credit value of knowledge. In this paper, the naive Bayes method is improved and applied in automatic knowledge extraction from course documents. Finally, we compare the method proposed in this paper with improved naive Bayes method based on experiments results. The results show that the average performance of this method is better than that of the naive Bayes method.", "venue": "2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "6bcb7510f2541de1a5d50062c6ffae7d14606029", "url": "https://www.semanticscholar.org/paper/6bcb7510f2541de1a5d50062c6ffae7d14606029", "title": "Knowledge extraction from scenery images and the recognition using fuzzy inference neural networks", "abstract": "A new system of knowledge extraction and recognition from scenery images is proposed in this paper. The system can extract different levels of knowledge automatically using Fuzzy Inference Neural Network (FINN). The proposed system consists of several Knowledge Extraction Networks (KENs). Each one is composed of FINN, and it can extract fuzzy if-then rules automatically. The KEN has an input-output (I/O) layer and two different-sized rule layers. The I/O layer includes the input part and the output part. The input part receives information on a pixel such as the position, the Intensity, the Hue and the Saturation. The output part receives the label of the corresponding pixel such as sky, mountains and woods, etc. The larger rule layer extracts detailed knowledge and it uses for the image recognition. On the other hand, the smaller rule layer extracts global knowledge which can correct contradiction of detailed knowledge and can remove trivial knowledge. It can be seen that the proposed system can recognize the image almost correctly by computer experiments. Knowledge is obtained by integrating rules from each KEN and then translating them into linguistic form. The extracted knowledge is quite natural.", "venue": "SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)", "citationCount": 11, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "13c5765932a0fdf5bce9c350421626f3427093ae", "url": "https://www.semanticscholar.org/paper/13c5765932a0fdf5bce9c350421626f3427093ae", "title": "Heuristic constraints enforcement for training of and knowledge extraction from a fuzzy/neural architecture. I. Foundation", "abstract": "Using fuzzy/neural architectures to extract heuristic information from systems has received increasing attention. A number of fuzzy/neural architectures and knowledge extraction methods have been proposed. Knowledge extraction from systems where the existing knowledge limited is a difficult task. One of the reasons is that there is no ideal rulebase, which can be used to validate the extracted rules. In most of the cases, using output error measures to validate extracted rules is not sufficient as extracted knowledge may not make heuristic sense, even if the output error may meet the specified criteria. The paper proposes a novel method for enforcing heuristic constraints on membership functions for rule extraction from a fuzzy/neural architecture. The proposed method not only ensures that the final membership functions conform to a priori heuristic knowledge, but also reduces the domain of search of the training and improves convergence speed. Although the method is described on a specific fuzzy/neural architecture, it is applicable to other realizations, including adaptive or static fuzzy inference systems. The foundations of the proposed method are given in Part I. The techniques for implementation and integration into the training are given in Part II, together with applications.", "venue": "IEEE transactions on fuzzy systems", "citationCount": 32, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "157b8138726f6b25ddfcd6ad9c7eff644602fd13", "url": "https://www.semanticscholar.org/paper/157b8138726f6b25ddfcd6ad9c7eff644602fd13", "title": "Knowledge extraction from reinforcement learning", "abstract": "This paper deals with knowledge extraction from reinforcement learners. It addresses two approaches towards knowledge extraction: the extraction of explicit, symbolic rules front neural reinforcement learners; and the extraction of complete plans from such learners. The advantages of such knowledge extraction include: the improvement of learning (especially with the rule extraction approach); and the improvement of the usability of results of learning.", "venue": "IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "a6eaade8664ab0b08bd46a54ce661c403dc458ba", "url": "https://www.semanticscholar.org/paper/a6eaade8664ab0b08bd46a54ce661c403dc458ba", "title": "Assessing Wireless Network Dependability through Knowledge Extraction via Decision Trees", "abstract": "Critical infrastructures such as wireless network systems demand dependability. Dependability attributes reported here include availability, reliability, maintainability and survivability (ARMS). This research uses computer simulation and knowledge extraction to introduce a new approach to measure dependability of wireless networks. Earlier research has used computer simulation for estimating wireless network dependability. This work introduces a new methodology which uses discrete time event simulation in-put/output to train an artificial neural network and then extract knowledge via decision trees. A comparison of decision tree extraction technique results are discussed, including those from neural (TREPAN) and non neural networks (C4.5). Significant insights are gained into increasing wireless infrastructure dependability through such knowledge extraction techniques; however the neural approach is superior from a parsimonious and comprehensibility perspective.", "venue": "Third International Conference on Systems (icons 2008)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "29f0afb2914231af8e40b44619d9da3993adc4a3", "url": "https://www.semanticscholar.org/paper/29f0afb2914231af8e40b44619d9da3993adc4a3", "title": "A rough set multi-knowledge extraction algorithm and its formal concept analysis", "abstract": "Rough set theory provides an effective method to reduce attributes and extract knowledge. This paper represents a rough set multi-knowledge extraction algorithm and its formal concept analysis. The proposed algorithm can obtain multi-reducts by using rough set in decision table. The formal concept analysis is used to obtain rules from the main values of the attributes influencing the decision making and these rules build a multi-knowledge. Experimental results show that the proposed multi-knowledge extraction algorithm is efficient.", "venue": "International Conference on Intelligent Systems Design and Applications", "citationCount": 1, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "44242e0aa8b865096b958b1485c9a93540513f34", "url": "https://www.semanticscholar.org/paper/44242e0aa8b865096b958b1485c9a93540513f34", "title": "Applying a Semantic Interpreter to a Knowledge Extraction Task", "abstract": "A system that extracts knowledge from encyclopedic texts is presented. The knowledge extraction component is based on a semantic interpreter of English based on an enhanced WordNet. The input to the knowledge extraction component is the output of the semantic interpreter. The extraction task was chosen in order to test the semantic interpreter. The following aspects are described: the definition of verb predicates and semantic roles, the organization of the inferences, an evaluation of the system, and a session with the system.", "venue": "NLUCS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "614d673cc4ad8ff4444ca4f9c19a8c2080314aeb", "url": "https://www.semanticscholar.org/paper/614d673cc4ad8ff4444ca4f9c19a8c2080314aeb", "title": "Rule extraction from recurrent neural networks using a symbolic machine learning algorithm", "abstract": "Addresses the extraction of knowledge from recurrent neural networks trained to behave like deterministic finite-state automata (DFAs). To date, methods used to extract knowledge from such networks have relied on the hypothesis that network states tend to cluster and that clusters of network states correspond to DFA states. The computational complexity of such a cluster analysis has led to heuristics which either limit the number of clusters that may form during training or limit the exploration of the output space of hidden recurrent state neurons. These limitations, while necessary, may lead to reduced fidelity, i.e. the extracted knowledge may not model the true behavior of a trained network, perhaps not even for the training set. The method proposed uses a polynomial-time symbolic learning algorithm to infer DFAs solely from the observation of a trained network's input/output behavior. Thus, this method has the potential to increase the fidelity of the extracted knowledge.", "venue": "ICONIP'99. ANZIIS'99 & ANNES'99 & ACNN'99. 6th International Conference on Neural Information Processing. Proceedings (Cat. No.99EX378)", "citationCount": 22, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "50398333009b1c7cc29f6db9b37bff35131379e9", "url": "https://www.semanticscholar.org/paper/50398333009b1c7cc29f6db9b37bff35131379e9", "title": "NLP-based Ontology Learning from Legal Texts. A Case Study", "abstract": "The paper reports on the methodology and preliminary results of a case study in automatically extracting ontological knowledge from Italian legislative texts in the environmental domain. We use a fully\u2013implemented ontology learning system (T2K) that includes a battery of tools for Natural Language Processing (NLP), statistical text analysis and machine language learning. Tools are dynamically integrated to provide an incremental representation of the content of vast repositories of unstructured documents. Evaluated results, however preliminary, are very encouraging, showing the great potential of NLP\u2013powered incremental systems like T2K for accurate large\u2013scale semi-automatic extraction of legal ontologies.", "venue": "LOAIT", "citationCount": 27, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 4, "onto": 4}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "bc36834e62b1cd098a2be81628c16b9b41693997", "url": "https://www.semanticscholar.org/paper/bc36834e62b1cd098a2be81628c16b9b41693997", "title": "What kind of knowledge is in Wikipedia? Unsupervised extraction of properties for similar concepts", "abstract": "This article presents a novel method for extracting knowledge from Wikipedia and a classification schema for annotating the extracted knowledge. Unlike the majority of approaches in the literature, we use the raw Wikipedia text for knowledge acquisition. The main assumption made is that the concepts classified under the same node in a taxonomy are described in a comparable way in Wikipedia. The annotation of the extracted knowledge is done at two levels: ontological and logical. The extracted properties are evaluated in the traditional way, that is, by computing the precision of the extraction procedure and in a clustering task. The second method of evaluation is seldom used in the natural language processing community, but it is regularly employed in cognitive psychology.", "venue": "J. Assoc. Inf. Sci. Technol.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "ke", "nlp", "ke"], "mention_counts": {"nlp": 1, "ke": 3, "onto": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "9fbba3ac8b3a55657c52b5704f3884e9b1af413d", "url": "https://www.semanticscholar.org/paper/9fbba3ac8b3a55657c52b5704f3884e9b1af413d", "title": "Building Semantic/Ontological Knowledge by Text Mining", "abstract": "People have long talked about having NLP systems employ semantic knowledge on a large scale. However, no-one has yet built a large ontology that was indeed practically useful for tasks such as question answering, machine translation, and information retrieval. Work on WordNet, a major contender, shows that it requires more content to realize its full potential, while efforts to use CYC show how hard it is to build general-purpose ontologies that can support NLP applications. In this talk I outline some recent efforts to automatically acquire knowledge that may be placed into terminological ontologies and used by NLP systems, and mention some problems in evaluating the quality of the results.", "venue": "COLING 2002", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "onto", "onto", "mt", "nlp"], "mention_counts": {"nlp": 3, "onto": 4, "mt": 1}, "nlp_mention_counts": {"nlp": 3, "mt": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "2d7511fcf6e74576a656f5ecb16cef0a0366ac2d", "url": "https://www.semanticscholar.org/paper/2d7511fcf6e74576a656f5ecb16cef0a0366ac2d", "title": "Towards applying text mining and natural language processing for biomedical ontology acquisition", "abstract": "The use of text mining and natural language processing can extend into the realm of knowledge acquisition and management for biomedical applications. In this paper, we describe how we implemented natural language processing and text mining techniques on the transcribed verbal descriptions from retinal experts of biomedical disease features. The feature-attribute pairs generated were then incorporated within a user interface for a collaborative ontology development tool. This tool, IDOCS, is being used in the biomedical domain to help retinal specialists reach a consensus on a common ontology for describing age-related macular degeneration (AMD). We compare the use of traditional text mining and natural language processing techniques with that of a retinal specialist's analysis and discuss how we might integrate these techniques for future biomedical ontology and user interface development.", "venue": "TMBIO '06", "citationCount": 18, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "nlp", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 4, "onto": 4}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "205d5151172384cdea7c5611e06244d7010ab566", "url": "https://www.semanticscholar.org/paper/205d5151172384cdea7c5611e06244d7010ab566", "title": "Bio-Ontology and text: bridging the modeling gap", "abstract": "MOTIVATION\nNatural language processing (NLP) techniques are increasingly being used in biology to automate the capture of new biological discoveries in text, which are being reported at a rapid rate. Yet, information represented in NLP data structures is classically very different from information organized with ontologies as found in model organisms or genetic databases. To facilitate the computational reuse and integration of information buried in unstructured text with that of genetic databases, we propose and evaluate a translational schema that represents a comprehensive set of phenotypic and genetic entities, as well as their closely related biomedical entities and relations as expressed in natural language. In addition, the schema connects different scales of biological information, and provides mappings from the textual information to existing ontologies, which are essential in biology for integration, organization, dissemination and knowledge management of heterogeneous phenotypic information. A common comprehensive representation for otherwise heterogeneous phenotypic and genetic datasets, such as the one proposed, is critical for advancing systems biology because it enables acquisition and reuse of unprecedented volumes of diverse types of knowledge and information from text.\n\n\nRESULTS\nA novel representational schema, PGschema, was developed that enables translation of phenotypic, genetic and their closely related information found in textual narratives to a well-defined data structure comprising phenotypic and genetic concepts from established ontologies along with modifiers and relationships. Evaluation for coverage of a selected set of entities showed that 90% of the information could be represented (95% confidence interval: 86-93%; n = 268). Moreover, PGschema can be expressed automatically in an XML format using natural language techniques to process the text. To our knowledge, we are providing the first evaluation of a translational schema for NLP that contains declarative knowledge about genes and their associated biomedical data (e.g. phenotypes).\n\n\nAVAILABILITY\nhttp://zellig.cpmc.columbia.edu/PGschema", "venue": "Bioinform.", "citationCount": 28, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "onto", "onto", "onto", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 4}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "c1e3f57a789cc0631e7ffa0503ffe4e7fa01e949", "url": "https://www.semanticscholar.org/paper/c1e3f57a789cc0631e7ffa0503ffe4e7fa01e949", "title": "oWSD: A Tool for Word Sense Disambiguation in Its Ontology Context", "abstract": "Word sense disambiguation (abbr. WSD) is very important to the semantic web/web 2.0. However, there is still no easy-to-use tool available. As a remedy, here a simple and very efficient tool called oWSD is demonstrated. It disambiguates the senses of words in their ontological contexts, and obtains the right word senses from WordNet. It is very helpful to applications involving ontologies and natural language processing as well.", "venue": "SEMWEB", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "onto", "onto", "onto", "wsd", "wsd", "wsd"], "mention_counts": {"nlp": 1, "sw": 1, "onto": 3, "wsd": 3}, "nlp_mention_counts": {"nlp": 1, "wsd": 3}, "ld_mention_counts": {"sw": 1, "onto": 3}, "relevance_score": 0.8807970779778823}, {"paperId": "b3efb95224c3a48d627306684429b6a53f02c75a", "url": "https://www.semanticscholar.org/paper/b3efb95224c3a48d627306684429b6a53f02c75a", "title": "Natural Language and Knowledge Representation", "abstract": "The aim of this track is to bring researchers from the knowledge representation (KR) and the natural language processing (NLP) communities together to discuss common \" representational \" and \" reasoning \" issues. In addition to the two main challenging concerns , namely, expressivity and fast reasoning, representations should attempt to be transparent and friendly. The NLP community has made some progress in terms of processing and handling ambiguity and the KR community has realized that a lot of knowledge is already \" coded \" in NL. Researchers on both sides consider benefiting from each other's progress and taking on issues that were left to be solved by the \" other \" community. The accepted papers and posters in this track discuss issues relating to the semantic web, semantic annotation, NL semantics and NLP-based techniques, the bottleneck KR problem, ontologies and NL interpretation, the use of KR and knowledge bases to resolve NL ambiguity, the use of NL to \" disambiguate and strengthen \" single observations for learning tasks, the use of NL to support constructing DB/ontology query interface , the possibility of using (controlled) NL as a KR, underspecified representations and reasoning, mapping \" syntax and semantics \" to a KR, and the disadvantages of using KR that is remote from NL semantics.", "venue": "J. Log. Comput.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "sw", "onto", "nlp", "nlp", "onto", "kg"], "mention_counts": {"nlp": 4, "sw": 1, "onto": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"sw": 1, "onto": 2, "kg": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "5f21507c873487d3c78dffd126cece5a1b8f8e32", "url": "https://www.semanticscholar.org/paper/5f21507c873487d3c78dffd126cece5a1b8f8e32", "title": "Commonsense knowledge, ontology and ordinary language", "abstract": "Over two decades ago a \"quite revolution\" overwhelmingly replaced knowledge- based approaches in natural language processing (NLP) by quantitative (e.g., statistical, corpus-based, machine learning) methods. Although it is our firm belief that purely quanti- tative approaches cannot be the only paradigm for NLP, dissatisfaction with purely engi- neering approaches to the construction of large knowledge bases for NLP are somewhat justified. In this paper we hope to demonstrate that both trends are partly misguided and that the time has come to enrich logical semantics with an ontological structure that reflects our commonsense view of the world and the way we talk about in ordinary language. In this paper it will be demonstrated that assuming such an ontological structure a number of challenges in the semantics of natural language (e.g., metonymy, intensionality, copredica- tion, nominal compounds, etc.) can be properly and uniformly addressed.", "venue": "Int. J. Reason. based Intell. Syst.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "kg", "onto", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 4, "onto": 3, "kg": 1}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 1, "onto": 3}, "relevance_score": 0.8807970779778823}, {"paperId": "1c816e8fdb826abde7dcd8b39307200eb54de839", "url": "https://www.semanticscholar.org/paper/1c816e8fdb826abde7dcd8b39307200eb54de839", "title": "Minimal training based semantic categorization in a voice activated question answering (VAQA) system", "abstract": "In this paper, we develop a knowledge based methodology that maps Automatic Speech Recognizer (ASR) transcriptions to prede\ufb01ned semantic categories in a Voice Activated Question Answering (VAQA) system. The proposed semantic categorization methodology, SemCat, uses a novel lexical chains/ontology based algorithm and relies heavily on customized but domain independent Natural Language Processing (NLP) tools and does not require any domain-speci\ufb01c utterance collections or manually annotated text data. SemCat requires minimal manual intervention during training, relying only on the semantics encoded in a brief, manually-created description for each prede\ufb01ned cat-egory/slot. SemCat uses these descriptions along with the eXtended WordNet Knowledge Base (XWN-KB) and several domain independent NLP tools including XWN lexical chains to accurately extract information and map user utterances to prede\ufb01ned categories. SemCat also uses the domain ontologies created automatically by the Jaguar knowledge acquisition tool to accurately extract domain/customer speci\ufb01c language/terms.", "venue": "Interspeech", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "kg", "onto", "nlp", "ie", "onto", "nlp"], "mention_counts": {"nlp": 3, "kg": 2, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "ie": 1}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.8807970779778823}, {"paperId": "111caf3b0a0028d9c92fd13a4a4a771fb5b30a9b", "url": "https://www.semanticscholar.org/paper/111caf3b0a0028d9c92fd13a4a4a771fb5b30a9b", "title": "Natural Language Processing for the Semantic Web", "abstract": "This book introduces core natural language processing (NLP) technologies to non-experts in an easily accessible way, as a series of building blocks that lead the user to understand key technologies, why they are required, and how to integrate them into Semantic Web applications. Natural language processing and Semantic Web technologies have different, but complementary roles in data management. Combining these two technologies enables structured and unstructured data to merge seamlessly. Semantic Web technologies aim to convert unstructured data to meaningful representations, which benefit enormously from the use of NLP technologies, thereby enabling applications such as connecting text to Linked Open Data, connecting texts to each other, semantic searching, information visualization, and modeling of user behavior in online networks. The first half of this book describes the basic NLP processing tools: tokenization, part-of-speech tagging, and morphological analysis, in addition to the main tools required for an information extraction system (named entity recognition and relation extraction) which build on these components. The second half of the book explains how Semantic Web and NLP technologies can enhance each other, for example via semantic annotation, ontology linking, and population. These chapters also discuss sentiment analysis, a key component in making sense of textual data, and the difficulties of performing NLP on social media, as well as some proposed solutions. The book finishes by investigating some applications of these tools, focusing on semantic search and visualization, modeling user behavior, and an outlook on the future.", "venue": "Natural Language Processing for the Semantic Web", "citationCount": 47, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "sw", "nlp", "ie", "nlp", "nlp", "nlp", "sw", "sw", "nlp", "sw", "nlp", "lod", "sw", "nlp"], "mention_counts": {"onto": 1, "nlp": 8, "sw": 5, "lod": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 8, "ie": 1}, "ld_mention_counts": {"sw": 5, "lod": 1, "onto": 1}, "relevance_score": 0.8728364547379447}, {"paperId": "b0dd9802dfb4ea0ee07289a2967e662d8fa15701", "url": "https://www.semanticscholar.org/paper/b0dd9802dfb4ea0ee07289a2967e662d8fa15701", "title": "NLP-driven ontology modeling for handling event semantics in NL constraints", "abstract": "To assist decision makers, there is need of providing an insight on current of scenario of market, considering really sensitive news involving economic events like acquisitions, stock splits, or dividend announcements. Similar to the work discussed above, to machine process natural language constraints, there is need of a mechanism that can automate events related information extraction and knowledge acquisition processes to facilitate software developers in fulfilling their cumbersome tasks, for quicker and accurate of software modeling. However, extraction of events and their related information from natural language constraints is a complex task considering the ambiguous nature of natural languages such as English. However, similar problems have been solved previously by other researchers with the help of using event semantic ontologies. However, there is need to plan a mechanism for developing an event semantic ontology for natural language constraints. It is discussed in the previous sections that the Ontology can provide assistance in identification of relations existing in various annotations. It is discussed in this thesis that it is significant to use ontology for identifying events from text. With respect to the scope of this thesis, a framework is developed to generate an event semantic ontology.", "venue": "2016 Sixth International Conference on Innovative Computing Technology (INTECH)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlg", "nll", "nle", "onto", "onto", "onto", "ie", "onto", "nlp", "nlu", "nlp"], "mention_counts": {"onto": 6, "nll": 1, "nlu": 1, "nlp": 3, "nlg": 1, "nle": 1, "ie": 1}, "nlp_mention_counts": {"nll": 1, "nlu": 1, "nlp": 3, "nlg": 1, "nle": 1, "ie": 1}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.8514061277791846}, {"paperId": "e35c8b4090bf5573359f15b6a5576b6908602e09", "url": "https://www.semanticscholar.org/paper/e35c8b4090bf5573359f15b6a5576b6908602e09", "title": "Yet another Platform for Extracting Knowledge from Corpora", "abstract": "The research field of \u0093extracting knowledge bases from text collections\u0094 seems to be mature: its target and its working hypotheses are clear. In this paper we propose a platform, YAPEK, i.e., Yet Another Platform for Extracting Knowledge from corpora, that wants to be the base to collect the majority of algorithms for extracting knowledge bases from corpora. The idea is that, when many knowledge extraction algorithms are collected under the same platform, relative comparisons are clearer and many algorithms can be leveraged to extract more valuable knowledge for final tasks such as Textual Entailment Recognition. As we want to collect many knowledge extraction algorithms, YAPEK is based on the three working hypotheses of the area: the basic hypothesis, the distributional hypothesis, and the point-wise assertion patterns. In YAPEK, these three hypotheses define two spaces: the space of the target textual forms and the space of the contexts. This platform guarantees the possibility of rapidly implementing many models for extracting knowledge from corpora as the platform gives clear entry points to model what is really different in the different algorithms: the feature spaces, the distances in these spaces, and the actual algorithm.", "venue": "LREC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke", "kg", "kg"], "mention_counts": {"ke": 6, "kg": 2}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"ke": 6, "kg": 2}, "relevance_score": 0.8514061277791846}, {"paperId": "c7bae7db1dbf0e73de04c0c1d5777c84cb5d5f47", "url": "https://www.semanticscholar.org/paper/c7bae7db1dbf0e73de04c0c1d5777c84cb5d5f47", "title": "Evaluating Ontologies with NLP-Based Terminologies - A Case Study on ACGT and Its Master Ontology", "abstract": "Natural language processing (NLP) plays a major role in knowledge engineering. However, NLP's usage is traditionally being seen as a means of extracting knowledge required for building knowledge resources, i.e. ontologies, knowledge bases. When it comes to the evaluation of these knowledge artefacts, then general trends are: expert reviewing, evaluating against existing ontologies and democratic ranking. We propose a new approach for evaluating domain coverage of application ontologies which is based on NLP techniques. The latter can be seen as one way of bridging the gap between terminologies and ontologies in order to create user-understandable expert systems.", "venue": "Formal Ontology in Information Systems", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "nlp", "nlp", "ke", "nlp", "onto", "onto", "kg", "onto", "nlp"], "mention_counts": {"nlp": 5, "kg": 1, "onto": 6, "ke": 1}, "nlp_mention_counts": {"nlp": 5, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 6, "kg": 1}, "relevance_score": 0.8514061277791846}, {"paperId": "61c1e8e41c731635ca5a8bf57f83e53574a1e80a", "url": "https://www.semanticscholar.org/paper/61c1e8e41c731635ca5a8bf57f83e53574a1e80a", "title": "Research on Sino-Tibetan Machine Translation Based on the Reusing of Domain Ontology", "abstract": "There are some problems on traditional machine translation and these problems have affected its application. The combination of ontology and machine translation can bring about change to machine translation. Machine translation based on the reusing of domain ontology can effectively solve the problems that exist in traditional machine translation. The domain ontology knowledge base of Tibetan folk culture is tried to be built and the Sino-Tibetan machine translation system based on the reusing of Tibetan folk culture domain ontology is tried to be constructed in our research. In this machine translation system, ambiguity problem which exits in traditional machine translation can be solved and the domain ontology can be constructed automatically. By that intelligent machine translation between Tibetan and Chinese can be achieved.", "venue": "Int. J. Online Biomed. Eng.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "mt", "kg", "mt", "mt", "mt", "onto", "mt", "onto", "mt", "onto", "onto", "mt", "mt", "mt", "mt"], "mention_counts": {"kg": 1, "onto": 6, "mt": 10}, "nlp_mention_counts": {"mt": 10}, "ld_mention_counts": {"kg": 1, "onto": 6}, "relevance_score": 0.8222931440238636}, {"paperId": "4bc3674b18d40d90dc922f293775cbac1fae7894", "url": "https://www.semanticscholar.org/paper/4bc3674b18d40d90dc922f293775cbac1fae7894", "title": "LinKBase, a Philosophically-Inspired Ontology for NLP/NLU Applications", "abstract": "LinKBase\u00ae is a biomedical ontology. Its hierarchical structure, coverage, use of operational, formal and linguistic relationships, combined with its underlying language technology, make it an excellent ontology to support Natural Language Processing and Understanding (NLP/NLU) and data integration applications. In this paper we will describe the structure and coverage of LinKBase\u00ae. In addition, we will discuss the editing of LinKBase\u00ae and how domain experts are guided by specific editing rules to ensure modeling quality and consistency. Finally, we compare the structure of LinKBase\u00ae to the structure of third party terminologies and ontologies and discuss the integration of these data sources into LinKBase\u00ae.", "venue": "KR-MED", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlu", "nlp", "onto", "nlp", "onto", "nlu"], "mention_counts": {"nlp": 3, "onto": 4, "nlu": 2}, "nlp_mention_counts": {"nlp": 3, "nlu": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "ee463f1f72a7e007bae274d2d42cd2e5d817e751", "url": "https://www.semanticscholar.org/paper/ee463f1f72a7e007bae274d2d42cd2e5d817e751", "title": "Automatically Extracting Qualia Relations for the Rich Event Ontology", "abstract": "Commonsense, real-world knowledge about the events that entities or \u201cthings in the world\u201d are typically involved in, as well as part-whole relationships, is valuable for allowing computational systems to draw everyday inferences about the world. Here, we focus on automatically extracting information about (1) the events that typically bring about certain entities (origins), (2) the events that are the typical functions of entities, and (3) part-whole relationships in entities. These correspond to the agentive, telic and constitutive qualia central to the Generative Lexicon. We describe our motivations and methods for extracting these qualia relations from the Suggested Upper Merged Ontology (SUMO) and show that human annotators overwhelmingly find the information extracted to be reasonable. Because ontologies provide a way of structuring this information and making it accessible to agents and computational systems generally, efforts are underway to incorporate the extracted information to an ontology hub of Natural Language Processing semantic role labeling resources, the Rich Event Ontology.", "venue": "COLING", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto", "onto", "ie", "onto", "nlp", "onto"], "mention_counts": {"nlp": 1, "onto": 5, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ie": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.8214593955366725}, {"paperId": "bb075b13e6c5b2f0290e24acf41f4a4e3538d19b", "url": "https://www.semanticscholar.org/paper/bb075b13e6c5b2f0290e24acf41f4a4e3538d19b", "title": "Spatial Ontology in Factored Statistical Machine Translation", "abstract": "This paper presents a statistical phrase-based machine translation system which is enriched with semantic data coming from a spatial ontology. Paper presents the spatial ontology, how it is integrated in statistical machine translation system using factored models and how it is being evaluated using both automatic and human evaluation. Spatial information is added as a factor in both translation and language models. SOLIM spatial ontology language is used to implement ontology and to infer necessary knowledge for training statistical machine translation system. The machine translation system is based on Moses toolkit.", "venue": "DB&IS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "mt", "mt", "mt", "mt", "mt"], "mention_counts": {"onto": 4, "mt": 5}, "nlp_mention_counts": {"mt": 5}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "aeaf24779e5c9ba6b28e21d9f768f37e84d2c46b", "url": "https://www.semanticscholar.org/paper/aeaf24779e5c9ba6b28e21d9f768f37e84d2c46b", "title": "An Nlp-Based Approach for Improving Human-Robot Interaction", "abstract": "Abstract This study aims to explore the possibility of improving human-robot interaction (HRI) by exploiting natural language resources and using natural language processing (NLP) methods. The theoretical basis of the study rests on the claim that effective and efficient human robot interaction requires linguistic and ontological agreement. A further claim is that the required ontology is implicitly present in the lexical and grammatical structure of natural language. The paper offers some NLP techniques to uncover (fragments of) the ontology hidden in natural language and to generate semantic representations of natural language sentences using that ontology. The paper also presents the implementation details of an NLP module capable of parsing English and Turkish along with an overview of the architecture of a robotic interface that makes use of this module for expressing the spatial motions of objects observed by a robot", "venue": "J. Artif. Intell. Soft Comput. Res.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp", "onto", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 5, "onto": 4}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "ff9018ae0fc2e254a055fb920f7f9eb65649d50d", "url": "https://www.semanticscholar.org/paper/ff9018ae0fc2e254a055fb920f7f9eb65649d50d", "title": "Ten Ways of Leveraging Ontologies for Rapid Natural Language Processing Customization for Multiple Use Cases in Disjoint Domains", "abstract": "With the ever-growing adoption of AI technologies by large enterprises, purely data-driven approaches have dominated the field in the recent years. For a single use case, a development process looks simple: agreeing on an annotation schema, labeling the data, and training the models. As the number of use cases and their complexity increases, the development teams face issues with collective governance of the models, scalability and reusablity of data and models. These issues are widely addressed on the engineering side, but not so much on the knowledge side. Ontologies have been a well-researched approach for capturing knowledge and can be used to augment a data-driven methodology. In this paper, we discuss 10 ways of leveraging ontologies for Natural Language Processing (NLP) and its applications. We use ontologies for rapid customization of a NLP pipeline, ontologyrelated standards to power a rule engine and provide standard output format. We also discuss various use cases for medical, enterprise, financial, legal, and security domains, centered around three NLP-based applications: semantic search, question answering and natural language querying.", "venue": "Open J. Semantic Web", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "nlp", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 5, "onto": 4}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "95178e23945ccc5000599ea6786f33870cc41e00", "url": "https://www.semanticscholar.org/paper/95178e23945ccc5000599ea6786f33870cc41e00", "title": "SPARQL/T A query language with SPARQL's syntax for semantic mining of textual complaints", "abstract": "Extracting information from complaints, either scraped from the Web or received directly from the client, is a necessity of many companies nowadays. The aim is to find inside them some actionable knowledge. To this purpose, verbal phrases must be analyzed, as many complaints refer to actions improperly performed. The Semantic Roles of the actions (who did what to whom) and the Named Entities involved need to be extracted. Moreover, for the correct interpretation of the claims, the software should be able to deal with some background knowledge (for example, a product\u2019s ontology). Although there are already many libraries and out of the shelf tools that allow tackling these problems singularly, it may be hard to find one that includes all the needed tasks. We propose here a query language that adopts the syntax of SPARQL to extracts information from natural language documents, pre-annotated with NLP information. The language provides the user with a simple and uniform interface to the most useful NLP tasks, isolating him or her from the details of the specific implementation. We argue that a query language is much easier and intuitive (from a laymen point of view) than an imperative one. Moreover, the adoption of the SPARQL syntax allows to seamlessly mix, inside the same query, NLP patterns with traditional RDF/OWL ones, simplifying the integration with Semantic Web technologies.", "venue": "IIR", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "rdf", "onto", "ie", "nlp", "ie", "onto", "nlp", "nlp"], "mention_counts": {"onto": 2, "nlp": 3, "sw": 1, "rdf": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 3, "ie": 2}, "ld_mention_counts": {"sw": 1, "onto": 2, "rdf": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "a9de628768529e509379b596919d06ab060ebcf8", "url": "https://www.semanticscholar.org/paper/a9de628768529e509379b596919d06ab060ebcf8", "title": "Experimenting with semantic web services to understand the role of NLP technologies in Healthcare", "abstract": "NLP technologies can play a significant role in healthcare where a predominant segment of the clinical documentation is in text form. In a graduate course focused on understanding semantic web services at West Virginia University, a class project was designed with the purpose of exploring potential use for NLP-based abstraction of clinical documentation. The role of NLP-technology was simulated using human abstractors and various workflows were investigated using public domain workflow and semantic web service technologies. This poster explores the potential use of NLP and the role of workflow and semantic web technologies in developing healthcare IT environments.", "venue": "AMIA", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "nlp", "sw", "nlp", "sw", "nlp", "sw", "sw", "nlp"], "mention_counts": {"nlp": 5, "sw": 4}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"sw": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "7413523e7c51ca070799b2795a1cd4373aaf8226", "url": "https://www.semanticscholar.org/paper/7413523e7c51ca070799b2795a1cd4373aaf8226", "title": "Parallelizing natural language techniques for knowledge extraction from cloud service level agreements", "abstract": "To efficiently utilize their cloud based services, consumers have to continuously monitor and manage the Service Level Agreements (SLA) that define the service performance measures. Currently this is still a time and labor intensive process since the SLAs are primarily stored as text documents. We have significantly automated the process of extracting, managing and monitoring cloud SLAs using natural language processing techniques and Semantic Web technologies. In this paper we describe our prototype system that uses a Hadoop cluster to extract knowledge from unstructured legal text documents. For this prototype we have considered publicly available SLA/terms of service documents of various cloud providers. We use established natural language processing techniques in parallel to speed up cloud legal knowledge base creation. Our system considerably speeds up knowledge base creation and can also be used in other domains that have unstructured data.", "venue": "2015 IEEE International Conference on Big Data (Big Data)", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "nlp", "sw", "ke", "nlp", "ke"], "mention_counts": {"nlp": 2, "kg": 2, "sw": 1, "ke": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 2}, "ld_mention_counts": {"kg": 2, "sw": 1, "ke": 2}, "relevance_score": 0.8214593955366725}, {"paperId": "3719cf1d86b93718d50886834ceaecd1c64eb1bd", "url": "https://www.semanticscholar.org/paper/3719cf1d86b93718d50886834ceaecd1c64eb1bd", "title": "State of the art in knowledge extraction from online polls: a survey of current technologies", "abstract": "The ongoing research and development in the field of Natural Language Processing has lead to a great number of technologies in its context. There have been major benefits when it comes to bringing together the worlds of natural language and semantic technologies, so more and more potential areas of application emerge. One of these is the subject of this paper, in particular the possible ways of knowledge extraction from single-question online polls. With concepts of the Social Web, internet users want to contribute and express their opinion. As a consequence, the popularity of online polls is rapidly increasing; they can be found in news articles of media sites, on blogs etc. It would be desirable to bring intelligence to the application of polls by using technologies of the SemanticWeb and Natural Language Processing as this would allow to build a great knowledge base and to draw conclusions from it. This paper surveys the current landscape of tools and state-of-the-art technologies and analyses them with regard to pre-defined requirements that need to be accomplished, in order to be useful for extracting knowledge from the results generated by online polls.", "venue": "ACSW", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "ke", "kg", "ke", "nlp"], "mention_counts": {"nlp": 2, "ke": 3, "kg": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "310126004335765babbb50c88504e7574edd1d92", "url": "https://www.semanticscholar.org/paper/310126004335765babbb50c88504e7574edd1d92", "title": "Mining User Queries with Information Extraction Methods and Linked Data", "abstract": "Purpose: Advanced usage of Web Analytics tools allows to capture the content of user queries. Despite their relevant nature, the manual analysis of large volumes of user queries is problematic. This paper demonstrates the potential of using information extraction techniques and Linked Data to gather a better understanding of the nature of user queries in an automated manner. \nDesign/methodology/approach: The paper presents a large-scale case-study conducted at the Royal Library of Belgium consisting of a data set of 83 854 queries resulting from 29 812 visits over a 12 month period of the historical newspapers platform BelgicaPress. By making use of information extraction methods, knowledge bases and various authority files, this paper presents the possibilities and limits to identify what percentage of end users are looking for person and place names. \nFindings: Based on a quantitative assessment, our method can successfully identify the majority of person and place names from user queries. Due to the specific character of user queries and the nature of the knowledge bases used, a limited amount of queries remained too ambiguous to be treated in an automated manner. \nOriginality/value: This paper demonstrates in an empirical manner both the possibilities and limits of gaining more insights from user queries extracted from a Web Analytics tool and analysed with the help of information extraction tools and knowledge bases. Methods and tools used are generalisable and can be reused by other collection holders.", "venue": "J. Documentation", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "kg", "kg", "ld", "ie", "ld", "ie", "kg", "ie"], "mention_counts": {"ld": 2, "kg": 3, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"ld": 2, "kg": 3}, "relevance_score": 0.8214593955366725}, {"paperId": "2c8dde814803d54844975cec6e543ee7ac7f6c3d", "url": "https://www.semanticscholar.org/paper/2c8dde814803d54844975cec6e543ee7ac7f6c3d", "title": "LODIE: Linked Open Data for Web-scale Information Extraction", "abstract": "This work analyzes research gaps and challenges for Web-scale Information Extraction and foresees the usage of Linked Open Data as a groundbreaking solution for the field. The paper presents a novel methodology for Web scale Information Extraction which will be the core of the LODIE project (Linked Open Data Information Extraction). LODIE aims to develop Information Extraction techniques able to (i) scale at web level and (ii) adapt to user information need. We argument that for the first time in the history of IE this will be possible given the availability of Linked Data, a very large-scale information resource, providing annotated data on a growing number of domains.", "venue": "SWAIE", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "ie", "lod", "ld", "ie", "lod", "lod", "ie"], "mention_counts": {"ld": 1, "lod": 3, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"ld": 1, "lod": 3}, "relevance_score": 0.8214593955366725}, {"paperId": "f965c0447ce22c06e8bb95bf4429d9c93be03d1d", "url": "https://www.semanticscholar.org/paper/f965c0447ce22c06e8bb95bf4429d9c93be03d1d", "title": "Distantly supervised Web relation extraction for knowledge base population", "abstract": "Extracting information from Web pages for populating large, cross-domain knowledge bases requires methods which are suitable across domains, do not require manual effort to adapt to new domains, are able to deal with noise, and integrate information extracted from different Web pages. Recent approaches have used existing knowledge bases to learn to extract information with promising results, one of those approaches being distant supervision. Distant supervision is an unsupervised method which uses background information from the Linking Open Data cloud to automatically label sentences with relations to create training data for relation classifiers. In this paper we propose the use of distant supervision for relation extraction from the Web. Although the method is promising, existing approaches are still not suitable for Web extraction as they suffer from three main issues: data sparsity, noise and lexical ambiguity. Our approach reduces the impact of data sparsity by making entity recognition tools more robust across domains and extracting relations across sentence boundaries using unsupervised co- reference resolution methods. We reduce the noise caused by lexical ambiguity by employing statistical methods to strategically select training data. To combine information extracted from multiple sources for populating knowledge bases we present and evaluate several information integration strategies and show that those benefit immensely from additional relation mentions extracted using co-reference resolution, increasing precision by 8%. We further show that strategically selecting training data can increase precision by a further 3%.", "venue": "Semantic Web", "citationCount": 34, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "lod", "kg", "kg", "ie", "ie", "ie", "ie"], "mention_counts": {"kg": 4, "lod": 1, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"kg": 4, "lod": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "4f73c1256423898d1eebafbb0daf87d63b80c5bd", "url": "https://www.semanticscholar.org/paper/4f73c1256423898d1eebafbb0daf87d63b80c5bd", "title": "From Natural Language to Ontology Population in the Cultural Heritage Domain. A Computational Linguistics-based approach", "abstract": "This paper presents an on-going Natural Language Processing (NLP) research based on Lexicon-Grammar (LG) and aimed at improving knowledge management of Cultural Heritage (CH) domain. We intend to demonstrate how our language formalization technique can be applied for both processing and populating a domain ontology. We also use NLP techniques for text extraction and mining to fill information gaps and improve access to cultural resources. The Linguistic Resources (LRs, i.e. electronic dictionaries) we built can be used in the structuring of effective Knowledge Management Systems (KMSs). In order to apply to Parts of Speech (POS) the classes and properties defined by the Conseil Interational des Musees (CIDOC) Conceptual Reference Model (CRM), we use Finite State Transducers/Automata (FSTs/FSA) and their variables built in the form of graphs. FSTs/FSA are also used for analysing corpora in order to retrieve recursive sentence structures, in which combinatorial and semantic constraints identify properties and denote relationship. Besides, FSTs/FSA are also used to match our electronic dictionary entries (ALUs, or Atomic Linguistic Units) to RDF subject, object and predicate (SKOS Core Vocabulary). This matching of linguistic data to RDF and their translation into SPARQL/SERQL path expressions allows the use ALUs to process natural-language queries.", "venue": "LREC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "skos", "nlp", "rdf", "rdf", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 4, "skos": 1, "onto": 2, "rdf": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"skos": 1, "onto": 2, "rdf": 2}, "relevance_score": 0.8214593955366725}, {"paperId": "30596da17f14943eb98f428844e100724e28456d", "url": "https://www.semanticscholar.org/paper/30596da17f14943eb98f428844e100724e28456d", "title": "DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction Corpus", "abstract": "In the past decade, the DBpedia community has put significant amount of effort on developing technical infrastructure and methods for efficient extraction of structured information from Wikipedia. These efforts have been primarily focused on harvesting, refinement and publishing semi-structured information found in Wikipedia articles, such as information from infoboxes, categorization information, images, wikilinks and citations. Nevertheless, still vast amount of valuable information is contained in the unstructured Wikipedia article texts. In this paper, we present DBpedia NIF - a large-scale and multilingual knowledge extraction corpus. The aim of the dataset is two-fold: to dramatically broaden and deepen the amount of structured information in DBpedia, and to provide large-scale and multilingual language resource for development of various NLP and IR task. The dataset provides the content of all articles for 128 Wikipedia languages. We describe the dataset creation process and the NLP Interchange Format (NIF) used to model the content, links and the structure the information of the Wikipedia articles. The dataset has been further enriched with about 25% more links and selected partitions published as Linked Data. Finally, we describe the maintenance and sustainability plans, and selected use cases of the dataset from the TextExt knowledge extraction challenge.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "ke", "nlp", "ke", "ke", "nlp"], "mention_counts": {"ld": 1, "ke": 3, "nlp": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 3}, "ld_mention_counts": {"ld": 1, "ke": 3}, "relevance_score": 0.8214593955366725}, {"paperId": "f9ce3ccfe7bd1c6d400a378941bd584612510410", "url": "https://www.semanticscholar.org/paper/f9ce3ccfe7bd1c6d400a378941bd584612510410", "title": "Vision and Knowledge Bimodal Fine-grained Recognition Based on Hierarchical Structured Knowledge Extraction", "abstract": "In the field of fine-grained recognition, the most important method is to use visual information as the most important modality to complete the task. In order to prevent the model from being greatly affected by the data of a single modality, and to improve the accuracy and robustness and security, existing methods fused information such as text, voice or knowledge graphs on a single modality basis. In view of the limitations of the previous methods in the way of integrating knowledge modalities, this paper optimizes the existing knowledge extraction methods for bird datasets, and proposes a hierarchical and structured knowledge extraction method, so that the model can be classified when only knowledge modalities are used. The accuracy reaches 48.69%, which improves the accuracy by about 10% compared with previous methods. After multimodal fusion of knowledge extracted by this method and visual modalities, about 2% improvement was achieved on the knowledge-visual fusion framework KGRF, and the results proved the effectiveness of this method.", "venue": "International Conference on Data Intelligence and Security", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "kg", "ke"], "mention_counts": {"ke": 4, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "797b89a225ad3c2d88c0bc12d76d606181b371ac", "url": "https://www.semanticscholar.org/paper/797b89a225ad3c2d88c0bc12d76d606181b371ac", "title": "Information Extraction from the Web: An Ontology-Based Method Using Inductive Logic Programming", "abstract": "Relevant information extraction from text and web pages in particular is an intensive and time-consuming task that needs important semantic resources. Thus, to be efficient, automatic information extraction systems have to exploit semantic resources (or ontologies) and employ machine-learning techniques to make them more adaptive. This paper presents an Ontology-based Information Extraction method using Inductive Logic Programming that allows inducing symbolic predicates expressed in Horn clausal logic that subsume information extraction rules. Such rules allow the system to extract class and relation instances from English corpora for ontology population purposes. Several experiments were conducted and preliminary experimental results are promising, showing that the proposed approach improves previous work over extracting instances of classes and relations, either separately or altogether.", "venue": "IEEE International Conference on Tools with Artificial Intelligence", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "ie", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 4, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "84feacad2733b423b39fda9119c878971cee4fae", "url": "https://www.semanticscholar.org/paper/84feacad2733b423b39fda9119c878971cee4fae", "title": "Document Summarization and Information Extraction for Generation of Presentation Slides", "abstract": "In this paper, a semi automated technique to generate slide presentations from english text documents is proposed. The technique discussed in this paper is considered to be a pioneering attempt in the field of NLP (Natural Language Processing). The technique involves an information extractor and a slide generator, which combines certain NLP methods such as segmentation, chunking, summarization etc.., with certain special linguistic features of the text such as the ontology of the words, noun phrases found, semantic links, sentence centrality etc., In order to aid the language processing task, two tools can be utilized namely, MontyLingua which helps in chunking and Doddle helps in creating an ontology for the input text represented as an OWL (Ontology Web Language) file. The process of the technique comprises of extracting text, creating an ontology, identifying important phrases for bullets and generating slides.", "venue": "2009 International Conference on Advances in Recent Technologies in Communication and Computing", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "onto", "onto", "ie", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 5, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "ie": 1}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.8214593955366725}, {"paperId": "8139b57e5542e7b7c742270ea8cbaecccc58cf4e", "url": "https://www.semanticscholar.org/paper/8139b57e5542e7b7c742270ea8cbaecccc58cf4e", "title": "Ontology-Based Information Extraction from PDF Documents with Xonto", "abstract": "Information extraction is of paramount importance in several real world applications in the areas of business, competitive and military intelligence because it enables to acquire information contained in unstructured documents and store them in structured forms. Unstructured documents have different internal encodings, one of the most diffused encoding is the visualization-oriented Adobe portable document format (PDF). Although several sophisticated and indeed complex approaches were proposed, they are still limited in many aspects. In particular, existing information extraction systems cannot be applied to PDF documents because of their completely unstructured nature that pose many issues in defining IE approaches. In this paper the novel ontology-based system named XONTO, that allows the semantic extraction of information from PDF documents, is presented. The XONTO system is founded on the idea of self-describing ontologies in which objects and classes can be equipped by a set of rules named descriptors. These rules represent patterns that allow to automatically recognize and extract ontology objects contained in PDF documents also when information is arranged in tabular form. This way a self-describing ontology expresses the semantic of the information to extract and the rules that, in turn, populate itself. In the paper XONTO system behaviors and structure are sketched by means of a running example.", "venue": "Int. J. Artif. Intell. Tools", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ie", "ie", "onto", "ie", "ie", "onto"], "mention_counts": {"onto": 5, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.8214593955366725}, {"paperId": "5ad44275fc5e81189bab1aa7a8a7847c50d514d7", "url": "https://www.semanticscholar.org/paper/5ad44275fc5e81189bab1aa7a8a7847c50d514d7", "title": "OWL as a Target for Information Extraction Systems (Statement of Interest)", "abstract": "Current information extraction systems can do a good job of discovering entities, relations and events in natural language text. The traditional output of such systems is XML, with the ACE Pilot Format (APF) schema as a common target. We are developing a system that will take the output of an information extraction system as APF documents and directly populate a knowledge base with the information extracted. We report on an initial OWL ontology that covers the APF schema, a simple program to convert a set of APF documents to RDF data and a demonstration system build with Exhibit to view the results.", "venue": "OWLED", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "ie", "onto", "kg", "ie", "rdf", "ie"], "mention_counts": {"kg": 1, "onto": 3, "rdf": 1, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"kg": 1, "onto": 3, "rdf": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "2ff13a0e606277b182c9810dcc81f850b577c077", "url": "https://www.semanticscholar.org/paper/2ff13a0e606277b182c9810dcc81f850b577c077", "title": "Ontology-guided Extraction of Complex Nested Relationships", "abstract": "Many applications call for methods to enable automatic extraction of structured information from unstructured natural language text. Due to inherent challenges of natural language processing, most of the existing methods for information extraction from text tend to be domain specific. We explore a modular ontology-based approach to information extraction that decouples domain-specific knowledge from the rules used for information extraction. We describe a framework for extraction of a subset of complex nested relationships (e.g., Joe reports that Jim is a reliable employee). The extracted relationships are output in the form of sets of RDF (resource description framework) triples, which can be queried using query languages for RDF and mined for knowledge acquisition.", "venue": "IEEE International Conference on Tools with Artificial Intelligence", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "rdf", "nlp", "onto", "ie", "rdf", "ie", "rdf"], "mention_counts": {"nlp": 1, "onto": 2, "rdf": 3, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ie": 3}, "ld_mention_counts": {"onto": 2, "rdf": 3}, "relevance_score": 0.8214593955366725}, {"paperId": "db328685d00ec35fe35f9350f884c7b4b8db3f4c", "url": "https://www.semanticscholar.org/paper/db328685d00ec35fe35f9350f884c7b4b8db3f4c", "title": "Unsupervised Ontology Induction from Text", "abstract": "Extracting knowledge from unstructured text is a long-standing goal of NLP. Although learning approaches to many of its subtasks have been developed (e.g., parsing, taxonomy induction, information extraction), all end-to-end solutions to date require heavy supervision and/or manual engineering, limiting their scope and scalability. We present OntoUSP, a system that induces and populates a probabilistic ontology using only dependency-parsed text as input. OntoUSP builds on the USP unsupervised semantic parser by jointly forming ISA and IS-PART hierarchies of lambda-form clusters. The ISA hierarchy allows more general knowledge to be learned, and the use of smoothing for parameter estimation. We evaluate OntoUSP by using it to extract a knowledge base from biomedical abstracts and answer questions. OntoUSP improves on the recall of USP by 47% and greatly outperforms previous state-of-the-art approaches.", "venue": "Annual Meeting of the Association for Computational Linguistics", "citationCount": 145, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "kg", "onto", "ke", "ie", "ke"], "mention_counts": {"onto": 2, "nlp": 1, "ke": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 2, "ie": 1}, "ld_mention_counts": {"kg": 1, "onto": 2, "ke": 2}, "relevance_score": 0.8214593955366725}, {"paperId": "6339ababdcda13b19979b314d5afeb53a83a6a4a", "url": "https://www.semanticscholar.org/paper/6339ababdcda13b19979b314d5afeb53a83a6a4a", "title": "DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population", "abstract": "We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a uni\ufb01ed framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Speci\ufb01cally, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain suf\ufb01cient modularity and extensibility. We release the source code at GitHub 1 with Google Colab tutorials and comprehensive documents 2 for be-ginners. Besides, we present an online system 3 for real-time extraction of various tasks, and a demo video 4 .", "venue": "ArXiv", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ie", "kg", "kg", "ie", "ke"], "mention_counts": {"kg": 3, "ke": 2, "ie": 2}, "nlp_mention_counts": {"ke": 2, "ie": 2}, "ld_mention_counts": {"kg": 3, "ke": 2}, "relevance_score": 0.8214593955366725}, {"paperId": "b15ec477c6b94c8490b47bcdba74e7f942d8af2c", "url": "https://www.semanticscholar.org/paper/b15ec477c6b94c8490b47bcdba74e7f942d8af2c", "title": "OMIR: Ontology-Based Multimedia Information Retrieval System for Web Usage Mining", "abstract": "ABSTRACT Information extraction relevant to the user queries is the challenging task in the ontology environment due to data varieties such as image, video, and text. The utilization of appropriate semantic entities enables the content-based search on annotated text. Recently, the automatic extraction of textual content in the audio-visual content is an advanced research area in a multimedia (MM) environment. The annotation of the video includes several tags and comments. This paper proposes the Collaborative Tagging (CT) model based on the Block Acquiring Page Segmentation (BAPS) method to retrieve the tag-based information. The information extraction in this model includes the Ontology-Based Information Extraction (OBIE) based on the single ontology utilization. The semantic annotation phase in the proposed work inserts the metadata with limited machine-readable terms. The insertion process is split into two major processes such as database uploading to server and extraction of images/web pages based on the results of semantic phase. Novel weight-based novel clustering algorithms are introduced to extract knowledge from MM contents. The ranking based on the weight value in the semantic annotation phase supports the image/web page retrieval process effectively. The comparative analysis of the proposed BAPS-CT with the existing information retrieval (IR) models regarding the average precision rate, time cost, and storage space rate assures the effectiveness of BAPS-CT in OMIR.", "venue": "Cybernetics and systems", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "onto", "onto", "ie", "onto", "onto", "ie"], "mention_counts": {"ke": 1, "onto": 4, "ie": 3}, "nlp_mention_counts": {"ke": 1, "ie": 3}, "ld_mention_counts": {"ke": 1, "onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "cf1c284c7870edc237affd09ecf0e2e01831f8f9", "url": "https://www.semanticscholar.org/paper/cf1c284c7870edc237affd09ecf0e2e01831f8f9", "title": "Automatic Extraction of Information about the Molecular Interactions in Biological Pathways from Texts Based on Ontology and Semantic Processing", "abstract": "We develop a framework using ontology inference and semantic processing techniques to help biologists to extract knowledge directly from a large scale of biological literature in NCBI PubMed. The system integrated various sharable thesauri of WordNet, MeSH (Medical Subject Heading), and GO (Gene ontology) to support the automatic semantic annotation and analysis. The natural language processing and semantic processing are facilitated by the ontological inference, and the system could automatically extract the correct molecular interactions from the complex sentences in an abstract automatically. It facilitates the biologists not only to save time and efforts to construct and analyze biological pathways, but also to discover the novel molecular interactions by comparing the information extracted from the literature with that in such existing pathway database as KEGG. We evaluated the system performance based on the pathways in Apoptosis domain.", "venue": "2006 IEEE International Conference on Systems, Man and Cybernetics", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ke", "onto", "nlp", "ie", "onto"], "mention_counts": {"nlp": 1, "ke": 1, "onto": 4, "ie": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "37f09b047152ba75f4bb3fa6aa387d7eb6c8988d", "url": "https://www.semanticscholar.org/paper/37f09b047152ba75f4bb3fa6aa387d7eb6c8988d", "title": "SeseiOnto: Interfacing NLP and Ontology Extraction", "abstract": "For many years, information retrieval tools have been used to try to solve the information overload problem which was accentuated by the coming of age of the World Wide Web. Some tools used Boolean search, others, natural language based processing (NLP). Ontology-based techniques were proposed to improve the quality of the search but none were widely adopted since they did not statistically enhance either the recall or the precision of the search. However, when it comes to information extraction, they may be of significant help. Their integration in professional search engines has been rather slow, partially due to the fact that the ontology building process is time consuming. In this paper, we describe the SeseiOnto software, which uses simple artificial intelligence techniques to improve information extraction and retrieval. To assist the NLP-based information retrieval on a corpus of documents, SeseiOnto employs an automatically generated ontology. Under our experiments, we found that SeseiOnto obtained results comparable to a traditional search engine, while providing a natural language interface to its user", "venue": "2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "ie", "nlp", "onto", "nlp", "ie", "onto"], "mention_counts": {"nlp": 3, "onto": 4, "ie": 2}, "nlp_mention_counts": {"nlp": 3, "ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "4b1e6f34e9570376224cfd82655b83ca4909d825", "url": "https://www.semanticscholar.org/paper/4b1e6f34e9570376224cfd82655b83ca4909d825", "title": "Design and Construction of a NLP Based Knowledge Extraction Methodology in the Medical Domain Applied to Clinical Information", "abstract": "Objectives This research presents the design and development of a software architecture using natural language processing tools and the use of an ontology of knowledge as a knowledge base. Methods The software extracts, manages and represents the knowledge of a text in natural language. A corpus of more than 200 medical domain documents from the general medicine and palliative care areas was validated, demonstrating relevant knowledge elements for physicians. Results Indicators for precision, recall and F-measure were applied. An ontology was created called the knowledge elements of the medical domain to manipulate patient information, which can be read or accessed from any other software platform. Conclusions The developed software architecture extracts the medical knowledge of the clinical histories of patients from two different corpora. The architecture was validated using the metrics of information extraction systems.", "venue": "Healthcare informatics research", "citationCount": 3, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["kg", "ke", "nlp", "onto", "onto", "nlp", "ie", "kg"], "mention_counts": {"onto": 2, "nlp": 2, "ke": 1, "kg": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 2, "onto": 2, "ke": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "ceac023219023e792cc8671cd5ad35d79822f8b0", "url": "https://www.semanticscholar.org/paper/ceac023219023e792cc8671cd5ad35d79822f8b0", "title": "Ontologies as a Source for the Automatic Generation of Grammars for Information Extraction Systems", "abstract": "Grammars for Natural Language Processing (NLP) applications are generally built either by linguists \u2013 on the basis of their language competence, or by automated tools applied to existing large corpora of language data \u2014 using either supervised or unsupervised methods (or a combination of both). Domain knowledge usually played just a little role in this process. The increasing availability of extended knowledge representation systems, like taxonomies and ontologies, is giving the opportunity to consider new approaches to the (automated) generation of processing grammars, especially in the field of domain-oriented Information Extraction (IE). The reason for this being that most of the taxonomies and ontologies are equipped with natural language expressions included in ontology elements like labels, comments or definitions. These de facto established relations between (domain) knowledge and natural language expressions can be exploited for the automatic generation of domain specific NLP and IE grammars. We describe in this paper steps leading to this automation.", "venue": "SWAIE", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "ie", "ie", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 3, "onto": 4, "ie": 2}, "nlp_mention_counts": {"nlp": 3, "ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "03c07b2e46de3707bb2cd9caed138c629f16ce75", "url": "https://www.semanticscholar.org/paper/03c07b2e46de3707bb2cd9caed138c629f16ce75", "title": "Knowledge representation and management: benefits and challenges of the semantic web for the fields of KRM and NLP.", "abstract": "OBJECTIVES\nTo summarize excellent current research in the field of knowledge representation and management (KRM).\n\n\nMETHOD\nA synopsis of the articles selected for the IMIA Yearbook 2011 is provided and an attempt to highlight the current trends in the field is sketched.\n\n\nRESULTS\nThis last decade, with the extension of the text-based web towards a semantic-structured web, NLP techniques have experienced a renewed interest in knowledge extraction. This trend is corroborated through the five papers selected for the KRM section of the Yearbook 2011. They all depict outstanding studies that exploit NLP technologies whenever possible in order to accurately extract meaningful information from various biomedical textual sources.\n\n\nCONCLUSIONS\nBringing semantic structure to the meaningful content of textual web pages affords the user with cooperative sharing and intelligent finding of electronic data. As exemplified by the best paper selection, more and more advanced biomedical applications aim at exploiting the meaningful richness of free-text documents in order to generate semantic metadata and recently to learn and populate domain ontologies. These later are becoming a key piece as they allow portraying the semantics of the Semantic Web content. Maintaining their consistency with documents and semantic annotations that refer to them is a crucial challenge of the Semantic Web for the coming years.", "venue": "Yearbook of medical informatics", "citationCount": 3, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "ke", "onto", "nlp", "sw", "sw", "nlp", "sw"], "mention_counts": {"nlp": 3, "sw": 3, "onto": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1}, "ld_mention_counts": {"ke": 1, "sw": 3, "onto": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "296247bad16a06df446dbc6897c9872056ed59de", "url": "https://www.semanticscholar.org/paper/296247bad16a06df446dbc6897c9872056ed59de", "title": "Rough Set Based Approach to Text Classification", "abstract": "Textual document set has become an important and rapidly growing information source in the web. Text classification is one of the crucial technologies for information organisation and management. Text classification has become more and more important and attracted wide attention of researchers from different research fields. In this paper, many feature selection methods, the implement algorithms and applications of text classification are introduced firstly. However, because there are much noise in the knowledge extracted by current data-mining techniques for text classification, it leads to much uncertainty in the process of text classification which is produced from both the knowledge extraction and knowledge usage, therefore, more innovative techniques and methods are needed to improve the performance of text classification. It has been a critical step with great challenge to further improve the process of knowledge extraction and effectively utilization of the extracted knowledge. Rough Set decision making approach is proposed to use Rough Set decision techniques to more precisely classify the textual documents which are difficult to separate by the classic text classification methods. The purpose of this paper is to give an overview of existing text classification technologies, to demonstrate the Rough Set concepts and the decision making approach based on Rough Set theory for building more reliable and effective text classification framework with higher precision, to set up an innovative evaluation metric named CEI which is very effective for the performance assessment of the similar research, and to propose a promising research direction for addressing the challenging problems in text classification, text mining and other relative fields.", "venue": "2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)", "citationCount": 12, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["ke", "tp", "ke", "ke", "ke"], "mention_counts": {"ke": 4, "tp": 1}, "nlp_mention_counts": {"ke": 4, "tp": 1}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "30c72c8c7973487d78a03f242fd46ec3947c562f", "url": "https://www.semanticscholar.org/paper/30c72c8c7973487d78a03f242fd46ec3947c562f", "title": "A Novel Patent Knowledge Extraction Method for Innovative Design", "abstract": "As an important source of inspiration, the great number of patent documents provides designers with valuable knowledge of design rationale (DR), including issues, intent, pros and cons of the solutions. Researchers have carried out a number of data analysis studies based on patent information, which is now a new discipline called Patinformatics, including the analysis of patent information from a macro perspective and the identification and extraction of patent knowledge from a micro perspective. If DR knowledge could be extracted automatically from the patent documents and provided to designers as a source of inspiration, it would greatly promote innovative design, and at the same time promote the reuse of patent documents and the wide application of DR theory, which can be like killing three birds with one stone. To address this issue, this study proposes an improved lexical-syntactic pattern method for DR centric patent knowledge extraction, including DR Vector Space model (DRVS), DRV Trigger Word (DRV-TW), Design Rationale Vector (DRV), DR credibility (DRC) and others, and DRV based knowledge extraction algorithms. Knowledge extraction experiments were conducted on 1491 patent documents to verify the feasibility and performance of the method. In addition, two other sets of comparative experiments were conducted using the FastText and BERT machine learning methods, and the results further confirmed the reliability of the proposed method for low-resource corpus.", "venue": "IEEE Access", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ke", "ke"], "mention_counts": {"ke": 4, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "1c26c5158e52b3cb101448322c6015a102d0f342", "url": "https://www.semanticscholar.org/paper/1c26c5158e52b3cb101448322c6015a102d0f342", "title": "Korean NLP2RDF Resources", "abstract": "The aim of Linked Open Data (LOD) is to improve information management and integration by enhancing accessibility to the existing various forms of open data. The goal of this paper is to make Korean resources linkable entities. By using NLP tools, which are suggested in this paper, Korean texts are converted to RDF resources and they can be connected with other RDF triples. It is worth noticing that to the best of our knowledge there is a few of publicy available Korean NLP tools. For this reason, the Korean NLP platform presented here will be available as open source. And it is shown in this paper that the result of this NLP platform can be used as Linked Data entities.", "venue": "ALR@COLING", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "nlp", "lod", "ld", "rdf", "lod", "nlp", "nlp", "nlp"], "mention_counts": {"ld": 1, "nlp": 4, "lod": 2, "rdf": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"ld": 1, "lod": 2, "rdf": 2}, "relevance_score": 0.8214593955366725}, {"paperId": "495015d21c26eac9a6bd64c836ee3370283641ec", "url": "https://www.semanticscholar.org/paper/495015d21c26eac9a6bd64c836ee3370283641ec", "title": "VisKE: Visual knowledge extraction and question answering by visual verification of relation phrases", "abstract": "How can we know whether a statement about our world is valid. For example, given a relationship between a pair of entities e.g., `eat(horse, hay)', how can we know whether this relationship is true or false in general. Gathering such knowledge about entities and their relationships is one of the fundamental challenges in knowledge extraction. Most previous works on knowledge extraction have focused purely on text-driven reasoning for verifying relation phrases. In this work, we introduce the problem of visual verification of relation phrases and developed a Visual Knowledge Extraction system called VisKE. Given a verb-based relation phrase between common nouns, our approach assess its validity by jointly analyzing over text and images and reasoning about the spatial consistency of the relative configurations of the entities and the relation involved. Our approach involves no explicit human supervision thereby enabling large-scale analysis. Using our approach, we have already verified over 12000 relation phrases. Our approach has been used to not only enrich existing textual knowledge bases by improving their recall, but also augment open-domain question-answer reasoning.", "venue": "Computer Vision and Pattern Recognition", "citationCount": 121, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "ke", "ke"], "mention_counts": {"ke": 4, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "e62cc6938ed468e97a63d49b80735d7a92fdd6f8", "url": "https://www.semanticscholar.org/paper/e62cc6938ed468e97a63d49b80735d7a92fdd6f8", "title": "Knowledge Extraction and Acquisition During Real-Time Navigation in Unknown Environments", "abstract": "This paper deals with the representation and acquisition of knowledge extracted during a real-time path planning process in an unknown space. In particular, the extracted knowledge from each current free navigation space is represented by attributed graphs. The graph knowledge representation forms are \u201cprocessed and combined\u201d appropriately by generating a scheme which acquires the extracted knowledge during the navigation. Illustrated examples are provided for simple navigation cases.", "venue": "Int. J. Pattern Recognit. Artif. Intell.", "citationCount": 12, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["ke", "ke", "kg", "ke", "ke"], "mention_counts": {"ke": 4, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "547f3e80bfbcfb0868d02aeaaf8261ba5ecceff9", "url": "https://www.semanticscholar.org/paper/547f3e80bfbcfb0868d02aeaaf8261ba5ecceff9", "title": "Knowledge extraction changes the way an expert thinks", "abstract": "The authors show an aspect of knowledge extraction that deserves attention: the process where a human domain expert becomes more cogent, in his or her own mind, about the algorithms, heuristics, and strategies that he or she uses in manipulating this factual knowledge base. The contention is that the knowledge engineer's constructs can be greatly strengthened by utilizing those constructs of the domain expert that may evolve through the knowledge extraction process. In addition, harnessing the domain expert's constructs by fostering such introspective processes early in the knowledge extraction stage will increase the cost effectiveness of the expert system design. The arguments are based on a case study of one particular human expert who has been involved from 1985 to 1992, in the construction of five expert systems, all of which pertain to the same domain and have the same objective to assist in the diagnosis of stuttering among young children.<<ETX>>", "venue": "[Proceedings] 1992 IEEE International Conference on Systems, Man, and Cybernetics", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ke", "ke"], "mention_counts": {"ke": 4, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "5ff3fc373fad4841fe98a4ef3986d76eb451844b", "url": "https://www.semanticscholar.org/paper/5ff3fc373fad4841fe98a4ef3986d76eb451844b", "title": "A framework for structured knowledge extraction and representation from natural language via deep sentence analysis", "abstract": "We present a framework that we are currently developing, that allows one to extract knowledge from natural language sentences using a deep analysis technique based on linguistic dependencies. The extracted knowledge is represented in OOLOT, an intermediate format that we have introduced, inspired by the Language of Thought (LOT) and based on Answer Set Programming (ASP). OOLOT uses an ontologyoriented lexicon and syntax. Therefore, it is possible to export the extracted knowledge into OWL and native ASP.", "venue": "Italian Conference on Computational Logic", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4, "onto": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "onto": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "bf9bb5f28c1c9e014e453d745c8a6e83d9d365f2", "url": "https://www.semanticscholar.org/paper/bf9bb5f28c1c9e014e453d745c8a6e83d9d365f2", "title": "Knowledge Extraction and Analysis on Collaborative Interaction", "abstract": "E-learning is popularized so fast and Collaborative Learning (CL) becomes so important an instructional strategy. There are huge Group Session (GS) texts needed to be analyzed to evaluate CL, thus the automatic or semi-automatic methods of analyzing the GS texts become very important. \n \nIn this paper we present a method called Interaction Analysis depended on Knowledge Extraction (IAKE) to analyze collaborative learning by extracting knowledge from the GS texts. This method is based on a GS text analysis approach we proposed it as Theme based Knowledge Extraction (TKE).", "venue": "International Conference on Artificial Intelligence in Education", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "ke", "ke"], "mention_counts": {"ke": 4, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "4274016dfe32971fcb5bb2043b35555f6fcf91c9", "url": "https://www.semanticscholar.org/paper/4274016dfe32971fcb5bb2043b35555f6fcf91c9", "title": "Building and Using a Lexical Knowledge Base of Near-Synonym Differences", "abstract": "Choosing the wrong word in a machine translation or natural language generation system can convey unwanted connotations, implications, or attitudes. The choice between near-synonyms such as error, mistake, slip, and blunderwords that share the same core meaning, but differ in their nuancescan be made only if knowledge about their differences is available. We present a method to automatically acquire a new type of lexical resource: a knowledge base of near-synonym differences. We develop an unsupervised decision-list algorithm that learns extraction patterns from a special dictionary of synonym differences. The patterns are then used to extract knowledge from the text of the dictionary. The initial knowledge base is later enriched with information from other machine-readable dictionaries. Information about the collocational behavior of the near-synonyms is acquired from free text. The knowledge base is used by Xenon, a natural language generation system that shows how the new lexical resource can be used to choose the best near-synonym in specific situations.", "venue": "Computational Linguistics", "citationCount": 80, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "kg", "nlg", "kg", "kg", "nlg", "kg", "ke"], "mention_counts": {"kg": 4, "nlg": 2, "ke": 1, "mt": 1}, "nlp_mention_counts": {"ke": 1, "nlg": 2, "mt": 1}, "ld_mention_counts": {"kg": 4, "ke": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "9cc8609f904c50b1be408abede7ab7f5cdbc9744", "url": "https://www.semanticscholar.org/paper/9cc8609f904c50b1be408abede7ab7f5cdbc9744", "title": "Research on Machine Translation of Deep Neural Network Learning Model Based on Ontology", "abstract": "To align different ontologies, it is necessary to find effective ways to achieve interoperability of information in the context of the Semantic Web. The development of accurate and reliable techniques to automatically perform this task, it is becoming more and more crucial as overlap between ontologies grows proportionally. In order to solve the problem that traditional machine translation cannot meet the needs of users because of the slow translation speed. According to the characteristics of Ontology's domain knowledge concept system, deep neural network learning model based machine translation method is proposed. Through the experimental design, we examine the translation time and BLEU score and other indicators. After junior translators use the tools, the translation time is reduced by 34.0% and the BLEU score increases by 7.59; after the senior translators use the tools, the translation time is reduced by 11.3%, and the BLEU score is increased by 1.67. Analysis of the experimental results shows that the essence of this method is to complement translation skills, so it is more effective for junior translators who are not good enough in translation skills. The machine translation method based on deep neural network learning can significantly improve the quality and efficiency of translation.", "venue": "Informatica", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "onto", "mt", "sw", "mt", "onto", "onto", "mt", "onto"], "mention_counts": {"sw": 1, "onto": 4, "mt": 4}, "nlp_mention_counts": {"mt": 4}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "9c01a3e29f62b7e8967b03af0016e2a555ddd2c2", "url": "https://www.semanticscholar.org/paper/9c01a3e29f62b7e8967b03af0016e2a555ddd2c2", "title": "FNLP\u2010ONT: A feasible ontology for improving NLP tasks in Persian", "abstract": "Natural language processing is a composition of several error\u2010prone and challenging tasks, including part of speech tagging, word sense disambiguation, named entity recognition, and compound verb detection. Studying intrasentence relations and roles is essential to improve the mentioned subtasks. Semi\u2010automatic schemes such as ontologies can be applied to clarify word's dependencies. This paper presents an ontology that is targeting to improve POS tagging, WSD, NER, and compound verb detection in Persian with extra properties that may ameliorate machine translation. The ontology is tested in combinations with several state\u2010of\u2010art algorithms on Dadegan corpus. The results show that coping semantic analysis with machine learning methods enhance relation detection and consequently precision of the mentioned subtasks, which is not widely addressed in Persian. Furthermore, the experimental results declare that the accuracy rate increases between 4.5 and 23% for different tasks.", "venue": "Expert Syst. J. Knowl. Eng.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "wsd", "wsd", "onto", "onto", "onto", "mt"], "mention_counts": {"nlp": 2, "wsd": 2, "onto": 4, "mt": 1}, "nlp_mention_counts": {"nlp": 2, "wsd": 2, "mt": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "aaabe8ed88532781c82d5eed36ef80f842a9f2e1", "url": "https://www.semanticscholar.org/paper/aaabe8ed88532781c82d5eed36ef80f842a9f2e1", "title": "Enhancing Linguistic Web Service Description with Non-functional NLP Properties", "abstract": "This paper deals with the enhancing of Linguistic Web Service (LingWS) description. It proposes an extension for the OWL-S approach and a Natural Language Processing (NLP) domain ontology based on linguistic standards. The proposed extension provides a classification of the Non-functional NLP properties which promotes the representation of their relationships. The extended OWL-S description is linked to the NLP domain ontology to semantically annotate the LingWS properties.", "venue": "ICSOFT", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp", "nlp", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 5, "onto": 4}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "29421992b3908b5cb39e5705374ec7cf504ae2ea", "url": "https://www.semanticscholar.org/paper/29421992b3908b5cb39e5705374ec7cf504ae2ea", "title": "Design of GA and Ontology based NLP Frameworks for Online Opinion Mining", "abstract": "\n\n For almost every domain, a tremendous degree of data is accessible in an\nonline and offline mode. Billions of users are daily posting their views or opinions by using different\nonline applications like WhatsApp, Facebook, Twitter, Blogs, Instagram etc.\n\n\n\nThese reviews are constructive for the progress of the venture, civilization, state and even nation.\nHowever, this momentous amount of information is useful only if it is collectively and effectively\nmined.\n\n\n\nOpinion mining is used to extract the thoughts, expression, emotions, critics, appraisal\nfrom the data posted by different persons. It is one of the prevailing research techniques that coalesce\nand employ the features from natural language processing. Here, an amalgamated approach has been\nemployed to mine online reviews.\n\n\n\nTo improve the results of genetic algorithm based opining mining patent, here, a hybrid genetic\nalgorithm and ontology based 3-tier natural language processing framework named GAO_NLP_OM has\nbeen designed. First tier is used for preprocessing and corrosion of the sentences. Middle tier is composed\nof genetic algorithm based searching module, ontology for English sentences, base words for the\nreview, complete set of English words with item and their features. Genetic algorithm is used to expedite\nthe polarity mining process. The last tier is liable for semantic, discourse and feature summarization.\nFurthermore, the use of ontology assists in progressing more accurate opinion mining model.\n\n\n\nGAO_NLP_OM is supposed to improve the performance of genetic algorithm based opinion\nmining patent. The amalgamation of genetic algorithm, ontology and natural language processing\nseems to produce fast and more precise results. The proposed framework is able to mine simple as well\nas compound sentences. However, affirmative preceded interrogative, hidden feature and mixed language\nsentences still be a challenge for the proposed framework.\n", "venue": "Recent Patents on Engineering", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "onto", "nlp", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 4, "onto": 5}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.8214593955366725}, {"paperId": "2612f8294454aab719376b411185d8a90b3510b9", "url": "https://www.semanticscholar.org/paper/2612f8294454aab719376b411185d8a90b3510b9", "title": "The Theoretical Status of Ontologies in Natural Language Processing", "abstract": "This paper discusses the use of `ontologies' in Natural Language Processing. It classifies various kinds of ontologies that have been employed in NLP and discusses various benefits and problems with those designs. Particular focus is then placed on experiences gained in the use of the Upper Model, a linguistically-motivated `ontology' originally designed for use with the Penman text generation system. Some proposals for further NLP ontology design criteria are then made.", "venue": "ArXiv", "citationCount": 55, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 4, "onto": 5}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.8214593955366725}, {"paperId": "9ee97ae8d5ee07c4e5c7acb6c125ec7a38aa0564", "url": "https://www.semanticscholar.org/paper/9ee97ae8d5ee07c4e5c7acb6c125ec7a38aa0564", "title": "Natural language processing based ontology learning", "abstract": "Though the utility of domain Ontologies is now widely acknowledged in an increasing number of domains, a critical task of identifying, defining, and entering the concept definitions is still intractable. Nowadays, natural language becomes a more and more important information source, and natural language processing (NLP) becomes more and more practical. To reduce time, cost and making use of NLP, it is highly advisable to refer, in constructing or updating an ontology, to the documents available in the field. In this paper we describe NLTPOnto, a natural language processing based tool is devised to help people to semi-automatically construct ontology.", "venue": "2010 International Conference on Computer Application and System Modeling (ICCASM 2010)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "nlp", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 5, "onto": 4}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "6a6e4f13a4577497a95ad8e1acddcee1d335130e", "url": "https://www.semanticscholar.org/paper/6a6e4f13a4577497a95ad8e1acddcee1d335130e", "title": "Integration of Domain Knowledge using Medical Knowledge Graph Deep Learning for Cancer Phenotyping", "abstract": "A key component of deep learning (DL) for natural language processing (NLP) is word embeddings. Word embeddings that effectively capture the meaning and context of the word that they represent can significantly improve the performance of downstream DL models for various NLP tasks. Many existing word embeddings techniques capture the context of words based on word co-occurrence in documents and text; however, they often cannot capture broader domain-specific relationships between concepts that may be crucial for the NLP task at hand. In this paper, we propose a method to integrate external knowledge from medical terminology ontologies into the context captured by word embeddings. Specifically, we use a medical knowledge graph, such as the unified medical language system (UMLS), to find connections between clinical terms in cancer pathology reports. This approach aims to minimize the distance between connected clinical concepts. We evaluate the proposed approach using a Multitask Convolutional Neural Network (MT-CNN) to extract six cancer characteristics \u2013 site, subsite, laterality, behavior, histology, and grade \u2013 from a dataset of 900K cancer pathology reports. The results show that the MT-CNN model which uses our domain informed embeddings outperforms the same MT-CNN using standard word2vec embeddings across all tasks, with an improvement in the overall microand macro-F1 scores by 4.97%and 22.5%, respectively. Keywords-Word embeddings; UMLS; convolutional neural networks; CNN; natural language processing; knowledge graph; This manuscript has been authored by UT Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a nonexclusive, paidup, irrevocable, world-wide license to publish or reproduce the published form of the manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-publicaccess-plan).", "venue": "ArXiv", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "nlp", "nlp", "nlp", "nlp", "kg", "kg", "onto"], "mention_counts": {"nlp": 5, "onto": 1, "kg": 3}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"kg": 3, "onto": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "1c4dc3adf6911ee35699a99aaf93127f6c01e763", "url": "https://www.semanticscholar.org/paper/1c4dc3adf6911ee35699a99aaf93127f6c01e763", "title": "The Effect of Text Ambiguity on creating Policy Knowledge Graphs", "abstract": "A growing number of web and cloud-based products and services rely on data sharing between consumers, service providers, and their subsidiaries and third parties. There is a growing concern around the security and privacy of data in such large-scale shared architectures. Most organizations have a human-written privacy policy that discloses all the ways that data is shared, stored, and used. The organizational privacy policies must also be compliant with government and administrative regulations. This raises a major challenge for providers as they try to launch new services. Thus they are moving towards a system of automatic policy maintenance and regulatory compliance. This requires extracting policy from text documents and representing it in a semi-structured, machine-processable framework. The most popular method to this end is extracting policy information into a Knowledge Graph (KG). There exists a significant body of work that converts text descriptions of regulations into policies expressed in languages such as OWL and XACML and is grounded in the control-based schema by using NLP approaches. In this paper, we show that the NLP-based approaches to extract knowledge from written policy documents and representing them in enforceable Knowledge Graphs fail when the text policies are ambiguous. Ambiguity can arise from lack of clarity, misuse of syntax, and/or the use of complex language. We describe a system to extract features from a policy document that affect its ambiguity and classify the documents based on the level of ambiguity present. We validate this approach using human annotators. We show that a large number of documents in a popular privacy policy corpus (OPP-115) are ambiguous. This affects the ability to automatically monitor privacy policies. We show that for policies that are more ambiguous according to our proposed measure, NLP-based text segment classifiers are less accurate.", "venue": "2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "kg", "kg", "nlp", "nlp", "nlp", "kg"], "mention_counts": {"nlp": 3, "ke": 1, "onto": 1, "kg": 3}, "nlp_mention_counts": {"nlp": 3, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 1, "kg": 3}, "relevance_score": 0.8214593955366725}, {"paperId": "8a9ca89285620840ae770f8e90438a1df919ef22", "url": "https://www.semanticscholar.org/paper/8a9ca89285620840ae770f8e90438a1df919ef22", "title": "Template Driven Information Extraction for Populating Ontologies", "abstract": "We address the integration of information extraction (IE) and ontologies. In particular, using an ontology to aid the IE process, and using the IE results to help populate the ontology. We perform IE by means of domain specific templates and the lightweight use of Natural Languages Processing techniques (NLP). Our main goal is to learn information from text by the use of templates and in this way to alleviate the main bottleneck in creating knowledge-base systems that is \"the extraction of knowledge\". Our domain of study is \"KMi Planet\", a Web-based news server for communication of stories between members in our institute. The main goals of our system are to classify an incoming story, obtain the relevant objects within the story, deduce the relationships between them, and to populate the ontology. Furthermore, we aim to do this with minimal help from the user.", "venue": "Workshop on Ontology Learning", "citationCount": 47, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ie", "onto", "kg", "onto", "ie", "nlp", "onto", "onto", "onto", "ke"], "mention_counts": {"onto": 5, "nlp": 2, "ke": 1, "kg": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 1, "ie": 2}, "ld_mention_counts": {"kg": 1, "onto": 5, "ke": 1}, "relevance_score": 0.8183448250315903}, {"paperId": "be22b8a3b0662ddc7c75792ead3175d674b07bbc", "url": "https://www.semanticscholar.org/paper/be22b8a3b0662ddc7c75792ead3175d674b07bbc", "title": "The State of Knowledge Extraction from Text for Thai Language", "abstract": "With the emergence of the Semantic Web (or Linked Data), increased efforts have been made to automatically extract formalized semantic knowledge from natural language text. Most research work and tools for knowledge extraction are focusing on text in English language. In this work, wepresent our research-in-progress on evaluating the state-of-the-art in knowledge extraction from text for Thai language. For this purpose, we investigate the existing knowledge extraction literature and group the available research work and tools into eight knowledge extraction tasks. Our preliminary results from the survey of the state-of-the-art show that there exist large research gaps and therefore future research opportunities in Thaiknowledge extraction, and we provide hints for available research directions.", "venue": "IIAI International Conference on Advanced Applied Informatics", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ke", "ke", "ke", "ld", "ke", "ke"], "mention_counts": {"ld": 1, "sw": 1, "ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ld": 1, "sw": 1, "ke": 5}, "relevance_score": 0.8183448250315903}, {"paperId": "484d9a9ddcabb4e0125c4ba5886afc390d616c94", "url": "https://www.semanticscholar.org/paper/484d9a9ddcabb4e0125c4ba5886afc390d616c94", "title": "Improving Machine Translation through Linked Data", "abstract": "Abstract With the ever increasing availability of linked multilingual lexical resources, there is a renewed interest in extending Natural Language Processing (NLP) applications so that they can make use of the vast set of lexical knowledge bases available in the Semantic Web. In the case of Machine Translation, MT systems can potentially benefit from such a resource. Unknown words and ambiguous translations are among the most common sources of error. In this paper, we attempt to minimise these types of errors by interfacing Statistical Machine Translation (SMT) models with Linked Open Data (LOD) resources such as DBpedia and BabelNet. We perform several experiments based on the SMT system Moses and evaluate multiple strategies for exploiting knowledge from multilingual linked data in automatically translating named entities. We conclude with an analysis of best practices for multilingual linked data sets in order to optimise their benefit to multilingual and cross-lingual applications.", "venue": "Prague Bull. Math. Linguistics", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "lod", "sw", "lod", "ld", "ld", "nlp", "mt", "mt", "ld", "nlp", "kg"], "mention_counts": {"ld": 3, "sw": 1, "lod": 2, "nlp": 2, "mt": 3, "kg": 1}, "nlp_mention_counts": {"nlp": 2, "mt": 3}, "ld_mention_counts": {"ld": 3, "sw": 1, "lod": 2, "kg": 1}, "relevance_score": 0.8183448250315903}, {"paperId": "79c9c846e0f977868c034e8a40837409ab9b802a", "url": "https://www.semanticscholar.org/paper/79c9c846e0f977868c034e8a40837409ab9b802a", "title": "Enriching OWL Ontologies with Linguistic and User-Related Annotations: The ELEON System", "abstract": "This paper introduces ELEON, an editor that allows the enrichment of OWL ontologies with linguistic and user-related annotations. The enriched ontologies are used by natural language generation (NLG) engines to generate textual descriptions of the objects represented in the ontologies in the selected language and according to user's model. ELEON provides a well-defined interface that can be used by different NLG engines. The paper presents the relevant functionalities of ELEON, describes the provided interface to NLG engines and discusses the advantages of exploiting such enriched ontologies in NLG.", "venue": "IEEE International Conference on Tools with Artificial Intelligence", "citationCount": 32, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "nlg", "onto", "onto", "nlg", "nlg", "onto", "onto", "onto", "onto", "nlg"], "mention_counts": {"nlg": 5, "onto": 7}, "nlp_mention_counts": {"nlg": 5}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.8183448250315903}, {"paperId": "b130d7e38a59e1194bd6430696bf96d386114ce7", "url": "https://www.semanticscholar.org/paper/b130d7e38a59e1194bd6430696bf96d386114ce7", "title": "Natural Language Generation at Scale: A Case Study for Open Domain Question Answering", "abstract": "Current approaches to Natural Language Generation (NLG) for dialog mainly focus on domain-specific, task-oriented applications (e.g. restaurant booking) using limited ontologies (up to 20 slot types), usually without considering the previous conversation context. Furthermore, these approaches require large amounts of data for each domain, and do not benefit from examples that may be available for other domains. This work explores the feasibility of applying statistical NLG to scenarios requiring larger ontologies, such as multi-domain dialog applications or open-domain question answering (QA) based on knowledge graphs. We model NLG through an Encoder-Decoder framework using a large dataset of interactions between real-world users and a conversational agent for open-domain QA. First, we investigate the impact of increasing the number of slot types on the generation quality and experiment with different partitions of the QA data with progressively larger ontologies (up to 369 slot types). Second, we perform multi-task learning experiments between open-domain QA and task-oriented dialog, and benchmark our model on a popular NLG dataset. Moreover, we experiment with using the conversational context as an additional input to improve response generation quality. Our experiments show the feasibility of learning statistical NLG models for open-domain QA with larger ontologies.", "venue": "INLG", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "onto", "nlg", "onto", "nlg", "kg", "nlg", "nlg", "nlg", "onto", "nlg"], "mention_counts": {"kg": 1, "nlg": 7, "onto": 4}, "nlp_mention_counts": {"nlg": 7}, "ld_mention_counts": {"kg": 1, "onto": 4}, "relevance_score": 0.8183448250315903}, {"paperId": "799403bbb2f10fbfaf6381c4555fd0a9e987a34b", "url": "https://www.semanticscholar.org/paper/799403bbb2f10fbfaf6381c4555fd0a9e987a34b", "title": "An exploratory analysis: extracting materials science knowledge from unstructured scholarly data", "abstract": "\nPurpose\nThe output of academic literature has increased significantly due to digital technology, presenting researchers with a challenge across every discipline, including materials science, as it is impossible to manually read and extract knowledge from millions of published literature. The purpose of this study is to address this challenge by exploring knowledge extraction in materials science, as applied to digital scholarship. An overriding goal is to help inform readers about the status knowledge extraction in materials science.\n\n\nDesign/methodology/approach\nThe authors conducted a two-part analysis, comparing knowledge extraction methods applied materials science scholarship, across a sample of 22 articles; followed by a comparison of HIVE-4-MAT, an ontology-based knowledge extraction and MatScholar, a named entity recognition (NER) application. This paper covers contextual background, and a review of three tiers of knowledge extraction (ontology-based, NER and relation extraction), followed by the research goals and approach.\n\n\nFindings\nThe results indicate three key needs for researchers to consider for advancing knowledge extraction: the need for materials science focused corpora; the need for researchers to define the scope of the research being pursued, and the need to understand the tradeoffs among different knowledge extraction methods. This paper also points to future material science research potential with relation extraction and increased availability of ontologies.\n\n\nOriginality/value\nTo the best of the authors\u2019 knowledge, there are very few studies examining knowledge extraction in materials science. This work makes an important contribution to this underexplored research area.\n", "venue": "Electronic library", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "onto", "ke", "ke", "ke", "ke", "onto", "ke", "ke", "ke", "onto"], "mention_counts": {"ke": 9, "onto": 3, "kg": 1}, "nlp_mention_counts": {"ke": 9}, "ld_mention_counts": {"ke": 9, "onto": 3, "kg": 1}, "relevance_score": 0.8180808589832839}, {"paperId": "447058cb448648af0adbd34db3feaddcaeba625a", "url": "https://www.semanticscholar.org/paper/447058cb448648af0adbd34db3feaddcaeba625a", "title": "A Methodology for Extracting Knowledge about Controlled Vocabularies from Textual Data using FCA-Based Ontology Engineering", "abstract": "We introduce an end-to-end methodology (from text processing to querying a knowledge graph) for the sake of knowledge extraction from text corpora with a focus on a list of vocabularies of interest. We propose a pipeline that incorporates Natural Language Processing (NLP), Formal Concept Analysis (FCA), and Ontology Engineering techniques to build an ontology from textual data. We then extract the knowledge about controlled vocabularies by querying that knowledge graph, i.e., the engineered ontology. We demonstrate the significance of the proposed methodology by using it for knowledge extraction from a text corpus that consists of 800 news articles and reports about companies and products in the IT and pharmaceutical domain, where the focus is on a given list of 250 controlled vocabularies.", "venue": "IEEE International Conference on Bioinformatics and Biomedicine", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["tp", "ke", "nlp", "ke", "nlp", "onto", "ke", "onto", "onto", "kg", "onto", "kg"], "mention_counts": {"onto": 4, "nlp": 2, "ke": 3, "kg": 2, "tp": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 3, "tp": 1}, "ld_mention_counts": {"ke": 3, "onto": 4, "kg": 2}, "relevance_score": 0.7967438898272832}, {"paperId": "2fe7c6c1bb727a8a71af2d2b4c328937c6ba0e1f", "url": "https://www.semanticscholar.org/paper/2fe7c6c1bb727a8a71af2d2b4c328937c6ba0e1f", "title": "A Survey of Thai Knowledge Extraction for the Semantic Web Research and Tools", "abstract": "As the manual creation of domain models and also of linked data is very costly, the extraction of knowledge from structured and unstructured data has been one of the central research areas in the Semantic Web field in the last two decades. Here, we look specifically at the extraction of formalized knowledge from natural language text, which is the most abundant source of human knowledge available. There are many tools on hand for information and knowledge extraction for English natural language, for written Thai language the situation is different. The goal of this work is to assess the state-of-the-art of research on formal knowledge extraction specifically from Thai language text, and then give suggestions and practical research ideas on how to improve the state-of-the-art. To address the goal, first we distinguish nine knowledge extraction for the Semantic Web tasks defined in literature on knowledge extraction from English text, for example taxonomy extraction, relation extraction, or named entity recognition. For each of the nine tasks, we analyze the publications and tools available for Thai text in the form of a comprehensive literature survey. Additionally to our assessment, we measure the self-assessment by the Thai research community with the help of a questionnaire-based survey on each of the tasks. Furthermore, the structure and size of the Thai community is analyzed using complex literature database queries. Combining all the collected information we finally identify research gaps in knowledge extraction from Thai language. An extensive list of practical research ideas is presented, focusing on concrete suggestions for every knowledge extraction task \u2013 which can be implemented and evaluated with reasonable effort. Besides the task-specific hints for improvements of the state-of-the-art, we also include general recommendations on how to raise the efficiency of the respective research community. key words: knowledge extraction, Thai language text, landscape analysis, semantic web", "venue": "IEICE Trans. Inf. Syst.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "sw", "ke", "ke", "ke", "ke", "sw", "ke", "ke", "sw", "ke", "ld", "sw", "ke"], "mention_counts": {"ld": 1, "ke": 9, "sw": 4}, "nlp_mention_counts": {"ke": 9}, "ld_mention_counts": {"ld": 1, "ke": 9, "sw": 4}, "relevance_score": 0.7825501203436873}, {"paperId": "1ee2492cef96d3bc7d5a7b91fac7b4a70368b24a", "url": "https://www.semanticscholar.org/paper/1ee2492cef96d3bc7d5a7b91fac7b4a70368b24a", "title": "An Ontology-Based Knowledge Methodology in the Medical Domain in the Latin America: the Study Case of Republic of Panama", "abstract": "Introduction: Nowadays in Panama, there is a lot of patient information stored in textual form which cannot be manipulated to manage adequate knowledge. There are multiple resources created to represent knowledge, including specialized glossaries, ontologies, among others. The ontologies are an important part within the scope of the recovery and organization of the information and the semantic web. Also in recent works they are used in applications of natural language processing (NLP), as a knowledge base. Aim: This research was conducted with the aim of creating a methodology that allows from a text written in NL, extract the necessary elements using NLP tools and with them create a knowledge base represented by one domain ontology and extract knowledge to help medical specialists. Material and Methods: In this study we carried out a methodology that allows the extraction of knowledge of patient clinical records, general medicine and palliative care, in order to show relevant knowledge elements to specialists. The methodology was validated with a data corpus of approximately 200 patient records. Conclusion: We have created a knowledge representation methodology, combining NLP techniques and tools and the automatic instantiation of an ontology, which can serve as a software agent for other applications or used to visualize the patient\u2019s clinical information. The study was validated using the traditional metrics of information retrieval systems precision, recall, F-measure obtaining excellent results, and can be used as a software agent or methodology for the development of information extraction software systems in the medical domain.", "venue": "Acta informatica medica : AIM : journal of the Society for Medical Informatics of Bosnia & Herzegovina : casopis Drustva za medicinsku informatiku BiH", "citationCount": 6, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["ke", "kg", "nlp", "kg", "ke", "onto", "onto", "kg", "sw", "nlp", "nlp", "onto", "ie", "onto", "onto", "nlp"], "mention_counts": {"onto": 5, "nlp": 4, "ke": 2, "sw": 1, "kg": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 4, "ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "sw": 1, "onto": 5, "kg": 3}, "relevance_score": 0.777069182404355}, {"paperId": "0c0d1816ebe35fa9f00d4f7c720b6a89c9acdb58", "url": "https://www.semanticscholar.org/paper/0c0d1816ebe35fa9f00d4f7c720b6a89c9acdb58", "title": "A Proposal for Screening Inconsistencies in Ontologies based on Query Languages using WSD", "abstract": "In this paper, we discuss a method to screen inconsistencies in ontologies by applying a natural language processing (NLP) technique, especially, those used for word sense disambiguation (WSD). In the database research field, it is claimed that queries over target ontologies should play a significant role because they represent every aspect of the terms described in each ontology. According to (Calvanese et al., 2001), considering the global and the local ontologies, the terms in the global ontology can be viewed as the query over the local ontology, and the mapping between the global and the local ontologies is given by, associating each term in the global ontology with a view. On the other hand, ontology screening systems should be able to take advantage of some popular techniques for WSD, which is supposed to decide the right sense where the target word is used in a specific context. We present several examples regarding inconsistencies in ontologies with the aid of DAML+OIL notation(DAML+OIL, 2001), and propose that WSD can be one of the promising method to screen such as inconsistencies.", "venue": "NLPXML@COLING", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "wsd", "onto", "wsd", "onto", "onto", "wsd", "onto", "onto", "onto", "wsd", "nlp", "wsd", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "wsd": 5, "onto": 11}, "nlp_mention_counts": {"nlp": 2, "wsd": 5}, "ld_mention_counts": {"onto": 11}, "relevance_score": 0.777069182404355}, {"paperId": "451dde1a7755831b46705faf44b2497fd975e295", "url": "https://www.semanticscholar.org/paper/451dde1a7755831b46705faf44b2497fd975e295", "title": "Towards Computational Guessing of Unknown Word Meanings: The Ontological Semantic Approach", "abstract": "Towards Computational Guessing of Unknown Word Meanings: The Ontological Semantic Approach Julia M. Taylor (jtaylor1@purdue.edu) CERIAS, Purdue University & RiverGlass, Inc West Lafayette, IN 47907 & Champaign, IL 61820 Victor Raskin (vraskin@purdue.edu) Linguistics & CERIAS, Purdue University West Lafayette, IN 47907 Christian F. Hempelmann (chempelm@purdue.edu) Linguistics, Purdue University & RiverGlass, Inc West Lafayette, IN 47907 & Champaign, IL 61820 Abstract The paper describes a computational approach for guessing the meanings of previously unaccounted words in an implemented system for natural language processing. Interested in comparing the results to what is known about human guessing, it reviews a largely educational approach, partially based on cognitive psychology, to teaching humans, mostly children, to acquire new vocabulary from contextual clues, as well as the lexicographic efforts to account for neologisms. It then goes over the previous NLP efforts in processing new words and establishes the difference\u2014mostly, much richer semantic resources\u2014of the proposed approach. Finally, the results of a computer experiment that guesses the meaning of a non-existent word, placed as the direct object of 100 randomly selected verbs, from the known meanings of these verbs, with methods of the ontological semantics technology, are presented and discussed. While the results are promising percentage-wise, ways to improve them within the approach are briefly outlined. Keywords: guessing word meaning, natural language understanding, ontological semantic technology Unknown Words in Text Along with ambiguity, unattested input is one of the major problems for natural language processing systems. An NLP system is robust only if it can deal with unknown words. Yet, to deal with such words only makes sense when the rest of the sentence is understood. We take an approach here similar to that of a human learner that encounters an unfamiliar word and is able to approximate its meaning based on the rest of the sentence or its subsequent usages in other sentences. There are some suggested strategies in the human acquisition and understanding of unknown words. Some cases stand out as easy and almost self-explanatory. One of these cases is when a word is immediately explained. Such an explanation may be introduced by a that is phrase (To lose weight, one may have to follow a diet, that is, to limit the amount of food and to avoid eating certain foods.), or by apposition (Computers programs follow algorithms, ordered lists of instructions to perform.), or by examples (The earliest records of felines, for example, cats, tigers, lions, or leopards, are from millions of years ago.), or by providing the presumably known opposites for comparison through words like but, rather then, not (It is frigid outside, rather than warm and comfortable like yesterday.). Both in the case of human acquisition of new vocabulary and the machine attempt at guessing its meaning, these somewhat trivial instances, where the meaning of a new word is immediately explained, either by giving its definition or by examples, present no particular interest for us here. Besides, such cases are rather rare in regular expository texts because most writers do not bother to allow for vocabulary deficiency with regards to words with which they are well familiar themselves. Thus, it is the non-trivial cases, those without an attached explanation or description, that it is necessary to address when one is interested in designing a computer system for natural language understanding. On the other side of the spectrum lie words that can only be guessed through their functional description, not necessarily following the first use of an unknown word. These functional descriptions should be gathered throughout the document, or a number of documents, narrowing the original functional description, if necessary, or supplying other facets of it. For example, They used a Tim-Tim to navigate their way to the cabin on the lake. It took them almost half a day. They hadn\u2019t checked if the maps had been recently updated on the device, and spent hours looking for roads that no longer existed. From the clues in the first sentence, Tim-Tim can be understood as a navigation instrument (including an atlas or a map) through an inverse function of the instrument of navigation. Since no other devices are mentioned, this navigation instrument can be considered the device from the third sentence whose maps can be periodically updated. It is essential, therefore, in situations of dispersed clues that co-reference (or antecedence) be established correctly\u2014in this case, between device and Tim-Tim. Towards the middle of the spectrum are the cases where the description may immediately follow the first use of the word but without being helpfully triggered by phrases like for example or that is (He was rather taciturn. He didn\u2019t like", "venue": "CogSci", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "onto", "nlp", "nlp", "nlu", "onto", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 4, "nlu": 2}, "nlp_mention_counts": {"nlp": 4, "nlu": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "e3a49ee315cb92da348beee18c8effcec5db9aa0", "url": "https://www.semanticscholar.org/paper/e3a49ee315cb92da348beee18c8effcec5db9aa0", "title": "NLP for the Generation of Training Data Sets for Ontology-Guided Weakly-Supervised Machine Learning in Digital Pathology", "abstract": "The combination of ontologies with machine learning (ML) approaches is a hot topic and not yet extensively investigated but having great future potential. This is due to the general fact that both, ontologies and ML, constitute two indispensable technologies for domain-specific knowledge extraction, actively used in knowledge-based systems. Whilst the primary goal of both these approaches are the same, knowledge discovery, little is yet known about how the two sources of knowledge can be successfully integrated. The main data source in digital pathology are whole slide images. For the effective generation of sufficiently large and high-quality training data we need to extract in addition information from medical reports, containing non-standardized text. Since full annotation on pixel level would be impracticably expensive, a practical solution is in weakly-supervised ML. In the project described in this paper we used ontology-guided natural language processing (NLP) for term extraction and a decision tree built with an expert-curated classification system. This demonstrates the practical value of our solution to analyze and structure training data sets for ML and as a tool for the generation of biobank catalogues.", "venue": "2019 IEEE Symposium on Computers and Communications (ISCC)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "onto", "nlp", "nlp", "onto", "onto", "nlp", "ke"], "mention_counts": {"nlp": 3, "kg": 1, "onto": 4, "ke": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1}, "ld_mention_counts": {"kg": 1, "onto": 4, "ke": 1}, "relevance_score": 0.7620593014579468}, {"paperId": "7415b111a760207d240b9cac859a91f6fd4b118e", "url": "https://www.semanticscholar.org/paper/7415b111a760207d240b9cac859a91f6fd4b118e", "title": "Web information extraction based on news domain ontology theory", "abstract": "For the current web information extraction can't adapt to the various page structures, this paper proposes a Web Information Extraction Method based on News Domain Ontology. The areas are accurately found out and the interested information was extracted exactly based on information extraction rules which is generated by news domain ontology. Using the technology of page processing, page conversion, XPath etc, the information extraction system based on news domain ontology is implemented. Testing from news site shows that the approach proposed doesn 't rely on the page structure and it can increase the recall and precision of information extraction.", "venue": "2010 IEEE 2nd Symposium on Web Society", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie", "ie", "ie", "ie", "onto", "ie", "onto"], "mention_counts": {"onto": 4, "ie": 6}, "nlp_mention_counts": {"ie": 6}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "5904186456ce1bd12574096b65a7c76960e1b98e", "url": "https://www.semanticscholar.org/paper/5904186456ce1bd12574096b65a7c76960e1b98e", "title": "Application of Domain Ontologies to Natural Language Processing: A Case Study for Drug-Drug Interactions", "abstract": "Natural Language Processing NLP techniques can provide an interesting way to mine the growing biomedical literature, and a promising approach for new knowledge discovery. However, the major bottleneck in this area is that these systems rely on specific resources providing the domain knowledge. Domain ontologies provide a contextual framework and a semantic representation of the domain, and they can contribute to a better performance of current NLP systems. However, their contribution to information extraction has not been well studied yet. The aim of this paper is to provide insights into the potential role that domain ontologies can play in NLP. To do this, the authors apply the drug-drug interactions ontology DINTO to named entity recognition and relation extraction from pharmacological texts. The authors use the DDI corpus, a gold-standard for the development and evaluation of IE systems in this domain, and evaluate their results in the framework of the last SemEval-2013 DDI Extraction task.", "venue": "Int. J. Inf. Retr. Res.", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "ie", "nlp", "onto", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 5, "onto": 4, "ie": 1}, "nlp_mention_counts": {"nlp": 5, "ie": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "2e614312ca22950b08697943c8a6a715ab285210", "url": "https://www.semanticscholar.org/paper/2e614312ca22950b08697943c8a6a715ab285210", "title": "Biomedical Concept Recognition Using Deep Neural Sequence Models", "abstract": "Background the automated identification of mentions of ontological concepts in natural language texts is a central task in biomedical information extraction. Despite more than a decade of effort, performance in this task remains below the level necessary for many applications. Results recently, applications of deep learning in natural language processing have demonstrated striking improvements over previously state-of-the-art performance in many related natural language processing tasks. Here we demonstrate similarly striking performance improvements in recognizing biomedical ontology concepts in full text journal articles using deep learning techniques originally developed for machine translation. For example, our best performing system improves the performance of the previous state-of-the-art in recognizing terms in the Gene Ontology Biological Process hierarchy, from a previous best F1 score of 0.40 to an F1 of 0.70, nearly halving the error rate. Nearly all other ontologies show similar performance improvements. Conclusions A two-stage concept recognition system, which is a conditional random field model for span detection followed by a deep neural sequence model for normalization, improves the state-of-the-art performance for biomedical concept recognition. Treating the biomedical concept normalization task as a sequence-to-sequence mapping task similar to neural machine translation improves performance.", "venue": "bioRxiv", "citationCount": 7, "fieldsOfStudy": ["Computer Science", "Biology"], "mentions": ["nll", "ie", "mt", "mt", "onto", "onto", "onto", "onto", "nlp", "nlp"], "mention_counts": {"onto": 4, "nll": 1, "nlp": 2, "mt": 2, "ie": 1}, "nlp_mention_counts": {"mt": 2, "nlp": 2, "nll": 1, "ie": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "992bedb7cf40b5f7f72f4d5a5d10ec1967f92d48", "url": "https://www.semanticscholar.org/paper/992bedb7cf40b5f7f72f4d5a5d10ec1967f92d48", "title": "Automatic Knowledge Extraction with Human Interface", "abstract": "OrbWeaver, an automatic knowledge extraction system paired with a human interface, streamlines the use of unintuitive natural language processing software for modeling systems from their documentation. OrbWeaver enables the indirect transfer of knowledge about legacy systems by leveraging open source tools in document understanding and processing as well as using web based user interface constructs. By design, OrbWeaver is scalable, extensible, and usable; we demonstrate its utility by evaluating its performance in processing a corpus of documents related to advanced persistent threats in the cyber domain. The results indicate better knowledge extraction by revealing hidden relationships, linking co-related entities, and gathering evidence. Keywords\u2014knowledge extraction, human interface, natural language processing, system modeling, model based system engineering, cyber, advanced persistent threat", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "ke", "nlp", "ke", "ke"], "mention_counts": {"nlp": 2, "ke": 4}, "nlp_mention_counts": {"nlp": 2, "ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "128d207b22353cbde0ed321156dc5187befa25cf", "url": "https://www.semanticscholar.org/paper/128d207b22353cbde0ed321156dc5187befa25cf", "title": "A High Precision Pipeline for Financial Knowledge Graph Construction", "abstract": "Motivated by applications such as question answering, fact checking, and data integration, there is significant interest in constructing knowledge graphs by extracting information from unstructured information sources, particularly text documents. Knowledge graphs have emerged as a standard for structured knowledge representation, whereby entities and their inter-relations are represented and conveniently stored as (subject,predicate,object) triples in a graph that can be used to power various downstream applications. The proliferation of financial news sources reporting on companies, markets, currencies, and stocks presents an opportunity for extracting valuable knowledge about this crucial domain. In this paper, we focus on constructing a knowledge graph automatically by information extraction from a large corpus of financial news articles. For that purpose, we develop a high precision knowledge extraction pipeline tailored for the financial domain. This pipeline combines multiple information extraction techniques with a financial dictionary that we built, all working together to produce over 342,000 compact extractions from over 288,000 financial news articles, with a precision of 78% at the top-100 extractions.The extracted triples are stored in a knowledge graph making them readily available for use in downstream applications.", "venue": "International Conference on Computational Linguistics", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "kg", "ie", "kg", "kg", "kg", "ie", "kg"], "mention_counts": {"ke": 1, "kg": 5, "ie": 3}, "nlp_mention_counts": {"ke": 1, "ie": 3}, "ld_mention_counts": {"ke": 1, "kg": 5}, "relevance_score": 0.7620593014579468}, {"paperId": "620e797f4bd23f2343a240f6c7e84a78be39bdd1", "url": "https://www.semanticscholar.org/paper/620e797f4bd23f2343a240f6c7e84a78be39bdd1", "title": "The PISAB Question Answering System", "abstract": "The PISAB Question Answering system is based on a combination of Information Extraction and Information Retrieval techniques. Knowledge extracted from documents is modeled as a set of entities extracted from text and by relations between them. During the learning phase we index documents using the entities they contain. In the answering phase we exploit the index previously built in order to focus the search for the answer to just the most relevant documents. As answers to a question we select from these documents the paragraphs containing entities most similar to those in the question. PISAB has been submitted to the TREC-9 Conference, achieving encouraging results despite it current prototypical development stage. Introduction The problem of finding answers to questions on a large document collection, could in principle be solved by creating a knowledge base with the information extracted from documents and then querying such knowledge base. Unfortunately this approach is not yet feasible, since it requires advanced techniques of natural language processing, knowledge extraction, knowledge representation and reasoning, which are beyond the current state of the art. On the other hand, Information Retrieval techniques are quite effective in retrieving documents relevant to a certain subject, so in particular those which might contain the answer to a question. Information Extraction techniques help identifying certain kinds of information, but their capabilities are quite domain dependent and limited to entities with predefined patterns. Neither of these techniques is sufficient to address the Question Answering problem, but we have explored a way of combining them to build a complete question answering system. The approach The meaning of a document might be expressed in terms of entities and relations between them. Entities are the semantic equivalent of nouns present in the document, while relations correspond to verbs. For example, the information contained in the following phrase: \u201cJohn reads a book\u201d can be represented by means of the entities \u201cJohn\u201d and \u201cbook\u201d, and by the relation \u201creads\u201d that links subject and object. Relations need not be binary: prepositional phrases and various kinds of syntactic adjuncts allow expressing n-ary relations.", "venue": "TREC", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "nlp", "ke", "ie", "ie", "kg", "ie"], "mention_counts": {"nlp": 1, "kg": 2, "ke": 2, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 2, "ie": 3}, "ld_mention_counts": {"kg": 2, "ke": 2}, "relevance_score": 0.7620593014579468}, {"paperId": "3aa14bd47c6177248e7ebd356436e9d894b202d4", "url": "https://www.semanticscholar.org/paper/3aa14bd47c6177248e7ebd356436e9d894b202d4", "title": "Joint Posterior Revision of NLP Annotations via Ontological Knowledge", "abstract": "Different well-established NLP tasks contribute to elicit the semantics of entities mentioned in natural language text, such as Named Entity Recognition and Classification (NERC) and Entity Linking (EL). However, combining the outcomes of these tasks may result in NLP annotations \u2014 such as a NERC organization linked by EL to a person \u2014 that are unlikely or contradictory when interpreted in the light of common world knowledge about the entities these annotations refer to. We thus propose a general probabilistic model that explicitly captures the relations between multiple NLP annotations for an entity mention, the ontological entity classes implied by those annotations, and the background ontological knowledge those classes may be consistent with. We use the model to estimate the posterior probability of NLP annotations given their confidences (prior probabilities) and the ontological knowledge, and consequently revise the best annotation choice performed by the NLP tools. In a concrete scenario with two stateof-the-art tools for NERC and EL, we experimentally show on three reference datasets that for these tasks, the joint annotation revision performed by the model consistently improves on the original results of the tools.", "venue": "International Joint Conference on Artificial Intelligence", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "onto", "nlp", "onto", "nlp", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 6, "onto": 4}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "9c6048f927762837820ee0c1d1e1b2ebb6c16be8", "url": "https://www.semanticscholar.org/paper/9c6048f927762837820ee0c1d1e1b2ebb6c16be8", "title": "Calculating Word Sense Probability Distributions for Semantic Web Applications", "abstract": "Researchers have found that Word Sense Disambiguation (WSD) is useful for tasks such as ontology alignment. Many other Semantic Web applications could also be enhanced with WSD results of Semantic Web documents. A system that can provide reusable intermediate WSD results is desirable. Compared to the top sense or a rank of senses, an output of meaningful scores of each possible sense informs subsequent processes of the certainty in results, and facilitates the application of other knowledge in choosing the correct sense. We propose that probabilistic models, which have proved successful in many other fields, can also be applied to WSD. Based on such observations, we focus on the problem of calculating probability distributions of senses for terms. In this paper we propose our novel WSD approach with our probability model, derive the problem formula into small computable pieces, and propose ways to estimate the values of these pieces.", "venue": "2010 IEEE Fourth International Conference on Semantic Computing", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "onto", "sw", "sw", "wsd", "wsd", "sw", "wsd", "wsd", "wsd"], "mention_counts": {"sw": 3, "wsd": 6, "onto": 1}, "nlp_mention_counts": {"wsd": 6}, "ld_mention_counts": {"sw": 3, "onto": 1}, "relevance_score": 0.7620593014579468}, {"paperId": "4ecf1f8c380f0e75dbf597b550e8193df368e926", "url": "https://www.semanticscholar.org/paper/4ecf1f8c380f0e75dbf597b550e8193df368e926", "title": "Semantic pattern learning through maximum entropy-based WSD technique", "abstract": "This paper describes a Natural Language Learning method that extracts knowledge in the form of semantic patterns with ontology elements associated to syntactic components in the text. The method combines the use of EuroWordNet\u2019s ontological concepts and the correct sense of each word assigned by a Word Sense Disambiguation(WSD) module to extract three sets of patterns: subject-verb, verb-direct object and verb-indirect object. These sets define the semantic behaviour of the main textual elements based on their syntactic role. On the one hand, it is shown that Maximum Entropy models applied to WSD tasks provide good results. The evaluation of the WSD module has revealed a accuracy rate of 64% in a preliminary test. On the other hand, we explain how an adequate set of semantic or ontological patterns can improve the success rate of NLP tasks such us pronoun resolution. We have implemented both modules in C++ and although the evaluation has been performed for English, their general features allow the treatment of other languages like Spanish. This paper has been partially supported by the Spanish Government (CICYT) project number TIC2000-0664-C0202.", "venue": "CoNLL", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["nll", "onto", "ke", "nlp", "onto", "wsd", "wsd", "wsd", "onto"], "mention_counts": {"onto": 3, "nll": 1, "nlp": 1, "ke": 1, "wsd": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "wsd": 3, "nll": 1}, "ld_mention_counts": {"ke": 1, "onto": 3}, "relevance_score": 0.7620593014579468}, {"paperId": "9b5cea95de709e889eb41572d9e404667e5a330b", "url": "https://www.semanticscholar.org/paper/9b5cea95de709e889eb41572d9e404667e5a330b", "title": "Arabic NLP tools for ontology construction from Arabic text: An overview", "abstract": "Natural Language Processing (NLP) is a technique used to extract data in natural language text that is used in many applications. In the last years, NLP techniques have been adapted for ontology construction and population. For example, Part-of-Speech (POS) tagging, filtering, lexical semantics tagging or tagged named entities used to extract concepts and relationships among entities. The effect of the NLP tools on ontology construction from Arabic text is an area of research. However, to the best of our knowledge, there are no comparative studies that show the effectiveness of these preprocessing techniques. The main goal of this paper is to present a brief review for the existing Arabic NLP tools and assess their capabilities on Arabic ontology construction.", "venue": "2015 International Conference on Electrical and Information Technologies (ICEIT)", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "onto", "nlp", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 6, "onto": 4}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "d25ed54de4aad27e86720d47b5dd7ee3764f3e63", "url": "https://www.semanticscholar.org/paper/d25ed54de4aad27e86720d47b5dd7ee3764f3e63", "title": "Towards Symbiosis in Knowledge Representation and Natural Language Processing for Structuring Clinical Practice Guidelines", "abstract": "The successful adoption by clinicians of evidence-based clinical practice guidelines (CPGs) contained in clinical information systems requires efficient translation of free-text guidelines into computable formats. Natural language processing (NLP) has the potential to improve the efficiency of such translation. However, it is laborious to develop NLP to structure free-text CPGs using existing formal knowledge representations (KR). In response to this challenge, this vision paper discusses the value and feasibility of supporting symbiosis in text-based knowledge acquisition (KA) and KR. We compare two ontologies: (1) an ontology manually created by domain experts for CPG eligibility criteria and (2) an upper-level ontology derived from a semantic pattern-based approach for automatic KA from CPG eligibility criteria text. Then we discuss the strengths and limitations of interweaving KA and NLP for KR purposes and important considerations for achieving the symbiosis of KR and NLP for structuring CPGs to achieve evidence-based clinical practice.", "venue": "Nursing Informatics", "citationCount": 5, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlp", "onto", "nlp", "nlp", "nlp", "onto", "kg", "nlp", "nlp"], "mention_counts": {"nlp": 6, "onto": 3, "kg": 1}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"kg": 1, "onto": 3}, "relevance_score": 0.7620593014579468}, {"paperId": "6a762246fbc4084def7d4745fce341cd5a90dad6", "url": "https://www.semanticscholar.org/paper/6a762246fbc4084def7d4745fce341cd5a90dad6", "title": "Natural Language Generation From Ontologies Using Grammatical Framework", "abstract": "The paper addresses the problem of automatic generation of natural language descriptions for ontology-described artifacts. The motivation for the work is the challenge of providing textual descriptions of automatically generated scientific workflows (e.g., paragraphs that scientists can include in their publications). The extended abstract presents a system which generates descriptions of sets of atoms derived from a collection of ontologies. The system, called nlgPhylogeny, demonstrates the feasibility of the task in the Phylotastic project, that aims at providing evolutionary biologists with a platform for automatic generation of phylogenetic trees given some suitable inputs. nlgPhylogeny utilizes the fact that the Grammatical Framework (GF) is suitable for the natural language generation (NLG) task; the abstract shows how elements of the ontologies in Phylotastic, such as web services, inputs and outputs of web services, can be encoded in GF for the NLG task. 2012 ACM Subject Classification Computing methodologies \u2192 Logic programming and answer set programming, Information systems \u2192 Web services, Computing methodologies \u2192 Natural language generation", "venue": "ICLP", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "onto", "onto", "nlg", "nlg", "nlg", "nlg", "onto", "nlg"], "mention_counts": {"nlg": 6, "onto": 4}, "nlp_mention_counts": {"nlg": 6}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "cfc47e5748bbeb071b2035ba691f3c5ea303a9c9", "url": "https://www.semanticscholar.org/paper/cfc47e5748bbeb071b2035ba691f3c5ea303a9c9", "title": "A syntactic method of extracting terms from special texts for replenishing domain ontologies", "abstract": "Natural Language Processing (NLP) is one of the principal areas of artificial intelligence. It can be argued that the use of ontologies increases the efficiency of natural language processing. However, most ontologies are built manually and require a lot of work. Thus, the problem of automated ontology replenishment is very relevant. One approach is to develop methods for replenishing ontologies using NLP for specific texts of a certain area. We applied the developed method of replenishing the OntoMathPro mathematical ontology, by extracting new terminology from mathematical documents. We developed a method for processing complex syntactic structures (structures with coordination reduction). The method includes certain rule schemata, conditions under which they are to be applied, and conditions determining the sequence of subtrees for which they are to be performed. In our studies, we investigated typical coordination models for mathematical works and performed experiments with a big mathematical collection.", "venue": "2017 Second Russia and Pacific Conference on Computer Technology and Applications (RPC)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "onto", "nlp", "onto", "nlp", "onto", "onto"], "mention_counts": {"nlp": 4, "onto": 6}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.7620593014579468}, {"paperId": "cd349397fe11eae15f5b9fecb8f4ededb99ab112", "url": "https://www.semanticscholar.org/paper/cd349397fe11eae15f5b9fecb8f4ededb99ab112", "title": "A Knowledge-Based Sense Disambiguation Method to Semantically Enhanced NL Question for Restricted Domain", "abstract": "Within the space of question answering (QA) systems, the most critical module to improve overall performance is question analysis processing. Extracting the lexical semantic of a Natural Language (NL) question presents challenges at syntactic and semantic levels for most QA systems. This is due to the difference between the words posed by a user and the terms presently stored in the knowledge bases. Many studies have achieved encouraging results in lexical semantic resolution on the topic of word sense disambiguation (WSD), and several other works consider these challenges in the context of QA applications. Additionally, few scholars have examined the role of WSD in returning potential answers corresponding to particular questions. However, natural language processing (NLP) is still facing several challenges to determine the precise meaning of various ambiguities. Therefore, the motivation of this work is to propose a novel knowledge-based sense disambiguation (KSD) method for resolving the problem of lexical ambiguity associated with questions posed in QA systems. The major contribution is the proposed innovative method, which incorporates multiple knowledge sources. This includes the question\u2019s metadata (date/GPS), context knowledge, and domain ontology into a shallow NLP. The proposed KSD method is developed into a unique tool for a mobile QA application that aims to determine the intended meaning of questions expressed by pilgrims. The experimental results reveal that our method obtained comparable and better accuracy performance than the baselines in the context of the pilgrimage domain.", "venue": "Inf.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "wsd", "kg", "nlp", "nlp", "nlp", "onto", "wsd", "wsd", "kg"], "mention_counts": {"nlp": 3, "kg": 3, "wsd": 3, "onto": 1}, "nlp_mention_counts": {"nlp": 3, "wsd": 3}, "ld_mention_counts": {"kg": 3, "onto": 1}, "relevance_score": 0.7620593014579468}, {"paperId": "364760501bf9f1a8b7e8cbd0a8e01d19c9ae684e", "url": "https://www.semanticscholar.org/paper/364760501bf9f1a8b7e8cbd0a8e01d19c9ae684e", "title": "From hyperlinks to Semantic Web properties using Open Knowledge Extraction", "abstract": "Open information extraction approaches are useful but insufficient alone for populating the Web with machine read- able information as their results are not directly linkable to, and immediately reusable from, other Linked Data sources. This work proposes a novel paradigm, named Open Knowledge Extraction, and its implementation (Legalo) that performs unsuper- vised, open domain, and abstractive knowledge extraction from text for producing machine readable information. The imple- mented method is based on the hypothesis that hyperlinks (either created by humans or knowledge extraction tools) provide a pragmatic trace of semantic relations between two entities, and that such semantic relations, their subjects and objects, can be revealed by processing their linguistic traces (i.e. the sentences that embed the hyperlinks) and formalised as Semantic Web triples and ontology axioms. Experimental evaluations conducted on validated text extracted from Wikipedia pages, with the help of crowdsourcing, confirm this hypothesis showing high performances. A demo is available at http://wit.istc.cnr.it/stlab-tools/ legalo.", "venue": "Semantic Web", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "ke", "ke", "sw", "onto", "ie", "sw", "ke", "ke"], "mention_counts": {"onto": 1, "ld": 1, "ke": 4, "sw": 2, "ie": 1}, "nlp_mention_counts": {"ke": 4, "ie": 1}, "ld_mention_counts": {"ld": 1, "ke": 4, "sw": 2, "onto": 1}, "relevance_score": 0.7607792748995437}, {"paperId": "a7c9a6686d143bb03bbebac37b4deb90b281dea5", "url": "https://www.semanticscholar.org/paper/a7c9a6686d143bb03bbebac37b4deb90b281dea5", "title": "Boosting Document Retrieval with Knowledge Extraction and Linked Data", "abstract": "Given a document collection, Document Retrieval is the task of returning the most relevant documents for a specified user query. In this paper, we assess a document retrieval approach exploiting Linked Open Data and Knowledge Extraction techniques. Based on Natural Language Processing methods (e.g., Entity Linking, Frame Detection), knowledge extraction allows disambiguating the semantic content of queries and documents, linking it to established Linked Open Data resources (e.g., DBpedia, YAGO) from which additional semantic terms (entities, types, frames, temporal information) are imported to realize a semantic-based expansion of queries and documents. The approach, implemented in the KE4IR system, has been evaluated on different state-of-the-art datasets, on a total of 555 queries and with document collections spanning from few hundreds to more than a million of resources. The results show that the expansion with semantic content extracted from queries and documents enables consistently outperforming retrieval performances when only textual information is exploited; on a specific dataset for semantic search, KE4IR outperforms a reference ontology-based search system. The experiments also validate the feasibility of applying knowledge extraction techniques for document retrieval \u2014i.e., processing the document collection, building the expanded index, and searching over it\u2014 on large collections (e.g., TREC WT10g).", "venue": "Semantic Web", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["lod", "ke", "ke", "onto", "lod", "ld", "ke", "ke", "nlp"], "mention_counts": {"onto": 1, "ld": 1, "ke": 4, "lod": 2, "nlp": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 4}, "ld_mention_counts": {"ld": 1, "ke": 4, "lod": 2, "onto": 1}, "relevance_score": 0.7607792748995437}, {"paperId": "bf85ac3b7dcb2901526c749e83bf4542042d50ef", "url": "https://www.semanticscholar.org/paper/bf85ac3b7dcb2901526c749e83bf4542042d50ef", "title": "A hybrid ontology-based information extraction system", "abstract": "Information Extraction is the process of automatically obtaining knowledge from plain text. Because of the ambiguity of written natural language, Information Extraction is a difficult task. Ontology-based Information Extraction (OBIE) reduces this complexity by including contextual information in the form of a domain ontology. The ontology provides guidance to the extraction process by providing concepts and relationships about the domain. However, OBIE systems have not been widely adopted because of the difficulties in deployment and maintenance. The Ontology-based Components for Information Extraction (OBCIE) architecture has been proposed as a form to encourage the adoption of OBIE by promoting reusability through modularity. In this paper, we propose two orthogonal extensions to OBCIE that allow the construction of hybrid OBIE systems with higher extraction accuracy and a new functionality. The first extension utilizes OBCIE modularity to integrate different types of implementation into one extraction system, producing a more accurate extraction. For each concept or relationship in the ontology, we can select the best implementation for extraction, or we can combine both implementations under an ensemble learning schema. The second extension is a novel ontology-based error detection mechanism. Following a heuristic approach, we can identify sentences that are logically inconsistent with the domain ontology. Because the implementation strategy for the extraction of a concept is independent of the functionality of the extraction, we can design a hybrid OBIE system with concepts utilizing different implementation strategies for extracting correct or incorrect sentences. Our evaluation shows that, in the implementation extension, our proposed method is more accurate in terms of correctness and completeness of the extraction. Moreover, our error detection method can identify incorrect statements with a high accuracy.", "venue": "Journal of information science", "citationCount": 26, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "onto", "onto", "onto", "onto", "ie", "ie", "onto", "ie", "onto"], "mention_counts": {"onto": 8, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 8}, "relevance_score": 0.7607792748995437}, {"paperId": "7fcb917cd4b6d0c77c41b4a1b1dc0c4d1965ba7b", "url": "https://www.semanticscholar.org/paper/7fcb917cd4b6d0c77c41b4a1b1dc0c4d1965ba7b", "title": "Comprehensive Event Representations using Event Knowledge Graphs and Natural Language Processing", "abstract": "Recent work has utilised knowledge-aware approaches to natural language understanding, question answering, recommendation systems, and other tasks. These approaches rely on well-constructed and large-scale knowledge graphs that can be useful for many downstream applications and empower knowledge-aware models with commonsense reasoning. Such knowledge graphs are constructed through knowledge acquisition tasks such as relation extraction and knowledge graph completion. This work seeks to utilise and build on the growing body of work that uses findings from the field of natural language processing (NLP) to extract knowledge from text and build knowledge graphs. The focus of this research project is on how we can use transformer-based approaches to extract and contextualise event information, matching it to existing ontologies, to build comprehensive knowledge graph-based event representations. Specifically, sub-event extraction is used as a way of creating sub-event-aware event representations. These event representations are then further enriched through fine-grained location extraction and contextualised through the alignment of historically relevant quotes.", "venue": "WWW", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlu", "nlp", "kg", "kg", "kg", "onto", "kg", "ke", "kg", "kg", "nlp"], "mention_counts": {"onto": 1, "nlu": 1, "ke": 1, "nlp": 3, "kg": 6}, "nlp_mention_counts": {"nlp": 3, "ke": 1, "nlu": 1}, "ld_mention_counts": {"kg": 6, "onto": 1, "ke": 1}, "relevance_score": 0.7607792748995437}, {"paperId": "ba1bc10e15ed312ec30e3ff576e227961ee385ac", "url": "https://www.semanticscholar.org/paper/ba1bc10e15ed312ec30e3ff576e227961ee385ac", "title": "Corpus-based Ontology Learning for Word Sense Disambiguation", "abstract": "This paper proposes to disambiguate word senses by corpus-based ontology learning. Our approach is a hybrid method. First, we apply the previously-secured dictionary information to select the correct senses of some ambiguous words with high precision, and then use the ontology to disambiguate the remaining ambiguous words. The mutual information between concepts in the ontology was calculated before using the ontology as knowledge for disambiguating word senses. If mutual information is regarded as a weight between ontology concepts, the ontology can be treated as a graph with weighted edges, and then we locate the least weighted path from one concept to the other concept. In our practical machine translation system, our word sense disambiguation method achieved a 9% improvement over methods which do not use ontology for Korean translation.", "venue": "PACLIC", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "wsd", "onto", "onto", "wsd", "onto", "onto", "onto", "wsd", "onto", "wsd", "onto"], "mention_counts": {"wsd": 4, "onto": 8, "mt": 1}, "nlp_mention_counts": {"wsd": 4, "mt": 1}, "ld_mention_counts": {"onto": 8}, "relevance_score": 0.7607792748995437}, {"paperId": "edfb109298a04bfac6a1617be291fb44de3b0f0b", "url": "https://www.semanticscholar.org/paper/edfb109298a04bfac6a1617be291fb44de3b0f0b", "title": "Exploiting Ontology Lexica for Generating Natural Language Texts from RDF Data", "abstract": "The increasing amount of machinereadable data available in the context of the Semantic Web creates a need for methods that transform such data into human-comprehensible text. In this paper we develop and evaluate a Natural Language Generation (NLG) system that converts RDF data into natural language text based on an ontology and an associated ontology lexicon. While it follows a classical NLG pipeline, it diverges from most current NLG systems in that it exploits an ontology lexicon in order to capture context-specific lexicalisations of ontology concepts, and combines the use of such a lexicon with the choice of lexical items and syntactic structures based on statistical information extracted from a domain-specific corpus. We apply the developed approach to the cooking domain, providing both an ontology and an ontology lexicon in lemon format. Finally, we evaluate fluency and adequacy of the generated recipes with respect to two target audiences: cooking novices and advanced cooks.", "venue": "European Workshop on Natural Language Generation", "citationCount": 46, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "onto", "nlg", "rdf", "nlg", "nlg", "onto", "ie", "rdf", "nlg", "onto", "onto", "onto", "onto", "sw"], "mention_counts": {"onto": 7, "sw": 1, "nlg": 5, "rdf": 2, "ie": 1}, "nlp_mention_counts": {"nlg": 5, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 7, "rdf": 2}, "relevance_score": 0.748145532632524}, {"paperId": "b3a1323ae79be250758df20fd3308f3adf9c9e5e", "url": "https://www.semanticscholar.org/paper/b3a1323ae79be250758df20fd3308f3adf9c9e5e", "title": "Translating XBRL Into Description Logic. An Approach Using Protege, Sesame & OWL", "abstract": "In the context of the eTen project, WINS, a web-based business intelligence service to public and private financial institutions has been designed and implemented. One of the goals of the project was to provide new financial knowledge on companies from information gathered through interoperable information services. The services were implemented under the new emerging standard XBRL used for financial reporting. We sketch how relevant financial information was extracted from annual financial reportings. We also show at the same time the limitations we encountererd with the XBRL schema, due to the lack of reasoning support over XML-based data and information extracted from documents. To overcome these difficulties, we describe the \u201contologization\u201d of XBRL, which we assume to be a necessary requisite for large intelligent web-based financial information and decision support systems. 1 General Background In the context of the eTen project, WINS, a web-based business intelligence service to public and private financial institutions has been designed and implemented. One of the goals of the project was to provide new financial knowledge on companies from information gathered through interoperable information services [1]. The services were implemented under the new emerging standard called XBRL (eXtensible Business Reporting Language) used for financial reporting. In the following, we sketch how relevant financial information was extracted from annual financial reportings. We also show at the same time the limitations we encountererd with the XBRL schema, due to the lack of reasoning support over XML-based data and information extracted from documents that is finally mapped onto XBRL instances. In the first part of our submission, we just summarize our way of information extraction guided by XBRL, and afterwards describing our work dedicated to the ontologization of XBRL, which we assume to be a necessary requisite for large intelligent web-based financial information and decision support systems. The work described here will be further carried out within an Integrated Project of the 6th Framework, called MUSING (MUlti-Industry, Semantic-based next generation business Intelligence). 1 ETEN 2003/1, Grant agreement nr. C51083. WINS stands for Web-based Intelligence for common-interest fiscal Networked Services\u2019. 2 For more information, see XBRL International: http://www.xbrl.org. 2 Incremental Information Extraction Guided by XBRL The next three subsections present the basic setting, viz., XBRL-guided information extraction of structured and unstructured documents. 2.1 Knowledge-Driven Information Extraction from Structured and Unstructured Documents The actual input for the information extraction (IE) task in WINS consists of balance sheets in PDF format, containing structured forms (tables) and free text (included, for example, in the annexes of balance sheets). Relevant information extracted from these sources are merged and mapped onto the XBRL format. A terminological clarification should be given at this place. IE often refers to the task of filling useror application-defined templates with the result of information detected by natural language analysis tools in textual documents. For certain applications, knowledge bases are available, supporting the IE task. Such knowledge might consist of taxonomies, thesauri, or ontologies. In this case, knowledge-driven IE tries to populate knowledge bases with instances detected in the textual documents. This was the situation in WINS, where the XBRL taxonomy was guiding the IE task through the analysis of both tabular data and free text. In the end, an XBRL structure should be instantiated with the information extracted from the annual reports of companies. 2.2 The Mapping Process from Text to XBRL The mapping process has been implemented within a Web service made available to the WINS partners. The Web service operates on PDF/text files from WINS data providers and returns files, containing the data in XBRL format. In a first step, text and tables from the PDF documents were extracted. It was also necessary to apporimatively recontructs the original layout, which is getting lost in the PDF-to-text conversion. Once this has been done, the WINS information extraction module inspects the generated HTML documents, trying to find correspondences in the text of the tables for labels of concepts contained in the overall XBRL taxonomy. But not only the detection of realizations of XBRL concepts in the document is important. The extraction tools must also detect relevant dates in the tables as well as currencies used, so that the figures contained in the tables , e.g., balance and profit & loss (P&L) tables, are getting their correct interpretation. Since the XBRL taxonomy is also considering information about a company as such (name, address, number of employees, etc), the extraction tools need to detect this kind of information. In order to obtain such information, we implemented a simple named entity recognition algorithm for detecting names of companies, locations, and relevant persons. 456 BUSINESS INFORMATION SYSTEMS BIS 2006", "venue": "Business Information Systems", "citationCount": 32, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto", "onto", "ie", "kg", "ie", "ie", "ie", "ie", "kg", "ie", "ie", "onto", "ie"], "mention_counts": {"kg": 2, "onto": 4, "ie": 10}, "nlp_mention_counts": {"ie": 10}, "ld_mention_counts": {"kg": 2, "onto": 4}, "relevance_score": 0.748145532632524}, {"paperId": "97b45b01e294b8c3ea617d82e0c0c545a66bce6b", "url": "https://www.semanticscholar.org/paper/97b45b01e294b8c3ea617d82e0c0c545a66bce6b", "title": "Ontological semantics, formal ontology, and ambiguity", "abstract": "Ontological semantics is a theory of meaning in natural languageand an approach to natural language processing (NLP) which uses anontology as the central resource for extracting and representingmeaning of natural language texts, reasoning about knowledgederived from texts as well as generating natural language textsbased on representations of their meaning. Ontological semanticsdirectly supports such applications as machine translation ofnatural languages, information extraction, text summarization,question answering, advice giving, collaborative work of networksof human and software agents, etc. Ontological semantics paysserious attention to its theoretical foundations by explicating itspremises; therefore, formal ontology and its relations withontological semantics are important. Besides a general briefdiscussion of these relations, the paper focuses on the importanttheoretical and practical issue of the distinction between ontologyand natural language. It is argued that this crucial distinctionlies not in the (inaccurately) presumed nonambiguity of the one andthe well-established ambiguity of the other but rather in theconstructed and overtly defined nature of ontological concepts andlabels on which no human background knowledge can operateunintentionally to introduce ambiguity, as opposed to pervasiveuncontrolled and uncontrollable ambiguity in natural language. Theemphasis on this distinction, we argue, will provide bettertheoretical support for the central tenets of formal ontology byfreeing it from the Wittgensteinian and Rortyan retreats from theanalytical paradigm; it also reinforces the methodology of NLP bymaintaining a productive demarcation between thelanguage-independent nature of ontology and language-specificnature of the lexicons, a demarcation that has paid off well inconsecutive implementations of ontological semantics and theirapplications in practical computer systems.", "venue": "Formal Ontology in Information Systems", "citationCount": 52, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlg", "onto", "nlp", "onto", "onto", "nlp", "onto", "ie", "onto", "onto", "onto", "mt", "onto", "onto", "onto"], "mention_counts": {"onto": 10, "nlp": 3, "nlg": 1, "mt": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "nlg": 1, "mt": 1, "ie": 1}, "ld_mention_counts": {"onto": 10}, "relevance_score": 0.748145532632524}, {"paperId": "90f5e40f9706e031ba57163245b593419ab4f754", "url": "https://www.semanticscholar.org/paper/90f5e40f9706e031ba57163245b593419ab4f754", "title": "Reconciling Heterogeneous Descriptions of Language Resources", "abstract": "Language resources are a cornerstone of linguistic research and for the development of natural language processing tools, but the discovery of relevant resources remains a challenging task. This is due to the fact that relevant metadata records are spread among different repositories and it is currently impossible to query all these repositories in an integrated fashion, as they use different data models and vocabularies. In this paper we present a first attempt to collect and harmonize the metadata of different repositories, thus making them queriable and browsable in an integrated way. We make use of RDF and linked data technologies for this and provide a first level of harmonization of the vocabularies used in the different resources by mapping them to standard RDF vocabularies including Dublin Core and DCAT. Further, we present an approach that relies on NLP and in particular word sense disambiguation techniques to harmonize resources by mapping values of attributes \u2010 such as the type, license or intended use of a resource \u2010 into normalized values. Finally, as there are duplicate entries within the same repository as well as across different repositories, we also report results of detection of these duplicates.", "venue": "LDL@IJCNLP", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "rdf", "ld", "rdf", "nlp", "wsd"], "mention_counts": {"ld": 1, "nlp": 2, "wsd": 1, "rdf": 2}, "nlp_mention_counts": {"nlp": 2, "wsd": 1}, "ld_mention_counts": {"ld": 1, "rdf": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "b618f990830123fb87e033d0b7e18a040d5b55c4", "url": "https://www.semanticscholar.org/paper/b618f990830123fb87e033d0b7e18a040d5b55c4", "title": "Refining Search Performance through Semantic based CBR Model and QoS Ranking Methodology", "abstract": "Objectives:\u00a0To refine search performance using semantic web with an improved algorithm to retrieve the information efficiently.\u00a0Methods:\u00a0In order to establish the SCBR model and improve the performance of Web search, this paper adopts the Natural Language Processing (NLP) technology and the Quality of Service (QoS) ranking method, and endeavors to develop a relevant reliable and efficient search engine.\u00a0Findings:\u00a0Mean average precision tests revealed for quickness and precious of search results, and achieves the values from 82.98% to 99.53%. The experimental results show that the NLP technique improves the performance of SCBR model, and achieves higher average precision and recall values.\u00a0Novelty:\u00a0This research focuses to develop a related reliable and an efficient search engine to retrieve the accurate results for the user\u2019s complex query. It even bears the human error in typing, and suggests the expected word to search for. It also aims to retrieving the same result for synonym words which prevent the appearance of irrelevant search results. \n \nKeywords \n\u00ad Semantic Web, Information Retrieval, SCBR Model, Ontology, Semantic Search, Quality of Service", "venue": "Indian Journal of Science and Technology", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlp", "nlp", "onto", "sw", "nlp"], "mention_counts": {"nlp": 3, "sw": 2, "onto": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "ffba67307a1b002859f6b87739ba91d8c39804ae", "url": "https://www.semanticscholar.org/paper/ffba67307a1b002859f6b87739ba91d8c39804ae", "title": "Extending a model for ontology-based Arabic-English machine translation", "abstract": "The acceleration in telecommunication needs leads to many groups of research, especially in communication facilitating and Machine Translation fields. While people contact with others having different languages and cultures, they need to have instant translations. However, the available instant translators are still providing somewhat bad Arabic-English Translations, for instance when translating books or articles, the meaning is not totally accurate. Therefore, using the semantic web techniques to deal with the homographs and homonyms semantically, the aim of this research is to extend a model for the ontology-based Arabic-English Machine Translation, named NAN, which simulate the human way in translation. The experimental results show that NAN translation is approximately more similar to the Human Translation than the other instant translators. The resulted translation will help getting the translated texts in the target language somewhat correctly and semantically more similar to human translations for the Non-Arabic Natives and the Non-English natives.", "venue": "International Journal of Artificial Intelligence & Applications", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "sw", "onto", "mt", "mt"], "mention_counts": {"sw": 1, "onto": 2, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "384152f62d0db5995b90ad441c435c4abce8048b", "url": "https://www.semanticscholar.org/paper/384152f62d0db5995b90ad441c435c4abce8048b", "title": "Semantic text mining for lignocellulose research", "abstract": "Semantic technologies, including natural language processing (NLP), ontologies, semantic web services and web-based collaboration tools, promise to support users in dealing with complex data, thereby facilitating knowledge-intensive tasks. An ongoing challenge is to select the appropriate technologies and combine them in a coherent system that brings measurable improvements to the users. We present our ongoing development of a semantic infrastructure in support of genomics-based lignocellulose research. Part of this effort is the automated curation of knowledge from information on enzymes from fungi that is available in the literature and genome resources. Fungi naturally break down lignocellulose, hence the identification and characterization of the enzymes that they use in lignocellulose hydrolysis is an important part in research and development of biomass-derived products and fuels. Working close to the biology researchers who manually curate the existing literature, we developed ontological NLP pipelines integrated in a Web-based interface to help them in two main tasks: mining the literature for relevant information, and at the same time providing rich and semantically linked information.", "venue": "DTMBIO '11", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "nlp", "sw"], "mention_counts": {"nlp": 3, "sw": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "cc56e8b52b314f77bc3e85fffaf70559197453dc", "url": "https://www.semanticscholar.org/paper/cc56e8b52b314f77bc3e85fffaf70559197453dc", "title": "Building Multilingual Lexical Resources on Semiotic Principles", "abstract": "Multilinguality permeates the web so that multilingual resources are fundamental in several NLP applications as cross language information retrieval as well as machine translation. Nonetheless the manual creation of such resources is very expensive. Semantic Web technologies can represent a great enhancement for NLP applications. In this paper, we show how Semantic Web technologies as an upper ontology based on well-founded semiotic theories can be applied to build multilingual lexical resources as Machine Readable Dictionaries (MRDs).", "venue": "KEOD", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "onto", "mt", "nlp", "sw", "nlp"], "mention_counts": {"nlp": 2, "sw": 2, "onto": 1, "mt": 1}, "nlp_mention_counts": {"nlp": 2, "mt": 1}, "ld_mention_counts": {"sw": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "89756bfc1dfec78c00c043e41006f731b14265a7", "url": "https://www.semanticscholar.org/paper/89756bfc1dfec78c00c043e41006f731b14265a7", "title": "Nature: A Tool Resulting from the Union of Artificial Intelligence and Natural Language Processing for Searching Research Projects in Colombia", "abstract": "This paper presents the final results of the research project that aimed for the construction of a tool which is aided by Artificial Intelligence through an Ontology with a model trained with Machine Learning, and is aided by Natural Language Processing to support the semantic search of research projects of the Research System of the University of Nari\u00f1o. For the construction of NATURE, as this tool is called, a methodology was used that includes the following stages: appropriation of knowledge, installation and configuration of tools, libraries and technologies, collection, extraction and preparation of research projects, design and development of the tool. The main results of the work were three: a) the complete construction of the Ontology with classes, object properties (predicates), data properties (attributes) and individuals (instances) in Proteg\u00e9, SPARQL queries with Apache Jena Fuseki and the respective coding with Owlready2 using Jupyter Notebook with Python within the virtual environment of anaconda; b) the successful training of the model for which Machine Learning algorithms were used and specifically Natural Language Processing algorithms such as: SpaCy, NLTK, Word2vec and Doc2vec, this was also performed in Jupyter Notebook with Python within the virtual environment of anaconda and with Elasticsearch; and c) the creation of NATURE by managing and unifying the queries for the Ontology and for the Machine Learning model. The tests showed that NATURE was successful in all the searches that were performed as its results were satisfactory.", "venue": "International Journal of Artificial Intelligence & Applications", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "23976900d27bddcdbfbbba4ba5bea11cfe51bbbf", "url": "https://www.semanticscholar.org/paper/23976900d27bddcdbfbbba4ba5bea11cfe51bbbf", "title": "An Application of the Semantic Web Inspired by Human Learning and Natural Language Processing", "abstract": "The prototype in this paper presents a semantic web application that is inspired from psychology experiments on human learning and natural language processing. It aims at improving the proficiency of present search engines when dealing with specific queries (question-answering). The prototype makes use of the idea that the world wide web itself contains an enormous number of documents written in natural language (appearing in formats such as .html, .xml, .cfm, .pdf, .net, .asp etc.). It is consistently trained to improve its language proficiency by extracting knowledge from these documents and by storing redundant information in a database (i.e. information dealing with the same concept but expressed in different words).", "venue": "Int. J. Emerg. Technol. Learn.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "sw", "nlp", "nlp", "sw"], "mention_counts": {"nlp": 2, "ke": 1, "sw": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"ke": 1, "sw": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "073385657675a186ff6386f8520c490121ed1bc6", "url": "https://www.semanticscholar.org/paper/073385657675a186ff6386f8520c490121ed1bc6", "title": "World Wide Web in the Service of Schooling: Semantic Web as a Solution for Language Teaching in Cypriot Secondary Education", "abstract": "This paper examines some suitability aspects of existing web search engines in relation to the content and the stated learning objectives of language teaching in Cypriot secondary education, focusing on the language course of the third high school grade (G9). The end goal is to put the internet in the service of schooling; specifically to categorize the results returned by the search engine in- to genres in order to facilitate user (teacher or student) in choosing the most ap- propriate texts for their learning purposes. The tools for categorizing texts are being sought in the field of Semantic Web technology, such as metadata, ontol- ogies, software agents, and, the techniques in the fields of Natural Language Processing (NLP), Information Retrieval (IR), Information Extraction (IE) and Text Mining. The paper proposes the categorization of texts into six major ge- nre categories according to their external (structural) and internal (linguistic, stylistic) characteristics. For the purpose of this research, the MeDa13 metadata model was designed on the basis of the standard metadata model Dublin Core, and the Textual Genres Ontology (TeGO) was developed for describing the concepts mentioned in genres. In this work, we present the theoretical back- ground for the development of the proposed models (MeDa13 and TeGO), and also the methodological plan to achieve the research objective, which is the ca- tegorization of texts into genres considering the content and learning objectives for language teaching.", "venue": "EC-TEL Doctoral Consortium", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "onto", "nlp", "sw", "ie"], "mention_counts": {"nlp": 2, "sw": 2, "onto": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"sw": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "a4ffca7760507690e9f05758899aebfd7e96075a", "url": "https://www.semanticscholar.org/paper/a4ffca7760507690e9f05758899aebfd7e96075a", "title": "Linguistic Linked Open Data (LLOD). Introduction and Overview", "abstract": "The explosion of information technology has led to a substantial growth in quantity, diversity and complexity of linguistic data accessible over the internet. The lack of interoperability between linguistic and language resources represents a major challenge that needs to be addressed, in particular, if information from different sources is to be combined, like, say, machine-readable lexicons, corpus data and terminology repositories. For these types of resources, domainspecific standards have been proposed, yet, issues of interoperability between different types of resources persist, commonly accepted strategies to distribute, access and integrate their information have yet to be established, and technologies and infrastructures to address both aspects are still under development. The goal of the 2nd Workshop on Linked Data in Linguistics (LDL-2013) has been to bring together researchers from various fields of linguistics, natural language processing, and information technology to present and discuss principles, case studies, and best practices for representing, publishing and linking linguistic data collections, including corpora, dictionaries, lexical networks, translation memories, thesauri, etc., infrastructures developed on that basis, their use of existing standards, and the publication and distribution policies that were adopted. Background: Integrating Information from Different Sources In recent years, the limited interoperability between linguistic resources has been recognized as a major obstacle for data use and re-use within and across discipline boundaries. After half a century of computational linguistics [8], quantitative typology [12], empirical, corpus-based study of language [10], and computational lexicography [16], researchers in computational linguistics, natural language processing (NLP) or information technology, as well as in Digital Humanities, are confronted with an immense wealth of linguistic resources, that are not only growing in number, but also in their heterogeneity. Interoperability involves two aspects [14]: Structural (\u2018syntactic\u2019) interoperability: Resources use comparable formalisms to represent and to access data (formats, protocols, query languages, etc.),", "venue": "Workshop on Linked Data in Linguistics", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["llod", "nlp", "nlp", "nlp", "llod", "lod"], "mention_counts": {"nlp": 3, "llod": 2, "lod": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"llod": 2, "lod": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "1834fd2594feed8eeaccd7fc0698c8de7ab42ee3", "url": "https://www.semanticscholar.org/paper/1834fd2594feed8eeaccd7fc0698c8de7ab42ee3", "title": "Adaptive information extraction for document annotation in amilcare", "abstract": "Amilcare is a tool for Adaptive Information Extraction (IE) designed for supporting active annotation of documents for the Semantic Web (SW). It can be used either for unsupervised document annotation or as a support for human annotation. Amilcare is portable to new applications/domains without any knowledge of IE, as it just requires users to annotate a small training corpus with the information to be extracted. It is based on (LP)2, a supervised learning strategy for IE able to cope with different texts types, from newspaper-like texts, to rigidly formatted Web pages and even a mixture of them[1][5].Adaptation starts with the definition of a tag set for annotation, possibly organized as an ontology. Then users have to manually annotate a small training corpus. Amilcare provides a default mouse-based interface called Melita, where annotations are inserted by first selecting a tag from the ontology and then identifying the text area to annotate with the mouse. Differently from similar annotation tools [4, 5], Melita actively supports training corpus annotation. While users annotate texts, Amilcare runs in the background learning how to reproduce the inserted annotation. Induced rules are silently applied to new texts and their results are compared with the user annotation. When its rules reach a (user-defined) level of accuracy, Melita presents new texts with a preliminary annotation derived by the rule application. In this case users have just to correct mistakes and add missing annotations. User corrections are inputted back to the learner for retraining. This technique focuses the slow and expensive user activity on uncovered cases, avoiding requiring annotating cases where a satisfying effectiveness is already reached. Moreover validating extracted information is a much simpler task than tagging bare texts (and also less error prone), speeding up the process considerably. At the end of the corpus annotation process, the system is trained and the application can be delivered. MnM [6] and Ontomat annotizer [7] are two annotation tools adopting Amilcare's learner.In this demo we simulate the annotation of a small corpus and we show how and when Amilcare is able to support users in the annotation process, focusing on the way the user can control the tool's proactivity and intrusivity. We will also quantify such support with data derived from a number of experiments on corpora. We will focus on training corpus size and correctness of suggestions when the corpus is increased.", "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "sw", "onto", "ie", "onto", "ie"], "mention_counts": {"sw": 1, "onto": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "09510e0761bce39ea0b23804347ef8c4ee967b66", "url": "https://www.semanticscholar.org/paper/09510e0761bce39ea0b23804347ef8c4ee967b66", "title": "Machine Learning-Based Algorithms to Knowledge Extraction from Time Series Data: A Review", "abstract": "To predict the future behavior of a system, we can exploit the information collected in the past, trying to identify recurring structures in what happened to predict what could happen, if the same structures repeat themselves in the future as well. A time series represents a time sequence of numerical values observed in the past at a measurable variable. The values are sampled at equidistant time intervals, according to an appropriate granular frequency, such as the day, week, or month, and measured according to physical units of measurement. In machine learning-based algorithms, the information underlying the knowledge is extracted from the data themselves, which are explored and analyzed in search of recurring patterns or to discover hidden causal associations or relationships. The prediction model extracts knowledge through an inductive process: the input is the data and, possibly, a first example of the expected output, the machine will then learn the algorithm to follow to obtain the same result. This paper reviews the most recent work that has used machine learning-based techniques to extract knowledge from time series data.", "venue": "International Conference on Data Technologies and Applications", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "aa2889ca477809de939ad7b6dd2cb96b1825a829", "url": "https://www.semanticscholar.org/paper/aa2889ca477809de939ad7b6dd2cb96b1825a829", "title": "Knowledge of extraction from trained neural network by using decision tree", "abstract": "Inside the sets of data, hidden knowledge can be acquired by using neural network. These knowledge are described within topology, using activation function and connection weight at hidden neurons and output neurons. Is hardly to be understanding since neural networks act as a black box. The black box problem can be solved by extracting knowledge (rule) from trained neural network. Thus, the aim of this paper is to extract valuable information from trained neural networks using decision. Further, the Levenberg Marquardt algorithm was applied to training 30 networks for each datasets, using learning parameters and basis weights differences. As the number of hidden neurons increase, mean squared error and mean absolute percentage error decrease, and more time they need to deal with the dataset, that is result of investigation from neural network architectures. Decision tree induction generally performs better in knowledge extraction result with accuracy and precision level from 84.07 to 93.17 percent. The extracted rule can be used to explaining the process of the neural network systems and also can be applied in other systems like expert systems.", "venue": "International Conference on Science in Information Technology", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "29b559b912543d191c3ef9871ac18a857664a341", "url": "https://www.semanticscholar.org/paper/29b559b912543d191c3ef9871ac18a857664a341", "title": "Vulnerability knowledge extraction method based on joint extraction model", "abstract": "Information extraction is an important semantic processing task to construct network security knowledge graph. Extracting entities and relationships in vulnerability description from public data sets will inevitably lead to waste of manpower and difficulty in accurate positioning. Another challenge is that there are multiple relationships among vulnerable descriptors. This paper proposes a framework for the common vulnerabilities and exposures (CVE) analysis, which consists of entity annotation algorithm and relational classification model. In particular, we apply the model to CVE dataset to solve the problem of information extraction and relationship classification in the CVE vulnerability analysis. Moreover, the predicted relationship is used to construct vulnerability security knowledge graph. The experimental results show that the framework can deal with the CVE vulnerability description effectively, and has good relationship classification performance.", "venue": "2021 Ninth International Conference on Advanced Cloud and Big Data (CBD)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ie", "ie", "kg"], "mention_counts": {"ke": 1, "kg": 2, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "kg": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "1dc2cb77823d391954314aa67cb5603d72e5686a", "url": "https://www.semanticscholar.org/paper/1dc2cb77823d391954314aa67cb5603d72e5686a", "title": "Challenges and Opportunities for Journalistic Knowledge Platforms", "abstract": "Journalism is under pressure from loss of advertisement and revenues, while experiencing an increase in digital consumption and user demands for quality journalism and trusted sources. Journalistic Knowledge Platforms (JKPs) are an emerging generation of platforms which combine state-of-the-art artificial intelligence (AI) techniques such as knowledge graphs, linked open data (LOD), and natural-language processing (NLP) for transforming newsrooms and leveraging information technologies to increase the quality and lower the cost of news production. In order to drive research and design better JKPs that allow journalists to get most benefits out of them, we need to understand what challenges and opportunities JKPs are facing. This paper presents an overview of the main challenges and opportunities involved in JKPs which have been manually extracted from literature with the support of natural language processing and understanding techniques. These challenges and opportunities are organised in: stakeholders, information, functionalities, components, techniques and other aspects.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "lod", "nlp", "lod", "kg"], "mention_counts": {"nlp": 3, "lod": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 1, "lod": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "61645daf1c3a20bbe5570efb30f43b1645b26b33", "url": "https://www.semanticscholar.org/paper/61645daf1c3a20bbe5570efb30f43b1645b26b33", "title": "An Information Extraction Method for Digitized Textbooks of Traditional Chinese Medicine", "abstract": "Digital libraries have shouldered the mission of preserving and spreading human culture in the era of information. However, knowledge extraction for digital libraries is not well studied, and that holds back the role promotion of digital libraries from information collector to knowledge provider. This paper presents an ontology-based approach, which extracts detailed attributes of Traditional Chinese Medicine (TCM) from digitized textbooks. According to the characters of digitized textbooks, we propose an extraction ontology that is compatible with both textbook extraction and TCM theory. To improve extraction tolerance for OCR errors, we extract features of different aspects. Finally, a structured pattern based extraction method is adopted to minimize extraction supervision. The result shows that our method is a practical and robust exploration to address the problem of information extraction for digitized textbooks of TCM.", "venue": "2010 10th IEEE International Conference on Computer and Information Technology", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ke", "ie", "onto"], "mention_counts": {"ke": 1, "onto": 2, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "e0abebbcc12baaf5d84e0c66c8731347c2c52ed7", "url": "https://www.semanticscholar.org/paper/e0abebbcc12baaf5d84e0c66c8731347c2c52ed7", "title": "Knowledge Extraction and Extrapolation Using Ancient and Modern Biomedical Literature", "abstract": "Extraction of knowledge from biomedical literature is one of the major problems for researchers. This primarily involves identification of novel associations between biological objects (genes, proteins, diseases, medicines etc.). These associations are commonly extracted by mining biomedical resources such as the PUBMED which contains a large volume of information. An automated approach towards this end will reduce a substantial amount of time for biomedical researchers. In this paper we discuss a methodology to extract such associations and to assign a significance measure to the generated hypotheses. The computed significance value for the extracted knowledge can be considered as association strength between biological objects. The generated hypotheses with large significance can be considered for further experimental validation by biologists. In this paper we conduct two different validation studies of the results, which provide justification for the approach that was followed to generate the hypotheses.", "venue": "2009 International Conference on Advanced Information Networking and Applications Workshops", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "d0947127e4d5bb20c7ae4c965aa289958a399575", "url": "https://www.semanticscholar.org/paper/d0947127e4d5bb20c7ae4c965aa289958a399575", "title": "Precise tweet classification and sentiment analysis", "abstract": "The rise of social media in couple of years has changed the general perspective of networking, socialization, and personalization. Use of data from social networks for different purposes, such as election prediction, sentimental analysis, marketing, communication, business, and education, is increasing day by day. Precise extraction of valuable information from short text messages posted on social media (Twitter) is a collaborative task. In this paper, we analyze tweets to classify data and sentiments from Twitter more precisely. The information from tweets are extracted using keyword based knowledge extraction. Moreover, the extracted knowledge is further enhanced using domain specific seed based enrichment technique. The proposed methodology facilitates the extraction of keywords, entities, synonyms, and parts of speech from tweets which are then used for tweets classification and sentimental analysis. The proposed system is tested on a collection of 40,000 tweets. The proposed methodology has performed better than the existing system in terms of tweets classification and sentiment analysis. By applying the Knowledge Enhancer and Synonym Binder module on the extracted information we have achieved increase in information gain in a range of 0.1% to 55%. The increase in information gain has enabled our proposed system to better summarize the twitter data for user sentiments regarding a keyword from a particular category.", "venue": "International Conference on Interaction Sciences", "citationCount": 94, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "663e7fb1582fe3622863f5fc2234bc876bbb9ece", "url": "https://www.semanticscholar.org/paper/663e7fb1582fe3622863f5fc2234bc876bbb9ece", "title": "Ontology supported information extraction for document of evidence-based nursing domain", "abstract": "The increasing amount of information available in electronic media is a strategic resource for the health professional who uses this source to support decision making. However, it is not feasible to analyze a significant number of documents in a short time without the support of a computational tool. The Information Extraction aims to extract from textual documents only the relevant information defined by the user. This information can be mapped and classified in the field of health through ontologies. This paper proposes the design of an information extracting mechanism that, in a hybrid form, can combine: an extractor for textual documents; the construction and use of a domain ontology for evidence-based nursing, aiming to assist in the classification of documents which will be extracted by the extractor and in the representation of this domain with respect to the computational area.", "venue": "2014 9th Iberian Conference on Information Systems and Technologies (CISTI)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "ie"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "41350b24e0766b40a90fede59adec1531340dea9", "url": "https://www.semanticscholar.org/paper/41350b24e0766b40a90fede59adec1531340dea9", "title": "Fine-grained Information Extraction from Biomedical Literature based on Knowledge-enriched Abstract Meaning Representation", "abstract": "Biomedical Information Extraction from scientific literature presents two unique and non-trivial challenges. First, compared with general natural language texts, sentences from scientific papers usually possess wider contexts between knowledge elements. Moreover, comprehending the fine-grained scientific entities and events urgently requires domain-specific background knowledge. In this paper, we propose a novel biomedical Information Extraction (IE) model to tackle these two challenges and extract scientific entities and events from English research papers. We perform Abstract Meaning Representation (AMR) to compress the wide context to uncover a clear semantic structure for each complex sentence. Besides, we construct the sentence-level knowledge graph from an external knowledge base and use it to enrich the AMR graph to improve the model\u2019s understanding of complex scientific concepts. We use an edge-conditioned graph attention network to encode the knowledge-enriched AMR graph for biomedical IE tasks. Experiments on the GENIA 2011 dataset show that the AMR and external knowledge have contributed 1.8% and 3.0% absolute F-score gains respectively. In order to evaluate the impact of our approach on real-world problems that involve topic-specific fine-grained knowledge elements, we have also created a new ontology and annotated corpus for entity and event extraction for the COVID-19 scientific literature, which can serve as a new benchmark for the biomedical IE community.", "venue": "Annual Meeting of the Association for Computational Linguistics", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "ie", "kg", "kg"], "mention_counts": {"kg": 2, "onto": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "4c70eb2ef48ec1aae22546e202e52bc0d1c820e2", "url": "https://www.semanticscholar.org/paper/4c70eb2ef48ec1aae22546e202e52bc0d1c820e2", "title": "Ontology mediated information extraction in financial domain with Mastro System-T", "abstract": "Information extraction (IE) refers to the task of turning text documents into a structured form, in order to make the information contained therein automatically processable. Ontology Mediated Information Extraction (OMIE) is a new paradigm for IE that seeks to exploit the semantic knowledge expressed in ontologies to improve query answering over unstructured data (properly raw text). In this paper we present Mastro System-T, an OMIE tool born from a joint collaboration between the University of Rome \"La Sapienza\" and IBM Research Almaden and its first application in a financial domain, namely to facilitate the access to and the sharing of data extracted from the EDGAR system.", "venue": "DSMM@SIGMOD", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "986ae00cc06f76e773c438a54e6d93c4f4f72a1c", "url": "https://www.semanticscholar.org/paper/986ae00cc06f76e773c438a54e6d93c4f4f72a1c", "title": "An ontology-based Web information extraction approach", "abstract": "An approach supervised by ontology is proposed for Web information extraction after analyzing two types of methods based on wrapper and concept model. Using concepts and taxonomy relation between concepts provided by ontology, this method can locate the wanted information blocks in Web page quickly by judging if adjacent sub-trees which are included in HTML Tree are isomorphic. Furthermore, combining text's data-modes the method can filter information which are irrelevant to the wanted information and achieve higher accuracy of information extraction.", "venue": "2010 2nd International Conference on Future Computer and Communication", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "ie", "onto"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "c35b261f1fdca6cfa51abbe74c9cf942fc25d0e9", "url": "https://www.semanticscholar.org/paper/c35b261f1fdca6cfa51abbe74c9cf942fc25d0e9", "title": "Domain-Targeted, High Precision Knowledge Extraction", "abstract": "Our goal is to construct a domain-targeted, high precision knowledge base (KB), containing general (subject,predicate,object) statements about the world, in support of a downstream question-answering (QA) application. Despite recent advances in information extraction (IE) techniques, no suitable resource for our task already exists; existing resources are either too noisy, too named-entity centric, or too incomplete, and typically have not been constructed with a clear scope or purpose. To address these, we have created a domain-targeted, high precision knowledge extraction pipeline, leveraging Open IE, crowdsourcing, and a novel canonical schema learning algorithm (called CASI), that produces high precision knowledge targeted to a particular domain - in our case, elementary science. To measure the KB\u2019s coverage of the target domain\u2019s knowledge (its \u201ccomprehensiveness\u201d with respect to science) we measure recall with respect to an independent corpus of domain text, and show that our pipeline produces output with over 80% precision and 23% recall with respect to that target, a substantially higher coverage of tuple-expressible science knowledge than other comparable resources. We have made the KB publicly available.", "venue": "International Conference on Topology, Algebra and Categories in Logic", "citationCount": 47, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ie"], "mention_counts": {"ke": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "f485717097e721531452134332e158d75e1167d7", "url": "https://www.semanticscholar.org/paper/f485717097e721531452134332e158d75e1167d7", "title": "EULAide: Interpretation of End-User License Agreements using Ontology-Based Information Extraction", "abstract": "Ignoring End-User License Agreements (EULAs) for online services due to their length and complexity is a risk undertaken by the majority of online and mobile service users. This paper presents an Ontology-Based Information Extraction (OBIE) method for EULA term and phrase extraction to facilitate a better understanding by humans. An ontology capturing important terms and relationships has been developed and used to guide the OBIE process. Through a feedback cycle we have improved its domain-specific coverage by identifying additional concepts. In the detection and extraction, we focus on three key rights and conditions: permission, prohibition and duty. We present the EULAide system, which comprises a custom information extraction pipeline and a number of custom extraction rules tailored for EULA processing. To evaluate our approach, we created and manually annotated a corpus of 20 well-known licenses. For the gold standard we achieved an Inter-Annotator Agreement (IAA) of 90%, resulting in 193 permissions, 185 prohibitions and 168 duties. An evaluation of the OBIE pipeline against this gold standard resulted in an F-measure of 70-74% which, in the context of the IAA, proves the feasibility of the approach.", "venue": "SEMANTiCS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "d1f157f6b11de095690979c32f330bbea77436d3", "url": "https://www.semanticscholar.org/paper/d1f157f6b11de095690979c32f330bbea77436d3", "title": "KNEWS: Using Logical and Lexical Semantics to Extract Knowledge from Natural Language", "abstract": "We present KNEWS, a pipeline of NLP tools that accepts natural language text as input and outputs knowledge in a machine-readable format. The tool outputs frame-based knowledge as RDF triples or XML, including the word-level alignment with the surface form, as well as first-order logical formulae. KNEWS is freely available for download. Moreover, thanks to its versatility, KNEWS has already been employed for a number of different applications for information extraction and automatic reasoning.", "venue": "European Conference on Artificial Intelligence", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "ie", "kg", "ke", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "kg": 1, "rdf": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 1, "rdf": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "cf2f6883a52e75e090e39b120c1a1dbf8790441c", "url": "https://www.semanticscholar.org/paper/cf2f6883a52e75e090e39b120c1a1dbf8790441c", "title": "Ontology-Based Enhanced Word Embedding for Automated Information Extraction from Geoscience Reports", "abstract": "Larger amount of geoscience reports brings both challenges and opportunities for data mining and analysis. This paper proposes an ontology-based enhanced word embedding (OEWE) information extraction methodology for extracting information about geoscience topic from regional geoscience reports. We first built the geoscience ontology to obtain a controlled vocabulary, and then the Skip-Gram model of word embedding was improved by Point-wise Mutual Information (PMI). Empirical experimental results on geoscience documents and benchmark datasets showed that the method is efficient.", "venue": "GEOINFORMATICS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "ie", "ie"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "36f2980b6e2698e3a97b2d1f980712ced96dc87f", "url": "https://www.semanticscholar.org/paper/36f2980b6e2698e3a97b2d1f980712ced96dc87f", "title": "Ontology Mediated Information Extraction with MASTRO SYSTEM-T", "abstract": "In several data-centric application domains, the need arises to extract valuable information from unstructured text documents. The recent paradigm of Ontology Mediated Information Extraction (OMIE) faces this problem by taking into account the knowledge expressed by a domain ontology, and reasoning over it to improve the quality of extracted data. MASTRO SYSTEM-T is a novel tool for OMIE, developed by Sapienza University and IBM Almaden Research. In this work, we demonstrate its usage for information extraction over real-world financial text documents from the U.S. EDGAR system.", "venue": "SEMWEB", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto", "ie", "onto"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "890f50f3c7418a98d4e88d7f7faa0d31c2b4fd3f", "url": "https://www.semanticscholar.org/paper/890f50f3c7418a98d4e88d7f7faa0d31c2b4fd3f", "title": "Concepts extraction for medical documents using ontology", "abstract": "In the biomedical domain large amount of text documents are unstructured information is available in digital text form. Text Mining is the method or technique to find for interesting and useful information from unstructured text. Text Mining is also an important task in medical domain. The technique uses for Information retrieval, Information extraction and natural language processing (NLP). Traditional approaches for information retrieval are based on key based similarity. These approaches are used to overcome these problems; Semantic text mining is to discover the hidden information from unstructured text and making relationships of the terms occurring in them. In the biomedical text, the text should be in the form of text which can be present in the books, articles, literature abstracts, and so forth. Most of information is stored in the text format, so in this paper we will focus on the role of ontology for semantic text mining by using WordNet. Specifically, we have presented a model for extracting concepts from text documents using linguistic ontology in the domain of medical.", "venue": "2015 International Conference on Advances in Computer Engineering and Applications", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "ie", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "eee552f14c70947f277395b3ebe2841e770767a2", "url": "https://www.semanticscholar.org/paper/eee552f14c70947f277395b3ebe2841e770767a2", "title": "Special issue on \u201cMulti\u2010modal information learning and analytics of cross\u2010media big data\u201d", "abstract": "We are living in the era of data deluge. Meanwhile, the world of big data includes a rich and complex set of cross\u2010media content, including text, images, video, audio, and graphics. To date, substantial research efforts have been dedicated to big data processing and cross\u2010media mining, yielding good theoretical underpinnings and great practical success. However, studies that jointly consider cross\u2010media big data analytics are relatively sparse (Zhang, Liu, Deng, Xu, and Choo (2018); Zhang, Wei, Wang, and Liao (2018)). This research gap needs more attention since it will benefit many real\u2010world applications. Despite its significance, it is important to analyse cross\u2010media big data due to the heterogeneity (Zhang, Zhao, Li, Chen, and Yuan (2015)), large\u2010scale volume, increasing size, lack of structure, correlations, and noise. Multi\u2010modal information learning (Demertzis, Iliadis, Avramidis, and El\u2010Kassaby (2017)), which can be treated as the most significant breakthrough in the past 10 years, has greatly affected the methodology of computer vision and achieved substantial progress in both academia and industry. Additionally, deep learning has been adopted in all kinds of computer vision applications, and many breakthroughs have been achieved in sub\u2010areas, such as DeepFace in the LFW competition for face verification and GoogleNet for the ImageNet Competition for object categorization. We can expect that more and more computer vision applications will benefit from multi\u2010modal information learning. This special issue focuses on learning methods to achieve high\u2010performance multi\u2010modal information analysis and understanding under uncontrolled environments on a large scale, which is also a very challenging problem. This problem has attracted considerable attention from both academia and industry. We hope this topic will aggregate top\u2010level research on new advances in multi\u2010modal information from cross\u2010media data. The purpose of this SI is to provide a forum for researchers and practitioners to exchange ideas and progress in related areas. After several rounds of review, six papers have been accepted for publication. Zhu, Ma, and Liu (2018) combined status\u2010striving motivation with opinion dynamics to study the decision mechanism and evolution law of employee knowledge sharing behaviour. A multiagent simulation method is used to design the decision\u2010making model of employee knowledge sharing behaviour. The influence of the network structure, number of people, and the proportion of employees with different statuses on knowledge\u2010sharing performance is considered. The simulation software is used to convert the theoretical model into a parametric experiment, and the sensitivity analysis is also employed. According to natural language processing and machine translation technology, to solve the problem, He (2018) aims to establish a parallel corpus for information extraction based on the OntoNotes corpus by combining automatic extraction and manual adjustment. To verify the validity of the parallel corpus constructed in this paper, a comparative experiment was carried out on the corpus. The corpus entity alignment rate, anaphora absence, and syntactic structure were analysed in detail based on statistics. To reduce the vagueness and subjectivity of customer demand in the process of product\u2013service system design, a fuzzy semantic calculation method is proposed by Zhang (2018) to obtain the importance of service demand. In addition, according to the demand of the clustering of service modules, a new clustering method is proposed to analyse discrete data based on the improved K\u2010means algorithm that is based on the Kruskal algorithm. Based on the actual situation of the knowledge flow during industrial transfer, a dynamic model of knowledge flow in a complex network is built by Zhou and Wu (2018). The mechanism of the knowledge flow in the network is analysed, and an analogue simulation is conducted using the Netlogo platform to identify the effects of the network's average degree, the number of enterprises involved in the transfer, the network size, and the effective transmission rate of the knowledge on the efficiency of the knowledge flow in the network. The work presented by Yang (2018) proposes an ontology mapping approach, in which the ontology element name and structure are combined. It uses the approaches based on linguistics and distance to generate a variable weight semantic graph. On this graph, the similarity of element names and structure are calculated through iterative computation. In the process of iteration, similarity result values are constantly adjusted. The approach avoids the problem of single methods that cannot use the entire amount of ontology information; therefore, it provides a more ideal mapping result. As an important resource of the company, the customer plays a vital role in the process of knowledge sharing. Therefore, research on the mechanism of customers' participation in knowledge sharing is especially important. In the work by Wang and Liu (2019), from the perspective of knowledge sharing between customers and enterprises, a customer participation model of a knowledge\u2010sharing mechanism is built based on three aspects: subjective influencing factors of knowledge sharing, objective influencing factors of knowledge sharing, and environmental influencing factors of knowledge sharing. As the special issue editors, we would like to take this opportunity to thank the various authors for their papers and the reviewers for their work. We are also grateful to Jon Hall, Editor\u2010in\u2010Chief of the Wiley journal Expert Systems.", "venue": "Expert Syst. J. Knowl. Eng.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "nlp", "mt"], "mention_counts": {"nlp": 1, "onto": 3, "mt": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "mt": 1, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "307e0ad03911072bc46639c14b30b053c2199867", "url": "https://www.semanticscholar.org/paper/307e0ad03911072bc46639c14b30b053c2199867", "title": "Identification des cat\u00e9gories de produits issus de catalogues publicitaires", "abstract": "In this paper, we propose an approach of information extraction, based on an ontology, and applied to documents from advertising catalogs. Documents are relatively poor descriptions of products. The information to be extracted, or annotations, concern the categories and features of the products, listed in a domain ontology. Thus, the information extraction about a product is actually an ontology population process, more precisely the population of concepts representing its categories and features. The poverty of the descriptions makes a fully automatic population impossible. We propose a two-step approach: (1) a first semi-automatic annotation step, which covers a small set of documents; (2) a second step, which annotates all other documents, in an entirely automatic way, based on machine learning mechanisms exploiting the results of the first step. The originality of this work relies on an incremental approach Revue d\u2019intelligence artificielle \u2013 n 5/2016, 557-578 558 RIA. Volume 30 \u2013 n 5/2016 to refine the extracted information. The work described has been applied on real data, in the toy domain. MOTS-CL\u00c9S : extraction d\u2019informations, peuplement d\u2019ontologie, annotation s\u00e9mantique, application dans le domaine du e-commerce.", "venue": "Rev. d'Intelligence Artif.", "citationCount": 0, "fieldsOfStudy": ["Geography", "Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "ie", "onto"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "81ce8f3dabcfc48ac89d5fda2f3da961e59918f2", "url": "https://www.semanticscholar.org/paper/81ce8f3dabcfc48ac89d5fda2f3da961e59918f2", "title": "An Automated Learner for Extracting New Ontology Relations", "abstract": "Recently, the NLP community has shown a renewed interest in automatic recognition of semantic relations between pairs of words in text which called lexical semantics. This approach to semantics is concerned with psychological facts associated with the meaning of words. Lexical semantics is an important task with many potential applications including but not limited to, Information Retrieval, Information Extraction, Text Summarization, and Language Modeling. As this task \"automatic recognition of semantic relations between pairs of words in text\" can be used in many NLP applications, its implementation are demanding and may include many potential methodologies. And as it includes semantic processing, the results produced still need enhancements and the outcome was always limited in terms of domain or coverage. In this research we developed a buffered system that handle the whole process of extracting causation relations in general domain ontologies. The main achievement of this work is the heavy analysis of statistical and semantic information of causation relation context to generate the learner. The system also builds relation resources that made it possible to learn from itself, were each time it runs the resources incremented with new relations information recording all the statistics of such relation, making its performance enhanced each time it runs. Also we present a novel approach of learning based on the best lexical patterns extracted, besides two new algorithms the CIA and PS that provide the final set of rules for mining causation to enrich ontologies.", "venue": "International Conference on Advanced Computer Science Applications and Technologies", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "onto", "ie"], "mention_counts": {"nlp": 2, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "fec04ff73c63fa05b1f886f114bb4db779a944f4", "url": "https://www.semanticscholar.org/paper/fec04ff73c63fa05b1f886f114bb4db779a944f4", "title": "Semantic Relatedness Metric for Wikipedia Concepts Based on Link Analysis and its Application to Word Sense Disambiguation", "abstract": "Wikipedia has grown into a high quality up-todate knowledge base and can enable many knowledge-based applications, which rely on semantic information. One of the most general and quite powerful semantic tools is a measure of semantic relatedness between concepts. Moreover, the ability to efficiently produce a list of ranked similar concepts for a given concept is very important for a wide range of applications. We propose to use a simple measure of similarity between Wikipedia concepts, based on Dice\u2019s measure, and provide very efficient heuristic methods to compute top k ranking results. Furthermore, since our heuristics are based on statistical properties of scale-free networks, we show that these heuristics are applicable to other complex ontologies. Finally, in order to evaluate the measure, we have used it to solve the problem of word-sense disambiguation. Our approach to word sense disambiguation is based solely on the similarity measure and produces results with high accuracy.", "venue": "Spring Young Researchers Colloquium on Databases and Information Systems", "citationCount": 59, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "kg", "wsd", "kg", "wsd", "onto"], "mention_counts": {"kg": 2, "wsd": 3, "onto": 1}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "36e3add5e8c53c962d25b869d81bae5e13ad98eb", "url": "https://www.semanticscholar.org/paper/36e3add5e8c53c962d25b869d81bae5e13ad98eb", "title": "Internet Data Analysis Methodology for Cyberterrorism Vocabulary Detection, Combining Techniques of Big Data Analytics, NLP and Semantic Web", "abstract": "This article presents a methodology for the analysis of data on the Internet, combining techniques of Big Data analytics, NLP and semantic web in order to find knowledge about large amounts of information on the web. To test the effectiveness of the proposed method, webpages about cyberterrorism were analyzed as a case study. The procedure implemented a genetic strategy in parallel, which integrates (Crawler to locate and download information from the web; to retrieve the vocabulary, using techniques of NLP (tokenization, stop word, TF, TFIDF), methods of stemming and synonyms). For the pursuit of knowledge was built a dataset through the description of a linguistic corpus with semantic ontologies, considering the characteristics of cyber-terrorism, which was analyzed with the algorithms, Random Forests (parallel), Boosting, SVM, neural network, K-nn and Bayes. The results reveal a percentage of the 95.62% accuracy in the detection of the vocabulary of cyber-terrorism, which were approved through cross validation, reaching 576% time savings with parallel processing.", "venue": "International Journal on Semantic Web and Information Systems (IJSWIS)", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "sw", "nlp", "nlp", "sw"], "mention_counts": {"nlp": 3, "sw": 2, "onto": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "fc6eedc69f5da8fa7f1b2aeae58c403c0c4746f3", "url": "https://www.semanticscholar.org/paper/fc6eedc69f5da8fa7f1b2aeae58c403c0c4746f3", "title": "Knowledge extraction from software engineering repositories", "abstract": "Software engineering processes are hard to understand, and related tasks frequently produce lot of information which can be used for development of strategy for future Projects. In the last decade, a large number of software data sets have been created for different purposes, however as the challenges in the development and maintenance of software are increased the need for novel approaches to make use of the collected data is also increased. The demands for reduced development time and increased reliability of software also necessitated the need for knowledge extraction from the previously collected data sets. In this paper a detailed survey is conducted on the available methods for the knowledge extraction from the software engineering data bases to forecast and aid in improved development and maintenance phases of software.", "venue": "2017 Intelligent Systems Conference (IntelliSys)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "7ccfc1787a230a90c41c60cbfe8167e9e3df6d11", "url": "https://www.semanticscholar.org/paper/7ccfc1787a230a90c41c60cbfe8167e9e3df6d11", "title": "Efficient Parallel Wikipedia Internal Link Extraction for NLP-Assisted Requirements Understanding", "abstract": "Requirements engineering (RE) is a critical set of activities in the software development life cycle (SDLC). Without effective requirements elicitation, organization, communication, and understanding software engineers cannot build quality soft-ware. Thus, it is necessary for software stakeholders to facilitate the SDLC by following best practices and utilizing software tools as needed to ensure requirements are well understood. One area where RE still faces issues, despite stakeholders' best efforts, is the communication of requirements amongst the various stakeholders. Software stakeholders consist of the customers, developers, managers, end users, and others with a vested interest in the software, and they typically all have different skillsets, backgrounds, vernaculars, and understanding of the requirements. These differences naturally lead to miscommunications which can lead to redundant, missing, or conflicting requirements, especially when customer and end user domains include complex vocabularies developers may not be accustomed to, and vice versa, e.g., biology, physics, and medicine. One approach in recent works to address this challenge has been to bridge the communication gap between stakeholders by constructing domain-specific ontologies using natural language processing (NLP) and Wikipedia [1]. With these ontologies, stakeholders have a convenient tool they can use to translate and understand specific requirements in the terminologies they're accustomed to. These techniques have shown promising potential, however there are computational challenges associated with efficiently handling a large dataset like Wikipedia. In particular, parsing internal links from Wikipedia article metadata can be a bottleneck in such ontology-construction systems. In this work we address this issue by implementing a program for memory-efficient parallel internal link extraction from Wikipedia articles. This builds on the work of Rodriguez et al. [2] by optimizing additional phases in the knowledge acquisition process.", "venue": "2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "nlp", "onto", "onto"], "mention_counts": {"nlp": 3, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "10db7bab795cd3c57280594803ed838e0c443947", "url": "https://www.semanticscholar.org/paper/10db7bab795cd3c57280594803ed838e0c443947", "title": "The RDF-based Information Capturing System from Web Pages", "abstract": "It is an investigative purpose to acquire the event information in the municipality website and extraction information is converted into the XML form of the RDF model. There is a problem that the extraction performance is controlled by the structure of the HTML tag though there is Web-wrapper method that uses the HTML tag as an information extraction technique on the Web page. In this paper, we propose an extraction method from a HTML document based on dictionary. HTML tag is deleted from the HTML document and it converts it into the text. It proposes the method for extracting a target character string by comparing the text with the collection of words prepared beforehand. Finally, extraction information is converted into the XML form of the RDF model. The evaluation experiment was done to the municipality in 23 Tokyo district and 56 Chiba prefecture in Japan. The proposal method was able to extract event information on as a whole 73\\%. The LR-Wrapper was 52\\%. The Tree-Wrapper was 55\\%. The PLR-Wrapper was 32\\%. The proposal method confirmed event information was rating higher than an existing method extractive by the combination of a simple algorithm and the collection of words.", "venue": "International Conference on P2P, Parallel, Grid, Cloud and Internet Computing", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "rdf", "ie", "ie", "ie", "rdf"], "mention_counts": {"rdf": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"rdf": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "0d4571f57b32a182a34ae49ac68257fbd174c1ba", "url": "https://www.semanticscholar.org/paper/0d4571f57b32a182a34ae49ac68257fbd174c1ba", "title": "PDFMEF: A Multi-Entity Knowledge Extraction Framework for Scholarly Documents and Semantic Search", "abstract": "We introduce PDFMEF, a multi-entity knowledge extraction framework for scholarly documents in the PDF format. It is implemented with a framework that encapsulates open-source extraction tools. Currently, it leverages PDFBox and TET for full text extraction, the scholarly document filter described in [5] for document classification, GROBID for header extraction, ParsCit for citation extraction, PDFFigures for figure and table extraction, and algorithm extraction [27]. While it can be run as a whole, the extraction tool in each module is highly customizable. Users can substitute default extractors with other extraction tools they prefer by writing a thin wrapper to implement the abstracts. The framework is designed to be scalable and is capable of running in parallel using a multi-processing technique in Python. Experiments indicate that the system with default setups is CPU bounded, and leaves a small footprint in the memory, which makes it best to run on a multi-core machine. The best performance using a dedicated server of 16 cores takes 1.3 seconds on average to process one PDF document. It is used to index extracted information and help users to quickly locate relevant results in published scholarly documents and to efficiently construct a large knowledge base in order to build a semantic scholarly search engine. Part of it is running on CiteSeerX digital library search engine.", "venue": "International Conference on Knowledge Capture", "citationCount": 39, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "b4c1c6895a588126db151dbe9fd6c2315be5e7bd", "url": "https://www.semanticscholar.org/paper/b4c1c6895a588126db151dbe9fd6c2315be5e7bd", "title": "Knowledge Extraction from Self-Organizing Map Using Minimization Entropy Principle Algorithm", "abstract": "Knowledge extraction using self-organizing map produced numeric values. This paper proposes knowledge extraction from self-organizing map using membership function from the minimization entropy principle algorithm to build linguistic intervals. The rough set theory was used in the rule extraction process for the minimum number of rules. The rules were in the form of linguistic \"if-then\" rule that user can understand easily. The benchmark data were iris database and Wisconsin breast cancer database. The experimental results received the fewer number of rules with high accuracy", "venue": "2006 International Symposium on Communications and Information Technologies", "citationCount": 1, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "92e8fbd6a3dc4d571551a74f737dd8abe13c70c2", "url": "https://www.semanticscholar.org/paper/92e8fbd6a3dc4d571551a74f737dd8abe13c70c2", "title": "Knowledge Extraction from Construction Cost Databases Using Fuzzy Queries", "abstract": "Construction business abandons in data, such as: prices of individual work items, site productivity data, details on construction materials, and similar, that are used to estimate the works and offer services, at contractor's side, or evaluate construction alternatives and decide on money spending, at owner's side. Such a wealth of data is usually queried using rudimentary tools, producing plain spreadsheet-like reports, such as bills of quantities and lists of materials. Improvements in handling of existing hisorical data, especially cost data, would result in better knowledge extraction during estimating and could generally enhance the quality of construction project management.Construction estimating and evaluation of technology options are not always based on fully determined database queries. Therefore, \"flexible\" queries such as: \"retrieve materials with 'good daily output' and 'acceptable cost'\" are likely to be utilized on a historical cost database. Flexible queries are not possible in standard relational databases and there is no such command in SQL relational database language that can be used to retrieve materials with \"good daily output\" or \"acceptable cost\". Instead, relational query must state bottom limit for daily output and top limit for acceptable material cost.A standard relational construction cost estimating database has been created, with all necessary functions required by the modern construction practice. In order to enable usage of so-called linguistic variables in queries, relational cost data model has than been extended with concepts from fuzzy theory. User interface has been created in a transparent manner, so software may be used by professionals and alike without knowledge on fuzzy concepts. All extensions to data model have been implemented using standard relational database language, as both relational data model and fuzzy sets are based on theory of sets.Extended data model has been tested on a large construction project to simulate expert's reasoning and provide automated professional advice to the user. Results matched expert's opinion and model proved to be useful knowledge extraction tool in construction estimating practice.", "venue": "HIS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "47c8ec833415797328936781a53c02ba8f37cc56", "url": "https://www.semanticscholar.org/paper/47c8ec833415797328936781a53c02ba8f37cc56", "title": "Semantic Networks for Engineering Design: State of the Art and Future Directions", "abstract": "\n In the past two decades, there has been increasing use of semantic networks in engineering design for supporting various activities, such as knowledge extraction, prior art search, idea generation and evaluation. Leveraging large-scale pre-trained graph knowledge databases to support engineering design-related natural language processing (NLP) tasks has attracted a growing interest in the engineering design research community. Therefore, this paper aims to provide a survey of the state-of-the-art semantic networks for engineering design and propositions of future research to build and utilize large-scale semantic networks as knowledge bases to support engineering design research and practice. The survey shows that WordNet, ConceptNet and other semantic networks, which contain common-sense knowledge or are trained on non-engineering data sources, are primarily used by engineering design researchers to develop methods and tools. Meanwhile, there are emerging efforts in constructing engineering and technical-contextualized semantic network databases, such as B-Link and TechNet, through retrieving data from technical data sources and employing unsupervised machine learning approaches. On this basis, we recommend six strategic future research directions to advance the development and uses of large-scale semantic networks for artificial intelligence applications in engineering design.", "venue": "Journal of Mechanical Design", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ke", "nlp", "nlp"], "mention_counts": {"nlp": 2, "kg": 2, "ke": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "aa0da1fd33d1e46faa2bf60afd871bcf2aa89667", "url": "https://www.semanticscholar.org/paper/aa0da1fd33d1e46faa2bf60afd871bcf2aa89667", "title": "Data Guided Approach to Generate Multi-dimensional Schema for Targeted Knowledge Discovery", "abstract": "Data mining and data warehousing are two key technologies which have made significant contributions to the field of knowledge discovery in a variety of domains. More recently, the integrated use of traditional data mining techniques such as clustering and pattern recognition with data warehousing technique of Online Analytical Processing (OLAP) have motivated diverse research areas for leveraging knowledge discovery from complex real-world datasets. Recently, a number of such integrated methodologies have been proposed to extract knowledge from datasets but most of these methodologies lack automated and generic methods for schema generation and knowledge extraction. Mostly data analysts need to rely on domain specific knowledge and have to cope with technological constraints in order to discover knowledge from high dimensional datasets. In this paper we present a generic methodology which incorporates semi-automated knowledge extraction methods to provide data-driven assistance towards knowledge discovery. In particular, we provide a method for constructing a binary tree of hierarchical clusters and annotate each node in the tree with significant numeric variables. Additionally, we propose automated methods to rank nominal variables and to generate candidate multidimensional schema with highly significant dimensions. We have performed three case studies on three real-world datasets taken from the UCI machine learning repository in order to validate the generality and applicability of our proposed methodology.", "venue": "Australasian Data Mining Conference", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4777cc52b0f8d3510712ec8bc74b717d8f5245c0", "url": "https://www.semanticscholar.org/paper/4777cc52b0f8d3510712ec8bc74b717d8f5245c0", "title": "A review on knowledge extraction for Business operations using data mining", "abstract": "The Knowledge Economy is of great importance in various business fields which had resulted in increased demand for the people having high order thinking skills and unpredicted-problem-solving at workplace. Every organization has a Knowledge Management (KM) department as Knowledge itself is a precious resource of the organization. The latest trends in KM include Customer and Vendor knowledge, Mobile Applications for KM, Collaborative Knowledge Management System (KMS) and Social intranet which can be integrated with business processes. The knowledge extracted can be stored and processed to enhance business intelligence. KM works with various business fields like Marketing, Sales, Human Resource, Operations, Supply Chain, etc. Due to frequent changes in operation of processes and Quality policies, the knowledge extracted from these processes can play a vital role in enhancing business processes. In this paper we had proposed various models of KM & Business Operations and the need of data mining technique which can be used to deliver appropriate knowledge to the user.", "venue": "2017 7th International Conference on Cloud Computing, Data Science & Engineering - Confluence", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "bd09b846c49095070040d9ffb9d8015d2ede9f99", "url": "https://www.semanticscholar.org/paper/bd09b846c49095070040d9ffb9d8015d2ede9f99", "title": "Knowledge extraction through etymological networks: Synonym discovery in Sino-Korean words", "abstract": "Extracting knowledge from a text is a very active area of research. Techniques such as word embedding and LSA have brought great breakthroughs and have been used in applications such as automatic translation. We propose a novel approach to extract knowledge from text that relies on a graph to express the complex etymological structures formed by the historical roots of words. Our approach is specially fit for the study of Sino-Korean vo- cabulary, where the etymological roots of words are clearly shown in their writing. We use our approach to build a bipartite graph based on the Chinese etymological roots of Sino-Korean words, and then use the network structure to extract features describing pairs of nodes. We used these features in a classification scheme to discover pairs of nodes that represent synonym characters. Our model is simpler than previous work on synonym discovery with Chinese characters, and obtains good results. The code and data for our work are made openly available.", "venue": "2016 IEEE International Conference on Knowledge Engineering and Applications (ICKEA)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4459750d8bd1f8ae5277503c9767663aaa9a8722", "url": "https://www.semanticscholar.org/paper/4459750d8bd1f8ae5277503c9767663aaa9a8722", "title": "Discovering Knowledge by Comparing Silhouettes Using K-Means Clustering for Customer Segmentation", "abstract": "A large amount of data is generated every day from different sources. Knowledge extraction is the discovery of some useful and potential information from data that can help to make better decisions. Today's business process requires a technique that is intelligent and has the capability to discover useful patterns in data called data mining. This research is about using silhouettes created from K-means clustering to extract knowledge. This paper implements K-means clustering technique in order to group customers into K clusters according to deals purchased in two different scenarios using evolutionary algorithm for optimization and compare silhouettes for different K values to analyze the improvement in extracted knowledge.", "venue": "International Journal of Knowledge Management", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "bf36475cf160915944d1d32ae41c77f383255ec3", "url": "https://www.semanticscholar.org/paper/bf36475cf160915944d1d32ae41c77f383255ec3", "title": "Simulation knowledge extraction and reuse in constrained random processor verification", "abstract": "This work proposes a methodology of knowledge extraction from constrained-random simulation data. Feature-based analysis is employed to extract rules describing the unique properties of novel assembly programs hitting special conditions. The knowledge learned can be reused to guide constrained-random test generation towards uncovered corners. The experiments are conducted based on the verification environment of a commercial processor design, in parallel with the on-going verification efforts. The experimental results show that by leveraging the knowledge extracted from constrained-random simulation, we can improve the test templates to activate the assertions that otherwise are difficult to activate by extensive simulation.", "venue": "Design Automation Conference", "citationCount": 25, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "384c0b21af45678a1a3675c08520afd71a73221f", "url": "https://www.semanticscholar.org/paper/384c0b21af45678a1a3675c08520afd71a73221f", "title": "Knowledge Extraction of Adaptive Structural Learning of Deep Belief Network for Medical Examination Data", "abstract": "Deep learning has a hierarchical network structure to represent multiple features of input data. The adaptive structural learning method of Deep Belief Network (DBN) can reach the high classification capability while searching the optimal network structure during the training. The method can find the optimal number of hidden neurons for given input data in a Restricted Boltzmann Machine (RBM) by neuron generation\u2013annihilation algorithm, and generate a new hidden layer in DBN by the extension of the algorithm. In this paper, the proposed adaptive structural learning of DBN (Adaptive DBN) was applied to the comprehensive medical examination data for cancer prediction. The developed prediction system showed higher classification accuracy for test data (99.5% for the lung cancer and 94.3% for the stomach cancer) than the several learning methods such as traditional RBM, DBN, Non-Linear Support Vector Machine (SVM), and Convolutional Neural Network (CNN). Moreover, the explicit knowledge that makes the inference process of the trained DBN is required in deep learning. The binary patterns of activated neurons for given input in RBM and the hierarchical structure of DBN can represent the relation between input and output signals. These binary patterns were classified by C4.5 for knowledge extraction. Although the extracted knowledge showed slightly lower classification accuracy than the trained DBN network, it was able to improve inference speed by about 1/40. We report that the extracted IF-THEN rules from the trained DBN for medical examination data showed some interesting features related to initial condition of cancer.", "venue": "International Journal of Semantic Computing", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "e5edb891d05ee9db97c09ee6fdb18aa0d2c4b357", "url": "https://www.semanticscholar.org/paper/e5edb891d05ee9db97c09ee6fdb18aa0d2c4b357", "title": "Web Data Knowledge Extraction", "abstract": "A constantly growing amount of information is available through the web. Unfortunately, extracting useful content from this massive amount of data still remains an open issue. The lack of standard data models and structures forces developers to create adhoc solutions from the scratch. The figure of the expert is still needed in many situations where developers do not have the correct background knowledge. This forces developers to spend time acquiring the needed background from the expert. In other directions, there are promising solutions employing machine learning techniques. However, increasing accuracy requires an increase in system complexity that cannot be endured in many projects. In this work, we approach the web knowledge extraction problem using an expertcentric methodology. This methodology defines a set of configurable, extendible and independent components that permit the reutilisation of large pieces of code among projects. Our methodology differs from similar solutions in its expert-driven design. This design, makes it possible for subject-matter expert to drive the knowledge extraction for a given set of documents. Additionally, we propose the utilization of machine assisted solutions that guide the expert during this process. To demonstrate the capabilities of our methodology, we present a real use case scenario in which public procurement data is extracted from the web-based repositories of several public institutions across Europe. We provide insightful details about the challenges we had to deal with in this use case and additional discussions about how to apply our methodology.", "venue": "ArXiv", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4108915df0c8ee27f5863d79c6d603e557bd51e0", "url": "https://www.semanticscholar.org/paper/4108915df0c8ee27f5863d79c6d603e557bd51e0", "title": "Research on Flow Object Knowledge Extraction Method", "abstract": "This paper in view of flow object, through the research for data mining and knowledge discovery technology, has designed the system structure used for flow object knowledge extraction, and has shown related algorithm used for the flow object knowledge extraction. Take the cement production process for an example, it has extracted operation rule in cement calcinations process, proposed suggestive rule for big operation which can result in bigger fluctuation of key parameter, thus to achieve the goal of control effect improvement and the enhancement of installment running stability.", "venue": "2008 First International Conference on Intelligent Networks and Intelligent Systems", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "1f4a4769e4d2fb846e59c2f185e0377190739f18", "url": "https://www.semanticscholar.org/paper/1f4a4769e4d2fb846e59c2f185e0377190739f18", "title": "Learning Structured Embeddings of Knowledge Bases", "abstract": "\n \n Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigorous symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like nat- ural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning meth- ods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text.\n \n", "venue": "AAAI Conference on Artificial Intelligence", "citationCount": 808, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["nlu", "kg", "ke", "kg", "wsd"], "mention_counts": {"kg": 2, "wsd": 1, "ke": 1, "nlu": 1}, "nlp_mention_counts": {"ke": 1, "wsd": 1, "nlu": 1}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "113418a03ea3b5b66cee1103ec83dca2e6f1062a", "url": "https://www.semanticscholar.org/paper/113418a03ea3b5b66cee1103ec83dca2e6f1062a", "title": "Challenging Knowledge Extraction to Support the Curation of Documentary Evidence in the Humanities (short paper)", "abstract": "The identification and cataloguing of documentary evidence from textual corpora is an important part of empirical research in the humanities. In this position paper, we ponder the applicability of knowledge extraction techniques to support the data acquisition process. Initially, we characterise the task by analysing the end-to-end process occurring in the data curation activity. After that, we examine general knowledge extraction tasks and discuss their relation to the problem at hand. Considering the case of the Listening Experience Database (LED), we perform an empirical analysis focusing on two roles: the 'listener' and the 'place'. The results show, among other things, how the entities are often mentioned many paragraphs away from the evidence text or are not in the source at all. We discuss the challenges emerged from the point of view of scientific knowledge acquisition.", "venue": "SciKnow@K-CAP", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "2a3c577ff1c2fb73cebb2e1b76c09e16039353e0", "url": "https://www.semanticscholar.org/paper/2a3c577ff1c2fb73cebb2e1b76c09e16039353e0", "title": "Formal Concept Analysis With Background Knowledge: Attribute Priorities", "abstract": "This paper deals with background knowledge in knowledge extraction from binary data. A background knowledge represents an additional piece of information a user may have along with the input data. Such information can be considered as specifying the type of knowledge a user is looking for in the data. In particular, we emphasize the need for taking into account background knowledge in formal concept analysis. We present an approach to modeling background knowledge that represents user's priorities regarding attributes and their relative importance. Such priorities serve as a constraint-only those formal concepts that are compatible with user's priorities are considered relevant, extracted from data, and presented to the user. Our approach has two main practical features. First, the number of formal concepts presented to the user may get significantly reduced. As a result, the user is supplied with relevant formal concepts only and is not overloaded by a large number of possibly nonrelevant formal concepts. Second, different priorities lead to different pieces of knowledge extracted from data. This way, the input data may be repeatedly used in knowledge extraction for different purposes corresponding to different priorities. We concentrate on foundational aspects such as mathematical feasibility, reasoning with background knowledge, removing redundancy from background knowledge, and computational tractability, and present several illustrative examples. In addition, we discuss directions for future research.", "venue": "IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)", "citationCount": 79, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "e018d81158c97f0e19b9d7eeabc339a2309c0e56", "url": "https://www.semanticscholar.org/paper/e018d81158c97f0e19b9d7eeabc339a2309c0e56", "title": "Compressor Fault Diagnosis Knowledge: A Benchmark Dataset for Knowledge Extraction From Maintenance Log Sheets Based on Sequence Labeling", "abstract": "Compressor fault diagnosis requires expert knowledge. Using the sequence labeling technology, this expert knowledge can be automatically extracted from compressor maintenance log sheets. Previous studies indicate that sequence labeling methods often need a substantial amount of annotation data for knowledge extraction, Unfortunately, the annotation data are very scarce in the field of compressor fault diagnosis. In this paper, we introduce a benchmark dataset for extraction of knowledge suitable for air compressor fault diagnosis. First, we collected 11,418 pieces of information from air compressor maintenance log sheets. Fault description, service requests, causes and troubleshooting solutions were stored in a dataset for data preprocessing and masking. In addition, 6196 valid text pairs were developed after the \u201cnoises\u201d in the raw dataset were cleaned. Second, five kinds of entities and sequences, such as equipment, faults, service requests, causes and troubleshooting solutions, were annotated by three subject experts. The annotation consistency was assessed with F1 scores. Furthermore, our proposed baseline model (or the BERT-BI-LSTM-CRF model) was compared against other five sequence labeling models (BI-LSTM-CRF, Lattice LSTM, BERT NER, ZEN, and ERNIE). The BERT-BI-LSTM-CRF model gives superior performance in extracting expert knowledge from the subject dataset. Although the baseline model is not the most cutting-edge model in the sequence labeling and named entity recognition fields, it indeed presents a great potential for compressor fault diagnosis. The dataset is available at https://github.com/chentao1999/CFDK.", "venue": "IEEE Access", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "d564e3b76d05b6ba834f2e6a5f31617e5874841b", "url": "https://www.semanticscholar.org/paper/d564e3b76d05b6ba834f2e6a5f31617e5874841b", "title": "Symbolic Knowledge Extraction from Opaque Predictors Applied to Cosmic-Ray Data Gathered with LISA Pathfinder", "abstract": "Machine learning models are nowadays ubiquitous in space missions, performing a wide variety of tasks ranging from the prediction of multivariate time series through the detection of specific patterns in the input data. Adopted models are usually deep neural networks or other complex machine learning algorithms providing predictions that are opaque, i.e., human users are not allowed to understand the rationale behind the provided predictions. Several techniques exist in the literature to combine the impressive predictive performance of opaque machine learning models with human-intelligible prediction explanations, as for instance the application of symbolic knowledge extraction procedures. In this paper are reported the results of different knowledge extractors applied to an ensemble predictor capable of reproducing cosmic-ray data gathered on board the LISA Pathfinder space mission. A discussion about the readability/fidelity trade-off of the extracted knowledge is also presented.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Physics"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "7f5e959d0af68e932ba6e00fbd3a7801e1497084", "url": "https://www.semanticscholar.org/paper/7f5e959d0af68e932ba6e00fbd3a7801e1497084", "title": "Resource-aware Federated Learning using Knowledge Extraction and Multi-model Fusion", "abstract": "With increasing concern about user data privacy, feder- ated learning (FL) has been developed as a unique training paradigm for training machine learning models on edge de- vices without access to sensitive data. Traditional FL and existing methods directly employ aggregation methods on all edges of the same models and training devices for a cloud server. Although these methods protect data privacy, they are not capable of model heterogeneity, even ignore the heterogeneous computing power, and incur steep communica- tion costs. In this paper, we purpose a resource-aware FL to aggregate an ensemble of local knowledge extracted from edge models, instead of aggregating the weights of each local model, which is then distilled into a robust global knowledge as the server model through knowledge distillation. The local model and the global knowledge are extracted into a tiny size knowledge network by deep mutual learning. Such knowledge extraction allows the edge client to deploy a resource- aware model and perform multi-model knowledge fusion while maintaining communication ef\ufb01ciency and model het- erogeneity. Empirical results show that our approach has signi\ufb01cantly improved over existing FL algorithms in terms of communication cost and generalization performance in heterogeneous data and models. Our approach reduces the com- munication cost of VGG-11 by up to 102 \u00d7 and ResNet-32 by up to 30 \u00d7 when training ResNet-20 as the knowledge net- work.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "35f471394a99f1eb02c67ff6a7b01093c1fa44d8", "url": "https://www.semanticscholar.org/paper/35f471394a99f1eb02c67ff6a7b01093c1fa44d8", "title": "An architectural framework for knowledge extraction from meteorological data", "abstract": "The methods of knowledge extraction from spatio-temporal data such as meteorological domain suffer from various problems like missing values, noise, improper format, and large in volume. Therefore, data pre-processing and reorganisation are the important concerns of this domain. In this paper, we have discussed a layered framework for data fetching, pre-processing and persisting. Further, we develop knowledge extraction modules for supporting the future extensibility and reusing well established tools. We have presented experimental results achieved using this architecture, in addition to its other potential benefits.", "venue": "International Journal of Applied Management Science", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "dde4d58c428ca01b63efccff17d147a92e7d5693", "url": "https://www.semanticscholar.org/paper/dde4d58c428ca01b63efccff17d147a92e7d5693", "title": "Neural Relational Learning Through Semi-Propositionalization of Bottom Clauses", "abstract": "Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph mining and link analysis in social networks. The CILP++ system is a neural-symbolic system which can perform efficient relational learning, by being able to process first-order logic knowledge into a neural network. CILP++ relies on BCP, a recently discovered propositionalization algorithm, to perform relational learning. However, efficient knowledge extraction from such networks is an open issue and features generated by BCP do not have an independent relational description, which prevents sound knowledge extraction from such networks. We present a methodology for generating independent propositional features for BCP by using semi-propositionalization of bottom clauses. Empirical results obtained in comparison with the original version of BCP show that this approach has comparable accuracy and runtimes, while allowing proper relational knowledge representation of features for knowledge extraction from CILP++ networks.", "venue": "AAAI Spring Symposia", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "8597da824cfdb3e1b212fad236dea66b835d1ce7", "url": "https://www.semanticscholar.org/paper/8597da824cfdb3e1b212fad236dea66b835d1ce7", "title": "Knowledge extraction from SID epidemiological data using neural networks", "abstract": "Knowledge extraction is an important problem that has been little addressed by neural networks. In this work, we try to analyse the knowledge stored in a trained three-layer perceptron, using sudden infant death syndrome data. It is shown that when analysing the internal structure of the network, the classification solution realised by the network may be optimal in terms of classification results but not optimal in terms of knowledge representation. A simple method is proposed in order to reorganise and to extract knowledge stored in the synaptic weights.<<ETX>>", "venue": "Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "9fc210d4f461cac8ceb334dbcd3560e568b22767", "url": "https://www.semanticscholar.org/paper/9fc210d4f461cac8ceb334dbcd3560e568b22767", "title": "Discrete approach for automatic knowledge extraction from precedent large-scale data, and classification", "abstract": "The proposed method for automatic knowledge extraction from large-scale data is based on the idea of analysing neighborhoods of \"supporting\" objects and construction of data covered by sets of hyper parallelepipeds. A simple procedure to choose the supporting objects is applied. Knowledge extraction (logical regularities search) is based on the solution of special discrete linear optimization tasks associated with supporting objects. Two practical tasks are considered for method illustration.", "venue": "Object recognition supported by user interaction for service robots", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "a2bbb8f9631b1965cfd2e851f61355142913f535", "url": "https://www.semanticscholar.org/paper/a2bbb8f9631b1965cfd2e851f61355142913f535", "title": "Multi-knowledge extraction algorithm using Group Search Optimization for brain dataset analysis", "abstract": "Knowledge is formed by a kind of mapping from the condition space to the decision space in rough set. This paper presents multi-knowledge extraction approaches with fuzzy population algorithms. The Group Search Optimization (GSO) and Particle Swarm Optimization (PSO) are compared. GSO not only has the rapid convergence speed, but also has low time complexity, especially for high dimensional datasets. We use the multi-knowledge extraction algorithm based on GSO to analyze the data of brain cognition datasets. The experimental results illustrate our algorithm is very promising to seek for the relationship between the active brain regions and stimuli.", "venue": "2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "781f6f2d95eba6825ad3d0b69784b3881c1c0c2f", "url": "https://www.semanticscholar.org/paper/781f6f2d95eba6825ad3d0b69784b3881c1c0c2f", "title": "A Knowledge Extraction System from Manager's Operation Sequences in System Development Project", "abstract": "In order to support training project managers with a project simulator, we address extracting knowledge from Manager's operation sequences in system development projects. Because it is hard to collect many operation sequences from project managers, the proposed knowledge extraction system generates operation sequences automatically by agent programs. The generated operation sequences are classified into clusters that consist of similar operation sequences. The clusters represent characteristic operation sequences to improve evaluation of a project. As a preliminary experiment based on clustering for time-series categorical data, we confirm that the generated clusters represent characteristic operations as knowledge to determine operations.", "venue": "2016 5th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "9073c82625d040c6d31abb38c73d2ced899b5005", "url": "https://www.semanticscholar.org/paper/9073c82625d040c6d31abb38c73d2ced899b5005", "title": "Knowledge Extraction in Digit Recognition Using MNIST Dataset: Evolution in Handwriting Analysis", "abstract": "In handwriting recognition, traditional systems have relied heavily on handcrafted features and a massive amount of prior data and knowledge. Deep learning techniques have been the focus of research in the field of handwriting digit recognition and have achieved breakthrough performance in the last few years for knowledge extraction and management. KM and knowledge pyramid helps the project with its relationship with big data and IoT. The layers were selected randomly by which the performance of all the cases was found different. Data layers of the knowledge pyramid are formed by the sensors and input devices, whereas knowledge layers are the result of knowledge extraction applied on data layers. The knowledge pyramid and KM helps in making the use of IoT and big data easily. In this manuscript, the knowledge management principles capture the handwritten gestures numerically and get it recognized correctly by the software. The application of AI and DNN has increased the acceptability significantly. The accuracy is better than other available software on the market.", "venue": "Int. J. Knowl. Manag.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "2f5b99ae60a17aef14cdd03f53d784ee4c7edb3a", "url": "https://www.semanticscholar.org/paper/2f5b99ae60a17aef14cdd03f53d784ee4c7edb3a", "title": "A Variable Precision Rough Set Model for Knowledge-assisted Management in Distance Education", "abstract": "To enable the teaching administrator to better obtain effective knowledge from a large amount of information to assist management and improve the efficiency and level of teaching management, a variable precision rough set model for knowledge assisted management of distance education was proposed. First, based on the theory of complete reduction and knowledge extraction, the proposed pedigree ambiguity tree was used as a strategy for obtaining complete reduction. An algorithm for obtaining a complete set of reductions was given. Then, by studying the process of knowledge extraction, a multi-knowledge extraction framework was put forward. The process of data conversion was completely realized. Finally, experimental verification was performed. The results showed that the proposed model overcame the effect of noise data in real data and improved the efficiency of the algorithm. Therefore, the model has high universality.", "venue": "Int. J. Emerg. Technol. Learn.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "3e4dd6bab08734aff28f6c19f0245e5d98198135", "url": "https://www.semanticscholar.org/paper/3e4dd6bab08734aff28f6c19f0245e5d98198135", "title": "Knowledge Transfer for Feature Generation in Document Classification", "abstract": "One important problem in machine learning is how to extract knowledge from prior experience, then transfer and apply this knowledge in new learning tasks. To address this problem, transfer learning leverages information from (supervised) learning on related tasks to facilitate the current learning task. Self-taught learning uses information extracted from (unsupervised) learning on related data. In this paper, we propose a new method for knowledge extraction, transfer and application in classification. We consider document classification where we mine correlation relationships among the words from a set of documents and compile a collection of correlation relationships as prior knowledge. This knowledge is then applied to generate new features for classifying documents in classes/types different from the ones from which we obtain the correlation relationships. Our experiment results show that the correlation-based knowledge transfer helps to reduce classification errors.", "venue": "2009 International Conference on Machine Learning and Applications", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "699313a1d735c32b06b8bf36367df6178953b4d3", "url": "https://www.semanticscholar.org/paper/699313a1d735c32b06b8bf36367df6178953b4d3", "title": "Objects and Goals Extraction from Semantic Networks : Applications of Fuzzy SetS Theory", "abstract": "Department of Mathematical and Data Processing, Preparatory Institute of Engineering Studies of Monastir Kairouan Road, 5019 Monastir, Tunisia E-mail : Nazih.omri@ipeim.rnu.tn , E-mail : Noureddine.chouigui@ipeim.rnu.tn Abstract. In this paper we present a short survey of fuzzy and Semantic approaches to Knowledge Extraction. The goal of such approaches is to define flexible Knowledge Extraction Systems able to deal with the inherent vagueness and uncertainty of the Extraction process. In this survey we address if and how some approaches met their goal. Keywords: Knowledge extraction, fuzzy Goal, Fuzzy Object, Semantic Network, Fuzzy sets.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "3f4b86a94ef1c8d58a07ca72396e2f203d5bf124", "url": "https://www.semanticscholar.org/paper/3f4b86a94ef1c8d58a07ca72396e2f203d5bf124", "title": "Natural Language Processing for Forecasting Innovative Development of the Energy Infrastructure", "abstract": "The article deals with the application of natural language processing methods to support research and forecasting the innovative development of energy infrastructure. The main methods of NLP, which are used to build an intelligent system to support scientific research, are considered. Methods of building infrastructure for processing Open Linked Data and Big Data are described. Semantic analysis and knowledge integration are based on ontology system. Applying suggested methods allow increasing quality of scientific research in this area and make it more effectively", "venue": "E3S Web of Conferences", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ld", "nlp", "onto", "nlp", "lod"], "mention_counts": {"ld": 1, "nlp": 3, "lod": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"ld": 1, "lod": 1, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "fee3fd2b000ca3e8d3a09606fd15f0a3c1cf69bc", "url": "https://www.semanticscholar.org/paper/fee3fd2b000ca3e8d3a09606fd15f0a3c1cf69bc", "title": "K4ThaiHealth: A Prototype for Thai Routine Medical Research Knowledge Extraction Sharing", "abstract": "\u201cRoutine to Research\u201d (R2R) is well known for Thai research related to the development of routine works of medical and public health practitioners. R2R research contains useful practical knowledge beneficial to the health of Thai people. However, this knowledge cannot be shared easily because it is unstructured and not classified text, moreover, no tool for R2R Thai knowledge sharing yet. In this research, we attempt to use text mining techniques to get insights of R2R research data and the K4ThaiHealth is first implemented as a prototype for basic R2R knowledge sharing. A set of basic medical corpus are developed using Thai medical International Statistical Classification of Diseases and Related Health Problems (ICD10TM) and several resources. They are used for R2R Thai medical text classification and key terms relationship extraction. The results are classified into diseases, organs, symptoms, and others. K4ThaiHealth is then used as a knowledge sharing prototype to offer health and medical practice knowledge extracted from R2R data sharing to Thai people. R2R WordCloud and R2R WordNet are used to display the diseases knowledge extracted from R2R research data and their relationships to diseases, organs, symptoms and others are visualized.", "venue": "2018 Seventh ICT International Student Project Conference (ICT-ISPC)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "56ac2d00f0466fba8498d56658639ad91ac972e9", "url": "https://www.semanticscholar.org/paper/56ac2d00f0466fba8498d56658639ad91ac972e9", "title": "Knowledge Extraction from Auto-Encoders on Anomaly Detection Tasks Using Co-activation Graphs", "abstract": "Deep neural networks have exploded in popularity and different types of networks are used to solve a multitude of complex tasks. One such task is anomaly detection, that a type of deep neural network called auto-encoder has become extremely proficient at solving. The low level neural activity, produced by such a network, generates extremely rich representations of the data, which can be used to extract task specific knowledge. In this paper, we built upon previous work and used co-activation graph analysis to extract knowledge from auto-encoders, that were trained for the specific task of anomaly detection. First, we outlined a method for extracting co-activation graphs from auto-encoders. Then, we performed graph analysis to discover that task specific knowledge from the auto-encoder was being encoded into the co-activation graph, and that the extracted knowledge could be used to reveal the role of individual neurons in the network.", "venue": "K-CAP", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "e757589891759f70b50fb972135fac4c5c1d65a6", "url": "https://www.semanticscholar.org/paper/e757589891759f70b50fb972135fac4c5c1d65a6", "title": "Identifying likely student dropouts using fuzzy inferencing", "abstract": "Fuzzy logic provides a methodology for reasoning using imprecise rules and assertions. Whereas a statement can only be true or false in classical logic, statements in fuzzy logic may be true or false to varying degrees. This enables fuzzy logic to deal with data and rules that are expressed in an imprecise manner using inexact linguistic expressions. Fuzzy inference is the process of formulating the mapping from a given input to an output using fuzzy logic. The mapping then provides a basis from which decisions can be made, or patterns discerned. Fuzzy expert systems are proving to be a powerful tool in business intelligence and decision support. This project concerns the development of a fuzzy expert system for identifying likely student dropouts at Columbus State University (CSU).\n According to a report released by the National Center for Public Policy and Higher Education, a low rate of college completion is a key concern in American higher education. According to ACT (the college testing service), the national average freshmen retention rate is 65.7%. From 2005 to 2010, this rate at CSU was 71%. Colleges and universities across the country, including CSU, are investigating this problem of student Retention, Progression and Graduation (RPG) in order to address it more effectively. The main aim of this research project is to build a fuzzy inference based model using a hybrid knowledge extraction process to predict how likely each freshman student will be to drop their program of study at the end of their first semester. Columbus State University database has student Retention, Progression and Graduation (RPG) data dating back to 1998. This historical data is being utilized to develop and evaluate the proposed fuzzy rule-based inferencing system.\n Knowledge extraction for the system will be performed using a top down (symbolic) as well as a bottom-up (data-based) approach. In the top-down approach, rules for the fuzzy model will be derived using the traditional knowledge extraction process involving domain expert interviews. Several persons in charge of university departments that have low retention rates will be interviewed to identify parameters that are significant determinants of student success. Fuzzy-rules designed using this knowledge will be weighted appropriately to reflect their level of significance. In the second phase of fuzzy rule derivation, results of data mining performed on student data will be utilized. A feedforward artificial neural network already trained using the student data will be subjected to weight analysis to derive additional rules for the fuzzy rule base, as well as for adjusting the significance of all rules. This hybrid neuro-fuzzy approach is expected to yield better performance than either a conventional fuzzy inferencing system or an artificial neural network.", "venue": "ACMSE '13", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "cd4bc7aa872fca023d2b5b03117cebf0979abbac", "url": "https://www.semanticscholar.org/paper/cd4bc7aa872fca023d2b5b03117cebf0979abbac", "title": "Constructing Bayesian networks by harvesting knowledge from online resources", "abstract": "In this paper, the development of a human-like intelligent system, named AKEOS (Automatic Knowledge Extraction from Online Sources), is introduced. AKEOS can automatically harvest knowledge from online resources to build a Bayesian network inference engine. Starting from a single event, the AKEOS system performs unsupervised knowledge extraction to convert unstructured text into structured knowledge. By performing repeated knowledge extraction for multiple similar events, AKEOS produces structured databases. Thus, various machine learning algorithms can be directly applied to explore relations between attributes, to discover patterns hidden in the data, or to build inference engines such as Bayesian networks for predictive and diagnostic reasoning. AKEOS is an end-to-end system with lists of events as input, structured databases as intermediate products, and inference engines as end products.", "venue": "2016 19th International Conference on Information Fusion (FUSION)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "3e81b09417029e09e01bd3bca7e2c590986505af", "url": "https://www.semanticscholar.org/paper/3e81b09417029e09e01bd3bca7e2c590986505af", "title": "A novel method for extracting interpretable knowledge from a spiking neural classifier with time-varying synaptic weights", "abstract": "This paper presents a novel method for information interpretability in an MC-SEFRON classifier. To develop a method to extract knowledge stored in a trained classifier, first, the binary-class SEFRON classifier developed earlier is extended to handle multi-class problems. MC-SEFRON uses the population encoding scheme to encode the real-valued input data into spike patterns. MC-SEFRON is trained using the same supervised learning rule used in the SEFRON. After training, the proposed method extracts the knowledge for a given class stored in the classifier by mapping the weighted postsynaptic potential in the time domain to the feature domain as Feature Strength Functions (FSFs). A set of FSFs corresponding to each output class represents the extracted knowledge from the classifier. This knowledge encoding method is derived to maintain consistency between the classification in the time domain and the feature domain. The correctness of the FSF is quantitatively measured by using FSF directly for classification tasks. For a given input, each FSF is sampled at the input value to obtain the corresponding feature strength value (FSV). Then the aggregated FSVs obtained for each class are used to determine the output class labels during classification. FSVs are also used to interpret the predictions during the classification task. Using ten UCI datasets and the MNIST dataset, the knowledge extraction method, interpretation and the reliability of the FSF are demonstrated. Based on the studies, it can be seen that on an average, the difference in the classification accuracies using the FSF directly and those obtained by MC-SEFRON is only around 0.9% & 0.1\\% for the UCI datasets and the MNIST dataset respectively. This clearly shows that the knowledge represented by the FSFs has acceptable reliability and the interpretability of classification using the classifier's knowledge has been justified.", "venue": "ArXiv", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "097a988d32639aa2d79f4858b692075b00440ca3", "url": "https://www.semanticscholar.org/paper/097a988d32639aa2d79f4858b692075b00440ca3", "title": "Social Computing and User Generated Content potential Pros and Cons: A Review", "abstract": "Social Computing has gained multidisplinary key research areas from academicians to professionals and to researchers. In couple of past decades several implementations, framework and research theories had been proposed and justified in such regards. Moreover Knowledge Extraction (KE) is a trending and emerging domain that addresses various proven techniques for extracting knowledge out of heavy and resilient social data. In this paper we have presented our literature review for novel approach of data intensive social computing for the purpose of knowledge extraction in social ties. Social Computing is a trending fuzzy term that act as a super set of Social Network Analysis, crowd management, crowd sourcing and many more. And as bigger population in involved in social computing therefore it becomes very crucial to cater overall monitoring functionality on such huge user generated data.", "venue": "International Journal of Computer Applications", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4d15d83ae7e61fded91a9455b4a268273718fd7d", "url": "https://www.semanticscholar.org/paper/4d15d83ae7e61fded91a9455b4a268273718fd7d", "title": "TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories", "abstract": "Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce. State-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-Commerce scenarios, which often contain thousands of diverse categories. This paper proposes TXtract, a taxonomy-aware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy. Through category conditional self-attention and multi-task learning, our approach is both scalable, as it trains a single model for thousands of categories, and effective, as it extracts category-specific attribute values. Experiments on products from a taxonomy with 4,000 categories show that TXtract outperforms state-of-the-art approaches by up to 10% in F1 and 15% in coverage across all categories.", "venue": "Annual Meeting of the Association for Computational Linguistics", "citationCount": 35, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "9ce4599f0e7529c0324c0b25e72b8e7ac1f21178", "url": "https://www.semanticscholar.org/paper/9ce4599f0e7529c0324c0b25e72b8e7ac1f21178", "title": "Interpretable Multi-Step Reasoning with Knowledge Extraction on Complex Healthcare Question Answering", "abstract": "Healthcare question answering assistance aims to provide customer healthcare information, which widely appears in both Web and mobile Internet. The questions usually require the assistance to have proficient healthcare background knowledge as well as the reasoning ability on the knowledge. Recently a challenge involving complex healthcare reasoning, HeadQA dataset, has been proposed, which contains multiple-choice questions authorized for the public healthcare specialization exam. Unlike most other QA tasks that focus on linguistic understanding, HeadQA requires deeper reasoning involving not only knowledge extraction, but also complex reasoning with healthcare knowledge. These questions are the most challenging for current QA systems, and the current performance of the state-of-the-art method is slightly better than a random guess. In order to solve this challenging task, we present a Multi-step reasoning with Knowledge extraction framework (MurKe). The proposed framework first extracts the healthcare knowledge as supporting documents from the large corpus. In order to find the reasoning chain and choose the correct answer, MurKe iterates between selecting the supporting documents, reformulating the query representation using the supporting documents and getting entailment score for each choice using the entailment model. The reformulation module leverages selected documents for missing evidence, which maintains interpretability. Moreover, we are striving to make full use of off-the-shelf pre-trained models. With less trainable weight, the pre-trained model can easily adapt to healthcare tasks with limited training samples. From the experimental results and ablation study, our system is able to outperform several strong baselines on the HeadQA dataset.", "venue": "ArXiv", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "287603a70267bbe0d11d4cdf9d3241fabefcb6f2", "url": "https://www.semanticscholar.org/paper/287603a70267bbe0d11d4cdf9d3241fabefcb6f2", "title": "Layerwise Knowledge Extraction from Deep Convolutional Networks", "abstract": "Knowledge extraction is used to convert neural networks into symbolic descriptions with the objective of producing more comprehensible learning models. The central challenge is to find an explanation which is more comprehensible than the original model while still representing that model faithfully. The distributed nature of deep networks has led many to believe that the hidden features of a neural network cannot be explained by logical descriptions simple enough to be comprehensible. In this paper, we propose a novel layerwise knowledge extraction method using M-of-N rules which seeks to obtain the best trade-off between the complexity and accuracy of rules describing the hidden features of a deep network. We show empirically that this approach produces rules close to an optimal complexity-error tradeoff. We apply this method to a variety of deep networks and find that in the internal layers we often cannot find rules with a satisfactory complexity and accuracy, suggesting that rule extraction as a general purpose method for explaining the internal logic of a neural network may be impossible. However, we also find that the softmax layer in Convolutional Neural Networks and Autoencoders using either tanh or relu activation functions is highly explainable by rule extraction, with compact rules consisting of as little as 3 units out of 128 often reaching over 99% accuracy. This shows that rule extraction can be a useful component for explaining parts (or modules) of a deep neural network.", "venue": "ArXiv", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "e9ecc83042f6d439a32815a19c1bc11cc82b6a8b", "url": "https://www.semanticscholar.org/paper/e9ecc83042f6d439a32815a19c1bc11cc82b6a8b", "title": "Introduction to MAchine Learning & Knowledge Extraction (MAKE)", "abstract": "The grand goal of Machine Learning is to develop software which can learn from previous experience\u2014similar to how we humans do. Ultimately, to reach a level of usable intelligence, we need (1) to learn from prior data, (2) to extract knowledge, (3) to generalize\u2014i.e., guessing where probability function mass/density concentrates, (4) to fight the curse of dimensionality, and (5) to disentangle underlying explanatory factors of the data\u2014i.e., to make sense of the data in the context of an application domain. To address these challenges and to ensure successful machine learning applications in various domains an integrated machine learning approach is important. This requires a concerted international effort without boundaries, supporting collaborative, cross-domain, interdisciplinary and transdisciplinary work of experts from seven sections, ranging from data pre-processing to data visualization, i.e., to map results found in arbitrarily high dimensional spaces into the lower dimensions to make it accessible, usable and useful to the end user. An integrated machine learning approach needs also to consider issues of privacy, data protection, safety, security, user acceptance and social implications. This paper is the inaugural introduction to the new journal of MAchine Learning & Knowledge Extraction (MAKE). The goal is to provide an incomplete, personally biased, but consistent introduction into the concepts of MAKE and a brief overview of some selected topics to stimulate future research in the international research community.", "venue": "Machine Learning and Knowledge Extraction", "citationCount": 50, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "0fdc8c3be4f0059c201beee2a6f61c8c06dfe9ee", "url": "https://www.semanticscholar.org/paper/0fdc8c3be4f0059c201beee2a6f61c8c06dfe9ee", "title": "Knowledge Extraction and Integration for Information Gathering in Penetration Testing", "abstract": "Assets identification is an important aspect of penetration test on which security practitioner develop their defense mechanism. In addition, assets identification is an essential piece of information for penetration testers to find a weakness in the targeted organization. Information gathering is the process of extracting knowledge to recognize the organizations' assets available on the internet. There are many open source tools available for information gathering. However, penetration tester needs to put manual effort (during several hours to multiple days) to extract useful knowledge from the output of one tool and integrate that knowledge in another tool. Penetration tester can increase speed and accuracy of the overall information gathering process by automating the knowledge extraction and integration. This paper review and identify open source subdomain enumeration and service scanning tools and present an approach to integrate and automate identified tools. The result reveals that there is a significant improvement of the information gathering process by using our approach due to the reduction of manual tasks.", "venue": "IEEE International Conference on Software Quality, Reliability and Security Companion", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "1162c416c4f0ac5d20cb2d636b1424435ca4bced", "url": "https://www.semanticscholar.org/paper/1162c416c4f0ac5d20cb2d636b1424435ca4bced", "title": "A rule-based named-entity recognition method for knowledge extraction of evidence-based dietary recommendations", "abstract": "Evidence-based dietary information represented as unstructured text is a crucial information that needs to be accessed in order to help dietitians follow the new knowledge arrives daily with newly published scientific reports. Different named-entity recognition (NER) methods have been introduced previously to extract useful information from the biomedical literature. They are focused on, for example extracting gene mentions, proteins mentions, relationships between genes and proteins, chemical concepts and relationships between drugs and diseases. In this paper, we present a novel NER method, called drNER, for knowledge extraction of evidence-based dietary information. To the best of our knowledge this is the first attempt at extracting dietary concepts. DrNER is a rule-based NER that consists of two phases. The first one involves the detection and determination of the entities mention, and the second one involves the selection and extraction of the entities. We evaluate the method by using text corpora from heterogeneous sources, including text from several scientifically validated web sites and text from scientific publications. Evaluation of the method showed that drNER gives good results and can be used for knowledge extraction of evidence-based dietary recommendations.", "venue": "PLoS ONE", "citationCount": 81, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "840a4bd0234c5ced74e71583121995cb4ac5d18e", "url": "https://www.semanticscholar.org/paper/840a4bd0234c5ced74e71583121995cb4ac5d18e", "title": "Design of a conceptual knowledge extraction framework for a social learning environment based on Social Network Analysis methods", "abstract": "The advent of social media in education has the potential to foster collaborative learning. Exploring students' interactions on the social media tools is an important research direction, which could bring an insight into the collaborative learning process. Therefore, our aim is to propose a conceptual framework for knowledge extraction and visualization from a social media-based learning environment. In particular, we focus on our in-house platform, called eMUSE, which has been successfully used in a project-based learning scenario. The paper addresses the construction of a social graph starting from students' interactions on the social media tools; the objective is to identify appropriate social network analysis techniques that can answer specific educational needs and integrate them in a conceptual knowledge extraction framework. The basis, rationale and analysis levels of the framework are discussed in the paper.", "venue": "International Conference on Innovative Computing and Cloud Computing", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "127744222fe31086c2c998a7fae2f16a3fb330a4", "url": "https://www.semanticscholar.org/paper/127744222fe31086c2c998a7fae2f16a3fb330a4", "title": "Lexicon Knowledge Extraction with Sentiment Polarity Computation", "abstract": "Sentiment analysis is one of the most popular natural language processing techniques. It aims to identify the sentiment polarity (positive, negative, neutral or mixed) within a given text. The proper lexicon knowledge is very important for the lexicon-based sentiment analysis methods since they hinge on using the polarity of the lexical item to determine a text's sentiment polarity. However, it is quite common that some lexical items appear positive in the text of one domain but appear negative in another. In this paper, we propose an innovative knowledge building algorithm to extract sentiment lexicon knowledge through computing their polarity value based on their polarity distribution in text dataset, such as in a set of domain specific reviews. The proposed algorithm was tested by a set of domain microblogs. The results demonstrate the effectiveness of the proposed method. The proposed lexicon knowledge extraction method can enhance the performance of knowledge based sentiment analysis.", "venue": "2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "nlp", "ke"], "mention_counts": {"nlp": 1, "ke": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "240f59d5f72e7c08a1de28a29f84a3b3be54e5bc", "url": "https://www.semanticscholar.org/paper/240f59d5f72e7c08a1de28a29f84a3b3be54e5bc", "title": "Relational Knowledge Extraction from Neural Networks", "abstract": "The effective integration of learning and reasoning is a well-known and challenging area of research within artificial intelligence. Neural-symbolic systems seek to integrate learning and reasoning by combining neural networks and symbolic knowledge representation. In this paper, a novel methodology is proposed for the extraction of relational knowledge from neural networks which are trainable by the efficient application of the backpropagation learning algorithm. First-order logic rules are extracted from the neural networks, offering interpretable symbolic relational models on which logical reasoning can be performed. The wellknown knowledge extraction algorithm TREPAN was adapted and incorporated into the first-order version of the neural-symbolic system CILP++. Empirical results obtained in comparison with a probabilistic model for relational learning, Markov Logic Networks, and a state-of-the-art Inductive Logic Programming system, Aleph, indicate that the proposed methodology achieves competitive accuracy results consistently in all datasets investigated, while either Markov Logic Networks or Aleph show considerably worse results in at least one dataset. It is expected that effective knowledge extraction from neural networks can contribute to the integration of heterogeneous knowledge representations.", "venue": "CoCo@NIPS", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "1d4ad0da9581c8d9d5a7d56765e53782995cbaae", "url": "https://www.semanticscholar.org/paper/1d4ad0da9581c8d9d5a7d56765e53782995cbaae", "title": "Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia", "abstract": "Wikipedia infoboxes are a valuable source of structured knowledge for global knowledge sharing. However, infobox information is very incomplete and imbalanced among the Wikipedias in different languages. It is a promising but challenging problem to utilize the rich structured knowledge from a source language Wikipedia to help complete the missing infoboxes for a target language. In this paper, we formulate the problem of cross-lingual knowledge extraction from multilingual Wikipedia sources, and present a novel framework, called WikiCiKE, to solve this problem. An instancebased transfer learning method is utilized to overcome the problems of topic drift and translation errors. Our experimental results demonstrate that WikiCiKE outperforms the monolingual knowledge extraction method and the translation-based method.", "venue": "ACL", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "66040e6c4261a9bdc9847c429440995105cdd2da", "url": "https://www.semanticscholar.org/paper/66040e6c4261a9bdc9847c429440995105cdd2da", "title": "Framework for web application domain knowledge extraction", "abstract": "A decade ago a web application e-Student was built with aim to provide electronic support for student enrolment and examination/alumni records management at the University of Ljubljana. Due to issues emerging from the Bologna reform a new e-Student is to be build using a modern technology in the near future. The old e-Student encapsulates a huge amount of domain knowledge. Unfortunately, it was developed using agile approach resulting in poor technical documentation, thus an alternative approach for the domain knowledge extraction has to be defined. In the paper a framework for an effective web application domain knowledge extraction is defined. It has five elements. The main principles (1) of extraction are defined to perform effective reengineering of different application views at a defined abstract level. A proper knowledge representation using diverse models (2) has to be determined next, and the Model Driven Architecture using UML models is considered a suitable choice. The procedure (3) for extraction has to be defined using appropriate (usually custom made) tools (4) and performed by skilled staff (5), possibly members of the old development team. The use of framework is demonstrated on the web application e-Student outlining several custom made tools, the results and the most valuable lessons learnt.", "venue": "International Convention on Information and Communication Technology, Electronics and Microelectronics", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "91fe19171bdb6925785d8b01a7d06d6bebb01f9b", "url": "https://www.semanticscholar.org/paper/91fe19171bdb6925785d8b01a7d06d6bebb01f9b", "title": "Parameterized Contrast in Second Order Soft Co-occurrences: A Novel Text Representation Technique in Text Mining and Knowledge Extraction", "abstract": "In this article, we present a novel statistical representation method for knowledge extraction from a corpus containing short texts. Then we introduce the contrast parameter which could be adjusted for targeting different conceptual levels in text mining and knowledge extraction. The method is based on second order co-occurrence vectors whose efficiency for representing meaning has been established in many applications, especially for representing word senses in different contexts and for disambiguation purposes. We evaluate our method on two tasks: classification of textual description of dreams, and classification of medical abstracts for systematic reviews.", "venue": "2009 IEEE International Conference on Data Mining Workshops", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "c3deb9745320563d5060e568aeb294469f6582f6", "url": "https://www.semanticscholar.org/paper/c3deb9745320563d5060e568aeb294469f6582f6", "title": "Knowledge Extraction from Structured Engineering Drawings", "abstract": "As a typical type of structured documents, table drawings are widely used in engineering fields. Knowledge extraction of such structured documents plays an important role in automatic interpretation systems. In this paper, we propose a new knowledge extraction method based on automatically analyzing drawing layout and extracting physical or logical structures from the given engineering table drawings. Then based on the automatic interpretation results, we further propose normalization method to integrate varied types of engineering tables with other engineering drawings and extract implied domain knowledge.", "venue": "2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "60269cd30224d2b75b202822052bb2bbff635f8a", "url": "https://www.semanticscholar.org/paper/60269cd30224d2b75b202822052bb2bbff635f8a", "title": "Knowledge extraction from human motion", "abstract": "Observing people is currently one of the most active application areas in computer vision. This strong interest is driven by a wide spectrum of promising applications in many areas such as virtual reality, smart surveillance, perceptual interface, etc [13]. This paper presents the concept of knowledge extraction from single human motion via a fixed camera in an enclosed environment in order to mine some movement attributes. We propose a framework based on five mining tools. The five measurements are extracting pixel coverage of a particular object, time domain, frequency distribution of pixels of interest, distances crossed in each frame and considering the object velocity. We assume that, taking into account the measurements mentioned above will introduce a robust knowledge extraction approach.", "venue": "2008 International Conference on Computer and Communication Engineering", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "ddbf8a207b3dde78881a1d20a27a5ea49d20e453", "url": "https://www.semanticscholar.org/paper/ddbf8a207b3dde78881a1d20a27a5ea49d20e453", "title": "Application of Genetic Algorithm and Rough Set Theory for Knowledge Extraction", "abstract": "This paper proposes a hybrid approach using the rough set theory and genetic algorithm (RS-GA) for knowledge extraction as one part of a substation level decision support system. The technique involved a process which learns and extracts knowledge from a set of events into a form of rules to identify the most probable faulted section in a network. Numerous case studies performed on a simulated distribution network [1] that consists of several relays models [2] using PSCAD/EMTDC have revealed the usefulness of the proposed technique for fault diagnosis. The test results demonstrated that the extracted rules are capable of identifying and isolating the faulted section and hence improve the outage response time.", "venue": "2007 IEEE Lausanne Power Tech", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4bd85ca5d9608b55624f9f5f3a9b191848c4d89f", "url": "https://www.semanticscholar.org/paper/4bd85ca5d9608b55624f9f5f3a9b191848c4d89f", "title": "Deep Logic Networks: Inserting and Extracting Knowledge From Deep Belief Networks", "abstract": "Developments in deep learning have seen the use of layerwise unsupervised learning combined with supervised learning for fine-tuning. With this layerwise approach, a deep network can be seen as a more modular system that lends itself well to learning representations. In this paper, we investigate whether such modularity can be useful to the insertion of background knowledge into deep networks, whether it can improve learning performance when it is available, and to the extraction of knowledge from trained deep networks, and whether it can offer a better understanding of the representations learned by such networks. To this end, we use a simple symbolic language\u2014a set of logical rules that we call confidence rules\u2014and show that it is suitable for the representation of quantitative reasoning in deep networks. We show by knowledge extraction that confidence rules can offer a low-cost representation for layerwise networks (or restricted Boltzmann machines). We also show that layerwise extraction can produce an improvement in the accuracy of deep belief networks. Furthermore, the proposed symbolic characterization of deep networks provides a novel method for the insertion of prior knowledge and training of deep networks. With the use of this method, a deep neural\u2013symbolic system is proposed and evaluated, with the experimental results indicating that modularity through the use of confidence rules and knowledge insertion can be beneficial to network performance.", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "citationCount": 85, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "6d355e6fc39865a1bc00b109719d72a981aeb494", "url": "https://www.semanticscholar.org/paper/6d355e6fc39865a1bc00b109719d72a981aeb494", "title": "Knowledge extraction from hierarchical fuzzy model obtained by fuzzy neural networks and genetic algorithm", "abstract": "Knowledge extraction from trained artificial neural networks has been studied by many researchers. Modeling of nonlinear systems using fuzzy neural networks (FNNs) is a promising approach to the knowledge acquisition, and a FNN is specially designed for knowledge extraction. The authors have proposed a hierarchical fuzzy modeling method using FNNs and a genetic algorithm (GA). This method can identify fuzzy models of nonlinear objects with strong nonlinearity. The disadvantage of the method is that the training of the FNN is time consuming. This paper presents a quick method for a rough search for proper structures in the antecedent of fuzzy models. The fine tuning of the acquired rough model is done by the FNN. This modeling method is quite efficient to identify precise fuzzy models of systems with strong nonlinearities. A simulation is done to show the effectiveness of the proposed method.", "venue": "International Conference on Neural Networks", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "7b5949433e482dec0acb9b6aa24a92b26417e916", "url": "https://www.semanticscholar.org/paper/7b5949433e482dec0acb9b6aa24a92b26417e916", "title": "Pertinent Knowledge Extraction from a Semantic Network: Application of Fuzzy Sets Theory", "abstract": "In this paper we describe an approach to define flexible Knowledge Extraction Systems able to deal with the inherent vagueness and uncertainty of the Extraction process. We also present a short survey of fuzzy and semantic approaches to knowledge extraction. The goal of such approaches is to address if and how some approaches met their goal.", "venue": "Int. J. Artif. Intell. Tools", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "c83352aeefffa5db36f5bad7944e8e0fd13eb01c", "url": "https://www.semanticscholar.org/paper/c83352aeefffa5db36f5bad7944e8e0fd13eb01c", "title": "HILDA: knowledge extraction from neural networks in legal rule based and case based reasoning", "abstract": "A major requirement for legal expert systems involved in generating legal advice and purporting to adjudicate on disputes is that they explain their reasoning. Even systems involved in predicting the outcomes of legal disputes are enhanced by this facility. Difficulties in extracting knowledge from neural networks (\"NNs\") have made their application to legal expert systems somewhat limited. HILDA incorporates some aspects of rule based reasoning (\"RBR\") and case based reasoning (\"CBR\") to assist the user in predicting case outcomes and generating arguments and case decisions. The system can use the NN to guide RBR and CBR in a number of ways. Knowledge extracted from a NN could also be used to iteratively refine the system's domain theory. This refined domain theory is one way in which HILDA can carry out RBR and CBR.", "venue": "Proceedings of ICNN'95 - International Conference on Neural Networks", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "c88ccba0cd963569e535b44e1c29071c93f92421", "url": "https://www.semanticscholar.org/paper/c88ccba0cd963569e535b44e1c29071c93f92421", "title": "Sub-symbolic knowledge extraction environment for teaching process assistance", "abstract": "We aim with this research is to build an integrated environment to serve as an assistant in the educational process. When we deal with unstructured knowledge, getting of useful information for the teaching process is very difficult. Neural networks can store subsymbolic knowledge, but until recently it was believed to be only in a \"black-box\" format. Knowledge extraction from NNs is a relatively new field, which tries to reduce these disadvantages and build a bridge between subsymbolic and symbolic knowledge. As the teaching process requires only symbolic knowledge, we believe this to be a chance for teachers to significantly improve their teaching materials and/or style by combining the symbolic knowledge of the domain theory with the rules extracted from the empirical subsymbolic knowledge stored in NNs trained on examples. Therefore, we developed a neural network's subsymbolic knowledge extraction environment for the teaching process assistance and also built a study case of teaching stock exchange developments.", "venue": "1998 Second International Conference. Knowledge-Based Intelligent Electronic Systems. Proceedings KES'98 (Cat. No.98EX111)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "f78ec72f044d23c8c55b7ce935ec34ac2edb8ac5", "url": "https://www.semanticscholar.org/paper/f78ec72f044d23c8c55b7ce935ec34ac2edb8ac5", "title": "Automatic knowledge extraction from OCR documents using hierarchical document analysis", "abstract": "Industries can improve their business efficiency by analyzing and extracting relevant knowledge from large numbers of documents. Knowledge extraction manually from large volume of documents is labor intensive, unscalable and challenging. Consequently there have been a number of attempts to develop intelligent systems to automatically extract relevant knowledge from OCR documents. Moreover, the automatic system can improve the capability of search engine by providing application-specific domain knowledge. However, extracting the efficient information from OCR documents is challenging due to highly unstructured format [1, 11, 18, 26]. In this paper, we propose an efficient framework for a knowledge extraction system that takes keywords based queries and automatically extracts their most relevant knowledge from OCR documents by using text mining techniques. The framework can provide relevance ranking of knowledge to a given query. We tested the proposed framework on corpus of documents at GE Power where document consists of more than hundred pages in PDF.", "venue": "Research in Adaptive and Convergent Systems", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "aa15ef60c2fb0164f06b650187b8e9b5592602d1", "url": "https://www.semanticscholar.org/paper/aa15ef60c2fb0164f06b650187b8e9b5592602d1", "title": "Methodology for Knowledge Extraction from Trained Artificial Neural Networks", "abstract": "Artificial neural networks are widely spread models that outperform more basic, but explainable machine learning models like classification decision tree. Although their lack of explainability severely limits their area of application. All mission critical areas or law regulated areas (like European GDPR) require model to be explained. Explainability allows model validation for correctness and lack of bias. Thus methods for knowledge extraction from artificial neural networks have gained attention and development efforts. Current paper addresses this problem and describes knowledge extraction methodology which can be applied to classification problems. It is based on previous research and allows knowledge to be extracted from trained fully connected feed-forward artificial neural network, from radial basis function neural network and from hyper-polytope based classifier in the form of binary classification decision tree, elliptical rules and If-Then rules.", "venue": "Information Technology and Management Science", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "a5f964d87101b52cdaf52e612b5f01353c4874de", "url": "https://www.semanticscholar.org/paper/a5f964d87101b52cdaf52e612b5f01353c4874de", "title": "Hyperspectral Imagery Semantic Interpretation Based on Adaptive Constrained Band Selection and Knowledge Extraction Techniques", "abstract": "In this paper, we propose a novel adaptive band selection approach for hyperspectral image semantic interpretation. This approach is based on constrained band selection (CBS) method and extracted knowledge coming from tensor locality preserving projection. The extracted knowledge is presented as a set of rules which are used to evaluate the importance of spectral bands for classes discrimination. Based on these extracted rules and the CBS approach, relevant bands are selected to enhance the hyperspectral image semantic interpretation. The main advantage of the proposed adaptive band selection approach is to allow the automatic selection of discriminant, distinctive and informative spectral bands, and improve the semantic interpretation of hyperspectral images. Experimental results on real images show that the proposed band selection approach reaches competitive good performances, in terms of classification accuracy.", "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "583159b66af823fc91c1113c68896d3c7645a4a1", "url": "https://www.semanticscholar.org/paper/583159b66af823fc91c1113c68896d3c7645a4a1", "title": "AKECP: Adaptive Knowledge Extraction from Feature Maps for Fast and Efficient Channel Pruning", "abstract": "Pruning can remove redundant parameters and structures of Deep Neural Networks (DNNs) to reduce inference time and memory overhead. As an important component of neural networks, the feature map (FM) has stated to be adopted for network pruning. However, the majority of FM-based pruning methods do not fully investigate effective knowledge in the FM for pruning. In addition, it is challenging to design a robust pruning criterion with a small number of images and achieve parallel pruning due to the variability of FMs. In this paper, we propose Adaptive Knowledge Extraction for Channel Pruning (AKECP), which can compress the network fast and efficiently. In AKECP, we first investigate the characteristics of FMs and extract effective knowledge with an adaptive scheme. Secondly, we formulate the effective knowledge of FMs to measure the importance of corresponding network channels. Thirdly, thanks to the effective knowledge extraction, AKECP can efficiently and simultaneously prune all the layers with extremely few or even one image. Experimental results show that our method can compress various networks on different datasets without introducing additional constraints, and it has advanced the state-of-the-arts. Notably, for ResNet-110 on CIFAR-10, AKECP achieves 59.9% of parameters and 59.8% of FLOPs reduction with negligible accuracy loss. For ResNet-50 on ImageNet, AKECP saves 40.5% of memory footprint and reduces 44.1% of FLOPs with only 0.32% of Top-1 accuracy drop.", "venue": "ACM Multimedia", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4e49a590cb119f0707d643fb1157dfdb33f6ef0d", "url": "https://www.semanticscholar.org/paper/4e49a590cb119f0707d643fb1157dfdb33f6ef0d", "title": "Conceptualisation and Annotation of Drug Nonadherence Information for Knowledge Extraction from Patient-Generated Texts", "abstract": "Approaches to knowledge extraction (KE) in the health domain often start by annotating text to indicate the knowledge to be extracted, and then use the annotated text to train systems to perform the KE. This may work for annotat- ing named entities or other contiguous noun phrases (drugs, some drug effects), but be- comes increasingly difficult when items tend to be expressed across multiple, possibly non- contiguous, syntactic constituents (e.g. most descriptions of drug effects in user-generated text). Other issues include that it is not al- ways clear how annotations map to actionable insights, or how they scale up to, or can form part of, more complex KE tasks. This paper reports our efforts in developing an approach to extracting knowledge about drug nonadher- ence from health forums which led us to con- clude that development cannot proceed in sep- arate steps but that all aspects\u2014from concep- tualisation to annotation scheme development, annotation, KE system training and knowl- edge graph instantiation\u2014are interdependent and need to be co-developed. Our aim in this paper is two-fold: we describe a generally ap- plicable framework for developing a KE ap- proach, and present a specific KE approach, developed with the framework, for the task of gathering information about antidepressant drug nonadherence. We report the conceptual- isation, the annotation scheme, the annotated corpus, and an analysis of annotated texts.", "venue": "Conference on Empirical Methods in Natural Language Processing", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "48597f5aaebf9cb280df0e2fe6d7723074e0a734", "url": "https://www.semanticscholar.org/paper/48597f5aaebf9cb280df0e2fe6d7723074e0a734", "title": "PharmKE: Knowledge Extraction Platform for Pharmaceutical Texts using Transfer Learning", "abstract": "Even though named entity recognition (NER) has seen tremendous development in recent years, some domain-specific use-cases still require tagging of unique entities, which is not well handled by pre-trained models. Solutions based on enhancing pre-trained models or creating new ones are efficient, but creating reliable labeled training for them to learn on is still challenging. In this paper, we introduce PharmKE, a text analysis platform tailored to the pharmaceutical industry that uses deep learning at several stages to perform an in-depth semantic analysis of relevant publications. The proposed methodology is used to produce reliably labeled datasets leveraging cutting-edge transfer learning, which are later used to train models for specific entity labeling tasks. By building models for the well-known text-processing libraries spaCy and AllenNLP, this technique is used to find Pharmaceutical Organizations and Drugs in texts from the pharmaceutical domain. The PharmKE platform also incorporates the NER findings to resolve co-references of entities and examine the semantic linkages in each phrase, creating a foundation for further text analysis tasks, such as fact extraction and question answering. Additionally, the knowledge graph created by DBpedia Spotlight for a specific pharmaceutical text is expanded using the identified entities. The obtained results with the proposed methodology result in about a 96% F1-score on the NER tasks, which is up to 2% better than those of the fine-tuned BERT and BioBERT models developed using the same dataset. The ultimate benefits of the platform are that pharmaceutical domain specialists may more easily identify the knowledge extracted from the input texts thanks to the platform\u2019s visualization of the model findings. Likewise, the proposed techniques can be integrated into mobile and pervasive systems to give patients more relevant and comprehensive information from scanned medication guides. Similarly, it can provide preliminary insights to patients and even medical personnel on whether a drug from a different vendor is compatible with the patient\u2019s prescription medication.", "venue": "Computers", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "tp"], "mention_counts": {"ke": 2, "tp": 1, "kg": 1}, "nlp_mention_counts": {"ke": 2, "tp": 1}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "65a2d594a595196a4faf066b821112eed05ef1bd", "url": "https://www.semanticscholar.org/paper/65a2d594a595196a4faf066b821112eed05ef1bd", "title": "A reasoning system about knowledge extraction in human-computer interaction", "abstract": "The development of Intelligent Decision Support System is a key application for artificial intelligence technology. And human-computer interaction is the most common action in Intelligent Decision Support System. In the process of human-computer interaction, the input of decision problem and the output of the solution will be converted into the knowledge that can be extracted by machine. Human-computer interaction is a dynamic process, which involves knowledge extraction and sharing between machines and people. In this paper, we establish a Knowledge Extraction System (KES) based on the analysis of the usual knowledge types that are involved in the interaction process. And then we prove its soundness and completeness, explore some properties about reasoning in human-computer interaction. In this system, we can find the interpretation as to how the machine understands human's questions and reasons intelligently. It also reflects how the cognitive agent extracts each other's private knowledge into common knowledge that later becomes its own knowledge during the interactive process.", "venue": "Chinese Control and Decision Conference", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "90afd8fa7ee5583be04812e95b719f12d80b4f84", "url": "https://www.semanticscholar.org/paper/90afd8fa7ee5583be04812e95b719f12d80b4f84", "title": "Medical knowledge extraction scheme for cloudlet-based healthcare system to avoid malicious attacks", "abstract": "The medical information sharing involves information collection, information storage, and sharing of this information. Protection to medical information against malicious attacks is an important concern due to its storage on remote cloud. In current online question answering system for health-related issues, extraction of medical knowledge from the clamorous question- answers pair is a challenge. To overcome these challenges, medical knowledge extraction scheme for cloudlet based healthcare system is proposed. The proposed and developed medical knowledge extraction (MKE) scheme finds valid remedial Triples from clamorous question-answer (Q-A) pairs and evaluate the reliability along with doctor's proficiency using truth discovery method the modified number theory research unit algorithm (NTRU) and collaborative intrusion detection system (CIDS) is used to avoid and detect malicious attacks. The response time of the proposed system is 6 to 8 seconds which results in a substantial reduction in time and cost for the end-user.", "venue": "International Journal of Cloud Computing", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "c54c81182aa7fb3dc38459c26c94287ba013ed4b", "url": "https://www.semanticscholar.org/paper/c54c81182aa7fb3dc38459c26c94287ba013ed4b", "title": "Knowledge extraction for assisted curation of summaries of bacterial transcription factor properties", "abstract": "Abstract Transcription factors (TFs) play a main role in transcriptional regulation of bacteria, as they regulate transcription of the genetic information encoded in DNA. Thus, the curation of the properties of these regulatory proteins is essential for a better understanding of transcriptional regulation. However, traditional manual curation of article collections to compile descriptions of TF properties takes significant time and effort due to the overwhelming amount of biomedical literature, which increases every day. The development of automatic approaches for knowledge extraction to assist curation is therefore critical. Here, we show an effective approach for knowledge extraction to assist curation of summaries describing bacterial TF properties based on an automatic text summarization strategy. We were able to recover automatically a median 77% of the knowledge contained in manual summaries describing properties of 177 TFs of Escherichia coli K-12 by processing 5961 scientific articles. For 71% of the TFs, our approach extracted new knowledge that can be used to expand manual descriptions. Furthermore, as we trained our predictive model with manual summaries of E. coli, we also generated summaries for 185 TFs of Salmonella enterica serovar Typhimurium from 3498 articles. According to the manual curation of 10 of these Salmonella typhimurium summaries, 96% of their sentences contained relevant knowledge. Our results demonstrate the feasibility to assist manual curation to expand manual summaries with new knowledge automatically extracted and to create new summaries of bacteria for which these curation efforts do not exist. Database URL: The automatic summaries of the TFs of E. coli and Salmonella and the automatic summarizer are available in GitHub (https://github.com/laigen-unam/tf-properties-summarizer.git).", "venue": "Database J. Biol. Databases Curation", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "c404755bedf4209f42e258610d32059e5e0b35d7", "url": "https://www.semanticscholar.org/paper/c404755bedf4209f42e258610d32059e5e0b35d7", "title": "Knowledge Extraction Using Image Features", "abstract": "A feature-based picture knowledge extraction methodology is developed in this research in order to acquire a more completed and systematic search result via the target picture. In the proposed methodology, as the target picture from the knowledge acquirer is obtained, a set of prototype pictures highly correlated with the target picture can be determined based on the image features of the target picture. Then the knowledge regarding the extracted prototype pictures are integrated to represent the knowledge of the target picture. The verification results demonstrate that the developed system can be applied to real cases to effectively assist picture knowledge extraction of distinct application domains.", "venue": "Int. J. Electron. Bus. Manag.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "6229c16188aa99d83f0afe7732a381daecb74b51", "url": "https://www.semanticscholar.org/paper/6229c16188aa99d83f0afe7732a381daecb74b51", "title": "Relevance Feedback for Goal's Extraction from Fuzzy Semantic Networks", "abstract": "In this paper we present a short survey of fuzzy a nd Semantic approaches to Knowledge Extraction. The goal of such approaches is to define flexible Knowl edge Extraction Systems able to deal with the inher ent vagueness and uncertainty of the Extraction process . It has long been recognised that interactivity im proves the effectiveness of Knowledge Extraction systems. Novice users queries is the most natural and interactiv e medium of communication and recent progress in recognitio n is making it possible to build systems that inter act with the user. However, given the typical novice users queri es submitted to Knowledge Extraction systems, it is easy to imagine that the effects of goal recognition errors in novice user\u2019s queries must be severely destruct ive on the system\u2019s effectiveness. The experimental work repor ted in this paper shows that the use of classical K nowledge Extraction techniques for novice user\u2019s query proce ssing is robust to considerably high levels of goal recognition errors. Moreover, both standard relevance feedback and pseudo relevance feedback can be effectively employed to improve the effectiveness of novice user\u2019s query processing.", "venue": "ArXiv", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "0230c06c771bb6411614d685432ef076927e66f4", "url": "https://www.semanticscholar.org/paper/0230c06c771bb6411614d685432ef076927e66f4", "title": "Extracting Biomedical Factual Knowledge Using Pretrained Language Model and Electronic Health Record Context", "abstract": "Language Models (LMs) have performed well on biomedical natural language processing applications. In this study, we conducted some experiments to use prompt methods to extract knowledge from LMs as new knowledge Bases (LMs as KBs). However, prompting can only be used as a low bound for knowledge extraction, and perform particularly poorly on biomedical domain KBs. In order to make LMs as KBs more in line with the actual application scenarios of the biomedical domain, we specifically add EHR notes as context to the prompt to improve the low bound in the biomedical domain. We design and validate a series of experiments for our Dynamic-Context-BioLAMA task. Our experiments show that the knowledge possessed by those language models can distinguish the correct knowledge from the noise knowledge in the EHR notes, and such distinguishing ability can also be used as a new metric to evaluate the amount of knowledge possessed by the model.", "venue": "ArXiv", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "nlp", "kg"], "mention_counts": {"nlp": 1, "ke": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "7a4e33214fa35b444ac3594343bdfe69bb0c7536", "url": "https://www.semanticscholar.org/paper/7a4e33214fa35b444ac3594343bdfe69bb0c7536", "title": "Dual Track Multimodal Automatic Learning through Human-Robot Interaction", "abstract": "Human beings are constantly improving their cognitive ability via automatic learning from the interaction with the environment. Two important aspects of automatic learning are the visual perception and knowledge acquisition. The fusion of these two aspects is vital for improving the intelligence and interaction performance of robots. Many automatic knowledge extraction and recognition methods have been widely studied. However, little work focuses on integrating automatic knowledge extraction and recognition into a unified framework to enable jointly visual perception and knowledge acquisition. To solve this problem, we propose a Dual Track Multimodal Automatic Learning (DTMAL) system, which consists of two components: Hybrid Incremental Learning (HIL) from the vision track and Multimodal Knowledge Extraction (MKE) from the knowledge track. HIL can incrementally improve recognition ability of the system by learning new object samples and new object concepts. MKE is capable of constructing and updating the multimodal knowledge items based on the recognized new objects from HIL and other knowledge by exploring the multimodal signals. The fusion of the two tracks is a mutual promotion process and jointly devote to the dual track learning. We have conducted the experiments through human-machine interaction and the experimental results validated the effectiveness of our proposed system.", "venue": "IJCAI", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "245a23daee7ebb4ad2c83da7526522b96c862ce0", "url": "https://www.semanticscholar.org/paper/245a23daee7ebb4ad2c83da7526522b96c862ce0", "title": "A medical diagnosis support system based on automatic knowledge extraction from databases through differential evolution", "abstract": "An intelligent system for supporting medical diagnosis is presented in this paper. The system automatically extracts knowledge from databases as sets of IF-THEN rules. The approach chosen to fulfil this task is based on the differential evolution DE algorithm and its implementation results in a tool called DEREx. This tool is aimed at supporting clinicians in their decision making in the diagnostic process, by providing them with clear explanations on the reasons why each item is assigned to a given class. Performance of the tool has been evaluated over seven medical databases and compared against that of fifteen well-known classification tools. Numerical results in terms of classification accuracy and their statistical analysis, have evidenced the effectiveness of the proposed approach, so DEREx is preferable because of its added value, i.e. the knowledge extracted automatically and provided to users in an easily comprehensible form.", "venue": "Int. J. Data Min. Bioinform.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "aae5d379820447cb6c45a490c224dda6c41263fe", "url": "https://www.semanticscholar.org/paper/aae5d379820447cb6c45a490c224dda6c41263fe", "title": "CHCI: A Crowdsourcing Human-computer Interaction Framework for Cultural Heritage Knowledge", "abstract": "This paper aims to extract knowledge including entities and relationships, from multi-source heterogeneous cultural heritage (CH) resources. The proposed crowdsourcing human-computer interaction framework utilizes museum-user-algorithm cooperation to achieve high-quality and scalable CH knowledge extraction. This paper also proposes crowdsourcing optimization mechanisms to improve participation and quality of crowdsourcing project. Finally, this paper discusses how extracted knowledge can support CH digital resource construction and knowledge-driven intelligent applications in Museum.", "venue": "ACM/IEEE Joint Conference on Digital Libraries", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4123075b5d2232b154be457c3493321182d8fa2e", "url": "https://www.semanticscholar.org/paper/4123075b5d2232b154be457c3493321182d8fa2e", "title": "Controlling Complexity and Accuracy of Classification Decision Tree Extracted from Trained Artificial Neural Network", "abstract": "There is growing number of publications devoted to knowledge extraction from fully connected feed-forward artificial neural networks. Although there are not many publications covering ways allowing to control extracted knowledge complexity and precision. The higher complexity is, the higher accuracy can be gained. But in case ANN should be validated by domain expert or just be explainable it should be simple enough - this will lower accuracy of extracted knowledge. The current paper explores influence of parameters used for ANN pruning and neurons outputs discretization and clustering onto accuracy of extracted classification decision tree. Hence reader is presented with experimental validation of effects produced by variation in parameters combination.", "venue": "2019 60th International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "711966fb0cbc93285fa907726239ad0d94ad6d62", "url": "https://www.semanticscholar.org/paper/711966fb0cbc93285fa907726239ad0d94ad6d62", "title": "Extracting heuristically acceptable information from fuzzy/neural architectures via heuristic constraint enforcement. I. Foundation", "abstract": "Knowledge extraction from systems where the existing knowledge is limited is a difficult task. Using fuzzy/neural architectures to extract heuristic information from systems has received increasing attention. In most cases, using output error measures to validate extracted knowledge is not sufficient; extracted knowledge may not make heuristic sense even if the output error may meet the specified criterion. Using the principles of set theoretic estimation, the paper proposes a method for enforcing heuristic constraints on the membership functions of fuzzy/neural architectures. The proposed method ensures that the final membership functions conform to a priori heuristic knowledge. Although the method is described on a specific fuzzy/neural architecture, it is applicable to other realizations of fuzzy inference systems including adaptive or static implementations. The organized yet flexible characteristic of the heuristic constraint enforcement method enables its application to a wide range of problems.", "venue": "1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36227)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "a45cd816a5151e29ea22dfaf31f6482ea9387b86", "url": "https://www.semanticscholar.org/paper/a45cd816a5151e29ea22dfaf31f6482ea9387b86", "title": "A Sampling-Based Framework for Transductive Classification in Information Networks", "abstract": "Knowledge extraction from large information networks has received increasing attention in recent years. Among existing methods for knowledge extraction, transductive classification is a well-known semi-supervised learning method, where both labeled and unlabeled vertices are used in the learning process. However, transductive classification tasks become impractical in large information networks and the use of network sampling techniques in the transductive classification setting is not a trivial task, since it is required that all the vertices of the original network be classified during the transductive learning \u2013 and not only the vertices of the sample. In this paper, we present a framework called TCSN (Transductive Classification for Sampled Networks). TCSN allows the use of various network sampling techniques, as well as enables the use of various methods of transductive classification for information networks. We present a variation of the Chernoff Bounds method to calculate the minimum size of a sampled network, thereby bounding sampling error within a pre-specified tolerance level. Moreover, TCSN extends the concept of evidence accumulation to combine the results of several rounds of transductive classification into a final classification. Experimental results from different information networks reveals that TCSN statistically outperformed the classification performance in the whole original network. These promising results show that the TCSN enables transductive classification in large information networks without loss of quality in the knowledge extraction process.", "venue": "2019 8th Brazilian Conference on Intelligent Systems (BRACIS)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "1772dde69bce9910a62442aa418e04d45a1dada9", "url": "https://www.semanticscholar.org/paper/1772dde69bce9910a62442aa418e04d45a1dada9", "title": "1+1>2: Programming Know-What and Know-How Knowledge Fusion, Semantic Enrichment and Coherent Application", "abstract": "\u2014Software programming requires both API reference (know-what) knowledge and programming task (know-how) knowledge. Lots of programming know-what and know-how knowledge is documented in text, for example, API reference documentation and programming tutorials. To improve knowledge accessibility and usage, several recent studies use Natural Language Processing (NLP) methods to construct API know-what knowledge graph (API-KG) and programming task know-how knowledge graph (Task-KG) from software documentation. Although being promising, current API-KG and Task-KG are independent of each other, and thus are void of inherent connections between the two types of knowledge. Our empirical study on Stack Over\ufb02ow questions con\ufb01rms that only 36% of the API usage problems can be answered by the know-how or the know-what knowledge alone, while the rest questions requires a fusion of both. Inspired by this observation, we make the \ufb01rst attempt to fuse API-KG and Task-KG by API entity linking. This fusion creates nine categories of API semantic relations and two types of task semantic relations which are not present in the stand-alone API-KG or Task-KG. According to the de\ufb01nitions of these new API and task semantic relations, our approach dives deeper than surface-level API linking of API-KG and Task-KG, and infer nine categories of API semantic relations from task descriptions and two types of task semantic relations with the assistance of API-KG, which enrich the declaration or syntactic relations in the current API-KG and Task-KG. Our fused and semantically-enriched API-Task KG supports coherent API/Task-centric knowledge search by text or code queries. We have implemented our approach on Java programming documentation and built a web tool to search and explore API and programming task knowledge. Our evaluation con\ufb01rms the high-accuracy of our knowledge extraction, fusion and enrichment methods, and the effectiveness and usefulness of our API-Task KG for answering Stack Over\ufb02ow questions.", "venue": "IEEE Transactions on Services Computing", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "kg", "nlp", "nlp"], "mention_counts": {"nlp": 2, "ke": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"ke": 1, "kg": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "2b4a9bab3474f7695c10d1aa276ccaa7b5f6cebb", "url": "https://www.semanticscholar.org/paper/2b4a9bab3474f7695c10d1aa276ccaa7b5f6cebb", "title": "Mobile based data retrieval using RDF and NLP in an efficient approach", "abstract": "Nowadays Youngsters like IT people and various sectors are showing massive interest and moving towards agriculture farming for self sustainable energy for our generation. People are in metro environment with family owned lands they are in need of proper guidance from farmers having huge experience But there is a gap in medium in approaching the farmers by staying with them for months together to get Knowledge Transfer for facing issues and clarifications. The farmers and ancestors traditional methods and subject knowledge need to be transferred to upcoming generations. In Digital world, farmers are using smart phones so to document the knowledge from farmers the Subject Matter Expertise of agriculture (SME) under database and to provide solution by queries in efficient approach through RDF (Resource Description Framework) and Natural Language Processing Technique (NLP) in the form of mobile application internally using SPARQL queries. In this paper, WordNet text mining processing method is used.", "venue": "2017 Third International Conference on Science Technology Engineering & Management (ICONSTEM)", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "rdf", "nlp", "rdf", "rdf"], "mention_counts": {"nlp": 3, "rdf": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"rdf": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "9d2c55d6132f3c7f438f9427a64a88ee042616af", "url": "https://www.semanticscholar.org/paper/9d2c55d6132f3c7f438f9427a64a88ee042616af", "title": "G\u00e9n\u00e9ration de phrase : entr\u00e9e, algorithmes et applications (Sentence Generation: Input, Algorithms and Applications)", "abstract": "Joint work with Paul Bedaride, Eric Kow, Shashi Narayan and Laura Perez-Beltrachini) Sentence Generation maps abstract linguistic representations into sentences. A necessary part of any natural language generation system, sentence generation has also recently received increasing attention in applications such as transfer based machine translation (cf. the LOGON project) and natural language interfaces to knowledge bases (e.g., to verbalise, to author and/or to query ontologies). One outstanding issue in Sentence Generation is what it starts from. What is the abstract linguistic representation it generates from? In my talk, I will explore sentence generation from two main input formats (flat semantic formulae and dependency structures) and discuss their impact on efficiency, algorithms and applications. I will start by describing an algorithm that generates from flat semantic formulae, explain why it is computationally intractable and presenting ways of optimising it to make it usable in practice. I will then show how this algorithm can be used to generate paraphrases; to support error mining and to generate teaching material for language learners from an ontology. In the second part of the talk, I will focus on generation from dependency structures. Based on the input data recently made available by the Generation Challenges Surface Realisation Shared Task, I will show how the algorithm previously used to generate from flat semantic formulae can be adapted to generate from dependency structures. I will moreover discuss various issues raised by the GenChal data such as, missing lexical entries and mismatches between dependency and grammar structures. Bio of Claire Gardent Claire Gardent is a senior researcher at the French National Center for Scientific Research (CNRS). She graduated in linguistics at the University of Toulouse in 1986, received an MSc in Artificial Intelligence from the University of Essex in 1987 and defended a PhD in Cognitive Science at the University of Edinburgh in 1991. From 1991 to 2000, she worked as a reseacher at the Universities of Utrecht and Amsterdam (The Netherlands), Clermont-Ferrand and Sarrebruecken (Germany). Since 2001 she has been working for the CNRS at the Lorraine Laboratory for Research in Computer Science (LORIA) in Nancy, France. Claire Gardent's research focuses on the computational treatment of natural language meaning. She has worked on the automatic acquisition of lexical resources for French, on syntactic parsing and semantic role labelling and on text generation. Recently, she has become interested in in exploring the interaction between virtual worlds and natural language processing. Claire Gardent has published a textbook on analysis and generation (with Karine Baschung) and about 100 articles in journals and conference proceedings. She has been nominated Chair of the European Chapter for the Association of Computational Linguistics (EACL), editor in chief of the french journal \"Traitement Automatique des Langues\" and member of the editorial board of the journals \"Computational Linguistics\", \"Journal of Semantics\". Each year she is on the programme committee of half a dozen international conferences or workshops, she also acted as scientific chair for various international conferences (EACL), workshops (TAG+, ENLG, DIALOR, SIGDIAL) and summer schools (ESSLLI).", "venue": "JEPTALNRECITAL", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlg", "kg", "nlp", "mt", "onto"], "mention_counts": {"onto": 2, "nlp": 1, "nlg": 1, "mt": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "nlg": 1, "mt": 1}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "a4571e19a53a9ee571e66a710a2bcc69ede5dff1", "url": "https://www.semanticscholar.org/paper/a4571e19a53a9ee571e66a710a2bcc69ede5dff1", "title": "A multilingual ontology matcher", "abstract": "State-of-the-art multilingual ontology matchers use machine translation to reduce the problem to the monolingual case. We investigate an alternative, self-contained solution based on semantic matching where labels are parsed by multilingual natural language processing and then matched using a language-independent knowledge base acting as an interlingua. As the method relies on the availability of domain vocabularies in the languages supported, matching and vocabulary enrichment become joint, mutually reinforcing tasks. In particular, we propose a vocabulary enrichment method that uses the matcher\u2019s output to detect and generate missing items semi-automatically. Vocabularies developed in this manner can then be reused for other domain-specific natural language understanding tasks.", "venue": "OM", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "nlp", "nlu", "onto", "kg"], "mention_counts": {"onto": 2, "nlu": 1, "nlp": 1, "mt": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "nlu": 1, "mt": 1}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "6c086a542ccac6fa52089513676924ab9256b733", "url": "https://www.semanticscholar.org/paper/6c086a542ccac6fa52089513676924ab9256b733", "title": "Representation of linguistic and domain knowledge for second language learning in virtual worlds", "abstract": "There has been much debate, both theoretical and practical, on how to link ontologies and lexicons in natural language processing (NLP) applications. In this paper, we focus on an application in which lexicon and ontology are used to generate teaching material. We briefly describe the application (a serious game for language learning). We then zoom in on the representation and interlinking of the lexicon and of the ontology. We show how the use of existing standards and of good practice principles facilitates the design of our resources while satisfying the expressivity requirements set by natural language generation.", "venue": "LREC", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "nlg": 1, "onto": 3}, "nlp_mention_counts": {"nlp": 2, "nlg": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "9c0864fb89db95b43824aab93d666136b4ef4730", "url": "https://www.semanticscholar.org/paper/9c0864fb89db95b43824aab93d666136b4ef4730", "title": "Reflections on KOS Based Data Alignment", "abstract": "This paper briefly reviews two contrasting case studies by the authors on semantic data integration within the archaeology field and reflects on some of the key issues encountered, relevant to the NKOS Workshop theme on strategies for alignment of metadata to KOS linked data. Both projects involved diverse datasets with different schema and which employed different terminology. Both projects combined datasets with information extracted from archaeological reports via natural language processing (NLP). In both cases, the semantic framework that afforded data integration was a combination of metadata element sets organised by an ontology with a relevant value vocabulary (eg thesauri and term lists). Ontologies and value vocabularies have been seen as complementary resources for this purpose [1, 2].", "venue": "NKOS@TPDL", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "ie", "onto", "ld"], "mention_counts": {"ld": 1, "nlp": 2, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"ld": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "1edf4d90c9ba9a9b29b429ab5838998bda6be07f", "url": "https://www.semanticscholar.org/paper/1edf4d90c9ba9a9b29b429ab5838998bda6be07f", "title": "Open-source Tools for Creation, Maintenance, and Storage of Lexical Resources for Language Generation from Ontologies", "abstract": "This paper describes reusable, open-source tools for creation, maintenance, storage, and access of Language Resources (LR) needed for generating natural language texts from ontologies. One advantage of these tools is that they provide a user-friendly interface for NLG LR manipulation. They also provide unified models for accessing NLG lexicons and mappings between lexicons and ontologies.", "venue": "LREC", "citationCount": 35, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "nlg", "onto", "onto", "nlg"], "mention_counts": {"nlg": 3, "onto": 3}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "f91c9a379b35455b6042793e9037d58e1ecdd7d9", "url": "https://www.semanticscholar.org/paper/f91c9a379b35455b6042793e9037d58e1ecdd7d9", "title": "A general ontology based multi-lingual multi-function multimedia intelligent system", "abstract": "A large variety of information processing applications deal with natural language texts. Many of these applications require extracting and processing the meanings of natural language texts, in addition to processing their syntactic forms. In order to extract meanings from texts and manipulate them, a natural language processing system must have a significant amount of knowledge about the world and the domain of discourse. However, different kinds of knowledge are required for different languages, for different functions, and for different media. We design a new general ontology based natural language processing system referred to as the multi-lingual multi-function multimedia intelligent system (MMM-IS). MMM-IS is a complex system with multiple functions that can deal with multiple languages and multiple media. The general ontology consists of a static layer and a dynamic layer, and provides a real world knowledge model for the natural language processing system. The paper presents the concept of MMM-IS and how to build MMM-IS for real applications.", "venue": "Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "5e579ba17a674b8dbfdf7670bbcd2784512095b9", "url": "https://www.semanticscholar.org/paper/5e579ba17a674b8dbfdf7670bbcd2784512095b9", "title": "Ontology-Based Approach to Semantically Enhanced Question Answering for Closed Domain: A Review", "abstract": "For many users of natural language processing (NLP), it can be challenging to obtain concise, accurate and precise answers to a question. Systems such as question answering (QA) enable users to ask questions and receive feedback in the form of quick answers to questions posed in natural language, rather than in the form of lists of documents delivered by search engines. This task is challenging and involves complex semantic annotation and knowledge representation. This study reviews the literature detailing ontology-based methods that semantically enhance QA for a closed domain, by presenting a literature review of the relevant studies published between 2000 and 2020. The review reports that 83 of the 124 papers considered acknowledge the QA approach, and recommend its development and evaluation using different methods. These methods are evaluated according to accuracy, precision, and recall. An ontological approach to semantically enhancing QA is found to be adopted in a limited way, as many of the studies reviewed concentrated instead on NLP and information retrieval (IR) processing. While the majority of the studies reviewed focus on open domains, this study investigates the closed domain.", "venue": "Inf.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 3, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "582089a00a6c9fb534f16d1dbbafc50cc4e3912a", "url": "https://www.semanticscholar.org/paper/582089a00a6c9fb534f16d1dbbafc50cc4e3912a", "title": "Schema Aware Semantic Reasoning for Interpreting Natural Language Queries in Enterprise Settings", "abstract": "Natural Language Query interfaces allow the end-users to access the desired information without the need to know any specialized query language, data storage, or schema details. Even with the recent advances in NLP research space, the state-of-the-art QA systems fall short of understanding implicit intents of real-world Business Intelligence (BI) queries in enterprise systems, since Natural Language Understanding still remains an AI-hard problem. We posit that deploying ontology reasoning over domain semantics can help in achieving better natural language understanding for QA systems. In this paper, we specifically focus on building a Schema Aware Semantic Reasoning Framework that translates natural language interpretation as a sequence of solvable tasks by an ontology reasoner. We apply our framework on top of an ontology based, state-of-the-art natural language question-answering system ATHENA, and experiment with 4 benchmarks focused on BI queries. Our experimental numbers empirically show that the Schema Aware Semantic Reasoning indeed helps in achieving significantly better results for handling BI queries with an average accuracy improvement of ~30%", "venue": "COLING", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlu", "nlp", "nlu"], "mention_counts": {"nlp": 1, "onto": 3, "nlu": 2}, "nlp_mention_counts": {"nlp": 1, "nlu": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "9ffc808f0ad8ef25d59f9b86ba1dc87d86505afb", "url": "https://www.semanticscholar.org/paper/9ffc808f0ad8ef25d59f9b86ba1dc87d86505afb", "title": "Gold-standard ontology-based anatomical annotation in the CRAFT Corpus", "abstract": "Abstract Gold-standard annotated corpora have become important resources for the training and testing of natural-language-processing (NLP) systems designed to support biocuration efforts, and ontologies are increasingly used to facilitate curational consistency and semantic integration across disparate resources. Bringing together the respective power of these, the Colorado Richly Annotated Full-Text (CRAFT) Corpus, a collection of full-length, open-access biomedical journal articles with extensive manually created syntactic, formatting and semantic markup, was previously created and released. This initial public release has already been used in multiple projects to drive development of systems focused on a variety of biocuration, search, visualization, and semantic and syntactic NLP tasks. Building on its demonstrated utility, we have expanded the CRAFT Corpus with a large set of manually created semantic annotations relying on Uberon, an ontology representing anatomical entities and life-cycle stages of multicellular organisms across species as well as types of multicellular organisms defined in terms of life-cycle stage and sexual characteristics. This newly created set of annotations, which has been added for v2.1 of the corpus, is by far the largest publicly available collection of gold-standard anatomical markup and is the first large-scale effort at manual markup of biomedical text relying on the entirety of an anatomical terminology, as opposed to annotation with a small number of high-level anatomical categories, as performed in previous corpora. In addition to presenting and discussing this newly available resource, we apply it to provide a performance baseline for the automatic annotation of anatomical concepts in biomedical text using a prominent concept recognition system. The full corpus, released with a CC BY 3.0 license, may be downloaded from http://bionlp-corpora.sourceforge.net/CRAFT/index.shtml. Database URL: http://bionlp-corpora.sourceforge.net/CRAFT/index.shtml", "venue": "Database J. Biol. Databases Curation", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "onto", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 3, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "5bd466cb7b6ce832e695fb0c5438f90eb75b4e48", "url": "https://www.semanticscholar.org/paper/5bd466cb7b6ce832e695fb0c5438f90eb75b4e48", "title": "Learning by Reading: A Prototype System, Performance Baseline and Lessons Learned", "abstract": "A traditional goal of Artificial Intelligence research has been a system that can read unrestricted natural language texts on a given topic, build a model of that topic and reason over the model. Natural Language Processing advances in syntax and semantics have made it possible to extract a limited form of meaning from sentences. Knowledge Representation research has shown that it is possible to model and reason over topics in interesting areas of human knowledge. It is useful for these two communities to reunite periodically to see where we stand with respect to the common goal of text understanding. \n \nIn this paper, we describe a coordinated effort among researchers from the Natural Language and Knowledge Representation and Reasoning communities. We routed the output of existing NL software into existing KR software to extract knowledge from texts for integration with engineered knowledge bases. We tested the system on a suite of roughly 80 small English texts about the form and function of the human heart, as well as a handful of \"confuser\" texts from other domains. We then manually evaluated the knowledge extracted from novel texts. \n \nOur conclusion is that the technology from these fields is mature enough to start producing unified machine reading systems. The results of our exercise provide a performance baseline for systems attempting to acquire models from text.", "venue": "AAAI", "citationCount": 57, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "nlp"], "mention_counts": {"nlp": 1, "ke": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "6d1f1829167420402edce708b3dc1208039b9107", "url": "https://www.semanticscholar.org/paper/6d1f1829167420402edce708b3dc1208039b9107", "title": "Managing Mathematical Texts with OWL and Their Graphical Representation", "abstract": "Mathematical knowledge contained in scientific digital publications poses a challenge for intelligent retrieval mechanisms. Many current approaches use statistical (e.g. Google) or natural language processing methods to find correlations in texts and annotate texts semantically. However both kinds of approaches face the problem of extracting and processing knowledge from mathematical equations. The presented system is based on natural language processing techniques, and benefits from characteristic linguistic structures defined by the language used in mathematical texts. It accumulates extracted information snippets from texts, symbols, and equations in knowledge bases. These knowledge bases provide the foundation for the information retrieval. This article describes the concepts and the prototypical technical implementation.", "venue": "Proceedings of the 41st Annual Hawaii International Conference on System Sciences (HICSS 2008)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "onto", "ie", "nlp", "kg"], "mention_counts": {"nlp": 2, "kg": 2, "onto": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "5ddeb7cbf6d879a31091eed56b80c2dc47838474", "url": "https://www.semanticscholar.org/paper/5ddeb7cbf6d879a31091eed56b80c2dc47838474", "title": "An ontology clarification tool for word sense disambiguation", "abstract": "This paper presents a method for word sense disambiguation based on Lesk algorithm which uses lexical database WordNet as knowledge base. The word sense disambiguation is the process of automatically clarifying a meaning of a word in its context. In general, ontology means the meaning or analogous term. It can be interpreted by relating the word with other words in the sentence. This tool accepts English statement as input and gives best possible meaning of given word. Method is experimented with senseval-2 test data for lexical sample task. The results show the betterment over the original Lesk algorithm.", "venue": "2011 3rd International Conference on Electronics Computer Technology", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "wsd", "wsd", "onto", "wsd"], "mention_counts": {"kg": 1, "wsd": 3, "onto": 2}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "9fba7523c25feb27ea8e65ed07160fc0bc150fe5", "url": "https://www.semanticscholar.org/paper/9fba7523c25feb27ea8e65ed07160fc0bc150fe5", "title": "Ontology Based Query Expansion Using Word Sense Disambiguation", "abstract": "Abstract - The existing information retrieval techniques do not consider the context of the keywords present in the user\u2019s queries. Therefore, the search engines sometimes do not provide sufficient information to the users. New methods based on the semantics of user keywords must be developed to search in the vast web space without incurring loss of information. The semantic based information retrieval techniques need to understand the meaning of the concepts in the user queries. This will improve the precision-recall of the search results. Therefore, this approach focuses on the concept based semantic information retrieval. This work is based on Word sense disambiguation, thesaurus WordNet and ontology of any domain for retrieving information in order to capture the context of particular concept(s) and discover semantic relationships between them. reaction. Index terms \u2013 Word Sense Disambiguation, Semantic Information Retrieval, Clustering, Ontology. I. INTRODUCTION", "venue": "ArXiv", "citationCount": 19, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "wsd", "wsd", "onto", "onto", "wsd"], "mention_counts": {"wsd": 3, "onto": 3}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "8976101ac01ff3d97f818f0bbc6a4af5d85e49d5", "url": "https://www.semanticscholar.org/paper/8976101ac01ff3d97f818f0bbc6a4af5d85e49d5", "title": "Ontological Approach for Knowledge Extraction from Clinical Documents", "abstract": "In clinical NLP (Natural Language Processing), Knowledge extraction is a very important task to develop a highly accurate information retrieval system. The various approaches used to develop such systems include rule-based approach, statistical approach, shortest path algorithm or hybrid of these approaches. Accuracy and coverage are the most important parameters while comparing different approaches. Some methodologies have good accuracy but low coverage and vice-versa. In this paper, our focus is to extract domain relationships, for example to extract the relationship between \u2018Disease\u2019 and \u2018Procedure\u2019 or \u2018Symptom\u2019 and \u2018Disease\u2019 etc. from the clinical documents using three different approaches. These three approaches are i) Statistical ii) Shortest Path iii) Shortest Path Using Body System. All three approaches use our in-house existing NLP system to extract entities from the un-structured documents. The Statistical approach applies a probabilistic algorithm on clinical documents, whereas the Shortest Path algorithm uses the Ontological knowledge base for the hierarchical relationship between entities. This Ontological knowledge base is built upon the curated Unified Medical Language System (UMLS). For the Shortest Path Using Body System approach, we have used the domain relationship as well as hierarchical relationship. The output of these approaches is further validated by a domain expert and this validated relationship is used to enrich our ontological knowledge base. We have presented the details of these approaches one-by-one along with the comparative results of these approaches. We finally go through the analysis of the result and conclude on further work.", "venue": "2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "onto", "nlp", "ke", "ke", "onto", "onto", "kg", "nlp", "kg", "onto"], "mention_counts": {"nlp": 3, "onto": 4, "kg": 3, "ke": 2}, "nlp_mention_counts": {"nlp": 3, "ke": 2}, "ld_mention_counts": {"kg": 3, "onto": 4, "ke": 2}, "relevance_score": 0.7095051064826536}, {"paperId": "2523efbccb6aa4a3fc9aac074a76db7395a78dc0", "url": "https://www.semanticscholar.org/paper/2523efbccb6aa4a3fc9aac074a76db7395a78dc0", "title": "The LODeXporter: Flexible Generation of Linked Open Data Triples from NLP Frameworks for Automatic Knowledge Base Construction", "abstract": "We present LODeXporter, a novel approach for exporting Natural Language Processing (NLP) results to a graph-based knowledge base, following Linked Open Data (LOD) principles. The rules for transforming NLP entities into Resource Description Framework (RDF) triples are described in a custom mapping language, which is defined in RDF Schema (RDFS) itself, providing a separation of concerns between NLP pipeline engineering and knowledge base engineering. LODeXporter is available as an open source component for the GATE (General Architecture for Text Engineering) framework.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "lod", "kg", "kg", "rdf", "nlp", "lod", "nlp", "nlp", "kg", "rdf", "rdf", "lod"], "mention_counts": {"nlp": 5, "lod": 3, "kg": 3, "rdf": 3}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"kg": 3, "lod": 3, "rdf": 3}, "relevance_score": 0.7095051064826536}, {"paperId": "e02076dcf9f5218d42d46980b84b6c74a84dfae5", "url": "https://www.semanticscholar.org/paper/e02076dcf9f5218d42d46980b84b6c74a84dfae5", "title": "BERT-based knowledge extraction method of unstructured domain text", "abstract": "With the development and business adoption of knowledge graph, there is an increasing demand for extracting entities and relations of knowledge graphs from unstructured domain documents. This makes the automatic knowledge extraction for domain text quite meaningful. This paper proposes a knowledge extraction method based on BERT, which is used to extract knowledge points from unstructured specific domain texts (such as insurance clauses in the insurance industry) automatically to save manpower of knowledge graph construction. Different from the commonly used methods which are based on rules, templates or entity extraction models, this paper converts the domain knowledge points into question and answer pairs and uses the text around the answer in documents as the context. The method adopts a BERT-based model similar to BERT\u2019s SQuAD reading comprehension task. The model is fine-tuned. And it is used to directly extract knowledge points from more insurance clauses. According to the test results, the model performance is good.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "kg", "ke", "ke", "ke", "ke", "ke", "kg"], "mention_counts": {"kg": 4, "ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"kg": 4, "ke": 5}, "relevance_score": 0.7095051064826536}, {"paperId": "8cf32a719d23a97a0b796108381e59579f92f67f", "url": "https://www.semanticscholar.org/paper/8cf32a719d23a97a0b796108381e59579f92f67f", "title": "HISTORIAE, History of Socio-Cultural Transformation as Linguistic Data Science. A Humanities Use Case", "abstract": "The paper proposes an interdisciplinary approach including methods from disciplines such as history of concepts, linguistics, natural language processing (NLP) and Semantic Web, to create a comparative framework for detecting semantic change in multilingual historical corpora and generating diachronic ontologies as linguistic linked open data (LLOD). Initiated as a use case (UC4.2.1) within the COST Action Nexus Linguarum, European network for Web-centred linguistic data science, the study will explore emerging trends in knowledge extraction, analysis and representation from linguistic data science, and apply the devised methodology to datasets in the humanities to trace the evolution of concepts from the domain of socio-cultural transformation. The paper will describe the main elements of the methodological framework and preliminary planning of the intended workflow. 2012 ACM Subject Classification Computing methodologies \u2192 Semantic networks; Computing methodologies \u2192 Ontology engineering; Computing methodologies \u2192 Temporal reasoning; Computing methodologies \u2192 Lexical semantics; Computing methodologies \u2192 Language resources; Computing methodologies \u2192 Information extraction", "venue": "LDK", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["llod", "lod", "ke", "sw", "llod", "onto", "nlp", "nlp", "onto", "ie"], "mention_counts": {"onto": 2, "sw": 1, "llod": 2, "lod": 1, "ke": 1, "nlp": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1, "ie": 1}, "ld_mention_counts": {"onto": 2, "sw": 1, "llod": 2, "lod": 1, "ke": 1}, "relevance_score": 0.7059547412717408}, {"paperId": "abfbc294890b9b1ed038ce91a43f46c883d8e1ba", "url": "https://www.semanticscholar.org/paper/abfbc294890b9b1ed038ce91a43f46c883d8e1ba", "title": "Learning from Mistakes: Combining Ontologies via Self-Training for Dialogue Generation", "abstract": "Natural language generators (NLGs) for task-oriented dialogue typically take a meaning representation (MR) as input, and are trained end-to-end with a corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue acts and domain attributes. Creation of such datasets is labor intensive and time consuming. Therefore, dialogue systems for new domain ontologies would benefit from using data for pre-existing ontologies. Here we explore, for the first time, whether it is possible to train an NLG for a new larger ontology using existing training sets for the restaurant domain, where each set is based on a different ontology. We create a new, larger combined ontology, and then train an NLG to produce utterances covering it. For example, if one dataset has attributes for family friendly and rating information, and the other has attributes for decor and service, our aim is an NLG for the combined ontology that can produce utterances that realize values for family friendly, rating, decor and service. Initial experiments with a baseline neural sequence-to-sequence model show that this task is surprisingly challenging. We then develop a novel self-training method that identifies (errorful) model outputs, automatically constructs a corrected MR input to form a new (MR, utterance) training pair, and then repeatedly adds these new instances back into the training data. We then test the resulting model on a new test set. The result is a self-trained model whose performance is an absolute 75.4% improvement over the baseline model. We also report a human qualitative evaluation of the final model showing that it achieves high naturalness, semantic coherence and grammaticality.", "venue": "SIGDIAL", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlg", "onto", "nlg", "nlg", "onto", "onto", "onto", "onto", "nlg"], "mention_counts": {"nlg": 4, "onto": 7}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.7059547412717408}, {"paperId": "6373a66a0af4eafba9b76053e6ba82fd438b5118", "url": "https://www.semanticscholar.org/paper/6373a66a0af4eafba9b76053e6ba82fd438b5118", "title": "Populating a domain ontology from web historical dictionaries and encyclopedias", "abstract": "An increasing volume of information is available on the web and usually is expressed as text, representing unstructured or semi-structured data. Thus, semantic information is implicit in these texts, since they are mainly intended for human consumption and interpretation. Therefore, it is not easy to automatically identify concepts or establish relations among them inside the texts. In particular, some web sites contain information on historical data about artistic manifestations like literature or music. This kind of site contains a body of knowledge on the domain, and usually is constructed with some format and content patterns that may be useful for information extraction. In order to make this information available as a structured knowledge base, an information extraction process is necessary. Ontologies are an appropriate way to represent structured knowledge bases, enabling sharing, reuse and inference. In this paper, it is described an information extraction process cycle for populating a domain ontology using texts available on the internet to extract instances of concepts, events and relations, based on existing ontology development methodologies and information extraction techniques. Through this process, latent concepts and relations expressed in natural language can be extracted and represented as an ontology, allowing new uses of the available content. A case study that applies this process is presented.", "venue": "EATIS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "onto", "ie", "onto", "ie", "kg", "onto", "ie", "onto", "ie"], "mention_counts": {"kg": 2, "onto": 5, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"kg": 2, "onto": 5}, "relevance_score": 0.7059547412717408}, {"paperId": "6dab169937c969e346bdf408d8e4ab39f1deda60", "url": "https://www.semanticscholar.org/paper/6dab169937c969e346bdf408d8e4ab39f1deda60", "title": "Using Ontology-Driven Methods to Develop Frameworks for Tackling NLP Problems", "abstract": "In this paper, we present the meta-tooling framework named TAISim that can be used both as a developer\u201fs tool for creating NLP systems and as a NLP learning environment, which allows helping students to construct NLP systems by example in a flexible way. TAISim enables the end user to combine different components of a typical NLP system in order to tackle specific NLP problems. We use ontology-engineering methods to accumulate meta-knowledge about the system construction and about users\u201f activities to control the process of development and using the NLP system. Thanks to ontologydriven methods TAISim can be modified and enriched with additional information resources and program modules by means of a high-level interface. Additionally, we demonstrate how the using of meta-ontology helps us to improve TAISim to tackle ontology design automation problems.", "venue": "AIST", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "nlp", "nlp", "onto", "onto", "nlp", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 7, "onto": 4}, "nlp_mention_counts": {"nlp": 7}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7059547412717408}, {"paperId": "3b6496a6947578049706600db09b70ab094ef535", "url": "https://www.semanticscholar.org/paper/3b6496a6947578049706600db09b70ab094ef535", "title": "Combining the Best of Two Worlds: NLP and IR for Intranet Search", "abstract": "Natural language processing (NLP) is becoming much more robust and applicable in realistic applications. One area in which NLP has still not been fully exploited is information retrieval (IR). In particular we are interested in search over intranets and other local Web sites. We see dialogue-driven search which is based on a largely automated knowledge extraction process as one of the next big steps. Instead of replying with a set of documents for a user query the system would allow the user to navigate through the extracted knowledge base by making use of a simple dialogue manager. Here we support this idea with a first task-based evaluation that we conducted on a university intranet. We automatically extracted entities like person names, organizations and locations as well as relations between entities and added visual graphs to the search results whenever a user query could be mapped into this knowledge base. We found that users are willing to interact and use those visual interfaces. We also found that users preferred such a system that guides a user through the result set over a baseline approach. The results represent an important first step towards full NLP-driven intranet search.", "venue": "2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "kg", "nlp", "nlp", "ke", "kg", "nlp", "nlp"], "mention_counts": {"nlp": 5, "kg": 2, "ke": 2}, "nlp_mention_counts": {"nlp": 5, "ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 2}, "relevance_score": 0.7059547412717408}, {"paperId": "6e621feba6fa2ff894807d38ee7b648be89b227b", "url": "https://www.semanticscholar.org/paper/6e621feba6fa2ff894807d38ee7b648be89b227b", "title": "Small in Size, Big in Precision: A Case for Using Language-Specific Lexical Resources for Word Sense Disambiguation", "abstract": "Linked open data (LOD) presents an ideal platform for connecting the multilingual lexical resources used in natural language processing (NLP) tasks, but the use of machine translation to fill in gaps in lexical coverage for resource-poor languages means that large amounts of data are potentially unverified. For graph-based word sense disambiguation (WSD), one approach has been to first translate terms into English in order to disambiguate using richer, fuller lexical knowledge bases (LKBs) such as WordNet. In this paper, we show that this approach actually creates more ambiguity and is far less accurate than using languagespecific resources, which, regardless of their smaller size, can provide results comparable in accuracy to the state-of-theart reported for graph-based WSD in English. For LOD, this demonstrates the importance of continuing to grow and extend language-specific resources in order to continually verify and reintegrate them as accurate resources.", "venue": "NLPLOD@RANLP", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["lod", "wsd", "nlp", "lod", "wsd", "wsd", "nlp", "wsd", "lod", "mt", "kg"], "mention_counts": {"nlp": 2, "lod": 3, "mt": 1, "kg": 1, "wsd": 4}, "nlp_mention_counts": {"nlp": 2, "wsd": 4, "mt": 1}, "ld_mention_counts": {"kg": 1, "lod": 3}, "relevance_score": 0.7059547412717408}, {"paperId": "d2152059477ca5ac6424e6890879fce18469c8b1", "url": "https://www.semanticscholar.org/paper/d2152059477ca5ac6424e6890879fce18469c8b1", "title": "Semi-Automatic Practical Ontology Construction by Using a Thesaurus, Computational Dictionaries, and Large Corpora", "abstract": "This paper presents the semi-automatic construction method of a practical ontology by using various resources. In order to acquire a reasonably practical ontology in a limited time and with less manpower, we extend the Kadokawa thesaurus by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. The former can be obtained by converting valency information and case frames from previously-built computational dictionaries used in machine translation. The latter can be acquired from concept co-occurrence information, which is extracted automatically from large corpora. The ontology stores rich semantic constraints among 1,110 concepts, and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology. In our practical machine translation system, our ontology-based word sense disambiguation method achieved an 8.7% improvement over methods which do not use an ontology for Korean translation.", "venue": "HTLKM@ACL", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "wsd", "onto", "mt", "nlp", "mt", "onto", "onto"], "mention_counts": {"nlp": 1, "wsd": 1, "onto": 7, "mt": 2}, "nlp_mention_counts": {"nlp": 1, "wsd": 1, "mt": 2}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.7059547412717408}, {"paperId": "83b4f1037269b4b7621cb249520262c3d3b12104", "url": "https://www.semanticscholar.org/paper/83b4f1037269b4b7621cb249520262c3d3b12104", "title": "Research on Automatically Building Tibetan Folk Culture Ontology Library for Sino-Tibetan Machine Translation", "abstract": "In order to achieve the Sino-Tibetan machine translation system, the key problem is the automatic construction of the domain ontology. We use a combination of concept hierarchy theory and KDD (knowledge discovery in databases) technology to automatically build domain ontology. For building domain ontology, we should first extract information and build model of information in this field to create the initial concepts for ontology. We will automatically build Tibetan folk culture ontology library. In it, we will collect more than 100 kinds of Tibetan folk culture characteristics and find the relationship between them. Then we will establish their concept of a hierarchical tree, and finally achieve the expansion of the ontology knowledge through KDD technology for Sino-Tibetan Machine translation.", "venue": "2013 6th International Conference on Intelligent Networks and Intelligent Systems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "onto", "mt", "onto", "onto", "onto", "onto", "ie", "onto", "mt", "onto"], "mention_counts": {"onto": 7, "mt": 3, "ie": 1}, "nlp_mention_counts": {"mt": 3, "ie": 1}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.7059547412717408}, {"paperId": "cae30d5914fcf473a7a7169dcb8dcff881b8910d", "url": "https://www.semanticscholar.org/paper/cae30d5914fcf473a7a7169dcb8dcff881b8910d", "title": "Ontology-Driven Self-Supervision for Adverse Childhood Experiences Identification Using Social Media Datasets", "abstract": ": Adverse Childhood Experiences (ACEs) are defined as a collection of highly stressful, and potentially traumatic, events or circumstances that occur throughout childhood and/or adolescence. They have been shown to be associated with increased risks of mental health diseases or other abnormal behaviours in later lives. However, the identification of ACEs from textual data with Natural Language Processing (NLP) is challenging because (a) there are no NLP ready ACE ontologies; (b) there are few resources available for machine learning, necessitating the data annotation from clinical experts; (c) costly annotations by domain experts and large number of documents for supporting large machine learning models. In this paper, we present an ontology-driven self-supervised approach (derive concept embeddings using an auto-encoder from baseline NLP results) for producing a publicly available resource that would support large-scale machine learning (e.g., training transformer based large language models) on social media corpus. This resource as well as the proposed approach are aimed to facilitate the community in training transferable NLP models for effectively surfacing ACEs in low-resource scenarios like NLP on clinical notes within Electronic Health Records. The resource including a list of ACE ontology terms, ACE concept embeddings and the NLP annotated corpus is available at https://github.com/knowlab/ACE-NLP.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "nlp", "onto", "nlp", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 7, "onto": 4}, "nlp_mention_counts": {"nlp": 7}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7059547412717408}, {"paperId": "9a95053b3175ee2a90707d6816571333173948f2", "url": "https://www.semanticscholar.org/paper/9a95053b3175ee2a90707d6816571333173948f2", "title": "Semantic Annotation and Querying Framework based on Semi-structured Ayurvedic Text", "abstract": "Knowledge bases (KB) are an important resource in a number of natural language processing (NLP) and information retrieval (IR) tasks, such as semantic search, automated question-answering etc. They are also useful for researchers trying to gain information from a text. Unfortunately, however, the state-of-the-art in Sanskrit NLP does not yet allow automated construction of knowledge bases due to unavailability or lack of sufficient accuracy of tools and methods. Thus, in this work, we describe our efforts on manual annotation of Sanskrit text for the purpose of knowledge graph (KG) creation. We choose the chapter Dh\u0101nyavarga from Bh\u0101vaprak\u0101\u015banigha\u1e47\u1e6du of the Ayurvedic text Bh\u0101vaprak\u0101\u015ba for annotation. The constructed knowledge graph contains 410 entities and 764 relationships. Since Bh\u0101vaprak\u0101\u015banigha\u1e47\u1e6du is a technical glossary text that describes various properties of different substances, we develop an elaborate ontology to capture the semantics of the entity and relationship types present in the text. To query the knowledge graph, we design 31 query templates that cover most of the common question patterns. For both manual annotation and querying, we customize the Sangrahaka framework previously developed by us. The entire system including the dataset is available from https://sanskrit.iitk.ac.in/ayurveda/. We hope that the knowledge graph that we have created through manual annotation and subsequent curation will help in development and testing of NLP tools in future as well as studying of the Bh\u0101vaprak\u0101\u015banigha\u1e47\u1e6du text.", "venue": "Online World Conference on Soft Computing in Industrial Applications", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "kg", "kg", "nlp", "kg", "kg", "nlp", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 4, "kg": 6, "onto": 1}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 6, "onto": 1}, "relevance_score": 0.7059547412717408}, {"paperId": "c147f5eaf647dfd22a011784f16c37d9002b1af9", "url": "https://www.semanticscholar.org/paper/c147f5eaf647dfd22a011784f16c37d9002b1af9", "title": "PhenoGO: Assigning Phenotypic Context to Gene Ontology Annotations with Natural Language Processing", "abstract": "Natural language processing (NLP) is a high throughput technology because it can process vast quantities of text within a reasonable time period. It has the potential to substantially facilitate biomedical research by extracting, linking, and organizing massive amounts of information that occur in biomedical journal articles as well as in textual fields of biological databases. Until recently, much of the work in biological NLP and text mining has revolved around recognizing the occurrence of biomolecular entities in articles, and in extracting particular relationships among the entities. Now, researchers have recognized a need to link the extracted information to ontologies or knowledge bases, which is a more difficult task. One such knowledge base is Gene Ontology annotations (GOA), which significantly increases semantic computations over the function, cellular components and processes of genes. For multicellular organisms, these annotations can be refined with phenotypic context, such as the cell type, tissue, and organ because establishing phenotypic contexts in which a gene is expressed is a crucial step for understanding the development and the molecular underpinning of the pathophysiology of diseases. In this paper, we propose a system, PhenoGO, which automatically augments annotations in GOA with additional context. PhenoGO utilizes an existing NLP system, called BioMedLEE, an existing knowledge-based phenotype organizer system (PhenOS) in conjunction with MeSH indexing and established biomedical ontologies. More specifically, PhenoGO adds phenotypic contextual information to existing associations between gene products and GO terms as specified in GOA. The system also maps the context to identifiers that are associated with different biomedical ontologies, including the UMLS, Cell Ontology, Mouse Anatomy, NCBI taxonomy, GO, and Mammalian Phenotype Ontology. In addition, PhenoGO was evaluated for coding of anatomical and cellular information and assigning the coded phenotypes to the correct GOA; results obtained show that PhenoGO has a precision of 91% and recall of 92%, demonstrating that the PhenoGO NLP system can accurately encode a large number of anatomical and cellular ontologies to GO annotations. The PhenoGO Database may be accessed at the following URL: http://www.phenoGO.org", "venue": "Pacific Symposium on Biocomputing", "citationCount": 93, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "nlp", "onto", "onto", "nlp", "onto", "onto", "onto", "onto", "nlp", "nlp", "onto", "kg", "kg", "ie", "kg", "onto"], "mention_counts": {"nlp": 5, "onto": 8, "kg": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 5, "ie": 1}, "ld_mention_counts": {"kg": 3, "onto": 8}, "relevance_score": 0.7048226948775973}, {"paperId": "4116b467f5f9ad167917215214cfcbda7a002683", "url": "https://www.semanticscholar.org/paper/4116b467f5f9ad167917215214cfcbda7a002683", "title": "An Efficient Approach for Semantic Relatedness Evaluation Based on Semantic Neighborhood", "abstract": "In the context of natural language processing and information retrieval, ontologies can improve the results of the word sense disambiguation (WSD) techniques. By making explicit the semantics of the term, ontology-based semantic measures play a crucial role to determine how different ontology classes have a similar meaning. In this context, it is common to use semantic similarity as a basis for WSD. However, the measures generally consider only taxonomic relationships, which affects negatively the discrimination of two ontology classes that are related by the other relationship types. On the other hand, semantic relatedness measures consider diverse types of relationships to determine how much two classes on the ontology are related. However, these measures, especially the path-based approaches, has as the main drawback a high computational complexity to be calculated in query execution time. Also, for both types of semantic measures, it is unpractical to store all similarity or relatedness values between all ontology classes in memory, especially for large ontologies. In this work, we propose a novel approach based on semantic neighbors that aim to improve the query time in path-based semantic measures without losing their effectiveness in relatedness analysis. We also propose an efficient algorithm to calculate the semantic distance between two ontology classes. We evaluate our proposal in WSD using a pre-existent domain ontology for well-core description. This ontology contains 929 classes related to rock facies and a set of sentences from four different corpora about geology in the Oil\\&Gas domain. In the experiments, we compared our approach with state-of-the-art semantic relatedness measures, such as path-based, feature-based, information content, and hybrid methods regarding the F-score, query time and the total number of classes in memory. The experimental results show that the proposed method obtains F-score gains in WSD, as well as an improvement in the query time concerning the traditional path-based approaches. Also, we reduce the total number of classes stored in memory for each ontology class.", "venue": "2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "wsd", "nlp", "onto", "wsd", "onto", "onto", "wsd", "onto", "wsd", "onto", "wsd", "onto", "onto", "onto"], "mention_counts": {"nlp": 1, "wsd": 5, "onto": 11}, "nlp_mention_counts": {"nlp": 1, "wsd": 5}, "ld_mention_counts": {"onto": 11}, "relevance_score": 0.7048226948775973}, {"paperId": "56bb03730f52a69de3abedef03584582011e36fc", "url": "https://www.semanticscholar.org/paper/56bb03730f52a69de3abedef03584582011e36fc", "title": "Enriching Answers in Question Answering Systems using Linked Data", "abstract": "Linked Data has emerged as the most widely used and the most powerful knowledge source for Question Answering (QA). Although Question Answering using Linked Data (QALD) fills in many gaps in the traditional QA models, the answers are still presented as factoids. This research introduces an answer presentation model for QALD by employing Natural Language Generation (NLG) to generate natural language descriptions to present an informative answer. The proposed approach employs lexicalization, aggregation, and referring expression generation to build a human-like enriched answer utilizing the triples extracted from the entities mentioned in the question as well as the entities contained in the answer.", "venue": "SEMWEB", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "nlg", "llod", "ld", "nlg", "ld", "lod"], "mention_counts": {"ld": 2, "llod": 1, "nlg": 3, "lod": 1}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"ld": 2, "llod": 1, "lod": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "f1a1cb1a2d585bec5c66230c28fefc46444db263", "url": "https://www.semanticscholar.org/paper/f1a1cb1a2d585bec5c66230c28fefc46444db263", "title": "Cross-lingual Knowledge Extraction (XLike)", "abstract": "The goal of the XLike project is to develop technology to monitor and aggregate knowledge that is currently spread across mainstream and social media, and to enable cross-lingual ser vices for publishers, media monitoring and business intelligence. The effort will combine scientific capabilities and insights from several areas of science \u2013 modern computational linguistics and NLP, machine learning, text mining and semantic technologies \u2013 in order to enable cross-lingual text \u201cunderstanding\u201d by machines. Specifically, we plan to pursue the following two key open research problems: (1) to extract and integrate formal knowledge relations from multilingual texts with cross-lingual knowledge bases, and (2) to adapt linguistic techniques and crowdsourcing to deal with irregularities in the informal language used primarily in social media. The developed technology will be language-independent to the largest possible extent, while within the project we will specifically address English, German, Spanish, Chinese and Hindi as major world languages and Catalan, Slovenian, and Croatian as less resourced languages. Knowledge resources from Linked Open Data cloud will be used, with special focus paid to using general common sense knowledge base CycKB as \u201csemantic Interlingua\u201d. The use of Machine Translation will be oriented towards translation from a natural language as the source language into a formal language of semantic representation as source language. For languages where not enough required linguistic resources are available, we will use a probabilistic Interlingua representation trained from a parallel corpora and/or from comparable corpus derived from the Wikipedia, or use MT as a fallback option for translating the text from less resourced language to English and then process this translation. Specifically, developed solutions will be applied and evaluated in two use cases: a \u201cBloomberg\u201d use case, covering the domain of financial news, and a \u201cSlovenian Press Agency\u201d use case, covering the domain of general news.", "venue": "MTSUMMIT", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "lod", "kg", "nlp", "mt"], "mention_counts": {"nlp": 1, "ke": 1, "lod": 1, "mt": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "mt": 1}, "ld_mention_counts": {"ke": 1, "lod": 1, "kg": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "cf2450d035df95ff44ee1c154ced3a48012fbc6f", "url": "https://www.semanticscholar.org/paper/cf2450d035df95ff44ee1c154ced3a48012fbc6f", "title": "Session details: Poster presentations", "abstract": "We are very pleased to welcome the 100 posters of this WWW 2012 poster track. We wish to thank all Program Committee members for the quality (and the quantity) of their work. And naturally, we also congratulate authors of the selected contributions that reflect the variety of the WWW series conferences: all posters present high quality work in various domains, poster authors come from various countries and represent both academic and industrial research teams. The poster track provides an opportunity for authors and conference attendees to interact and exchange about innovative theoretical and technical work in progress around, about and on the Web. In addition to being displayed during three whole days from 4/18 to 4/20, WWW'2012 posters will also be shortly presented on Thursday 4/19 PM. Seven \"spotlight events\" are organized to give conference attendees a 5-minute glimpse on each poster content, in the following areas: 1 - Web search / Web mining: WWW contains rich data, yet how to effectively use such rich data is challenging. Web search is mainly about searching information with a proper query, while web mining has a boarder scope of knowledge exaction from the Web. 2 - Natural Language Processing / Information Search and Retrieval: NLP and IR are two major foundations of Web search. With more and more applications on the Web, the boundary between NLP and IR is getting smaller. How to efficiently and effectively handle large scale online data is a challenge for NLP and IR communities. 3 - Web engineering / Performance / Security / Privacy: this session copes with the everyday challenges of Web development: how to improve contents and applications design processes, how to serve them efficiently and securely to users and how to face privacy issues. 4 - Recommender Systems / Semantic Web: Recommender Systems allow Web communities access the best of users' opinions. Semantic Web and Linked Data rely on communities' data to harvest and process complex information. This session presents posters that cope with graph algorithms and semantic/social data. 5 - Social Web / Human Factors / Accessibility: this session deals with human-oriented topics. The presented posters mainly concern analysis and prediction of social behavior and influence, as well as of individual behavior, affective computing and Web accessibility. 6 - Microblogging / Social media / Catch up TV: social media and microblogging provide rich online information with a different perspective from traditional static Web page data. How to effectively search, mine and utilize such valuable information is a very stimulating research area. 7 - Monetization: posters in this session explore efficient ways to spread ads on the Web or analyze the efficiency of existing algorithms on actual e-commerce activity. This year's edition aims at \"augmenting posters\" in several ways: poster views are also available in virtual worlds, and attendees can access poster metadata and additional information and even chat with authors using their smartphones and tabs. Numerous efforts have been made to encourage discussions about posters for the largest audience, inside and outside the conference. So come, see and participate!", "venue": "Proceedings of the 21st International Conference on World Wide Web", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "sw", "nlp", "nlp", "ld", "nlp"], "mention_counts": {"ld": 1, "nlp": 4, "sw": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"ld": 1, "sw": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "b641ab44b1ab70665c58b10f7d175be99bb9a16f", "url": "https://www.semanticscholar.org/paper/b641ab44b1ab70665c58b10f7d175be99bb9a16f", "title": "Ontology-Based Interpretation of Natural Language Philipp Cimiano, Christina Unger, and John McCrae (University of Arminia Bielefeld, Germany) Morgan & Claypool, Synthesis Lectures on Human Language Technologies, March 2014, 178 pages, (doi:10.2200/S00561ED1V01Y201401HLT024) , $45.00", "abstract": "A book aiming to build a bridge between two fields that share the subject of research but do not share the same views necessarily puts itself in a difficult position: The authors have either to strike a fair balance at peril of dissatisfying both sides or nail their colors to the mast and cater mainly to one of two communities. For semantic processing of natural language with either NLP methods or Semantic Web approaches, the authors clearly favor the latter and propose a strictly ontology-driven interpretation of natural language. The main contribution of the book, driving semantic processing from the ground up by a formal domain-specific ontology, is elaborated in ten well-structured chapters spanning 143 pages of content.", "venue": "Computational Linguistics", "citationCount": 1, "fieldsOfStudy": ["Sociology", "Computer Science"], "mentions": ["onto", "sw", "onto", "hlt", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 3, "hlt": 1}, "nlp_mention_counts": {"nlp": 2, "hlt": 1}, "ld_mention_counts": {"sw": 1, "onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "c376766991bb481c200b9ebdefb073d5b610d97c", "url": "https://www.semanticscholar.org/paper/c376766991bb481c200b9ebdefb073d5b610d97c", "title": "Natural Multi-language Interaction between Firefighters and Fire Fighting Robots", "abstract": "Due to rapid development of agent systems and robotics, more and more chances are available for humans to interact with agent-based robotic technology (e.g., Robotic vacuums, robotic surgery, etc.), this trend increases the importance of human-robot interaction including human-robot communication. For the robust human-robot communication, natural language processing (NLP) can be implemented, among various existing natural language processing techniques, Ontological Semantic Technology (OST), which addresses meanings in an easily comprehensible way as a human does, was selected, the OST is a system in an ontology-based structure to deal with multiple natural languages. This research specifically addresses a concept of natural language-based communication with fire fighting robots and humans, and the main domain of this study targets fire fighting situations. In order to implement ontology-based communication with different languages, Korean and English were used for this particular study. This study extends the domain of Ontological Semantic Technology, specifically for communication with robots in a fire fighting domain using Korean and English.", "venue": "2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "5bb28ae9adf604c5acc56fd51d0be3ea392048ce", "url": "https://www.semanticscholar.org/paper/5bb28ae9adf604c5acc56fd51d0be3ea392048ce", "title": "The Value of Paraphrase for Knowledge Base Predicates", "abstract": "Paraphrase, i.e., differing textual realizations of the same meaning, has proven useful for many natural language processing (NLP) applications. Collecting paraphrase for predicates in knowledge bases (KBs) is the key to comprehend the RDF triples in KBs. Existing works have published some paraphrase datasets automatically extracted from large corpora, but have too many redundant pairs or don't cover enough predicates, which cannot be improved by computer only and need the help of human beings. This paper shows a full process of collecting large-scale and high-quality paraphrase dictionaries for predicates in knowledge bases, which takes advantage of existing datasets and combines the technologies of machine mining and crowdsourcing. Our dataset comprises 2284 distinct predicates in DBpedia and 31130 paraphrase pairs in total, the quality of which is a great leap over previous works. Then it is demonstrated that such good paraphrase dictionaries can do great help to natural language processing tasks such as question answering and language generation. We also publish our own dictionary for further research.", "venue": "AAAI Conference on Artificial Intelligence", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "rdf", "kg", "nlp", "kg", "nlp"], "mention_counts": {"nlp": 3, "kg": 3, "rdf": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 3, "rdf": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "4b87c1e8798b172ea00e788963628f8695f6501e", "url": "https://www.semanticscholar.org/paper/4b87c1e8798b172ea00e788963628f8695f6501e", "title": "Semantic analysis of natural language software requirement", "abstract": "In the recent past, domain specific solutions for detailed semantic analysis have got acceptable by natural language processing community and use of applications involving natural language based user interface. Different approaches that has been previously used is focusing on quality of text and improving the text contents by adding semantic information with text then the existing approaches used for semantic analysis can provide better results. In this, an approach was presented to address the problem of non-availability of semantic information required for better semantic analysis. This problem is solved by using semantic technology to annotate text of software requirements expressed in a natural language with their domain specific semantics and investigate the effect of semantic analysis with attached semantics. The presented approach uses a semantic framework specifically designed for interpretation and detailed semantic analysis of natural language software requirement specifications. The used framework is based on semantic technology involves knowledge extracted from existing software requirement documents and knowledge extracted from existing applications. The presented approach shows that by adapting and combing existing ontologies to support knowledge management, developing system and performing experiments on requirement of real world software systems. In this approach start with software requirement specification, after this clean the irrelevant requirements, convert the cleaned requirements into graph that represents inter related different elements. Represent the requirement graph into sparse matrix, after these all steps; we generate ontology with the help of OntoGen tool.", "venue": "InTech", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "onto", "ke", "onto"], "mention_counts": {"nlp": 1, "onto": 2, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "8c557b7df46e6c54a5fef4a78f38151aa0c4ede5", "url": "https://www.semanticscholar.org/paper/8c557b7df46e6c54a5fef4a78f38151aa0c4ede5", "title": "Natural Language Processing Model for Automatic Analysis of Cybersecurity-Related Documents", "abstract": "This paper describes the development and implementation of a natural language processing model based on machine learning which performs cognitive analysis for cybersecurity-related documents. A domain ontology was developed using a two-step approach: (1) the symmetry stage and (2) the machine adjustment. The first stage is based on the symmetry between the way humans represent a domain and the way machine learning solutions do. Therefore, the cybersecurity field was initially modeled based on the expertise of cybersecurity professionals. A dictionary of relevant entities was created; the entities were classified into 29 categories and later implemented as classes in a natural language processing model based on machine learning. After running successive performance tests, the ontology was remodeled from 29 to 18 classes. Using the ontology, a natural language processing model based on a supervised learning model was defined. We trained the model using sets of approximately 300,000 words. Remarkably, our model obtained an F1 score of 0.81 for named entity recognition and 0.58 for relation extraction, showing superior results compared to other similar models identified in the literature. Furthermore, in order to be easily used and tested, a web application that integrates our model as the core component was developed.", "venue": "Symmetry", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 4, "onto": 3}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "5c162159543b068e20d6ec73d7bb0ef86bee4646", "url": "https://www.semanticscholar.org/paper/5c162159543b068e20d6ec73d7bb0ef86bee4646", "title": "Application of Natural Language Processing and Evidential Analysis to Web-Based Intelligence Information Acquisition", "abstract": "The quality of decisions made in business and government relates directly to the quality of the information used to formulate the decision. This information may be retrieved from an organization's knowledge base (Intranet) or from the World Wide Web. Intelligence services Intranet held information can be efficiently manipulated by technologies based upon either semantics such as ontologies, or statistics such as meaning-based computing. These technologies require complex processing of large amount of textual information. However, they cannot currently be effectively applied to Web-based search due to various obstacles, such as lack of semantic tagging. A new approach proposed in this paper supports Web-based search for intelligence information utilizing evidence-based natural language processing (NLP). This approach combines traditional NLP methods for filtering of Web-search results, Grounded Theory to test the completeness of the evidence, and Evidential Analysis to test the quality of gathered information. The enriched information derived from the Web-search will be transferred to the intelligence services knowledge base for handling by an effective Intranet search system thus increasing substantially the information for intelligence analysis. The paper will show that the quality of retrieved information is significantly enhanced by the discovery of previously unknown facts derived from known facts.", "venue": "European Intelligence and Security Informatics Conference", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "kg", "kg", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "c6e4e528c5ea975f04fadb462c6b6e3284a471a1", "url": "https://www.semanticscholar.org/paper/c6e4e528c5ea975f04fadb462c6b6e3284a471a1", "title": "Issues and Challenges in Annotating Urdu Action Verbs on the IMAGACT4ALL Platform", "abstract": "In South-Asian languages such as Hindi and Urdu, action verbs having compound constructions and serial verbs constructions pose serious problems for natural language processing and other linguistic tasks. Urdu is an Indo-Aryan language spoken by 51, 500, 0001 speakers in India. Action verbs that occur spontaneously in day-to-day communication are highly ambiguous in nature semantically and as a consequence cause disambiguation issues that are relevant and applicable to Language Technologies (LT) like Machine Translation (MT) and Natural Language Processing (NLP). IMAGACT4ALL is an ontology-driven web-based platform developed by the University of Florence for storing action verbs and their inter-relations. This group is currently collaborating with Jawaharlal Nehru University (JNU) in India to connect Indian languages on this platform. Action verbs are frequently used in both written and spoken discourses and refer to various meanings because of their polysemic nature. The IMAGACT4ALL platform stores each 3d animation image, each one of them referring to a variety of possible ontological types, which in turn makes the annotation task for the annotator quite challenging with regard to selecting verb argument structure having a range of probability distribution. The authors, in this paper, discuss the issues and challenges such as complex predicates (compound and conjunct verbs), ambiguously animated video illustrations, semantic discrepancies, and the factors of verb-selection preferences that have produced significant problems in annotating Urdu verbs on the IMAGACT ontology.", "venue": "LREC", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "nlp", "onto", "onto", "mt"], "mention_counts": {"nlp": 3, "onto": 3, "mt": 1}, "nlp_mention_counts": {"nlp": 3, "mt": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "28386672818a489af94879534e6e644ae5231371", "url": "https://www.semanticscholar.org/paper/28386672818a489af94879534e6e644ae5231371", "title": "plWordNet as the Cornerstone of a Toolkit of Lexico-semantic Resources", "abstract": "A wordnet is many things to many people: a graph of inter-related lexicalised concepts, a taxonomy, a thesaurus, and so on. A wordnet makes good sense as the mainstay of any deep automated semantic analysis of text. We have begun the construction of a multi-component, multi-use toolkit of natural language processing tools with plWordNet, a very large Polish wordnet, at its centre. The components will include plWordNet and its mapping onto an ontology (the upper level and elements of the middle level), a lexicon of proper names and a semantic valency lexicon. Some of those elements will be aligned with plWordNet, and there will be a mapping onto Princeton WordNet. Several challenging applications will show the utility of the toolkit in practice. 1 How wordnets evolve Wordnets start small but quickly grow to account for much of the lexical material of the given language. The size of version 3.1 of Princeton WordNet (PWN) (Fellbaum, 1998) is a de facto standard, even if this mature wordnet also keeps growing, albeit slowly.1 One of the resources which approach this size standard is plWordNet (Piasecki et al., 2009), now in version 2.1. Languages change continually, so lexicographers never rest, but one can still ask when the development of a wordnet ought to slow down, and whether there is an appropriate steady state of a wordnet. That clearly is a loaded question, and much depends on the language. For example, suppose that a wordnet for PWN began as a test of a theory of human semantic representation and memory (Collins and Quillian, 1969). It now features a comprehensive vocabulary, a set of universally useful semantic relations, glosses, links to ontologies, and more. a richly inflected language with complex and varied derivation was originally a translation of PWN. Such a wordnet should, sooner or later, acquire semantic relations which account accurately for its unique lexical system.. A wordnet, even as developed as PWN, GermaNet (Hamp and Feldweg, 1997) or plWordNet (Maziarz et al., 2013a), serves many natural language processing (NLP) applications, yet it seems neither feasible nor necessary to remake wordnets into universal NLP resources. Instead, we propose to mark clear boundaries around a wordnet (what it should and what it should not include), and treat it as a pivotal element of an organic toolkit of inter-connected tools and resources for the semantic analysis of texts, along with the auxiliary morphological and syntactic analysis tools. Our case study is such a toolkit, now under development, centred on plWordNet 3.0 (also in development), and intended first and foremost for research in the humanities. In the remainder of the paper, we present the main design assumptions and principles of that project. We explain how comprehensive we want plWordNet 3.0 to become, what size and what coverage we envisage. We attempt to describe how the toolkit will be built around plWordNet, and we outline plans for its large-scale illustrative applications in several domains. We discuss how the components of the toolkit will be expanded or constructed: plWordNet 3.0, its mapping to an ontology, and a semantic lexicon of proper names. We also briefly present resources for morphological and structural description, associated with the plWordNet system, among them a lexicon of lexico-syntactic structures of multiword expressions and a valency lexicon linked to plWordNet but developed independently. This work is meant to take several years of initial effort and years of maintenance. We cannot answer many design questions yet, but many will be answered as the project unfolds. That is to say. we want to interlace theory and practice.", "venue": "GWC", "citationCount": 16, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 3}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "5f2dd73a4437008e792023f0a8a51662b66edc51", "url": "https://www.semanticscholar.org/paper/5f2dd73a4437008e792023f0a8a51662b66edc51", "title": "Combining NLP And Semantics For Mining Software Technologies From Research Publications", "abstract": "The natural language processing (NLP) community has developed a variety of methods for extracting and disambiguating information from research publications. However, they usually focus only on standard research entities such as authors, affiliations, venues, references and keywords. We propose a novel approach, which combines NLP and semantic technologies for generating from the text of research publications an OWL ontology describing software technologies used or introduced by researchers, such as applications, systems, frameworks, programming languages, and formats. The method was tested on a sample of 300 publications in the Semantic Web field, yielding promising results.", "venue": "WWW", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "sw", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 4, "sw": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "2f6bb3c5dd032fcb37b79828be446f26c2f47654", "url": "https://www.semanticscholar.org/paper/2f6bb3c5dd032fcb37b79828be446f26c2f47654", "title": "Research and Design of Knowledge System Construction System Based on Natural Language Processing", "abstract": "The digital processing of content resources has subverted the traditional paper content processing model and has also spread widely. The digital resources processed by text structure need to be structured and processed by professional knowledge, which can be saved as a professional digital content resource of knowledge base and provide basic metadata for intelligent knowledge service platform. The professional domain-based knowledge system construction system platform explored in this study is designed based on natural language processing. Natural language processing is an important branch of artificial intelligence, which is the application of artificial intelligence technology in linguistics. The system first extracts the professional thesaurus and domain ontology in the digital resources and then uses the new word discovery algorithm based on the label weight designed by artificial intelligence technology to intelligently extract and clean the new words of the basic thesaurus. At the same time, the relationship system between knowledge points and elements is established to realize the association extraction of targeted knowledge points, and finally the output content is enriched from knowledge points into related knowledge systems. In order to improve the scalability and universality of the system, the extended architecture of the thesaurus, algorithms, computational capabilities, tags, and exception thesaurus was taken into account when designing. At the same time, the implementation of \u201cartificial intelligence [Formula: see text] manual assistance\u201d was adopted. On the basis of improving the system availability, the experimental basis of the optimization algorithm is provided. The results of this research will bring an artificial intelligence innovation after the digitization to the publishing industry and will transform the content service into an intelligent service based on the knowledge system.", "venue": "Int. J. Pattern Recognit. Artif. Intell.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "kg", "tp", "kg", "onto"], "mention_counts": {"nlp": 3, "tp": 1, "kg": 2, "onto": 1}, "nlp_mention_counts": {"nlp": 3, "tp": 1}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "4585294526e2a0c3c9bc6d1c0b742ad16d00a7d6", "url": "https://www.semanticscholar.org/paper/4585294526e2a0c3c9bc6d1c0b742ad16d00a7d6", "title": "NLP Technologies and Semantic Web: Risks, Opportunities and Challenges", "abstract": "In this paper we provide a set of hypotheses about possible interactions between the raising paradigm of the Semantic Web and NLP technologies. We show that there is some role to be played by NLP both on the ground of creation and maintenance of the Semantic Web and on the one of its access by humans. We also provide the skeleton of a running application which emulates a situation where the Semantic Web has reached its mature state.", "venue": "Intelligenza Artificiale", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "nlp", "sw", "nlp", "sw", "nlp"], "mention_counts": {"nlp": 3, "sw": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "1486ceab09a38b31160840d66ec9bd4c76bc943d", "url": "https://www.semanticscholar.org/paper/1486ceab09a38b31160840d66ec9bd4c76bc943d", "title": "Semantic Web based Machine Translation", "abstract": "This paper describes the experimental combination of traditional Natural Language Processing (NLP) technology with the Semantic Web building stack in order to extend the expert knowledge required for a Machine Translation (MT) task. Therefore, we first give a short introduction in the state of the art of MT and the Semantic Web and discuss the problem of disambiguation being one of the common challenges in MT which can only be solved using world knowledge during the disambiguation process. In the following, we construct a sample sentence which demonstrates the need for world knowledge and design a prototypical program as a successful solution for the outlined translation problem. We conclude with a critical view on the developed approach.", "venue": "ESIRMT/HyTra@EACL", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "sw", "nlp", "mt", "mt", "nlp"], "mention_counts": {"nlp": 2, "sw": 3, "mt": 2}, "nlp_mention_counts": {"nlp": 2, "mt": 2}, "ld_mention_counts": {"sw": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "5016c3ed744329a4e17aee25b9742267bfce70ed", "url": "https://www.semanticscholar.org/paper/5016c3ed744329a4e17aee25b9742267bfce70ed", "title": "\"Linked data as background knowledge for information extraction on the web\" by Ziqi Zhang, Anna Lisa Gentile and Isabelle Augenstein with Martin Vesely as coordinator", "abstract": "Information Extraction (IE) is the technique for transforming textual data into structured representation that can be understood by machines. It is a crucial technique in enabling the Semantic Web, where increasing interest has been seen in recent years. This article reports recent progress in the LODIE project - Linked Open Data for Information Extraction, aimed at advancing Web IE to a new frontier by exploiting largely available, semantically annotated, Linked Open Data as background knowledge. We cover topics of wrapper induction, IE from semi-structured content such as tables and lists, and IE from free-text. We describe new challenges in the research and methods proposed to address them, together with summaries of recent evaluations showing encouraging results.", "venue": "LINK", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "ie", "ie", "lod", "ie", "lod", "sw"], "mention_counts": {"ld": 1, "sw": 1, "lod": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"ld": 1, "sw": 1, "lod": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "3b61d2494e7ca8405b64d4f0634103a821dedaa1", "url": "https://www.semanticscholar.org/paper/3b61d2494e7ca8405b64d4f0634103a821dedaa1", "title": "Integrating Open and Closed Information Extraction: Challenges and First Steps", "abstract": "Over the past years, state-of-the-art information extraction (IE) systems such as NELL [5] and ReVerb [9] have achieved impressive results by producing very large knowledge resources at web scale with minimal supervision. However, these resources lack the schema information, exhibit a high degree of ambiguity, and are difficult even for humans to interpret. Working with such resources becomes easier if there is a structured information base to which the resources can be linked. In this paper, we introduce the integration of open information extraction projects with Wikipedia-based IE projects that maintain a logical schema, as an important challenge for the NLP, semantic web, and machine learning communities. We describe the problem, present a gold-standard benchmark, and take the first steps towards a data-driven solution to the problem. This is especially promising, since NELL and ReVerb typically achieve a very large coverage, but still still lack a fullfl edged clean ontological structure which, on the other hand, could be provided by large-scale ontologies like DBpedia [2] or YAGO [13].", "venue": "NLP-DBPEDIA@ISWC", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ie", "nlp", "onto", "onto", "ie", "ie"], "mention_counts": {"nlp": 1, "sw": 1, "onto": 2, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ie": 3}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "2d6a08ab89d5b5b6dbcb779c8162392f9cbb261a", "url": "https://www.semanticscholar.org/paper/2d6a08ab89d5b5b6dbcb779c8162392f9cbb261a", "title": "Representing Texts as Contextualized Entity-Centric Linked Data Graphs", "abstract": "The integration of a small fraction of the information present in the Web of Documents to the Linked Data Web can provide a significant shift on the amount of information available to data consumers. However, information extracted from text does not easily fit into the usually highly normalized structure of ontology-based datasets. While the representation of structured data assumes a high level of regularity, relatively simple and consistent conceptual models, the representation of information extracted from texts need to take into account large terminological variation, complex contextual/dependency patterns, and fuzzy or conflicting semantics. This work focuses on bridging the gap between structured and unstructured data, proposing the representation of text as structured discourse graphs (SDGs), targeting an RDF representation of unstructured data. The representation focuses on a semantic best-effort information extraction scenario, where information from text is extracted under a pay-as-you-go data quality perspective, trading terminological normalization for domain-independency, context capture, wider representation scope and maximization of textual information capture.", "venue": "2013 24th International Workshop on Database and Expert Systems Applications", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "ie", "ld", "rdf", "ld", "onto"], "mention_counts": {"ld": 2, "onto": 1, "rdf": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"ld": 2, "onto": 1, "rdf": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "d2c499c6ffa9a95e9404faacacd43f01f7298491", "url": "https://www.semanticscholar.org/paper/d2c499c6ffa9a95e9404faacacd43f01f7298491", "title": "Benchmarking the Extraction and Disambiguation of Named Entities on the Semantic Web", "abstract": "Named entity recognition and disambiguation are of primary importance for extracting information and for populating knowledge bases. Detecting and classifying named entities has traditionally been taken on by the natural language processing community, whilst linking of entities to external resources, such as those in DBpedia, has been tackled by the Semantic Web community. As these tasks are treated in different communities, there is as yet no oversight on the performance of these tasks combined. We present an approach that combines the state-of-the art from named entity recognition in the natural language processing domain and named entity linking from the semantic web community. We report on experiments and results to gain more insights into the strengths and limitations of current approaches on these tasks. Our approach relies on the numerous web extractors supported by the NERD framework, which we combine with a machine learning algorithm to optimize recognition and linking of named entities. We test our approach on four standard data sets that are composed of two diverse text types, namely newswire and microposts.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 69, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "kg", "nlp", "sw", "sw", "ie", "nlp"], "mention_counts": {"nlp": 2, "sw": 3, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"sw": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "0ba745b89f6e5e71821a9d88e9e989d233e09dba", "url": "https://www.semanticscholar.org/paper/0ba745b89f6e5e71821a9d88e9e989d233e09dba", "title": "RULIE: Rule Unification for Learning Information Extraction", "abstract": "In this paper we are presenting RULIE (Rule Unification for Learning Information Extraction), an adaptive information extraction algorithm which works by employing a hybrid technique of Rule Learning and Rule Unification in order to extract relevant information from all types of documents which can be found and used in the semantic web. This algorithm combines the techniques of the LP2 and the BWI algorithms for improved performance. In this paper we are also presenting the experimental results of this algorithm and respective details of evaluation. This evaluation compares RULIE to other information extraction algorithms based on their respective performance measurements and in almost all cases RULIE outruns the other algorithms which are namely: LP2, BWI, RAPIER, SRV and WHISK. This technique would aid current techniques of linked data which would eventually lead to fullier realisation of the semantic web.", "venue": "LDH", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "sw", "ie", "ld", "ie", "ie", "sw"], "mention_counts": {"ld": 1, "sw": 2, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"ld": 1, "sw": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "d2e5207bfe52eb75b79414bda95b6c91d02e8490", "url": "https://www.semanticscholar.org/paper/d2e5207bfe52eb75b79414bda95b6c91d02e8490", "title": "KESeDa: Knowledge Extraction from Heterogeneous Semi-Structured Data Sources", "abstract": "A large part of the free knowledge existing on the Web is available as heterogeneous, semi-structured data, which is only weakly interlinked and in general does not include any semantic classification. Due to the enormous amount of information the necessary preparation of this data for integrating it in the Web of Data requires automated processes. The extraction of knowledge from structured as well as unstructured data has already been the topic of research. But especially for the semi-structured data format JSON, which is widely used as a data exchange format e.g., in social networks, extraction solutions are missing. Based on the findings we made by analyzing existing extraction methods, we present our KESeDa approach for extracting knowledge from heterogeneous, semi-structured data sources. We show how knowledge can be extracted by describing different analysis and processing steps. With the resulting semantically enriched data the potential of Linked Data can be utilized.", "venue": "International Conference on Semantic Systems", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ld"], "mention_counts": {"ld": 1, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ld": 1, "ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "8ca06ceaf94d8d25fbdc845f67d1b4806e19d0fa", "url": "https://www.semanticscholar.org/paper/8ca06ceaf94d8d25fbdc845f67d1b4806e19d0fa", "title": "Knowledge Extraction for Art History: the Case of Vasari's The Lives of The Artists", "abstract": "Knowledge Extraction (KE) techniques are used to convert unstructured information present in texts to Knowledge Graphs (KGs) which can be queried and explored. Despite their potential for cultural heritage domains, such as Art History, these techniques often encounter limitations if applied to domain-specific data. In this paper we present the main challenges that KE has to face on art-historical texts, by using as case study Giorgio Vasari\u2019s The Lives of The Artists . This paper discusses the following NLP tasks for art-historical texts, namely entity recognition and linking, coreference resolution, time extraction, motif extraction and artwork extraction. Several strategies to annotate art-historical data for these tasks and evaluate NLP models are also proposed.", "venue": "Qurator", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "nlp", "kg", "ke"], "mention_counts": {"nlp": 2, "ke": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "bfd71e2e6bf1974f2194fb66cccc855d00b7a1a9", "url": "https://www.semanticscholar.org/paper/bfd71e2e6bf1974f2194fb66cccc855d00b7a1a9", "title": "A Semantic Extraction and Sentimental Assessment of Risk Factors (SESARF): An NLP Approach for Precision Medicine: A Medical Decision Support Tool for Early Diagnosis from Clinical Notes", "abstract": "Clinical notes contain information that is crucial for the diagnosis process. However, it is usually not properly manually analyzed due to the tremendous efforts and time it takes. Hence, an automated approach is eagerly needed to maximize clinical knowledge management and reduce cost. In this paper, we propose a framework SESARF: a Semantic Extractor to identify hidden risk factors in clinical notes and a Sentimental Analyzer to assess the severity levels associated with the identified Risk Factors. This tool can be customized to any disease using Linked Open Data (LOD) by selecting a specific disease and collecting its risk factors list from medical ontologies. The extracted knowledge can serve two purposes: 1) a feature vector is prepared, for any classifier in machine learning, containing risk factors and their weights based on our semantic enrichment and sentimental analyzer and 2) a proper comparison of the extracted information with wearable body sensors that can alert any major changes in a patient's health status to personalize treatment.", "venue": "2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "lod", "ie", "ke", "nlp", "lod"], "mention_counts": {"onto": 1, "nlp": 1, "ke": 1, "lod": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "lod": 2, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "dca3c960ec60b321c0bc5c546aa8e3fbe0eb7e82", "url": "https://www.semanticscholar.org/paper/dca3c960ec60b321c0bc5c546aa8e3fbe0eb7e82", "title": "RAKER: Resource-Aware Knowledge Extraction aRchitecture on Mobile Grid", "abstract": "In this paper, we, based on mobile agent technology, propose a Resource-Aware Knowledge Extraction aRchitecture on mobile grid, named RAKER. RAKER can dynamically determine the processing and policy for achieving high-performance and high availability of knowledge extracting based on our previous proposed Resource Estimation Model. On the RAKER, users can extract information or knowledge in efficient,effective and transparent way that kept on the mobile grid without caring about the energy consumption that is most important issue in mobile computing. We show the implementation and an example to demonstrate the use of RAKER. In addition, we also measure the latency and the energy consumption and simulate the system availability to show that the performance of RAKER.", "venue": "2009 First Asian Conference on Intelligent Information and Database Systems", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ie"], "mention_counts": {"ke": 3, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "fe2b08135fbfe3d3a8846f5b328afb8a0ab1567d", "url": "https://www.semanticscholar.org/paper/fe2b08135fbfe3d3a8846f5b328afb8a0ab1567d", "title": "KIM \u2013 a semantic platform for information extraction and retrieval", "abstract": "The KIM platform provides a novel Knowledge and Information Management framework and services for automatic semantic annotation, indexing, and retrieval of documents. It provides a mature and semantically enabled infrastructure for scalable and customizable information extraction (IE) as well as annotation and document management, based on GATE.General Architecture for Text Engineering (GATE) (http://gate.ac.uk), leading NLP and IE platform developed at the University of Sheffield. Our understanding is that a system for semantic annotation should be based upon a simple model of real-world entity concepts, complemented with quasi-exhaustive instance knowledge. To ensure efficiency, easy sharing, and reusability of the metadata we introduce an upper-level ontology. Based on the ontology, a large-scale instance base of entity descriptions is maintained. The knowledge resources involved are handled by use of state-of-the-art Semantic Web technology and standards, including RDF(S) repositories, ontology middleware and reasoning. From a technical point of view, the platform allows KIM-based applications to use it for automatic semantic annotation, for content retrieval based on semantic queries, and for semantic repository access. As a framework, KIM also allows various IE modules, semantic repositories and information retrieval engines to be plugged into it. This paper presents the KIM platform, with an emphasis on its architecture, interfaces, front-ends, and other technical issues.", "venue": "Natural Language Engineering", "citationCount": 338, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "sw", "ie", "nlp", "onto", "ie", "onto"], "mention_counts": {"nlp": 1, "sw": 1, "onto": 3, "ie": 2}, "nlp_mention_counts": {"nlp": 1, "ie": 2}, "ld_mention_counts": {"sw": 1, "onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "49badb6822d79d7c3954b6da257972c3d14a2896", "url": "https://www.semanticscholar.org/paper/49badb6822d79d7c3954b6da257972c3d14a2896", "title": "Discovering Inconsistencies in PubMed Abstracts through Ontology-Based Information Extraction", "abstract": "Searching for a cure for cancer is one of the most vital pursuits in modern medicine. In that aspect microRNA research plays a key role. Keeping track of the shifts and changes in established knowledge in the microRNA domain is very important. In this paper, we introduce an Ontology-Based Information Extraction method to detect occurrences of inconsistencies in microRNA research paper abstracts. We propose a method to first use the Ontology for MIcroRNA Targets (OMIT) to extract triples from the abstracts. Then we introduce a new algorithm to calculate the oppositeness of these candidate relationships. Finally we present the discovered inconsistencies in an easy to read manner to be used by medical professionals. To our best knowledge, this study is the first ontology-based information extraction model introduced to find shifts in the established knowledge in the medical domain using research paper abstracts. We downloaded 36877 abstracts from the PubMed database. From those, we found 102 inconsistencies relevant to the microRNA domain.", "venue": "ACM International Conference on Bioinformatics, Computational Biology and Biomedicine", "citationCount": 19, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 4, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "5f4eb3e0ee8e0e6e842d1b855bb6ef22dbc098e0", "url": "https://www.semanticscholar.org/paper/5f4eb3e0ee8e0e6e842d1b855bb6ef22dbc098e0", "title": "NLP Techniques for Term Extraction and Ontology Population", "abstract": "This chapter investigates NLP techniques for ontology population, using a combination of rule-based approaches and machine learning. We describe a method for term recognition using linguistic and statistical techniques, making use of contextual information to bootstrap learning. We then investigate how term recognition techniques can be useful for the wider task of information extraction, making use of similarity metrics and contextual information. We describe two tools we have developed which make use of contextual information to help the development of rules for named entity recognition. Finally, we evaluate our ontology-based information extraction results using a novel technique we have developed which makes use of similarity-based metrics first developed for term recognition.", "venue": "Ontology Learning and Population", "citationCount": 149, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "ie", "ie", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 3, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "3b8ef03b38c2bb52053709b7a1c2a9f554590bf5", "url": "https://www.semanticscholar.org/paper/3b8ef03b38c2bb52053709b7a1c2a9f554590bf5", "title": "Ontology-Based Scalable and Portable Information Extraction System to Extract Biological Knowledge from Huge Collection of Biomedical Web Documents", "abstract": "Automated discovery and extraction of biological knowledge from biomedical web documents has become essential because of the enormous amount of biomedical literature published each year. In this paper we present an ontology-based scalable and portable information extraction system to automatically extract biological knowledge from huge collection of online biomedical web documents. Our method integrates ontology-based semantic tagging, information extraction and data mining together, automatically learns the patterns based on a few user seed tuples, and then extract new tuples from the biomedical web documents based on the discovered patterns. A novel system SPIE (Scalable and Portable Information Extraction) is implemented and tested on the PuBMed to find the chromatin protein-protein interaction and the experimental results indicate our approach is very effective in extracting biological knowledge from huge collection of biomedical web documents.", "venue": "International Conference on Wirtschaftsinformatik", "citationCount": 19, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "ie", "ie"], "mention_counts": {"onto": 3, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "8f1bdfc8ed2a1420d32c5f3b81e2b31c8fdc2cb7", "url": "https://www.semanticscholar.org/paper/8f1bdfc8ed2a1420d32c5f3b81e2b31c8fdc2cb7", "title": "The Use of Ontology in Clinical Information Extraction", "abstract": "Extracting clinical data from medical or clinical reports is a crucial effort. These records contain the most valuable pieces of evidence of treatments in humans. Integration of information extraction (IE) and ontology can produce a great tool for clinical concept extraction. The aim of this paper is to present a quick overview of the research work which has applied IE and ontology approaches in medical or clinical concepts extraction. This paper also presents our proposed framework based on the integration of both approaches mentioned above for extracting clinical concepts.", "venue": "Journal of Physics: Conference Series", "citationCount": 7, "fieldsOfStudy": ["Physics", "Computer Science"], "mentions": ["ke", "onto", "onto", "ie", "onto", "ie"], "mention_counts": {"ke": 1, "onto": 3, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "2fd2729412765cfaebdf78272aba7068bdbdc19b", "url": "https://www.semanticscholar.org/paper/2fd2729412765cfaebdf78272aba7068bdbdc19b", "title": "Open Knowledge Extraction through Compositional Language Processing", "abstract": "We present results for a system designed to perform Open Knowledge Extraction, based on a tradition of compositional language processing, as applied to a large collection of text derived from the Web. Evaluation through manual assessment shows that well-formed propositions of reasonable quality, representing general world knowledge, given in a logical form potentially usable for inference, may be extracted in high volume from arbitrary input sentences. We compare these results with those obtained in recent work on Open Information Extraction, indicating with some examples the quite different kinds of output obtained by the two approaches. Finally, we observe that portions of the extracted knowledge are comparable to results of recent work on class attribute extraction.", "venue": "Conference on Semantics in Text Processing", "citationCount": 53, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke", "ke"], "mention_counts": {"ke": 3, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "dec1e7340b69ed6b7044ffa2b73b27169e0cf000", "url": "https://www.semanticscholar.org/paper/dec1e7340b69ed6b7044ffa2b73b27169e0cf000", "title": "Ontology-Based Information Extraction System in E-Commerce Websites", "abstract": "Information extraction may help the users to query their needed information from the endless useful information. Now there are a variety of information extraction technologies, and we usually use the search engines to find our information we need from the Internet. But it is difficult to find the \"real\" information we want. So in this paper, we use the concept of the ontology to analyze the structure and content of the website, to build ontology model, in order to extract the information based on ontology from the e-commerce website for the users. In the end, the paper makes an experiment test of the text tool GATE to extract from websites and evaluate the results objectively.", "venue": "CASE", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "ie", "onto"], "mention_counts": {"onto": 4, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "97cbb076fd2bda102a8d27170669229c680cc705", "url": "https://www.semanticscholar.org/paper/97cbb076fd2bda102a8d27170669229c680cc705", "title": "Information extraction from unstructured data using RDF", "abstract": "The Internet exhibits a gigantic measure of helpful data which is generally designed for its users, which makes it hard to extract applicable information from different sources. Accordingly, the accessibility of strong, adaptable Information Extraction framework that consequently concentrate structured data such as, entities, relationships between entities, and attributes from unstructured or semi-structured sources. But somewhere during extraction of information may lead to the loss of its meaning, which is absolutely not feasible. Semantic Web adds solution to this problem. It is about providing meaning to the data and allow the machine to understand and recognize these augmented data more accurately. The proposed system is about extracting information from research data of IT domain like journals of IEEE, Springer, etc., which aid researchers and the organizations to get the data of journals in an optimized manner so the time and hard work of surfing and reading the entire journal's papers or articles reduces. Also the accuracy of the system is taken care of using RDF, the data extracted has a specific declarative semantics so that the meaning of the research papers or articles during extraction remains unchanged. In addition, the same approach shall be applied on multiple documents, so that time factor can get saved.", "venue": "2016 International Conference on ICT in Business Industry & Government (ICTBIG)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "ie", "ie", "ie", "rdf", "sw", "ie"], "mention_counts": {"sw": 1, "rdf": 2, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"sw": 1, "rdf": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "226716d236d08cb291779a2b96347ea8bf532711", "url": "https://www.semanticscholar.org/paper/226716d236d08cb291779a2b96347ea8bf532711", "title": "Ontology-Based Information Extraction of Crop Diseases on Chinese Web Pages", "abstract": "This paper proposes a method for extracting information of crop diseases on Chinese web pages. First, we define some special labels of the DOM tree[1] to partition the web page into some content blocks. Then the noise content in the web pages is eliminated according to the location and the word number of a content block. We employ an ontology-based way to implement information extraction from the content blocks. A top-down method is adopted to construct the ontology of crop diseases. In the extraction process, the concepts, relations and instances of ontology is used to extract the entities. The event is extracted by an optimal classification of paragraph groups in a content block. Experiments demonstrate the performance of the proposed method is satisfactory.", "venue": "J. Comput.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 4, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "b9dd3d0315b8811fd819b76f3d8b778c885593fa", "url": "https://www.semanticscholar.org/paper/b9dd3d0315b8811fd819b76f3d8b778c885593fa", "title": "Automatic Knowledge Base Construction using Probabilistic Extraction, Deductive Reasoning, and Human Feedback", "abstract": "We envision an automatic knowledge base construction system consisting of three inter-related components. MADden is a knowledge extraction system applying statistical text analysis methods over database systems (DBMS) and massive parallel processing (MPP) frameworks; ProbKB performs probabilistic reasoning over the extracted knowledge to derive additional facts not existing in the original text corpus; CAMeL leverages human intelligence to reduce the uncertainty resulting from both the information extraction and probabilistic reasoning processes.", "venue": "AKBC-WEKEX@NAACL-HLT", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ke", "ke", "ie"], "mention_counts": {"kg": 2, "ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"kg": 2, "ke": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "505251351b44f8db8ce7219fcea960f56eb3ae0b", "url": "https://www.semanticscholar.org/paper/505251351b44f8db8ce7219fcea960f56eb3ae0b", "title": "A system for coreference resolution for the clinical narrative", "abstract": "OBJECTIVE\nTo research computational methods for coreference resolution in the clinical narrative and build a system implementing the best methods.\n\n\nMETHODS\nThe Ontology Development and Information Extraction corpus annotated for coreference relations consists of 7214 coreferential markables, forming 5992 pairs and 1304 chains. We trained classifiers with semantic, syntactic, and surface features pruned by feature selection. For the three system components--for the resolution of relative pronouns, personal pronouns, and noun phrases--we experimented with support vector machines with linear and radial basis function (RBF) kernels, decision trees, and perceptrons. Evaluation of algorithms and varied feature sets was performed using standard metrics.\n\n\nRESULTS\nThe best performing combination is support vector machines with an RBF kernel and all features (MUC score=0.352, B(3)=0.690, CEAF=0.486, BLANC=0.596) outperforming a traditional decision tree baseline.\n\n\nDISCUSSION\nThe application showed good performance similar to performance on general English text. The main error source was sentence distances exceeding a window of 10 sentences between markables. A possible solution to this problem is hinted at by the fact that coreferent markables sometimes occurred in predictable (although distant) note sections. Another system limitation is failure to fully utilize synonymy and ontological knowledge. Future work will investigate additional ways to incorporate syntactic features into the coreference problem.\n\n\nCONCLUSION\nWe investigated computational methods for coreference resolution in the clinical narrative. The best methods are released as modules of the open source Clinical Text Analysis and Knowledge Extraction System and Ontology Development and Information Extraction platforms.", "venue": "J. Am. Medical Informatics Assoc.", "citationCount": 42, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ie", "onto", "ie", "onto", "onto", "ke"], "mention_counts": {"ke": 1, "onto": 3, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "2533e6ef309f625f891d506d013af47715f8b95f", "url": "https://www.semanticscholar.org/paper/2533e6ef309f625f891d506d013af47715f8b95f", "title": "Components for information extraction: ontology-based information extractors and generic platforms", "abstract": "Information Extraction (IE) has existed as a field for several decades and has produced some impressive systems in the recent past. Despite its success, widespread usage and commercialization remain elusive goals for this field. We identify the lack of effective mechanisms for reuse as one major reason behind this situation. Here, we mean not only the reuse of the same IE technique in different situations but also the reuse of information related to the application of IE techniques (e.g., features used for classification). We have developed a comprehensive component-based approach for information extraction that promotes reuse to address this situation. We designed this approach starting from our previous work on the use of multiple ontologies in information extraction [24]. The key ideas of our approach are \"information extractors,\" which are components of an IE system that make extractions with respect to particular components of an ontology and \"platforms for IE,\" which are domain and corpus independent implementations of IE techniques. A case study has shown that this component-based approach can be successfully applied in practical situations.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 39, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 3, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "459474126c67ee0c5517ac3eac9f2e1c0fcafda2", "url": "https://www.semanticscholar.org/paper/459474126c67ee0c5517ac3eac9f2e1c0fcafda2", "title": "Named Entity Disambiguation using Freebase and Syntactic Parsing", "abstract": "Named Entity Disambiguation (NED) is a fundamental task of semantic annotation for the Semantic Web. The task of Word Sense Disambiguation (WSD) in Ontology-Based Information Extraction (OBIE) aims to establish a link between the textual entity mention and the corresponding class in the ontology. In this paper, we propose a NED process integrated in a rule-based OBIE system for French. We show that our SVM approach can improve disambiguation efficiency using syntactic features provided by the Fips parser and popularity score features extracted from the Freebase knowledge base.", "venue": "LD4IE@ISWC", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "sw", "wsd", "ie", "onto", "onto", "kg"], "mention_counts": {"onto": 2, "sw": 1, "kg": 1, "wsd": 2, "ie": 1}, "nlp_mention_counts": {"wsd": 2, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 2, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "b3488844ad80daf10a3d6bceef64058769390306", "url": "https://www.semanticscholar.org/paper/b3488844ad80daf10a3d6bceef64058769390306", "title": "Knowledge Extraction System from Unstructured Documents", "abstract": "In this paper, we design and implement a knowledge extraction system from unstructured documents (unstructured documents are documents where the information is in natural language, and require natural language processing techniques for processing) in HTML format. Basically, the system allows to transform the content of a text that is in natural language, into structured and organized knowledge, semantically described (a Semantic Ontology). Therefore, it is proposed to generate semantic knowledge based on the extraction of entities and relationships, where entities are anything about which something can be said, and relations the interactions between entities. From the generated semantic knowledge model, it is possible to infer new knowledge, such as lexicons, taxonomies and specialized terminological bases. The system can be used by any semantic processing application, in its processes of enriching its information and knowledge", "venue": "IEEE Latin America Transactions", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "ke", "ke", "kg"], "mention_counts": {"nlp": 1, "kg": 1, "onto": 1, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "9d41230ef26d7aef8ded112f712081ad9c6ecbf8", "url": "https://www.semanticscholar.org/paper/9d41230ef26d7aef8ded112f712081ad9c6ecbf8", "title": "Identifying semantic and syntactic relations from text documents", "abstract": "Semantic and syntactic relations play an important role of applications in recent years, especially on Semantic Web, Information Retrieval, Information Extraction, and Question Answering. Semantic and syntactic relations content main ideas in the sentences or paragraphs. This paper presents our proposed algorithms for identifying semantic and syntactic relations between objects and their properties in order to enrich a domain specific ontology, namely Computing Domain Ontology, which is used in Information extraction system. We combine the methodologies of Natural Language Processing with Machine Learning in these proposed algorithms in order to extract the explicit and implicit relations. We exploit these relations from distinct resources, such as WordNet, Wikipedia and text documents of ACM Digital Libraries. We also use Natural Language Processing tools, such as OpenNLP, Stanford Lexical Dependency Parser in order to analyze and parse sentences. A random sample among 245 categories of ACM Categories is used to evaluate. Results generated show that our proposed approach achieves high precision.", "venue": "The 2015 IEEE RIVF International Conference on Computing & Communication Technologies - Research, Innovation, and Vision for Future (RIVF)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "sw", "nlp", "ie", "ie", "onto"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 2, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "1df13ee06629e5d19623855f98b32fde6f4fb6f4", "url": "https://www.semanticscholar.org/paper/1df13ee06629e5d19623855f98b32fde6f4fb6f4", "title": "Financial Knowledge Graph Based Financial Report Query System", "abstract": "Annual Financial Reports are the core in the Banking Sector to publish its financial statistics. Extracting useful information from these complex and lengthy reports involves manual process to resolve the financial queries, resulting in delays and ambiguity in investment decisions. One of the major reasons is the lack of any standardization in the format and vocabulary used in the reports. An automated system for resolution of intelligent financial queries is therefore difficult to design. Several works have been proposed to overcome these problems using Information Extraction; however, they do not address the semantic interoperability of the reports across different institutions. This work proposed an automated querying engine to answer the financial queries using Ontology based Information Extraction. For Semantic modeling of financial reports, a Financial Knowledge Graph, assisted by Financial Ontology, has been proposed. The nodes are populated with entities, while links are populated with relationships using Information Extraction applied on annual reports. Two benefits have been provided by this system to stakeholders through automation: decision making through queries and generation of custom financial stories. The work can further be extended to other domains including healthcare and academia where physical reports are used for communication.", "venue": "IEEE Access", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "kg", "ie", "kg", "ie", "onto"], "mention_counts": {"kg": 2, "onto": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "c533fc1b75166f16604611240950fbfb7ec2b019", "url": "https://www.semanticscholar.org/paper/c533fc1b75166f16604611240950fbfb7ec2b019", "title": "Constrained Semi-supervised Learning in the Presence of Unanticipated Classes", "abstract": "Traditional semi-supervised learning (SSL) techniques consider the missing labels of unlabeled datapoints as latent/unobserved variables, and model these variables, and the parameters of the model, using techniques like Expectation Maximization (EM). Such semisupervised learning techniques are widely used for Automatic Knowledge Base Construction (AKBC) tasks.\n We consider two extensions to traditional SSL methods which make it more suitable for a variety of AKBC tasks. First, we consider jointly assigning multiple labels to each instance, with a flexible scheme for encoding constraints between assigned labels: this makes it possible, for instance, to assign labels at multiple levels from a hierarchy. Second, we account for another type of latent variable, in the form of unobserved classes. In open-domain webscale information extraction problems, it is an unrealistic assumption that the class ontology or topic hierarchy we are using is complete. Our proposed framework combines structural search for the best class hierarchy with SSL, reducing the semantic drift associated with erroneously grouping unanticipated classes with expected classes. Together, these extensions allow a single framework to handle a large number of knowledge extraction tasks, including macro-reading, noun-phrase classification, word sense disambiguation, alignment of KBs to wikipedia or on-line glossaries, and ontology extension.\n To summarize, this thesis argues that many AKBC tasks which have previously been addressed separately can be viewed as instances of single abstract problem: multiview semisupervised learning with an incomplete class hierarchy. In this thesis we present a generic EM framework for solving this abstract task.", "venue": "SIGF", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "wsd", "kg", "ke"], "mention_counts": {"onto": 2, "ke": 1, "kg": 1, "wsd": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "wsd": 1, "ie": 1}, "ld_mention_counts": {"kg": 1, "onto": 2, "ke": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "70afaf892e1e88f8e9e0c98b3fcc1bf3baadd226", "url": "https://www.semanticscholar.org/paper/70afaf892e1e88f8e9e0c98b3fcc1bf3baadd226", "title": "Knowledge extraction and integration for semi-structural information in digital libraries", "abstract": "Recent years, the development of digital library builds up a tremendous digital world with millions of digitized books. However, despite the fact that digital libraries now have much more data compared with traditional libraries, extracting information and providing knowledge access are still challenges for the development of library services. Our application aims to extract information from traditional Chinese medicine (TCM) textbooks of China America Digital Academic Library (CADAL). Specifically, we extract both detailed properties of TCM entities (herbal medicines, prescriptions and attending illnesses) and their relations. The advantage is that with the extracted entity properties, relations can be discovered on semantic level. Besides, external information like famous TCM doctors as well as relevant images and videos is also integrated according to a predefined ontology model, so that the result of the integration tends to make library a knowledge provider. The primary contribution is that our application extracts knowledge out of digitized textbooks and comes to form a platform of preserving and popularizing TCM, one of the greatest culture heritages that have served health of Chinese people for over 4,000 years.", "venue": "JCDL '09", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "ke", "ke"], "mention_counts": {"ke": 2, "onto": 1, "ie": 2}, "nlp_mention_counts": {"ke": 2, "ie": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "ad384645fbef115e5b2648c20d8dd58f4f3505be", "url": "https://www.semanticscholar.org/paper/ad384645fbef115e5b2648c20d8dd58f4f3505be", "title": "The research of information extraction based on knowledge engineering", "abstract": "The system of arms information extraction based on the ontology, consists of two parts: knowledge base, processing program. It realizes the arms category determination based on text categorization, and realizes the arms object determination based on named entity recognition. It realizes the information extraction according to information extraction rules based on syntax and semantic constraint. It realizes the information integration in semantic level to some extent based on the ontology.", "venue": "2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "ie", "ie", "onto", "kg"], "mention_counts": {"kg": 1, "onto": 2, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "419d6ca6faf224c98a62ddbb5f75bd0d4ea31b6c", "url": "https://www.semanticscholar.org/paper/419d6ca6faf224c98a62ddbb5f75bd0d4ea31b6c", "title": "SPOT: Knowledge-Enhanced Language Representations for Information Extraction", "abstract": "Knowledge-enhanced pre-trained models for language representation have been shown to be more effective in knowledge base construction tasks (i.e.,~relation extraction) than language models such as BERT. These knowledge-enhanced language models incorporate knowledge into pre-training to generate representations of entities or relationships. However, existing methods typically represent each entity with a separate embedding. As a result, these methods struggle to represent out-of-vocabulary entities and a large amount of parameters, on top of their underlying token models (i.e., the transformer), must be used and the number of entities that can be handled is limited in practice due to memory constraints. Moreover, existing models still struggle to represent entities and relationships simultaneously. To address these problems, we propose a new pre-trained model that learns representations of both entities and relationships from token spans and span pairs in the text respectively. By encoding spans efficiently with span modules, our model can represent both entities and their relationships but requires fewer parameters than existing models. We pre-trained our model with the knowledge graph extracted from Wikipedia and test it on a broad range of supervised and unsupervised information extraction tasks. Results show that our model learns better representations for both entities and relationships than baselines, while in supervised settings, fine-tuning our model outperforms RoBERTa consistently and achieves competitive results on information extraction tasks.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ke", "ie", "ie", "ie"], "mention_counts": {"kg": 2, "ke": 1, "ie": 3}, "nlp_mention_counts": {"ke": 1, "ie": 3}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "4fcbf12ed7391d9863c4a39d2d27b359c964562e", "url": "https://www.semanticscholar.org/paper/4fcbf12ed7391d9863c4a39d2d27b359c964562e", "title": "Text Mining for Personalized Knowledge Extraction From Online Support Groups", "abstract": "The traditional approach to health care is being revolutionized by the rapid adoption of patient\u2010centered healthcare models. The successful transformation of patients from passive recipients to active participants is largely attributed to increased access to healthcare information. Online support groups present a platform to seek and exchange information in an inclusive environment. As the volume of text on online support groups continues to grow exponentially, it is imperative to improve the quality of retrieved information in terms of relevance, reliability, and usefulness. We present a text\u2010mining approach that generates a knowledge extraction layer to address this void in personalized information retrieval from online support groups. The knowledge extraction layer encapsulates an ensemble of text\u2010mining techniques with a domain ontology to interpose an investigable and extensible structure on hitherto unstructured text. This structure is not limited to personalized information retrieval for patients, as it also imparts aggregates for crowdsourcing analytics by healthcare researchers. The proposed approach was successfully trialed on an active online support group consisting of 800,000 posts by 72,066 participants. Demonstrations for both patient and researcher use cases accentuate the value of the proposed approach to unlock a broad spectrum of personalized and aggregate knowledge concealed within crowdsourced content.", "venue": "J. Assoc. Inf. Sci. Technol.", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "onto", "ke"], "mention_counts": {"ke": 3, "onto": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "2d03b7f30a7ab644b842bba4ae6f0f35b5721032", "url": "https://www.semanticscholar.org/paper/2d03b7f30a7ab644b842bba4ae6f0f35b5721032", "title": "A Knowledge-Based Method for Grammatical Knowledge Extraction Process", "abstract": "\u2014 This paper describes a new approach for the development of systems that requires natural language parsing or generation. This method is based on the use of Descriptive Grammars \u2013in particular, the descriptive grammar for Spanish is used\u2013 as the source for linguistic knowledge extraction. This knowledge source allows the use of classical knowledge-engineering methodologies for the extraction of rules that represent partial or complete aspects of the language, without the necessity of appealing to linguistic theories or experts. This easy method opens a new range of possibilities to the development of reliable applications that require parsing or language generation, dialog systems, information extraction, or semantic web applications.", "venue": "IKE", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ie", "ke", "sw", "ke"], "mention_counts": {"kg": 1, "sw": 1, "ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"kg": 1, "sw": 1, "ke": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "affad79c3df7719b209a3dad7e1be5324281fa9a", "url": "https://www.semanticscholar.org/paper/affad79c3df7719b209a3dad7e1be5324281fa9a", "title": "Enabling type/condition-specified entity/fact retrieval using semantic knowledge extracted from wikipedia", "abstract": "Wikipedia has recently become an important semantic knowledge resource, thanks to its semi-structured semantic features and the huge amount of user-generated content covering a wide range of topics. The mode of information retrieval on Wikipedia, as on the Web in general, however, remains that of conventional keyword-based page/document retrieval. The project presented in this paper, entitled PanAnthropon FilmWorld, aims at demonstrating direct, sophisticated entity/fact retrieval by extracting/deriving semantic knowledge from Wikipedia and by representing facts using domain-relevant classes, entities, attributes, and categories. To this end, a semantic knowledge base containing the extracted data and a semantic search interface demonstrating the proposed retrieval capability have been constructed. The focus of this paper is on the details concerning semantic knowledge extraction and derivation. However, the interface is fully functional. The results of evaluation confirm both the quality of knowledge extraction and the effectiveness of entity/fact retrieval using the interface.", "venue": "SMER '11", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "ke"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "fc2145325a3e5612d936d0da3e90de26ab85015d", "url": "https://www.semanticscholar.org/paper/fc2145325a3e5612d936d0da3e90de26ab85015d", "title": "Description and Evaluation of Semantic Similarity Measures Approaches", "abstract": "ABSTRACT In recent years, semantic similarity measure has a great interest in Semantic Web and Natural Language Processing (NLP). Several similarity measures have been developed, being given the existence of a structured knowledge representation offered by ontologies and corpus which enable semantic interpretation of terms. Semantic similarity measures compute the similarity between concepts/terms included in knowledge sources in order to perform estimations. This paper discusses the existing semantic similarity methods based on structure, information content and feature approaches. Additionally, we present a critical evaluation of several categories of semantic similarity approaches based on two standard benchmarks. The aim of this paper is to give an efficient evaluation of all these measures which help researcher and practitioners to select the measure that best fit for their requirements. General Terms Similarity Measures, Ontology, Semantic Web, NLP", "venue": "ArXiv", "citationCount": 112, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlp", "onto", "nlp", "sw", "onto", "nlp"], "mention_counts": {"nlp": 3, "sw": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 2, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "a05dc7e7e554df8a72bb154e022aaefe24c2489f", "url": "https://www.semanticscholar.org/paper/a05dc7e7e554df8a72bb154e022aaefe24c2489f", "title": "Knowledge Extraction From Texts Based on Wikidata", "abstract": "This paper presents an effort within our company of developing knowledge extraction pipeline for English, which can be further used for constructing an entreprise-specific knowledge base. We present a system consisting of entity detection and linking, coreference resolution, and relation extraction based on the Wikidata schema. We highlight existing challenges of knowledge extraction by evaluating the deployed pipeline on real-world data. We also make available a database, which can serve as a new resource for sentential relation extraction, and we underline the importance of having balanced data for training classification models.", "venue": "NAACL", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 1, "ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "13af5acb07914748d8372af6c4c43d94c7cfead3", "url": "https://www.semanticscholar.org/paper/13af5acb07914748d8372af6c4c43d94c7cfead3", "title": "Automated knowledge extraction from the federal acquisition regulations system (FARS)", "abstract": "With increasing regulation of Big Data, it is becoming essential for organizations to ensure compliance with various data protection standards. The Federal Acquisition Regulations System (FARS) within the Code of Federal Regulations (CFR) includes facts and rules for individuals and organizations seeking to do business with the US Federal government. Parsing and gathering knowledge from such lengthy regulation documents is currently done manually and is time and human intensive. Hence, developing a cognitive assistant for automated analysis of such legal documents has become a necessity. We have developed semantically rich approach to automate the analysis of legal documents and have implemented a system to capture various facts and rules contributing towards building an efficient legal knowledge base that contains details of the relationships between various legal elements, semantically similar terminologies, deontic expressions and cross-referenced legal facts and rules. In this paper, we describe our framework along with the results of automating knowledge extraction from the FARS document (Title 48, CFR). Our approach can be used by Big Data Users to automate knowledge extraction from Large Legal documents.", "venue": "2017 IEEE International Conference on Big Data (Big Data)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "871f1562a709cadad79d3ba299d311f7c68e15fa", "url": "https://www.semanticscholar.org/paper/871f1562a709cadad79d3ba299d311f7c68e15fa", "title": "Using Frame Semantics for Knowledge Extraction from Twitter", "abstract": "\n \n Knowledge bases have the potential to advance artificial intelligence, but often suffer from recall problems, i.e., lack of knowledge of new entities and relations. On the contrary, social media such as Twitter provide abundance of data, in a timely manner: information spreads at an incredible pace and is posted long before it makes it into more commonly used resources for knowledge extraction. In this paper we address the question whether we can exploit social media to extract new facts, which may at first seem like finding needles in haystacks. We collect tweets about 60 entities in Freebase and compare four methods to extract binary relation candidates, based on syntactic and semantic parsing and simple mechanism for factuality scoring. The extracted facts are manually evaluated in terms of their correctness and relevance for search. We show that moving from bottom-up syntactic or semantic dependency parsing formalisms to top-down frame-semantic processing improves the robustness of knowledge extraction, producing more intelligible fact candidates of better quality. In order to evaluate the quality of frame semantic parsing on Twitter intrinsically, we make a multiply frame-annotated dataset of tweets publicly available.\n \n", "venue": "AAAI Conference on Artificial Intelligence", "citationCount": 27, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "f424b6029a507c145aba8aef6646228565ec09ca", "url": "https://www.semanticscholar.org/paper/f424b6029a507c145aba8aef6646228565ec09ca", "title": "KEFST: a knowledge extraction framework using finite-state transducers", "abstract": "\nPurpose\nThe purpose of this research study is to extract and identify named entities from Hadith literature. Named entity recognition (NER) refers to the identification of the named entities in a computer readable text having an annotation of categorization tags for information extraction. NER is an active research area in information management and information retrieval systems. NER serves as a baseline for machines to understand the context of a given content and helps in knowledge extraction. Although NER is considered as a solved task in major languages such as English, in languages such as Urdu, NER is still a challenging task. Moreover, NER depends on the language and domain of study; thus, it is gaining the attention of researchers in different domains.\n\n\nDesign/methodology/approach\nThis paper proposes a knowledge extraction framework using finite-state transducers (FSTs) \u2013 KEFST \u2013 to extract the named entities. KEFST consists of five steps: content extraction, tokenization, part of speech tagging, multi-word detection and NER. An extensive empirical analysis using the data corpus of Urdu translation of Sahih Al-Bukhari, a widely known hadith book, reveals that the proposed method effectively recognizes the entities to obtain better results.\n\n\nFindings\nThe significant performance in terms of f-measure, precision and recall validates that the proposed model outperforms the existing methods for NER in the relevant literature.\n\n\nOriginality/value\nThis research is novel in this regard that no previous work is proposed in the Urdu language to extract named entities using FSTs and no previous work is proposed for Urdu hadith data NER.\n", "venue": "Electron. Libr.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "ke", "ke"], "mention_counts": {"ke": 3, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "608088c0135c469812cfb99f6d764bcfa1c5609c", "url": "https://www.semanticscholar.org/paper/608088c0135c469812cfb99f6d764bcfa1c5609c", "title": "Contour tracking based knowledge extraction and object recognition using deep learning neural networks", "abstract": "Object recognition in digital images are carried out using syntactic or spectral domain pattern recognition techniques. Due to ever increasing size of data collected by digital image acquisition systems there is a need to go in for developing faster, reliable and intelligent pattern recognition methods which would mostly supplement human intelligence in recognizing objects which otherwise remain latent and unnoticed. One such effort is use of deep learning neural networks for object recognition. The input to this system is knowledge extracted from the contours of various objects pre valent in a digital image. This paper advocates a novel method for extracting knowledge about the contours of various objects and components in a digital image and for recognizing objects using a neural network.", "venue": "International Conference on Next Generation Computing Technologies", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ke"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "acbfa46c0cf6d95a3ad77a0d696b8527a546f1e6", "url": "https://www.semanticscholar.org/paper/acbfa46c0cf6d95a3ad77a0d696b8527a546f1e6", "title": "Knowledge Extraction Experiment Based on Tourism Knowledge Graph Q & A Data Set", "abstract": "At present, the most natural language processing tasks use common data sets for experiments. However, as the concept of domain knowledge graphs is proposed, domain-based data sets have gradually become a demand. In this article, we collect data from various travel websites and official websites of tourist attractions, and use this to build a question and answer data set. At the same time, we also introduce the current Bert model with outstanding effect in the nlp field, and use this model to conduct experiments in the travelling question and answer data set. The experimental results not only show the feasibility of the constructed tourism data set, but also lay a foundation for the subsequent construction of a knowledge question answering system for tourism knowledge graph.", "venue": "2020 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "kg", "kg", "kg", "ke"], "mention_counts": {"nlp": 2, "kg": 3, "ke": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"kg": 3, "ke": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "29324138bcbb3ce0abde7fc281f4b2e917746b6d", "url": "https://www.semanticscholar.org/paper/29324138bcbb3ce0abde7fc281f4b2e917746b6d", "title": "Using Deep Learning for Extracting User-Generated Knowledge from Web Communities", "abstract": "The interest in user-generated content (UGC) is steadily growing, as it provides a valuable data source for companies to extract information that can be used for competitive advantage. However, mining UGC and extracting knowledge is a costly and labor-intense endeavour. Against this backdrop, the steep advancements in deep learning (DL) during the last years offer the potential to counteract this. However, DL is still in its infancy in the realm of UGC. Thus, we aim at contributing to the field of knowledge extraction of UGC by comparing traditional machine learning (ML) approaches with state-of-the-art DL models (e.g., BERT) for mining usergenerated instructions. We follow the knowledge discovery process to construct a novel corpus of user-generated instructions and develop a best-practice approach to mine knowledge from UGC. Thereby, we intend to foster a better understanding of knowledge extraction systems for UGC and provide a valuable solution to extend existing information systems.", "venue": "ECIS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke", "ke"], "mention_counts": {"ke": 3, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "b7287ef449e765c7dea2175a35fcc028b4b9bdb9", "url": "https://www.semanticscholar.org/paper/b7287ef449e765c7dea2175a35fcc028b4b9bdb9", "title": "Towards Building Open Knowledge Base From Programming Question-Answering Communities", "abstract": "In this paper, we propose the first system, so-called Open Programming Knowledge Extraction (OPKE), to automatically extract knowledge from programming Question-Answering (QA) communities. OPKE is the first step of building a programming-centric knowledge base. Data mining and Natural Language Processing techniques are leveraged to identify duplicate questions and construct structured information. Preliminary evaluation shows the effectiveness of OPKE.", "venue": "SEMWEB", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "ke", "ke", "kg"], "mention_counts": {"nlp": 1, "kg": 2, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"kg": 2, "ke": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "3de2db6e0fd81767599cb23197a3da2a883947f4", "url": "https://www.semanticscholar.org/paper/3de2db6e0fd81767599cb23197a3da2a883947f4", "title": "PERLEX: A Bilingual Persian-English Gold Dataset for Relation Extraction", "abstract": "Relation extraction is the task of extracting semantic relations between entities in a sentence. It is an essential part of some natural language processing tasks such as information extraction, knowledge extraction, and knowledge base population. The main motivations of this research stem from a lack of a dataset for relation extraction in the Persian language as well as the necessity of extracting knowledge from the growing big-data in the Persian language for different applications. In this paper, we present \"PERLEX\" as the first Persian dataset for relation extraction, which is an expert-translated version of the \"Semeval-2010-Task-8\" dataset. Moreover, this paper addresses Persian relation extraction utilizing state-of-the-art language-agnostic algorithms. We employ six different models for relation extraction on the proposed bilingual dataset, including a non-neural model (as the baseline), three neural models, and two deep learning models fed by multilingual-BERT contextual word representations. The experiments result in the maximum f-score 77.66% (provided by BERTEM-MTB method) as the state-of-the-art of relation extraction in the Persian language.", "venue": "Sci. Program.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "kg", "ke", "nlp"], "mention_counts": {"nlp": 1, "ke": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "4665f291020b54f758e47081f00ba4c4014006c9", "url": "https://www.semanticscholar.org/paper/4665f291020b54f758e47081f00ba4c4014006c9", "title": "EKDB&W'05: Workshop on Extraction of Knowledge from Databases and Warehouses", "abstract": "This chapter includes 13 selected papers for the 2005 EKDB&W Extracting Knowledge from Databases and Warehouses workshop which objective was to attract contributions related to methods for non-trivial extraction of information from data. Unsupervised Learning (Clustering, in particular) was addressed in 2 papers referring to the following proposals (i) The use of K-Means algorithm to yield clusters, based on attributes which summarize TV watching temporal behaviour; (ii) A weighted combination of the results of multiple clustering procedures. Supervised Learning was addressed in 6 papers including: (i) A new approach to evaluate the trade-off between redundancy and learning investment on the binary coding of multi-class problems' output (ECOCS); (ii) A two-stage approach for the discretization of continuous attributes which specifically provided means to evaluate incremental learning in classification problems (iii) The comparison of several supervised techniques in order to predict algae blooms.(iv) A LVQ network trained to discriminate classes of breast tissue. (v) A new evolutionary algorithm to train realistic ANN's (vi) Rough sets to generate decision rules from biometric databases which eventually helped to reduce the load of a verification system. Market Basket Analysis was addressed in 2 papers: (i) The generalization of Association Rules using a new algorithm GART based on the use of items' taxonomies; (ii) A new approach for finding the next-item for each customer in large database marketing presented a new solution to improve automatic cross-selling strategies. Finally, 3 papers addressed the issue of data and knowledge extraction dealing directly with databases and data warehouses gathering the following contributions: (i) The concept of staging schema mappings which helped to solve issues of consistency and integrity when uploading small and medium sized data repositories into a data warehouse; (ii) A flexible, and yet inexpensive, metadata repository based on standard XML technologies that eased the task of managing an Information System. (iii) A new tool to explore large amounts of spacial data with On-Line Analytical Processing techniques. Application domains were very diverse and illustrated the practical utility of the presented methodologies. Data originating from TV Audiometer systems, Retail, Water metrics, Biometric and Medical databases were considered. The EKDB&W Workshop would not have been possible without the contribution of Authors, Program Committee members and EPIA 2005 Organizers. All deserve our thanks and appreciation.", "venue": "2005 portuguese conference on artificial intelligence", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "ke", "ke"], "mention_counts": {"ke": 3, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "e24d0ed38b08b5542d8a0ca5cee3ebdfdcad31d2", "url": "https://www.semanticscholar.org/paper/e24d0ed38b08b5542d8a0ca5cee3ebdfdcad31d2", "title": "Towards Knowledge Enhanced Language Model for Machine Reading Comprehension", "abstract": "Machine reading comprehension is a crucial and challenging task in natural language processing (NLP). Recently, knowledge graph (KG) embedding has gained massive attention as it can effectively provide side information for downstream tasks. However, most previous knowledge-based models do not take into account the structural characteristics of the triples in KGs, and only convert them into vector representations for direct accumulation, leading to deficiencies in knowledge extraction and knowledge fusion. In order to alleviate this problem, we propose a novel deep model KCF-NET, which incorporates knowledge graph representations with context as the basis for predicting answers by leveraging capsule network to encode the intrinsic spatial relationship in triples of KG. In KCF-NET, we fine-tune BERT, a highly performance contextual language representation model, to capture complex linguistic phenomena. Besides, a novel fusion structure based on multi-head attention mechanism is designed to balance the weight of knowledge and context. To evaluate the knowledge expression and reading comprehension ability of our model, we conducted extensive experiments on multiple public datasets such as WN11, FB13, SemEval-2010 Task 8 and SQuAD. Experimental results show that KCF-NET achieves state-of-the-art results in both link prediction and MRC tasks with negligible parameter increase compared to BERT-Base, and gets competitive results in triple classification task with significantly reduced model size.", "venue": "IEEE Access", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "ke", "kg", "kg", "nlp"], "mention_counts": {"nlp": 2, "kg": 3, "ke": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"kg": 3, "ke": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "686f51d2b291268c86c68129285211160c1c4886", "url": "https://www.semanticscholar.org/paper/686f51d2b291268c86c68129285211160c1c4886", "title": "Coupled Semi-Supervised Learning for Chinese Knowledge Extraction", "abstract": "Robust intelligent systems may leverage knowledge about the world to cope with a variety of contexts. While automatic knowledge extraction algorithms have been successfully used to build knowledge bases in En-glish, little progress has been made in extracting non- alphabetic languages, e.g. Chinese. This paper iden-ti\ufb01es the key challenge in instance and pattern ex- traction for Chinese and presents the Coupled Chinese Pattern Learner that utilizes part-of-speech tagging and language-dependent grammar rules for gener- alized matching in the Chinese never-ending language learner framework for large-scale knowledge extraction from online documents. Experiments showed that the proposed system is scalable and achieves a precision of 79 . 9% in learning categories after a small number of iterations.", "venue": "AAAI Workshop: Knowledge Extraction from Text", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ke"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "9c2511f90acf52ae436ea81177476b23b7409728", "url": "https://www.semanticscholar.org/paper/9c2511f90acf52ae436ea81177476b23b7409728", "title": "A Korean Knowledge Extraction System for Enriching a KBox", "abstract": "The increased demand for structured knowledge has created considerable interest in knowledge extraction from natural language sentences. This study presents a new Korean knowledge extraction system and web interface for enriching a KBox knowledge base that expands based on the Korean DBpedia. The aim is to create an endpoint where knowledge can be extracted and added to KBox anytime and anywhere.", "venue": "International Conference on Computational Linguistics", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "119a5d9902a67e15e6e62d068459ad26fe749f41", "url": "https://www.semanticscholar.org/paper/119a5d9902a67e15e6e62d068459ad26fe749f41", "title": "Knowledge Extraction from C-Code", "abstract": "In this paper we present first ideas for extracting knowledge from C source code of control programs. The extracted knowledge is intended to be used in our smart control engine which takes a rule set and decides which rules to use based on the internal and environmental conditions. The extraction of rules is based on the control-flow graph of the supplied C program: Basically, our method extracts rules that correspond to paths to given high-level function calls. The advantage of this method is to get a first knowledge-base from available source code which makes using a smart control engine more applicable for industry. We use an industrial control program as example within the paper in order to justify the usefulness of our approach.", "venue": "2007 Fifth Workshop on Intelligent Solutions in Embedded Systems", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 1, "ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "fb4de23525a425c673c719771cd6593499843213", "url": "https://www.semanticscholar.org/paper/fb4de23525a425c673c719771cd6593499843213", "title": "A Novel End-to-End Multiple Tagging Model for Knowledge Extraction", "abstract": "It is an emerging research topic in NLP to joint extraction of knowledge including entities and relations from unstructured text and representing them as meaningful triplets. Despite significant progresses made by recent deep neural network based solutions, these methods still confront the overlapping issue that different relational triplets may have overlapped entities in a sentence, and it is troublesome to address this issue by current solutions. In this paper, we propose a novel multiple tagging model to address the overlapping issue and extract knowledge from unstructured text. Specifically, we devise a multiple tagging scheme that transforms the problem of joint entity and relation extraction into a multiple sequence tagging problem. By using GRU as the building block for encoding-decoding, the proposed model is capable of handling the triplet overlapping problem because the decoder layer allows one entity to take part in more than one triplet. The whole network is end-to-end trianable and outputs all triplets in a sentence directly. Experimental results on the NYT and KBP benchmarks demonstrate that the proposed model siginificantly improves the recall of triplet, and consequently, achieving the new state-of-the-art in the task of triplet extraction on both datasets.", "venue": "2019 International Joint Conference on Neural Networks (IJCNN)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "nlp", "ke"], "mention_counts": {"nlp": 1, "ke": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "2ee39a9a7d7186a351f2170ce7ebd61f8264a860", "url": "https://www.semanticscholar.org/paper/2ee39a9a7d7186a351f2170ce7ebd61f8264a860", "title": "Representing Knowledge by Spans: A Knowledge-Enhanced Model for Information Extraction", "abstract": "Knowledge-enhanced pre-trained models for language representation have been shown to be more effective in knowledge base construction tasks (i.e., relation extraction) than language models such as BERT. These knowledge-enhanced language models incorporate knowledge into pre-training to generate representations of entities or relationships. However, existing methods typically represent each entity with a separate embedding. As a result, these methods struggle to represent out-of-vocabulary entities and a large amount of parameters, on top of their underlying token models (i.e., the transformer), must be used and the number of entities that can be handled is limited in practice due to memory constraints. Moreover, existing models still struggle to represent entities and relationships simultaneously. To address these problems, we propose a new pre-trained model that learns representations of both entities and relationships from token spans and span pairs in the text respectively. By encoding spans efficiently with span modules, our model can represent both entities and their relationships but requires fewer parameters than existing models. We pre-trained our model with the knowledge graph extracted from Wikipedia and test it on a broad range of supervised and unsupervised information extraction tasks. Results show that our model learns better representations for both entities and relationships than baselines, while in supervised settings, fine-tuning our model outperforms RoBERTa consistently and achieves competitive results on information extraction tasks.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "kg", "ie", "ke", "kg", "ie"], "mention_counts": {"kg": 2, "ke": 1, "ie": 3}, "nlp_mention_counts": {"ke": 1, "ie": 3}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "a2900b21897ca70e02afee1a94550c9b6a15dcb4", "url": "https://www.semanticscholar.org/paper/a2900b21897ca70e02afee1a94550c9b6a15dcb4", "title": "Probabilistic Task Content Modeling for Episodic Textual Narratives", "abstract": "Episodic knowledge is often stored in the form of textual narratives written in natural language. However, a large repository of such narratives will contain both repetitive and novel knowledge. In this paper, we propose an approach for discovering interesting pieces of knowledge by using a priori task knowledge. By considering the narratives as generated by an underlying task structure, the elements of the task can be regarded as topics that generate the text. Then, by capturing task content in a probabilistic model, the model can be used, e.g., to identify the semantic orientation of textual phrases. An evaluation for a real world corpus of episodic narratives provides strong evidence for the feasibility of the proposed approach. Episodic Textual Narratives Discussing the use of knowledge in knowledge systems, (Richter 1998) distinguishes among three types of knowledge: background, contextual, and episodic knowledge. Out of all types of knowledge systems, episodic knowledge\u2014 which is of narrative character, because it tells the story of something that happened in the past\u2014is directly employed in Case-Based Reasoning (CBR) systems only. Since episodic knowledge has a narrative character, a natural means of preserving it is in the form of textual narratives written in natural language by human users. Extraction of valuable pieces of knowledge from such narratives, which can serve as cases in the context of a Textual CBR (TCBR) system, is the focus of our research. A common way of extracting knowledge from text documents in the context of a TCBR system is by considering a priori domain knowledge (Lenz 1999). The underlying idea is that text can be regarded as a container of domain objects (or information entities) and by using different types of knowledge acquisition, text documents can be reduced to a set of such information entities. In contrast, we take an alternative perspective. We consider text as generated by an underlying process, which consists of a series of related events. Domain objects are then participants of such events. By recognizing events and their participants in text, it is not only possible to discover domain objects, but also their respective roles in the event. In this way, text will not be a Copyright c \u00a9 2007, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. mere set of information entities, but a network of interconnected, semantically labeled entities. Additionally, a second difference to (Lenz 1999)\u2019s approach is that instead of a priori domain knowledge, we use a priori task knowledge to perform knowledge extraction. Task Knowledge: An Example Consider the task MONITOR-and-DIAGNOSE, a task that starts with the monitoring of an object and continues with diagnosis only if something problematic is observed during the monitoring step. In general, there is a distinct temporal order in the way the MONITOR-and-DIAGNOSE task is performed. 1. Some entities of interest are observed and the respective findings are noticed. 2. These findings are then explained and evaluated. 3. If findings are evaluated as negative, actions for maintenance are recommended. These three steps can be referred to as observation (OBS), explanation (EXP), and take action (ACT), and can be regarded as events that occur during the execution of the MONITOR-and-DIAGNOSE task. In the same way in which these events occur in reality, they will be described in written form, too. Each of the events is constituted by relations between different elements of the task. For example, an OBS relates an observed object to a finding, or an EXP relates a symptom to a possible cause. In our previous research, we have described an active learning approach (named LARC) that learns to annotate episodic narratives with task knowledge roles such as observed object, finding, cause, etc. (Mustafaraj, Hoof, & Freisleben 2006). Probabilistic Task Content Modeling The previous description of a task as a series of interconnected events and participant roles constitutes an abstract model of task structure. An instantiation of the task structure in a real situation produces the task content. The instantiation consists of the verbalization of the abstract events and roles with concrete sentences and phrases. Because realworld events and natural language are of stochastic nature, a probabilistic model is an appropriate means for capturing task content.", "venue": "FLAIRS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "4f152e62986f887f252b8b51859f2297ca1296f2", "url": "https://www.semanticscholar.org/paper/4f152e62986f887f252b8b51859f2297ca1296f2", "title": "Ontology learning from Italian legal texts", "abstract": "The paper reports on the methodology and preliminary results of a case study in automatically extracting ontological knowledge from Italian legislative texts. We use a fully--implemented ontology learning system (T2K) that includes a battery of tools for Natural Language Processing (NLP), statistical text analysis and machine language learning. Tools are dynamically integrated to provide an incremental representation of the content of vast repositories of unstructured documents. Evaluated results, however preliminary, show the great potential of NLP--powered incremental systems like T2K for accurate large--scale semi--automatic extraction of legal ontologies.", "venue": "Law, Ontologies and the Semantic Web", "citationCount": 35, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "0b556bf90622f059dcc5667156127e85e1af4c49", "url": "https://www.semanticscholar.org/paper/0b556bf90622f059dcc5667156127e85e1af4c49", "title": "Enhancing Question Answering by Injecting Ontological Knowledge through Regularization", "abstract": "Deep neural networks have demonstrated high performance on many natural language processing (NLP) tasks that can be answered directly from text, and have struggled to solve NLP tasks requiring external (e.g., world) knowledge. In this paper, we present OSCR (Ontology-based Semantic Composition Regularization), a method for injecting task-agnostic knowledge from an Ontology or knowledge graph into a neural network during pre-training. We evaluated the performance of BERT pre-trained on Wikipedia with and without OSCR by measuring the performance when fine-tuning on two question answering tasks involving world knowledge and causal reasoning and one requiring domain (healthcare) knowledge and obtained 33.3%, 18.6%, and 4% improved accuracy compared to pre-training BERT without OSCR.", "venue": "Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out", "citationCount": 3, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["kg", "nlp", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 3, "kg": 1, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 1, "onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "dea3e3f809e7ec2ec6ac5346f11e67c77987c0cd", "url": "https://www.semanticscholar.org/paper/dea3e3f809e7ec2ec6ac5346f11e67c77987c0cd", "title": "Knowledge Extraction from Learning Traces in Continuous Domains", "abstract": "A method is introduced to extract and transfer knowledge between a source and a target task in continuous domains and for direct policy search algorithms. The principle is (1) to use a direct policy search on the source task, (2) extract knowledge from the learning traces and (3) transfer this knowledge with a reward shaping approach. The knowledge extraction process consists in analyzing the learning traces, i.e. the behaviors explored while learning on the source task, to identify the behavioral features specific to successful solutions. Each behavioral feature is then attributed a value corresponding to the average reward obtained by the individuals exhibiting it. These values are used to shape rewards while learning on a target task. The approach is tested on a simulated ball collecting task in a continuous arena. The behavior of an individual is analyzed with the help of the generated knowledge bases.", "venue": "AAAI Fall Symposia", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 1, "ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "d5010c762c8dea4c55aeb5e9fd7b3675aa7480df", "url": "https://www.semanticscholar.org/paper/d5010c762c8dea4c55aeb5e9fd7b3675aa7480df", "title": "KB-NLG: From Knowledge Base to Natural Language Generation", "abstract": "We perform the natural language generation (NLG) task by mapping sets of Resource Description Framework (RDF) triples into text. First we investigate the impact of increasing the number of entity types in delexicalisaiton on the generation quality. Second we conduct different experiments to evaluate two widely applied language generation systems, encoder-decoder with attention and the Transformer model on a large benchmark dataset. We evaluate different models on automatic metrics, as well as the training time. To our knowledge, we are the first to apply Transformer model to this task.", "venue": "WNLP@ACL", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "rdf", "nlg", "nlg", "kg", "nlg", "rdf"], "mention_counts": {"kg": 1, "nlg": 4, "rdf": 2}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"kg": 1, "rdf": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "223b0cc07f7fae7d4e17ea3227a6006ff6e0d7fe", "url": "https://www.semanticscholar.org/paper/223b0cc07f7fae7d4e17ea3227a6006ff6e0d7fe", "title": "Translating Domain-Specific Expressions in Knowledge Bases with Neural Machine Translation", "abstract": "Our work presented in this paper focuses on the translation of domain-specific expressions represented in semantically structured resources, like ontologies or knowledge graphs. To make knowledge accessible beyond language borders, these resources need to be translated into different languages. The challenge of translating labels or terminological expressions represented in ontologies lies in the highly specific vocabulary and the lack of contextual information, which can guide a machine translation system to translate ambiguous words into the targeted domain. Due to the challenges, we train and translate the terminological expressions in the medial and financial domain with statistical as well as with neural machine translation methods. We evaluate the translation quality of domainspecific expressions with translation systems trained on a generic dataset and experiment domain adaptation with terminological expressions. Furthermore we perform experiments on the injection of external knowledge into the translation systems. Through these experiments, we observed a clear advantage in domain adaptation and terminology injection of NMT methods over SMT. Nevertheless, through the specific and unique terminological expressions, subword segmentation within NMT does not outperform a word based neural translation model.", "venue": "ArXiv", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "onto", "mt", "kg", "kg", "mt"], "mention_counts": {"kg": 2, "onto": 2, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "1f6c3532cc16467b2a9aecb679c358d4589cf04f", "url": "https://www.semanticscholar.org/paper/1f6c3532cc16467b2a9aecb679c358d4589cf04f", "title": "The Multilingual Procedural Semantic Web", "abstract": "The stated goal of the Semantic Web community is to turn the Web into a richly annotated resource, making its content more amenable to applications that involve machine reasoning. The most widely discussed language-oriented aspect of this vision involves the creation and use of an inventory of markup tags that indicate select semantic types. So, the \u201csemantics\u201d of the Semantic Web is not the semantics of full texts or even full sentences, but rather of select elements of text and extra-textual information. Moreover, the annotations are expected to be largely carried out manually, so broad coverage is unlikely, as are consistency and universal public-spiritedness on the part of annotators (cf. Doctorow, no date). Compare this to the ideal semantic web, which would be automatically generated from the unadorned web by processors that would carry out lexical disambiguation, referential disambiguation, and the interpretation of textual implicatures, such as the recognition of irony and indirect speech acts. Such full semantic interpretations of web content would serve as optimal input for machine reasoners. It is common practice in the field of AI to assume the availability of such knowledge structures \u2013 in fact, practically all work on machine reasoning over the past decades has used hand-crafted, complete, unambiguous knowledge structures as input. How that could be achieved automatically was always considered a separate issue, delegated to the NLP community. The NLP community, however, by and large abandoned the task of deep semantic analysis some 20 years ago, opting to pursue either (a) knowledge lean, \u201clowhanging fruit\u201d tasks that contribute to the configuration of better NLP applications in the near term but do not contribute to the ultimate goal of automatic text understanding or (b) method-oriented work, in which the methods themselves are of first priority and natural language serves primarily as a source of data sets. 1", "venue": "MSW", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "nlp", "nlp", "sw", "sw", "nlp"], "mention_counts": {"nlp": 3, "sw": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "d50a4877f02762a8c0e3ec6a2619575f2e759c3c", "url": "https://www.semanticscholar.org/paper/d50a4877f02762a8c0e3ec6a2619575f2e759c3c", "title": "An Ontology-Based Approach to Natural Language Generation from Coded Data in Electronic Health Records", "abstract": "The worldwide adoption of the HL7 Clinical Document Architecture (CDA) is promoting the availability of coded data (CDA entries) within sections of clinical documents. At the moment, an increasing number of studies are investigating ways to transform the narratives of CDA documents into machine process able CDA entries. This paper addresses the reverse problem, i.e. obtaining linguistic representations (sentences) from CDA entries. The approach presented employs Natural Language Generation (NLG) techniques and deals with two major tasks: content selection and content expression. The current research proposes a formal semantic representation of CDA entries and investigates how expressive domain ontologies in OWL and SPARQL SELECT queries can contribute to NLG. To validate the proposal, the study has focused on CDA entries from the History of Present Illness sections of CDA consultation notes. The results obtained are encouraging, as the clinical narratives automatically generated from these CDA entries fulfil the clinicians' expectations.", "venue": "2011 UKSim 5th European Symposium on Computer Modeling and Simulation", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "nlg", "onto", "onto", "onto", "nlg", "nlg"], "mention_counts": {"nlg": 4, "onto": 3}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "448d72e3059fc7cc2852b26236e5b11acf1ca898", "url": "https://www.semanticscholar.org/paper/448d72e3059fc7cc2852b26236e5b11acf1ca898", "title": "A Repository of Frame Instance Lexicalizations for Generation", "abstract": "Robust, statistical Natural Language Generation from Web knowledge bases is hindered by the lack of text-aligned resources. We aim to fill this gap by presenting a method for extracting knowledge from natural language text, and encode it in a format based on frame semantics and ready to be distributed in the Linked Open Data space. We run an implementation of such methodology on a collection of short documents and build a repository of frame instances equipped with fine-grained lex-icalizations. Finally, we conduct a pilot stody to investigate the feasibility of an approach to NLG based on said resource. We perform error analysis to assess the quality of the resource and manually evaluate the output of the NLG prototype.", "venue": "WebNLG", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["lod", "kg", "ke", "nlg", "nlg", "nlg"], "mention_counts": {"kg": 1, "lod": 1, "nlg": 3, "ke": 1}, "nlp_mention_counts": {"ke": 1, "nlg": 3}, "ld_mention_counts": {"kg": 1, "lod": 1, "ke": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "4b28157e005f885cb91cb582cdd965f08372f731", "url": "https://www.semanticscholar.org/paper/4b28157e005f885cb91cb582cdd965f08372f731", "title": "Integration of Neuroimaging and Microarray Datasets through Mapping and Model-Theoretic Semantic Decomposition of Unstructured Phenotypes", "abstract": "An approach towards heterogeneous neuroscience dataset integration is proposed that uses Natural Language Processing (NLP) and a knowledge-based phenotype organizer system (PhenOS) to link ontology-anchored terms to underlying data from each database, and then maps these terms based on a computable model of disease (SNOMED CT\u00ae). The approach was implemented using sample datasets from fMRIDC, GEO, The Whole Brain Atlas and Neuronames, and allowed for complex queries such as \u201cList all disorders with a finding site of brain region X, and then find the semantically related references in all participating databases based on the ontological model of the disease or its anatomical and morphological attributes\u201d. Precision of the NLP-derived coding of the unstructured phenotypes in each dataset was 88% (n = 50), and precision of the semantic mapping between these terms across datasets was 98% (n = 100). To our knowledge, this is the first example of the use of both semantic decomposition of disease relationships and hierarchical information found in ontologies to integrate heterogeneous phenotypes across clinical and molecular datasets.", "venue": "Summit on translational bioinformatics", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "onto", "kg", "nlp", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 3, "kg": 1, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 1, "onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "798489d2b0d3619246e95c0b553133183f35186a", "url": "https://www.semanticscholar.org/paper/798489d2b0d3619246e95c0b553133183f35186a", "title": "Business Insights Using Knowledge Graphs by Text Analytics in Dynamic Environments", "abstract": "Business Intelligence (BI) requires the collection and organization of important pieces of information (i.e. entities) from multiple sources to provide valuable insights (e.g. business trends) as events (i.e. a specific happening linked with a specific location and time) to users. Online news articles are one of the important information sources that present business news offered by various companies in the market every day all around the world. These news articles often cover the same events and report redundant information. Existing news platforms aim at collecting the key entities from news articles and providing a mechanism to view the latest and relevant business events based on user interest. However, they do not provide a method to model business events and understand them temporally, spatially, and contextually (i.e. changes in the event). For instance, it is crucial to know for how long a business event has been active? How important is its evolution locally, or worldwide? Or how did different companies come up with this event as competitors in the market? The contribution of this research is the exploration of the possibilities of modeling spatial, temporal, and contextual information evolution related to business events through the application of knowledge graphs and text analytics, more specifically, Natural Language Processing (NLP) methods. The constructed knowledge graphs through Named-Entity Recognition (NER), i.e., an NLP technique, present a compact news representation that tells the key entities of the business event at one glance using linked open data concepts. It enables the assessment of other related news events as well as provides the means for analysis of the influence and evolution of business events.", "venue": "International ACM Conference on Management of Emergent Digital EcoSystems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "lod", "kg", "kg", "nlp", "nlp"], "mention_counts": {"nlp": 3, "kg": 3, "lod": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 3, "lod": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "01fd5d3a300b6b8f8f2478d0d5e5250b1a851df0", "url": "https://www.semanticscholar.org/paper/01fd5d3a300b6b8f8f2478d0d5e5250b1a851df0", "title": "The lexical properties of the gene ontology", "abstract": "The Gene Ontology (GO) is a construct developed for the purpose of annotating molecular information about genes and their products. The ontology is a shared resource developed by the GO Consortium, a group of scientists who work on a variety of model organisms. In this paper we investigate the nature of the strings found in the Gene Ontology and evaluate them for their usefulness in natural language processing (NLP). We extend previous work that identified a set of properties that reliably identifies natural language phrases in the Unified Medical Language System (UMLS). The results indicate that a large percentage (79%) of GO terms are potentially useful for NLP applications. Some 35% of the GO terms were found in a corpus derived from the MEDLINE bibliographic database, and 27% of the terms were found in the current edition of the UMLS.", "venue": "American Medical Informatics Association Annual Symposium", "citationCount": 49, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "onto", "nlp", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "9a36f572553dd3e1c69ff16dd694ad2f07a55a13", "url": "https://www.semanticscholar.org/paper/9a36f572553dd3e1c69ff16dd694ad2f07a55a13", "title": "Semantic Restructuring of Natural Language Image Captions to Enhance Image Retrieval", "abstract": "The rapid growth in the volume of visual information can make the task of finding and accessing visual information of interest, overwhelming for users. Semantic analysis of image captions can be used in conjunction with image retrieval systems (IMR) to retrieve selected images more precisely. To do this, we first exploit a Natural Language Processing (NLP) framework in order to extract concepts from image captions. Next, an ontology-based framework is deployed in order to resolve natural language ambiguities. The novelty of the proposed framework is that the combination of LSI with the Ontology framework enables the combined framework to tolerate ambiguities and variations in the Ontology. A key feature is that the system can find indirectly relevant concepts in image captions and thus leverage these to represent the semantics of images at a higher level. Experimental results show that the use of LSI based NLP combined with an ontological framework significantly enhances image retrieval.", "venue": "J. Multim.", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "7d02d1e7e1b65cf2877d8167360c8a8881617574", "url": "https://www.semanticscholar.org/paper/7d02d1e7e1b65cf2877d8167360c8a8881617574", "title": "Coupling Ontology Driven Semantic Representation with Multilingual Natural Language Generation for Tuning International Terminologies", "abstract": "OBJECTIVES\nThe importance of clinical communication between providers, consumers and others, as well as the requisite for computer interoperability, strengthens the need for sharing common accepted terminologies. Under the directives of the World Health Organization (WHO), an approach is currently being conducted in Australia to adopt a standardized terminology for medical procedures that is intended to become an international reference.\n\n\nMETHOD\nIn order to achieve such a standard, a collaborative approach is adopted, in line with the successful experiment conducted for the development of the new French coding system CCAM. Different coding centres are involved in setting up a semantic representation of each term using a formal ontological structure expressed through a logic-based representation language. From this language-independent representation, multilingual natural language generation (NLG) is performed to produce noun phrases in various languages that are further compared for consistency with the original terms.\n\n\nRESULTS\nOutcomes are presented for the assessment of the International Classification of Health Interventions (ICHI) and its translation into Portuguese. The initial results clearly emphasize the feasibility and cost-effectiveness of the proposed method for handling both a different classification and an additional language.\n\n\nCONCLUSION\nNLG tools, based on ontology driven semantic representation, facilitate the discovery of ambiguous and inconsistent terms, and, as such, should be promoted for establishing coherent international terminologies.", "venue": "MedInfo", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlg", "nlg", "nlg", "onto", "onto", "nlg", "onto"], "mention_counts": {"nlg": 4, "onto": 3}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "1667ba6154ed69d8870ad27ea2db1fba92f063af", "url": "https://www.semanticscholar.org/paper/1667ba6154ed69d8870ad27ea2db1fba92f063af", "title": "Construcci\u00f3n de una base de conocimiento l\u00e9xico multil\u00edng\u00fce de amplia cobertura: Multilingual Central Repository", "abstract": "The use of wide coverage and general domain semantic resources has become a common practice and often necesary by existing systems Natural Language Processing (NLP). WordNet is by far the most widely used semantic resource in NLP. Following the success of WordNet, the EuroWordNet project has designed a multilingual semantic infrastructure to develop wordnets for a set of European languages. In EuroWordNet, these wordnets are interconnected with links stored in the Inter-Lingual Index (ILI). Following the EuroWordNet architecture, the MEANING project has developed the first versions of Multilingual Central Repository (MCR) using WordNet 1.6 as ILI. Thus, maintaining the compatibility between wordnets of different languages \u200b\u200band versions. This version of the MCR integrates six different versions of the English WordNet (1.6 to 3.0) and wordnets in Spanish, Catalan, Basque and Italian, along with more than a million semantic relationships between concepts and semantic properties different ontologies. We recently developed a new version of MCR using WordNet 3.0 as ILI. This new version of the MCR integrates wordnets of five different languages: English, Spanish, Catalan, Basque and Galician. The current version of MCR, like the previous one, systematically integrates thousands of semantic relations between concepts. In addition, the MCR is enriched with about 460,000 semantic and ontological properties including Base Level Concepts, Top Ontology, WordNet Domains and AdimenSUMO, providing all ontological consistency the integrated semantic wordnets and resources on it.", "venue": "Linguam\u00e1tica", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "2c13b0d69a5a1c05d231ce3eeb45a95cc42976ec", "url": "https://www.semanticscholar.org/paper/2c13b0d69a5a1c05d231ce3eeb45a95cc42976ec", "title": "An NLP-based ontology population for a risk management generic structure", "abstract": "In this paper we propose an NLP-based Ontology Population approach for a Generic Structure instantiation from natural language texts, in the domain of Risk Management. The approach is semi-automatic and based on combined NLP techniques using domain expert intervention for control and validation. It relies on the predicative power of verbs in the instantiation process. It is not domain dependent since it heavily relies on linguistic knowledge.\n We demonstrate the effectiveness of our method on the ontology of the PRIMA project (supported by the European community) and we populate this generic domain ontology via an available corpus. A first validation of the approach is done through an experiment with Chemical Fact Sheets from Environmental Protection Agency.", "venue": "International Conference on Soft Computing as Transdisciplinary Science and Technology", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "6a714159fd367d904309544632d7b4633d9be119", "url": "https://www.semanticscholar.org/paper/6a714159fd367d904309544632d7b4633d9be119", "title": "Information retrieval in folktales using natural language processing", "abstract": "Our aim is to extract information about literary characters in unstructured texts. We employ natural language processing and reasoning on domain ontologies. The first task is to identify the main characters and the parts of the story where these characters are described or act. We illustrate the system in a scenario in the folktale domain. The system relies on a folktale ontology that we have developed based on Propp's model for folktales morphology. Index Terms\u2014Natural language processing, ontologies, literary character, folktales.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "4e223a0864b4f79b21a3b21c2575a589d7c02975", "url": "https://www.semanticscholar.org/paper/4e223a0864b4f79b21a3b21c2575a589d7c02975", "title": "An Ontological Metro Accident Case Retrieval Using CBR and NLP", "abstract": "Metro accidents are apt to cause serious consequences, such as casualties or heavy economic loss. Once accidents occur, quick and accurate decision-making is essential to prevent emergent accidents from getting worse, which remains a challenge due to the lack of efficient knowledge representation and retrieval. In this research, an ontological method that integrates case-based reasoning (CBR) and natural language processing (NLP) techniques was proposed for metro accident case retrieval. An ontological model was developed to formalize the representation of metro accident knowledge, and then, the CBR aimed to retrieve similar past cases for supporting decision-making after the accident cases were annotated by the NLP technique. Rule-based reasoning (RBR), as a complementary of CBR, was used to decide the appropriate measures based on those that are recorded in regulations, such as emergency plans. A total of 120 metro accident cases were extracted from the safety monthly reports during metro operations and then built into the case library. The proposed method was tested in MyCBR and evaluated by expert reviews, which had an average precision of 91%.", "venue": "Applied Sciences", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 4, "onto": 3}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "02dcfe30ec70f75b8471fc255c94d8f1ed671c10", "url": "https://www.semanticscholar.org/paper/02dcfe30ec70f75b8471fc255c94d8f1ed671c10", "title": "Knowledge-based best of breed approach for automated detection of clinical events based on German free text digital hospital discharge letters", "abstract": "Objectives The secondary use of medical data contained in electronic medical records, such as hospital discharge letters, is a valuable resource for the improvement of clinical care (e.g. in terms of medication safety) or for research purposes. However, the automated processing and analysis of medical free text still poses a huge challenge to available natural language processing (NLP) systems. The aim of this study was to implement a knowledge-based best of breed approach, combining a terminology server with integrated ontology, a NLP pipeline and a rules engine. Methods We tested the performance of this approach in a use case. The clinical event of interest was the particular drug-disease interaction \u201cproton-pump inhibitor [PPI] use and osteoporosis\u201d. Cases were to be identified based on free text digital discharge letters as source of information. Automated detection was validated against a gold standard. Results Precision of recognition of osteoporosis was 94.19%, and recall was 97.45%. PPIs were detected with 100% precision and 97.97% recall. The F-score for the detection of the given drug-disease-interaction was 96,13%. Conclusion We could show that our approach of combining a NLP pipeline, a terminology server, and a rules engine for the purpose of automated detection of clinical events such as drug-disease interactions from free text digital hospital discharge letters was effective. There is huge potential for the implementation in clinical and research contexts, as this approach enables analyses of very high numbers of medical free text documents within a short time period.", "venue": "PLoS ONE", "citationCount": 10, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["nlp", "nlp", "onto", "nlp", "nlp", "kg", "kg"], "mention_counts": {"nlp": 4, "onto": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "9109aeeb4225fb438f3dcd4403a5e80087c511ee", "url": "https://www.semanticscholar.org/paper/9109aeeb4225fb438f3dcd4403a5e80087c511ee", "title": "Domain Experts and Natural language Processing in the Evaluation of Circular Economy Business Model Ontology", "abstract": "There are efforts been made globally, to switch from the take-make-dispose linear economy to Circular Economy (CE) due to the effects and wasteful nature of linear economic model. However, there are challenges and barriers that are keeping businesses from this transition despite the benefits. One of such challenge is uncertainty regarding the residual value of used products which often lead to incentives misalignment among others. While research efforts are being made to proffer solutions by proposing circular business models, the question of how reliable the new models are remains unanswered. In this paper, we propose a hybrid model of domain experts and Natural Language Processing (NLP) techniques for evaluating and validating CE Business Model (CEBM) ontology. Textual data on competency questions and comments is collected from domain experts and used in the proposed model. The model identified gaps in the ontology used here, calling for further work on the ontology and the result appear satisfactory to the domain experts who participated.", "venue": "2021 IEEE 15th International Conference on Semantic Computing (ICSC)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "4b002556fd2a95beb50024be18379acd322632d3", "url": "https://www.semanticscholar.org/paper/4b002556fd2a95beb50024be18379acd322632d3", "title": "Cognitive Computing based Question-Answering System for Teaching Electrical Motor Concepts", "abstract": "Today is the era of \u201dBig Data\u201d, and one has to spend ample amount of time to extract the meaningful information from such a huge store of data. It leads towards such a question answering system which can offer exact and precise answers to user queries. For that there is a requirement of understanding user queries effectively. Thus this paper proposes a cognitive computing powered question answering system in the field of education, which posses the power of Natural Language Processing (NLP). Here, cognitive computing provides the methods for synergism of several powers into a single architecture, NLP provides understanding of the user questions effectively, and Ontology endows with the techniques for the construction of robust knowledge base. So for the realistic implementation of the proposed architecture, the education domain has chosen and will be teaching electrical motor concepts to the edification of the students. General Terms Question-Answering System, NLP, Ontology.", "venue": "International Journal of Computer Applications", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "nlp", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 4, "kg": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "0389b12b6f752cb060652f0f05b83612db8f9788", "url": "https://www.semanticscholar.org/paper/0389b12b6f752cb060652f0f05b83612db8f9788", "title": "Knowledge-leveraged Computational Thinking through Natural Language Processing and Statistical Logic (NII Shonan Meeting 2011-4)", "abstract": "This talk describes a recent effort on the development of a textual entailment data set. Rather than assuming a sub-component of applications like question answering and multi-document summarization, we focus on a realworld task to judge whether a natural language proposition is true or false according to a given text. I will describe the design of resource development and features of the obtained resource. Unsupervised Semantic Parsing Pedro Domingos, Washington University Extracting knowledge from text has long been a goal of AI and NLP, but progress has been difficult. Manual approaches are too brittle, and supervised learning ones require an unrealistic quantity and quality of labeled data. To address this problem, we have recently developed the first unsupervised approach to semantic parsing (i.e., translating raw text into a formal meaning representation). It is based on Markov logic, which combines Markov networks and first-order logic. Our USP system transforms dependency trees into quasilogical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The most probable semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We have evaluated our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP greatly outperforms previous approaches on this task. (Joint work with Hoifung Poon.) A general MCMC method for logic-based probabilistic modeling Taisuke Sato, Titech I present a general MCMC method for PRISM, a logic-based probabilistic modeling language. It is a generalization of an MCMC method for PCFGs to the one for PRISM that covers from Bayesian networks to probabilistic grammars. I describe how to estimate the marginal probability of data from MCMC samples and how to perform Bayesian Viterbi inference using an example of Naive Bayes model augmented with a hidden variable.", "venue": "NII Shonan Meet. Rep.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "kg", "nlp", "ke"], "mention_counts": {"nlp": 2, "ke": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "ef38c6e46b7dea556c4253d7fdcc533b4db08ebf", "url": "https://www.semanticscholar.org/paper/ef38c6e46b7dea556c4253d7fdcc533b4db08ebf", "title": "PoliGraph: Automated Privacy Policy Analysis using Knowledge Graphs", "abstract": "Privacy policies disclose how an organization collects and handles personal information. Recent work has made progress in leveraging natural language processing (NLP) to automate privacy policy analysis and extract collection statements from different sentences, considered in isolation from each other. In this paper, we view and analyze, for the \ufb01rst time, the entire text of a privacy policy in an integrated way. In terms of methodology: (1) we de\ufb01ne P OLI G RAPH , a type of knowledge graph that captures different relations between different parts of the text in a privacy policy; and (2) we develop an NLP-based tool, P OLI G RAPH - ER , to automatically extract P OLI G RAPH from the text. In addition, (3) we revisit the notion of ontologies, previously de\ufb01ned in heuristic ways, to capture subsumption relations between terms. We make a clear distinction between local and global ontologies to capture the context of individual privacy policies, application domains, and privacy laws. Using a public dataset for evaluation, we show that P OLI G RAPH - ER identi\ufb01es 61% more collection statements than prior state-of-the-art, with over 90% precision. In terms of applications, P OLI G RAPH enables automated analysis of a corpus of privacy policies and allows us to: (1) reveal common patterns in the texts across different privacy policies, and (2) assess the correctness of the terms as de\ufb01ned within a privacy policy. We also apply P OLI G RAPH to: (3) detect contradictions in a privacy policy\u2014we show false posi-tives by prior work, and (4) analyze the consistency of privacy policies and network traf\ufb01c, where we identify signi\ufb01cantly more clear disclosures than prior work.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "onto", "onto", "kg", "kg"], "mention_counts": {"nlp": 3, "onto": 2, "kg": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "8853907ba99148819d309702b566149fbaebee0c", "url": "https://www.semanticscholar.org/paper/8853907ba99148819d309702b566149fbaebee0c", "title": "The MultiTal NLP tool infrastructure", "abstract": "This paper gives an overview of the MultiTal project, which aims to create a research infrastructure that ensures long-term distribution of NLP tools descriptions. The goal is to make NLP tools more accessible and usable to end-users of different disciplines. The infrastructure is built on a meta-data scheme modelling and standardising multilingual NLP tools documentation. The model is conceptualised using an OWL ontology. The formal representation of the ontology allows us to automatically generate organised and structured documentation in different languages for each represented tool.", "venue": "LT4DH@COLING", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 3}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "03b7fe82dd7b08a9d86e39ed9a4d4000d90308b4", "url": "https://www.semanticscholar.org/paper/03b7fe82dd7b08a9d86e39ed9a4d4000d90308b4", "title": "Abstractive Text Summarization: Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization", "abstract": "Abstract Nowadays, most research conducted in the field of abstractive text summarization focuses on neural-based models alone, without considering their combination with knowledge-based approaches that could further enhance their efficiency. In this direction, this work presents a novel framework that combines sequence-to-sequence neural-based text summarization along with structure and semantic-based methodologies. The proposed framework is capable of dealing with the problem of out-of-vocabulary or rare words, improving the performance of the deep learning models. The overall methodology is based on a well-defined theoretical model of knowledge-based content generalization and deep learning predictions for generating abstractive summaries. The framework is composed of three key elements: (i) a pre-processing task, (ii) a machine learning methodology, and (iii) a post-processing task. The pre-processing task is a knowledge-based approach, based on ontological knowledge resources, word sense disambiguation, and named entity recognition, along with content generalization, that transforms ordinary text into a generalized form. A deep learning model of attentive encoder-decoder architecture, which is expanded to enable a coping and coverage mechanism, as well as reinforcement learning and transformer-based architectures, is trained on a generalized version of text-summary pairs, learning to predict summaries in a generalized form. The post-processing task utilizes knowledge resources, word embeddings, word sense disambiguation, and heuristic algorithms based on text similarity methods in order to transform the generalized version of a predicted summary to a final, human-readable form. An extensive experimental procedure on three popular data sets evaluates key aspects of the proposed framework, while the obtained results exhibit promising performance, validating the robustness of the proposed approach.", "venue": "International Conference on Computational Logic", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "wsd", "kg", "wsd", "kg", "wsd", "onto"], "mention_counts": {"kg": 3, "wsd": 3, "onto": 1}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"kg": 3, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "fa562cd4a1e794273780296e77d9a72dc1dd9433", "url": "https://www.semanticscholar.org/paper/fa562cd4a1e794273780296e77d9a72dc1dd9433", "title": "Triple Prediction from Texts by Using Distributed Representations of Words", "abstract": "Knowledge graphs have been shown to be useful to many tasks in artificial intelligence. Triples of knowledge graphs are traditionally structured by human editors or extracted from semi-structured information; however, editing is expensive, and semi-structured information is not common. On the other hand, most such information is stored as text. Hence, it is necessary to develop a method that can extract knowledge from texts and then construct or populate a knowledge graph; this has been attempted in various ways. Currently, there are two approaches to constructing a knowledge graph. One is open information extraction (Open IE), and the other is knowledge graph embedding; however, neither is without problems. Stanford Open IE, the current best such system, requires labeled sentences as training data, and knowledge graph embedding systems require numerous triples. Recently, distributed representations of words have become a hot topic in the field of natural language processing, since this approach does not require labeled data for training. These require only plain text, but Mikolov showed that it can perform well with the word analogy task, answering questions such as, \u201ca is to b as c is to ?.\u201d This can be considered as a knowledge extraction task from a text for finding the missing entity of a triple. However, the accuracy is not sufficiently high when applied in a straightforward manner to relations in knowledge graphs, since the method uses only one triple as a positive example. In this paper, we analyze why distributed representations perform such tasks well; we also propose a new method for extracting knowledge from texts that requires much less annotated data. Experiments show that the proposed method achieves considerable improvement compared with the baseline; in particular, the improvement in HITS@10 was more than doubled for some relations. key words: distributed representations of words, knowledge extraction, knowledge graph completion", "venue": "IEICE Trans. Inf. Syst.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "kg", "ke", "kg", "kg", "kg", "kg", "kg", "ke", "ie", "ke", "kg", "kg"], "mention_counts": {"nlp": 1, "ke": 4, "kg": 8, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 4, "ie": 1}, "ld_mention_counts": {"ke": 4, "kg": 8}, "relevance_score": 0.666059299203733}, {"paperId": "6a878e4b4e92d213a376a381957ee23adb379163", "url": "https://www.semanticscholar.org/paper/6a878e4b4e92d213a376a381957ee23adb379163", "title": "Ontology Guided Information Extraction from Unstructured Text", "abstract": "In this paper, we describe an approach topopulate an existing ontology with instance information present in the natural language text provided as input.An ontology is defined as an explicit conceptualizatio n of a shared domain [18]. This approach starts with a list of relevant domain ontologies created by human experts, and techniques for identifying the most appropriateontology to be extended with information from a given text. Then we demonstrate heuristics to extract information from the unstructured text and for adding it as structured information to the selected ontology . This identification of the relevant ontology is critical, as it is used in identifying relevant informationin the text. We extract information in the form of semantic triples from the text, guided by the concepts in the ontology. We then convert the extracted information about the semantic class instances into Resource Description Framework (RDF 3 ) and append it to the existing domain ontology. This enables us to perform more precise semantic queries over the semantic triple store thus created. We have achieved 95% accuracy of information extraction in our implementation.", "venue": "ArXiv", "citationCount": 41, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie", "onto", "rdf", "onto", "ie", "onto", "rdf", "onto", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 8, "rdf": 2, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 8, "rdf": 2}, "relevance_score": 0.663953241522736}, {"paperId": "be389da5786bde0fd9e9f8fdda83fba730ab1463", "url": "https://www.semanticscholar.org/paper/be389da5786bde0fd9e9f8fdda83fba730ab1463", "title": "HO2IEV: Heavyweight ontology based web information extraction technique for visionless users", "abstract": "As the internet grows rapidly, millions of web pages are being added on a daily basis. The extraction of precise information is becoming more and more difficult as the volume of data on the internet increases. Several search engines and information fetching tools are available on the internet, all of which claim to provide the best crawling facilities. For the most part, these search engines are keyword based. This poses a problem for visually impaired people who want to get the full use from online resources available to other users. Visually impaired users require special aid to get along with any given computer system. Interface and content management are no exception, and special tools are required to facilitate the extraction of relevant information from the internet for visually impaired users. The HO2IEV (Heavyweight Ontology Based Information Extraction for Visually impaired User) architecture provides a mechanism for highly precise information extraction using heavyweight ontology and built-in vocal command system for visually impaired internet users. Our prototype intelligent system not only integrates and communicates among different tools, such as voice command parsers, domain ontology extractors and short message engines, but also introduces an autonomous mechanism of information extraction (hereafter referred to as IE) using heavyweight ontology. In this paper we designed domain specific heavyweight ontology using OWL (Web Ontology Language) for ontology modeling and PAL (Prot\u00e9g\u00e9 Axiom Language) for axiom writing. We introduced a novel autonomous mechanism for IE by developing prototype software. A series of experiments were designed for the testing and analysis of the performance of heavyweight ontology in general, and our information extraction prototype specifically.", "venue": "The 7th International Conference on Networked Computing and Advanced Information Management", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "onto", "ie", "onto", "onto", "ie", "ie", "onto", "ie", "onto", "onto", "onto"], "mention_counts": {"onto": 10, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 10}, "relevance_score": 0.663953241522736}, {"paperId": "b046a50a36589ae031bbf2924be9b2888c3100c8", "url": "https://www.semanticscholar.org/paper/b046a50a36589ae031bbf2924be9b2888c3100c8", "title": "Knowledge Extraction Method for Power Grid Fault Text Based on Ontology", "abstract": "The power industry usually records equipment failures, defects and other information in the form of text, which contains lots of regular patterns. Knowledge extraction in fault text is of great significance to improve efficiency and reduce the labor cost in the power industry. However, the research for knowledge extraction of text information in this field is rare, it is even more difficult to use machine learning algorithms to mine the deep patterns. To solve this problem, a method of knowledge extraction is proposed in this field. We use power equipment fault texts and relevant guidance as raw materials. Firstly, the knowledge base of this field is designed and constructed based on the ontology concepts, including ontology concept base, description base and regular expression base. Then, the knowledge extraction algorithm is designed according to the knowledge base. After that we conduct the knowledge merge operation to make the extraction results more accurate. Experiments on the real fault texts shows the feasibility and the high accuracy of our method when compared with artificial extraction.", "venue": "Journal of Physics: Conference Series", "citationCount": 0, "fieldsOfStudy": ["Physics", "Computer Science"], "mentions": ["ke", "ke", "kg", "ke", "onto", "kg", "onto", "ke", "ke", "onto"], "mention_counts": {"ke": 5, "onto": 3, "kg": 2}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5, "onto": 3, "kg": 2}, "relevance_score": 0.663953241522736}, {"paperId": "148941d7167df6a75659eca4dad7cd9143b5f71c", "url": "https://www.semanticscholar.org/paper/148941d7167df6a75659eca4dad7cd9143b5f71c", "title": "Ontology-Driven Human Language Technology for Semantic-Based Business Intelligence", "abstract": "In this poster submission, we describe the actual state of development of textual analysis and ontology-based information extraction in real world applications, as they are defined in the context of the European R&D project \u201cMUSING\u201d dealing with Business Intelligence. We present in some details the actual state of ontology development, including a time and domain ontologies, which are guiding information extraction onto an ontology population task.", "venue": "ECAI", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "onto", "ie", "onto", "hlt"], "mention_counts": {"hlt": 1, "onto": 5, "ie": 2}, "nlp_mention_counts": {"hlt": 1, "ie": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "2140e45b351137f53bd85c70f80c19d0c82c1614", "url": "https://www.semanticscholar.org/paper/2140e45b351137f53bd85c70f80c19d0c82c1614", "title": "Feeding OWL: Extracting and Representing the Content of Pathology Reports", "abstract": "This paper reports on an ongoing project that combines NLP with semantic web technologies to support a content-based storage and retrieval of medical pathology reports. We describe the NLP component of the project (a robust parser) and the background knowledge component (a domain ontology represented in OWL), and how they work together during extraction of domain specific information from natural language reports. The system provides a good example of how NLP techniques can be used to populate the Semantic Web.", "venue": "NLPXML@ACL", "citationCount": 22, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "sw", "nlp", "onto", "sw", "nlp", "onto"], "mention_counts": {"nlp": 3, "sw": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 2, "onto": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "a98f7b2bd334973a8ecaacf4aa5b4fe8e48ac3b9", "url": "https://www.semanticscholar.org/paper/a98f7b2bd334973a8ecaacf4aa5b4fe8e48ac3b9", "title": "A Heuristic Grafting Strategy for Manufacturing Knowledge Graph Extending and Completion Based on Nature Language Processing: KnowTree", "abstract": "Applied to search, question answering, and semantic web of close-or-open domain, knowledge graph (KG) is known for its incompleteness subject to the rapid knowledge growing pace. Inspired by the agricultural grafting technology to fruit variety, this paper proposes a heuristic knowledge grafting strategy (HGS) for manufacturing knowledge graph (MKG) named KnowTree extending and completion with natural language processing (NLP) mining engineering cases document. Based on similarity analysis, firstly the grafting related definitions and mechanisms (completeness, relatedness, connectivity and reutilization) are defined. Then, focused on the four mechanisms, HGS takes a pair same engineering documents as input. KnowWords is built as a collection of KnowScion, and each scion is mined from engineering documents based on the SAO structure network, whose importance is evaluated by SAORank counting the in-out degree of centrality. On another hand, the KnowRoot system is designed based on the extended P & S ontology model to characterize the structure of abstract document into four sub-space of knowledge: know-what (problem), know-why (context), know-how (solution) and know-with (result), where a pre-trained language representation model K-BERT is used to classify the KnowScion candidates into the designed KnowRoot system with a fine-tuning classification task. In the knowledge grafting process, the connection unit is constructed based on the extracted domain knowledge triples of the K-BERT model, where the head element of a triple is from the KnowScion candidate set KnowWords satisfying the threshold value, the tail element is from the domain MKG to be extended, and a connection factor is used to evaluate the relationship of union combination. To the goal of knowledge reuse, the path based reasoning rules are designed for KnowTree reutilization. Finally, take the latest engineering case abstract (ECA) in whitegoods domain as resources, a case study is conducted to validate the proposed HGS strategy.", "venue": "IEEE Access", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "kg", "nlp", "kg", "nlp", "kg", "nlp", "onto"], "mention_counts": {"nlp": 3, "sw": 1, "onto": 1, "kg": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 1, "onto": 1, "kg": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "0e182e9b05b687c5ddc2d162b45dea80923e025f", "url": "https://www.semanticscholar.org/paper/0e182e9b05b687c5ddc2d162b45dea80923e025f", "title": "Artificial Intelligence for the Early Design Phases of Space Missions", "abstract": "Recent introduction of data mining methods has led to a paradigm shift in the way we can analyze space data. This paper demonstrates that Artificial Intelligence (AI), and especially the field of Knowledge Representation and Reasoning (KRR), could also be successfully employed at the start of the space mission life cycle via an Expert System (ES) used as a Design Engineering Assistant (DEA). An ES is an AI-based agent used to solve complex problems in particular fields. There are many examples of ES being successfully implemented in the aeronautical, agricultural, legal or medical fields. Applied to space mission design, and in particular, in the context of concurrent engineering sessions, an ES could serve as a knowledge engine and support the generation of the initial design inputs, provide easy and quick access to previous design decisions or push to explore new design options. Integrated to the User design environment, the DEA could become an active assistant following the design iterations and flagging model inconsistencies. Today, for space missions design, experts apply methods of concurrent engineering and Model-Based System Engineering, relying both on their implicit knowledge (i.e., past experiences, network) and on available explicit knowledge (i.e., past reports, publications, data sheets). The former knowledge type represents still the most significant amount of data, mostly unstructured, non-digital or digital data of various legacy formats. Searching for information through this data is highly time-consuming. A solution is to convert this data into structured data to be stored into a Knowledge Graph (KG) that can be traversed by an inference engine to provide reasoning and deductions on its nodes. Knowledge is extracted from the KG via a User Interface (UI) and a query engine providing reliable and relevant knowledge summaries to the Human experts. The DEA project aims to enhance the productivity of experts by providing them with new insights into a large amount of data accumulated in the field of space mission design. Natural Language Processing (NLP), Machine Learning (ML), Knowledge Management (KM) and Human-Machine Interaction (HMI) methods are leveraged to develop the DEA. Building the knowledge base manually is subjective, time-consuming, laborious and error bound. This is why the knowledge base generation and population rely on Ontology Learning (OL) methods. This OL approach follows a modified model of the Ontology Layer Cake. This paper describes the approach and the parameters used for the qualitative trade-off for the selection of the software to be adopted in the architecture of the ES. The study also displays the first results of the multi-word extraction and highlights the importance of Word Sense Disambiguation for the identification of synonyms in the context. This paper includes the detailed software architecture of both front and back-ends, as well as the tool requirements. Both architectures and requirements were refined after a set of interviews with experts from the European Space Agency. The paper finally presents the preliminary strategy to quantify and mitigate uncertainties within the ES.", "venue": "IEEE Aerospace Conference", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "wsd", "kg", "onto", "kg", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "kg": 3, "wsd": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 2, "wsd": 1}, "ld_mention_counts": {"kg": 3, "onto": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "e1ae1c82a47c35f1c6f4bc34081fea6f4eb84337", "url": "https://www.semanticscholar.org/paper/e1ae1c82a47c35f1c6f4bc34081fea6f4eb84337", "title": "Mitigating linked data quality issues in knowledge-intense information extraction methods", "abstract": "Advances in research areas such as named entity linking and sentiment analysis have triggered the emergence of knowledge-intensive information extraction methods that combine classical information extraction with background knowledge from the Web. Despite data quality concerns, linked data sources such as DBpedia, GeoNames and Wikidata which encode facts in a standardized structured format are particularly attractive for such applications. This paper addresses the problem of data quality by introducing a framework that elaborates on linked data quality issues relevant to different stages of the background knowledge acquisition process, their impact on information extraction performance and applicable mitigation strategies. Applying this framework to named entity linking and data enrichment demonstrates the potential of the introduced mitigation strategies to lessen the impact of different kinds of data quality problems. An industrial use case that aims at the automatic generation of image metadata from image descriptions illustrates the successful deployment of knowledge-intensive information extraction in real-world applications and constraints introduced by data quality concerns.", "venue": "WIMS", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "ie", "ie", "ld", "ie", "ie", "ld", "ie"], "mention_counts": {"ld": 3, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"ld": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "8b8b695c57daeda6aa2eca81621e40b33bb73279", "url": "https://www.semanticscholar.org/paper/8b8b695c57daeda6aa2eca81621e40b33bb73279", "title": "NLP techniques associated with the OpenGALEN ontology for semi-automatic textual extraction of medical knowledge: abstracting and mapping equivalent linguistic and logical constructs", "abstract": "This research project presents methodological and theoretical issues related to the inter-relationship between linguistic and conceptual semantics, analysing the results obtained by the application of a NLP parser to a set of radiology reports. Our objective is to define a technique for associating linguistic methods with domain specific ontologies for semi-automatic extraction of intermediate representation (IR) information formats and medical ontological knowledge from clinical texts. We have applied the Edinburgh LTG natural language parser to 2810 clinical narratives describing radiology procedures. In a second step, we have used medical expertise and ontology formalism for identification of semantic structures and abstraction of IR schemas related to the processed texts. These IR schemas are an association of linguistic and conceptual knowledge, based on their semantic contents. This methodology aims to contribute to the elaboration of models relating linguistic and logical constructs based on empirical data analysis. Advance in this field might lead to the development of computational techniques for automatic enrichment of medical ontologies from real clinical environments, using descriptive knowledge implicit in large text corpora sources.", "venue": "AMIA", "citationCount": 20, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "nlp", "tp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 5, "tp": 1}, "nlp_mention_counts": {"nlp": 2, "tp": 1}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "994862df927cdb2331c29c5e581f2100dff6147b", "url": "https://www.semanticscholar.org/paper/994862df927cdb2331c29c5e581f2100dff6147b", "title": "Mining and Leveraging Background Knowledge for Improving Named Entity Linking", "abstract": "Knowledge-rich Information Extraction (IE) methods aspire towards combining classical IE with background knowledge obtained from third-party resources. Linked Open Data repositories that encode billions of machine readable facts from sources such as Wikipedia play a pivotal role in this development. The recent growth of Linked Data adoption for Information Extraction tasks has shed light on many data quality issues in these data sources that seriously challenge their usefulness such as completeness, timeliness and semantic correctness. Information Extraction methods are, therefore, faced with problems such as name variance and type confusability. If multiple linked data sources are used in parallel, additional concerns regarding link stability and entity mappings emerge. This paper develops methods for integrating Linked Data into Named Entity Linking methods and addresses challenges in regard to mining knowledge from Linked Data, mitigating data quality issues, and adapting algorithms to leverage this knowledge. Finally, we apply these methods to Recognyze, a graph-based Named Entity Linking (NEL) system, and provide a comprehensive evaluation which compares its performance to other well-known NEL systems, demonstrating the impact of the suggested methods on its own entity linking performance.", "venue": "Web Intelligence, Mining and Semantics", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ld", "ld", "lod", "ld", "ie", "ld", "ie"], "mention_counts": {"ld": 4, "lod": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"ld": 4, "lod": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "f4c3c5ed768195d6dfbb71d9c5dc222ac92744b4", "url": "https://www.semanticscholar.org/paper/f4c3c5ed768195d6dfbb71d9c5dc222ac92744b4", "title": "Linked Open Data Perspectives: Incorporating Linked Open Data into Information Extraction on the Web", "abstract": "Abstract Currently, the World Wide Web can be divided into two separate fields. The traditional Web of Documents consisting of hyperlinked web documents and the emerging Web of Data consisting of linked open data. We present ontology-based information extraction as core technology for bridging the gap between both fields. Based on this, we list three basic applications that integrate web data to web documents. Our SCOOBIE system can extract information of a linked open dataset mentioned as textual phrases in web documents. SCOOBIE returns machine interpretable metadata summarizing the content of a web document from the perspective of a linked open dataset. Based on SCOOBIE we present EPIPHANY, a system that returns extracted metadata back to the originating web document in form of semantic annotations. This allows users to request the Web of Data for more information about annotated subjects inside the web document. STERNTALER is a system that analyses extracted metadata from search results of a search engine. It generates semantic filters filled with facets of things that were extracted from web documents inside search results. This allows users filtering those web documents that contain information about specific subjects and facets. Zusammenfassung Das aktuelle \u201cWorld Wide Web\u201d l\u00e4sst sich in zwei Welten untergliedern. Einerseits das traditionelle Netz der Dokumente, bestehend aus verkn\u00fcpften Webseiten, andererseits das Netz der Daten, bestehend aus offenen und miteinander verkn\u00fcpften Datens\u00e4tzen (engl. \u201cLinked Open Data\u201d). Wir stellen ontologiebasierte Informationsextraktion als Basistechnologie vor, um beide Welten miteinander zu vereinen. Drei Anwendungen zeigen hierbei, wie sich das Netz der Dokumente mit dem Netz der Daten anreichern l\u00e4sst. Beim Analysieren von Webseiten erkennt das SCOOBIE System, ob einzelne Textfragmente als Entit\u00e4ten eines \u201cLinked Open Data\u201d-Datensatzes weitergehend beschrieben werden. Das Resultat von SCOOBIE sind maschinenverst\u00e4ndliche Metadaten, die den Inhalt der Webseite aus der Perspektive des jeweilig verwendeten Datensatzes heraus zusammenfassen. Basierend auf den Resultaten von SCOOBIE pr\u00e4sentieren wir das System EPIPHANY. EPIPHANY reichert das Quelldokument mit von SCOOBIE extrahierten Metadaten an, indem semantische Annotationen \u00fcber die von SCOOBIE ber\u00fccksichtigten Textfragmente erstellt werden. Dies erlaubt es Benutzern, weitere Informationen aus dem Netz der Daten \u00fcber annotierte Textpassagen anzufragen. Das System STERNTALER erweitert eine Suchmaschine, in dem es automatisch Metadaten aus Dokumenten der Suchresultate extrahiert. STERNTALER generiert auf Basis der extrahierten Metadaten semantische Filter, die mit Eigenschaften der im Dokument gefundenen Dingen gef\u00fcllt werden. Benutzern wird es hierdurch erm\u00f6glicht, solche Dokumente heraus zu filtern, die die gesuchten Informationen zu gew\u00fcnschten Dingen mit bestimmten Eigenschaften enthalten.", "venue": "it Inf. Technol.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "lod", "lod", "lod", "lod", "ie"], "mention_counts": {"lod": 4, "onto": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"lod": 4, "onto": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "98878502e7c8db6cb0a49a1d4c72fa868656118d", "url": "https://www.semanticscholar.org/paper/98878502e7c8db6cb0a49a1d4c72fa868656118d", "title": "Clinical named entity recognition: Challenges and opportunities", "abstract": "Information Extraction (IE), one of the important tasks in text analysis and Natural Language Processing (NLP), involves extracting meaningful pieces of knowledge from unstructured information sources, as unstructured data is computationally opaque. The intent of IE is to produce a knowledge base i.e. organize the information in a way that it is useful to people and arrange the information in a semantic way so that algorithms can make certain useful inferences from it. Named Entity Recognition (NER) is a sub-task of IE which finds and classifies the names/entities. Once these Named Entities (NE) are extracted, they can then be indexed and made searchable, relations can be derived, questions can be answered and many more. NER techniques are different for different domains, because of the uniqueness that exists in each of the domains, although the process depends on a number of fundamental Natural Language Processing (NLP) steps such as tokenization, part-of-speech tagging, parsing and model building. As an example, NER in the medical domain involves handling of a number of vital tasks such as identification of medical terms, attributes such as negation, severity, identification of relationships between entities and mapping terms in the document to concepts in domain specific ontologies. There is also a heavy dependence on domain specific resources such as medical dictionaries and ontologies such as the Unified Medical Language System (UMLS)[34]. In this paper, we focus on NER in the clinical domain. In particular, we will focus on the NER challenges and the qualitative analysis of clinical reports on the approaches we took for the named entities: anatomies, findings, location qualifier, and procedures.", "venue": "2016 IEEE International Conference on Big Data (Big Data)", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "nlp", "nlp", "nlp", "nlp", "kg"], "mention_counts": {"nlp": 4, "onto": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 4, "ie": 1}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "46b7f5c48f88d7472dda1cadd9b4ee321191428c", "url": "https://www.semanticscholar.org/paper/46b7f5c48f88d7472dda1cadd9b4ee321191428c", "title": "Query refinement for ontology information extraction", "abstract": "Ontology information extraction has gain popularity due to the increasing amount of ontologies developed over the years. World Wide Web Consortium (W3C) has introduced SPARQL query language to extract information. However SPARQL query language follow a specific pattern in order to find the triple through subgraph matching. The keywords used in the SPARQL query have to be exactly same as the existing keywords in the RDF data in-order to extract the required information. The paper introduced a method to ease the job of the user by generating SPARQL query from the query entered by the user. The method uses the object property list generated from the ontology and word's synonym to aid in SPARQL query generation. The result has shown that the proposed method clearly eases the need of the user to learn SPARQL syntax.", "venue": "2016 Third International Conference on Information Retrieval and Knowledge Management (CAMP)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "rdf", "onto", "onto", "ie", "ie", "onto", "ie"], "mention_counts": {"onto": 4, "rdf": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 4, "rdf": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "c3b2795a965643462a065eabc0fd617f23bb1854", "url": "https://www.semanticscholar.org/paper/c3b2795a965643462a065eabc0fd617f23bb1854", "title": "Building Information Extraction System Based on Computing Domain Ontology", "abstract": "In this paper, we present an Information Extraction (IE) system, which is built from unstructured text based on Computing domain ontology. The IE system comprises four sequential processing steps: preprocessing, topic identifier, building domain specific ontology and extracting information from text corpus. The first two steps perform generic Natural Language Processing (NLP) and Machine Learning tasks, while the last two phases are application-specific and require a thorough understanding of the application domain. Furthermore, the paper focuses on evaluating the IE IEsystem by selected methods. One of these methods that we introduced here, is comparative. Comparative evaluation performed in this study use of Key Exchange Algorithm with the same corpus to contrast results. Results generated by such experiments show that this IE system outperforms Key Exchange Algorithm, respectably.", "venue": "International Conference on Information Integration and Web-based Applications & Services", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "nlp", "ie", "ie", "ie"], "mention_counts": {"nlp": 2, "onto": 3, "ie": 3}, "nlp_mention_counts": {"nlp": 2, "ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "53bca484a7aba3253d884a1cf67288ee875eedac", "url": "https://www.semanticscholar.org/paper/53bca484a7aba3253d884a1cf67288ee875eedac", "title": "Odaies: ontology-driven adaptive Web information extraction system", "abstract": "This paper proposes an ontology-driven self-adapting approach in the semi-structured Web information extraction field, where ontology provides semantic support and plays a central role during the extraction process. It outperforms traditional wrapper systems in adaptiveness and maintenance. Firstly, we build a domain-dependant ontology. Then we design three templates generating algorithms, which have self-adaptiveness and self-maintenance based on the ontology, to perform Web page information extraction. Experiment results show that our prototype system can achieve 100% recall and 97.64% precision.", "venue": "IEEE/WIC International Conference on Intelligent Agent Technology, 2003. IAT 2003.", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "a1e404af8343b0924798e23972668e9af958ff11", "url": "https://www.semanticscholar.org/paper/a1e404af8343b0924798e23972668e9af958ff11", "title": "Ontology-based Information Extraction from Technical Documents", "abstract": "This paper presents a novel system for extracting user relevant tabular information from documents. The presented system is generic and can be applied to any documents irrespective of their domain and the information they contain. In addition to the generic nature of the presented approach, it is robust and can deal with different document layouts followed while creating those documents. The presented system has two main modules; table detection and ontological information extraction. The table detection module extracts all tables from a given technical document while, the ontological information extraction module extracts only relevant tables from all of the detected tables. The generalization in this system is achieved by using ontologies, thus enabling the system to adapt itself, to a new set of documents from any other domain, according to any provided ontology. Furthermore, the presented system also provides a confidence score and explanation of the score for each of the extracted tables in terms of its relevancy. The system was evaluated on 80 real technical documents of hardware parts containing 2033 tables from 20 different brands of Industrial Boilers domain. The evaluation results show that the presented system extracted all of the relevant tables and achieves an overall precision, recall, and F-measure of 0.88, 1 and 0.93 respectively.", "venue": "ICAART", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "onto", "ie", "ie", "onto"], "mention_counts": {"onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "5fd01ab3f5c39511f4a9311b3b7abb06a13d2be6", "url": "https://www.semanticscholar.org/paper/5fd01ab3f5c39511f4a9311b3b7abb06a13d2be6", "title": "XONTO: An Ontology-Based System for Semantic Information Extraction from PDF Documents", "abstract": "Information extraction is of paramount importance in several real world applications in the areas of business intelligence, competitive and military intelligence. Although several sophisticated and indeed complex approaches were proposed, they are still limited in many aspects. In this paper the novel ontology-based system named XONTO, that allows the semantic extraction of information from PDF unstructured documents, is presented. The XONTO system is founded on the idea of self-describing ontologies in which objects and classes can be equipped by a set of rules named descriptors. These rules represent patterns that allow to automatically recognize and extract ontology objects contained in PDF documents also when information is arranged in tabular form. This way a self-describing ontology expresses the semantic of the information to extract and the rules that, in turn, populate itself. In the paper XONTO system behaviors and structure are sketched by means of a running example.", "venue": "IEEE International Conference on Tools with Artificial Intelligence", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "42792ce88c1e98577a6e99c9ae318f71fde2ede2", "url": "https://www.semanticscholar.org/paper/42792ce88c1e98577a6e99c9ae318f71fde2ede2", "title": "Ontology-Based Information Extraction from Handwritten Documents", "abstract": "In this paper we introduce a new layer for the task of handwriting recognition. We add semantic information by means of ontologies. The task of our recognizer therefore is not only to recognize the ASCII transcription of the handwritten document, but also to identify the semantic concepts which appear in the text. This task is called ontology-based information extraction (OBIE), which has been applied to electronic documents recently. OBIE methods first segment the text into tokens, then identify their values and their corresponding instances of the ontology, and finally try to generate new facts based on the text. To the authors\u2019 knowledge, in this paper OBIE is proposed for the first time in handwriting literature. In our experiments we have evaluated the process up to the instantiation. We have found that using not only the top alternative, but also the k-best alternatives increases the performance of information extraction. Furthermore, the use of an ontology-based lexicon results in another performance increase.", "venue": "International Conference on Frontiers in Handwriting Recognition", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "ie", "onto", "onto"], "mention_counts": {"onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "bff822f93ca2f6c5d90ce5933b99465c10870426", "url": "https://www.semanticscholar.org/paper/bff822f93ca2f6c5d90ce5933b99465c10870426", "title": "Towards a Semantic Information Extraction Approach from Unstructured Documents", "abstract": "Recognizing and extracting meaningful information from semiand unstructured documents, taking into account their semantics, and storing them into database is an important problem in the context of information access and retrieval. This paper describes a novel logic-based approach to information extraction from both semiand unstructured documents. The approach, implemented in the HiLeX system, is founded on a new two-dimensional representation of documents constituting a unified abstract representation of both HTML pages and flat text documents. The semantics of information to be extracted is encoded by means of ontology expressed in DLP, an extension of disjunctive logic programming for ontology representation and reasoning, which has been recently implemented on top of the DLV system. Unlike previous systems, which are mainly syntactic, HiLeX combines both semantic and syntactic knowledge for a powerful information extraction. Each extraction pattern belongs to an ontology class and is expressed using regular expressions and/or an ad hoc two-dimensional language exploiting the document two-dimensional representation. The execution of DLP reasoning modules, encoding the HiLeX language expressions in term of logic rules, yields the actual extraction of information from the input document. HiLeX allows the semantic information extraction from both HTML pages and flat text documents by using synthetic and very expressive extraction patterns.", "venue": "SEBD", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "ie", "ie", "ie", "ie"], "mention_counts": {"onto": 3, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "312e6b42a75b2dc510aa30924da4ea7018697deb", "url": "https://www.semanticscholar.org/paper/312e6b42a75b2dc510aa30924da4ea7018697deb", "title": "Structural semantic interconnections: a knowledge-based approach to word sense disambiguation", "abstract": "Word sense disambiguation (WSD) is traditionally considered an AI-hard problem. A break-through in this field would have a significant impact on many relevant Web-based applications, such as Web information retrieval, improved access to Web services, information extraction, etc. Early approaches to WSD, based on knowledge representation techniques, have been replaced in the past few years by more robust machine learning and statistical techniques. The results of recent comparative evaluations of WSD systems, however, show that these methods have inherent limitations. On the other hand, the increasing availability of large-scale, rich lexical knowledge resources seems to provide new challenges to knowledge-based approaches. In this paper, we present a method, called structural semantic interconnections (SSI), which creates structural specifications of the possible senses for each word in a context and selects the best hypothesis according to a grammar G, describing relations between sense specifications. Sense specifications are created from several available lexical resources that we integrated in part manually, in part with the help of automatic procedures. The SSI algorithm has been applied to different semantic disambiguation problems, like automatic ontology population, disambiguation of sentences in generic texts, disambiguation of words in glossary definitions. Evaluation experiments have been performed on specific knowledge domains (e.g., tourism, computer networks, enterprise interoperability), as well as on standard disambiguation test sets.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citationCount": 382, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "kg", "kg", "wsd", "wsd", "wsd", "wsd", "ie"], "mention_counts": {"kg": 2, "wsd": 4, "onto": 1, "ie": 1}, "nlp_mention_counts": {"wsd": 4, "ie": 1}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "f5f679ef556b38f4361d24364965a968d9ebeea7", "url": "https://www.semanticscholar.org/paper/f5f679ef556b38f4361d24364965a968d9ebeea7", "title": "Query based information retrieval and knowledge extraction using Hadith datasets", "abstract": "In Natural language processing, one of the fundamental tasks is Named Entity Recognition (NER) that include identifying names of peoples, locations and other entities. Applications of NER include catboats, speech recognition, machine translation, knowledge extraction and intelligent search systems. NER is an active research domain for the last 10 years. In this paper, we propose a knowledge extraction framework to extract Named entities from Sahih AlBukhari Urdu translation which is a world known Hadith book. The proposed framework is based on finite state transducer system to extract entities and process the Hadith content using Part of Speech (POS) tagging. Conditional Random Field, an ensemble based algorithm, processes the extracted nouns for NER and classification. In the future, we aim to implement the proposed framework to rank the hadith content and apply the Vector Space Model.", "venue": "International Conference on Emerging Technologies", "citationCount": 20, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "mt", "nlp"], "mention_counts": {"nlp": 1, "ke": 3, "mt": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 3, "mt": 1}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "d65f7de6bd94cd810ee4475e141531bcec332be1", "url": "https://www.semanticscholar.org/paper/d65f7de6bd94cd810ee4475e141531bcec332be1", "title": "Mining information from sentences through Semantic Web data and Information Extraction tasks", "abstract": "The Semantic Web provides guidelines for the representation of information about real-world objects (entities) and their relations (properties). This is helpful for the dissemination and consumption of information by people and applications. However, the information is mainly contained within natural language sentences, which do not have a structure or linguistic descriptions ready to be directly processed by computers. Thus, the challenge is to identify and extract the elements of information that can be represented. Hence, this article presents a strategy to extract information from sentences and its representation with Semantic Web standards. Our strategy involves Information Extraction tasks and a hybrid semantic similarity measure to get entities and relations that are later associated with individuals and properties from a Knowledge Base to create RDF triples (Subject\u2013Predicate\u2013Object structures). The experiments demonstrate the feasibility of our method and that it outperforms the accuracy provided by a pattern-based method from the literature.", "venue": "J. Inf. Sci.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "sw", "sw", "ie", "kg", "sw", "rdf", "ie"], "mention_counts": {"sw": 3, "kg": 1, "rdf": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"sw": 3, "kg": 1, "rdf": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "15c444758e9ce29f724fcf243de8a9f8c4a75f8a", "url": "https://www.semanticscholar.org/paper/15c444758e9ce29f724fcf243de8a9f8c4a75f8a", "title": "Intelligent Chatbot-LDA Recommender System", "abstract": "With the proliferation of distance platforms, in particular that of an open access such as Massive Online Open Courses (MOOC), the learner finds himself overwhelmed with data which are not all efficient for his interest. Besides, the MOOC has tools that allow learners to seek information, express their ideas, and participate in discussions in an online forum. This tool is a huge repository of rich data, which continues to evolve, however its exploitation is fiddly in the search for information relevant to the learner. Similarly, the task of the tutor seems to be difficult in management of a large number of learners. To this end, the development of a Chatbot able to meet the requests of learners in a natural language is necessary to the deroulement a course in the MOOC. The ChatBot plays the role of assistant and guide for the learners and for the tutors. However, ChatBot responses come from a knowledge base, which must be relevant. Knowledge extraction to answer questions is a difficult task due to the number of MOOC participants. Learners' interactions with the MOOC platform gen-erate massive information, particularly in discussion forums by seeking answers to their questions. Identifying and extracting knowledge from online forums requires collaborative interactions between learners. In this article we propose a new approach to answer learners' questions in a relevant and instantaneous way in a ChatBot in natural language. Our model is based on the LDA Bayesian statistical method, applied to threads posted in the forum and classifies them to provide the learner with a rich semantic response. These threads taken from the discussion forum in the form of knowledge will enrich the ChatBot knowledge database. In parallel, we will map the extracted knowledge to ontology, to provide the learner with pedagogical resources that will serve as learning support.", "venue": "International Journal of Emerging Technologies in Learning (iJET)", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "onto", "ke", "ke"], "mention_counts": {"ke": 3, "onto": 1, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 1, "kg": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "e96a74d3cb4375a55dbf5b4c23b0fff1a8acf8bb", "url": "https://www.semanticscholar.org/paper/e96a74d3cb4375a55dbf5b4c23b0fff1a8acf8bb", "title": "NLP and ontology based clustering \u2014 An integrated approach for optimal information extraction from social web", "abstract": "Social Web is a heterogeneous collection of both structured and unstructured dataprimarily composed of contents from various social networking sites, blogs, online shopping sites and much more. The knowledge extracted from such data can be valuable for accuracy of search results in light of present explosion of information exchange over social web. The extraction of information patterns from unstructured data sets available at social networking sites viz. facebook, twitter, linkedin is a challenging task as it cannot be understood by machine to robotically process the data. Also, the major data source in the form of naive users triggers the significance of filtration of the relevant results. For effectual analysis of social web contents, this paper proposes integrated NLP and ontology based clustering TVC algorithm that generates semantically meaningful concepts from the social web content. The algorithm promises to optimize the web search results and to provide accuracy in searching the well treated unstructured social web contents.", "venue": "International Conference on Computing for Sustainable Global Development", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "nlp", "onto", "nlp", "ke", "ie", "onto"], "mention_counts": {"nlp": 2, "onto": 2, "ke": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "1daff1fdc6aa9427eb2e6af4c283effd0b67a111", "url": "https://www.semanticscholar.org/paper/1daff1fdc6aa9427eb2e6af4c283effd0b67a111", "title": "Multilingual Epidemic Event Extraction : From Simple Classification Methods to Open Information Extraction (OIE) and Ontology", "abstract": "There is an incredible amount of information available in the form of textual documents due to the growth of information sources. In order to get the information into an actionable way, it is common to use information extraction and more specifically the event extraction, it became crucial in various domains even in public health. In this paper, we address the problem of the epidemic event extraction in potentially any language, so that we tested different corpuses on an existed multilingual system for tele-epidemiology: the Data Analysis for Information Extraction in any Language(DANIEL) system. We focused on the influence of the number of documents on the performance of the system, on average results show that it is able to achieve a precision and recall around 82%, but when we resorted to the evaluation by event by checking whether it has been really detected or not, the results are not satisfactory according to this paper\u2019s evaluation. Our idea is to propose a system that uses an ontology which includes information in different languages and covers specific epidemiological concepts, it is also based on the multilingual open information extraction for the relation extraction step to reduce the expert intervention and to restrict the content for each text. We describe a methodology of five main stages: Pre-processing, relation extraction, named entity recognition (NER), event recognition and the matching between the information extracted and the ontology.", "venue": "RANLP", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto", "ie", "ie", "onto", "ie"], "mention_counts": {"onto": 3, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "7f00241198142680e9f6db15101c6a229869066f", "url": "https://www.semanticscholar.org/paper/7f00241198142680e9f6db15101c6a229869066f", "title": "An Ontology-Based Information Extraction System for Residential Land Use Suitability Analysis", "abstract": "We propose an Ontology-Based Information Extraction (OBIE) system to automate the extraction of the criteria and values applied in Land Use Suitability Analysis (LUSA) from bylaw and regulation documents related to the geographic area of interest. The results obtained by our proposed LUSA OBIE system (land use suitability criteria and their values) are presented as an ontology populated with instances of the extracted criteria and property values. This latter output ontology is incorporated into a Multi-Criteria Decision Making (MCDM) model applied for constructing suitability maps for different kinds of land uses. The resulting maps may be the final desired product or can be incorporated into the cellular automata urban modeling and simulation for predicting future urban growth. A case study has been conducted where the output from LUSA OBIE is applied to help produce a suitability map for the City of Regina, Saskatchewan, to assist in the identification of suitable areas for residential development. A set of Saskatchewan bylaw and regulation documents were downloaded and input to the LUSA OBIE system. We accessed the extracted information using both the populated LUSA ontology and the set of annotated documents. In this regard, the LUSA OBIE system was effective in producing a final suitability map.", "venue": "Int. J. Softw. Eng. Knowl. Eng.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "onto", "onto", "ie"], "mention_counts": {"onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "d8bfa6df6dcc7f2b66bdf77a6b009ab438bdefd9", "url": "https://www.semanticscholar.org/paper/d8bfa6df6dcc7f2b66bdf77a6b009ab438bdefd9", "title": "Automatic rules extraction from medical texts", "abstract": "The majority of existing knowledge is encoded in unstructured texts and is not linked to formalized knowledge, like ontologies and rules. The potential solution to this problem is to acquire this knowledge through natural language processing (NLP) tools and text mining techniques. Prior work has focused on the automatic extraction of ontologies from texts, but the acquired knowledge is generally limited to simple hierarchies of terms. This paper presents a polyvalent framework for acquiring more complex relationships from texts and codes them in the form of rules. Our approach starts with existing domain knowledge represented as OWL ontology and SWRL \"Semantic Web Rule Language\" rules by applying NLP tools and text matching techniques to deduce different atoms as classes, properties etc. This is to capture the deductive knowledge in the form of new rules. We evaluate our approach thereafter by applying it on medical field more precisely Gynecology specialty, showing that this approach can generate automatically and accurately SWRL rules for the representation of more formal knowledge necessary for reasoning.", "venue": "2014 International Workshop on Advanced Information Systems for Enterprises", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "sw", "nlp", "onto", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 3, "sw": 1, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.6605978084834118}, {"paperId": "d8a1706b59818a31cd687581475678fcf513a0e1", "url": "https://www.semanticscholar.org/paper/d8a1706b59818a31cd687581475678fcf513a0e1", "title": "Construction of Semantic Web-based Knowledge Using Text Processing", "abstract": "Looking through the application examples of a semantic Web-based service, we explain how text processing can efficiently help to construct the ontology instance which is obligatorily required in establishing the service. Text processing is applied to class instantiation in the manner of identity resolution, to metadata expansion through topic and category assignment, and to object property add-up by using citation relation extraction. Consequently, 8,543 author URIs were constructed. For assigning multiple topics and categories to each literature, index terms were matched with thesaurus terms of which category names are mapped in advance. We also acquired total 135 citation networks using the citation relation automatically extracted from references in 7,237 papers. It expects that the above-mentioned methods which are put through this research will be multilaterally used in the semantic Web applications", "venue": "Fourth International Conference on Information Technology (ITNG'07)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "onto", "sw", "kg", "tp", "tp", "tp"], "mention_counts": {"sw": 3, "tp": 3, "onto": 1, "kg": 1}, "nlp_mention_counts": {"tp": 3}, "ld_mention_counts": {"sw": 3, "onto": 1, "kg": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "e5b95c50756ed9b259523b4b436313d36681b660", "url": "https://www.semanticscholar.org/paper/e5b95c50756ed9b259523b4b436313d36681b660", "title": "Knowledge Extraction and Inference from Text: Shallow, Deep, and Everything in Between", "abstract": "Systems for structured knowledge extraction and inference have made giant strides in the last decade. Starting from shallow linguistic tagging and coarse-grained recognition of named entities at the resolution of people, places, organizations, and times, modern systems link billions of pages of unstructured text with knowledge graphs having hundreds of millions of entities belonging to tens of thousands of types, and related by tens of thousands of relations. Via deep learning, systems build continuous representations of words, entities, types, and relations, and use these to continually discover new facts to add to the knowledge graph, and support search systems that go far beyond page-level \"ten blue links''. We will present a comprehensive catalog of the best practices in traditional and deep knowledge extraction, inference and search. We will trace the development of diverse families of techniques, explore their interrelationships, and point out various loose ends.", "venue": "SIGIR", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "kg", "ke", "ke"], "mention_counts": {"kg": 2, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 2, "ke": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "a16af0a0f67350ec15e970fee8fdfac18d4294ed", "url": "https://www.semanticscholar.org/paper/a16af0a0f67350ec15e970fee8fdfac18d4294ed", "title": "Knowledge Extraction and Applications utilizing Context Data in Knowledge Graphs", "abstract": "Context is widely considered for NLP and knowledge discovery since it highly influences the exact meaning of natural language. The scientific challenge is not only to extract such context data, but also to store this data for further NLP approaches. Here, we propose a multiple step knowledge graphbased approach to utilize context data for NLP and knowledge expression and extraction. We introduce the graph-theoretic foundation for a general context concept within semantic networks and show a proof-of-concept-based on biomedical literature and text mining. We discuss the impact of this novel approach on text analysis, various forms of text recognition and knowledge extraction and retrieval.", "venue": "Conference on Computer Science and Information Systems", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "nlp", "ke", "ke", "nlp"], "mention_counts": {"nlp": 3, "kg": 1, "ke": 2}, "nlp_mention_counts": {"nlp": 3, "ke": 2}, "ld_mention_counts": {"kg": 1, "ke": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "765ac6eaa0731a31de66e2b81c1837b6522d1219", "url": "https://www.semanticscholar.org/paper/765ac6eaa0731a31de66e2b81c1837b6522d1219", "title": "Knowledge Extraction from Rise-Time Auto-Correlated Patterns", "abstract": "In this paper, the author presents an approach for automated knowledge extraction from rise time auto-correlated patterns by using self-organizing maps and k-means clustering. The extracted knowledge in terms of rules will be used as knowledge base for an expert system. Rise-time auto-correlated data patterns are used as a learning data set. The produced knowledge based was verified by using a conventional expert system.", "venue": "Int. J. Inf. Acquis.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ke", "ke", "ke"], "mention_counts": {"kg": 2, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 2, "ke": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "0c49994dd561b2d987e4ea0c164c4aca3e7992c3", "url": "https://www.semanticscholar.org/paper/0c49994dd561b2d987e4ea0c164c4aca3e7992c3", "title": "Knowledge Extraction from Source Code Based on Hidden Markov Model: Application to EPICAM", "abstract": "Large software systems evolve rapidly and these evolutions are usually integrated directly into source code without updating the conceptual model. As a consequence, implementation platforms evolve faster than business logic. Thus, when extracting knowledge to enrich or build an ontology, business logic is not always a complete data source. To solve this problem, some authors have suggested to adopt an ontology learning approach in order to extract knowledge from the source code. In this paper, we show how to realize this task using Hidden Markov Models. experiments on EPICAM, a tuberculosis surveillance system shows the relevance of this approach.", "venue": "2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "ke", "ke", "onto"], "mention_counts": {"ke": 3, "onto": 2}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "64bf8608cd216593e297868f0ad4dc653cd792da", "url": "https://www.semanticscholar.org/paper/64bf8608cd216593e297868f0ad4dc653cd792da", "title": "KBCT: a knowledge extraction and representation tool for fuzzy logic based systems", "abstract": "This paper presents a user-friendly portable tool designed and developed in order to make easier knowledge extraction and representation for fuzzy logic based systems. KBCT is an open source software that could be executed under Linux or Windows operating systems. Main goal of KBCT is the generation or refinement of fuzzy knowledge bases with a particular interest of obtaining interpretable partitions and rules. The use of fuzzy logic simplifies the knowledge extraction process and increase interpretability of rules because of the fuzzy rule expression is closed to expert natural language. KBCT lets the user define expert variables and rules, but also provide induction capabilities for partitions and rules. Both types of knowledge, expert and induced, are integrated under the expert control. In addition to this, the user can check consistency and quality of rule base at any moment. A simplify option is implemented in order to allow the user to reduce the size of rule base. The main objective consists of ensuring interpretability, non-redundancy and consistency of the knowledge base along the whole process.", "venue": "2004 IEEE International Conference on Fuzzy Systems (IEEE Cat. No.04CH37542)", "citationCount": 27, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 2}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "d4c679b725250dd59513fb00229a8b3e0fdb9c83", "url": "https://www.semanticscholar.org/paper/d4c679b725250dd59513fb00229a8b3e0fdb9c83", "title": "Automatic textual Knowledge Extraction based on Paragraph Constitutive Relations", "abstract": "People can understand the paragraph according to the paragraph constitutive relations, solve coreference resolution, dependency syntax, but the machine cannot. To let the machine better understand the paragraph, the paper proposes the automatic textual knowledge extraction considering paragraph constitutive relations. At first, the paragraph constitutive relations are defined, and the detailed analysis is given, including the grammar, semantics, structures of words, sentences and paragraphs. Then, the automatic textual knowledge extraction framework is proposed, where 3 additional databases are added and the adjust strategies are given for extracting entities and relations. Next, the method which extracting the knowledge graph of a paragraph through constitutive relations is given, especially the fusion extractions combining entity with relation are proposed besides entity extraction and relation extraction; the Bi-directional Long Short-Term Memory (Bi-LSTM) + conditional random field (CRF) model on the existing CoNLL dataset as assistance to spacy is applied in entity recognition, the dependency parsing is well processed in relation extraction. Finally, we developed the small B/S application to extract the entities and relationships and visualize the corresponding knowledge graph. The test experiment has achieved good results and applies this method to ICDM competition, the achievements made in the twelfth. More experiments showed our model is available and feasible, performs well, and it is able to build the corresponding knowledge according to the short paragraph.", "venue": "International Conference on Systems and Informatics", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 2}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "dafa062747bf7fbe0a913dbb044996a1c650937c", "url": "https://www.semanticscholar.org/paper/dafa062747bf7fbe0a913dbb044996a1c650937c", "title": "Muf: tool for knowledge extraction and knowledge base building", "abstract": "Muf is a tool for knowledge base (KB) building by extract-ing knowledge from texts. It is intended for cases when all documents have to be processed manually in order to ensure correctness of KB. Muf provides visual tools and some degree of mechanization to facilitate manual knowledge extraction and KB building. We also believe that manual processing of documents implies small number of docu-ments and it further implies that KB use case is well de-fined. Well defined use case allows us to decide which knowledge is worth of extraction and which not. Reducing amount of extracted knowledge also leads to less complex structure of KB. This all makes extraction and KB building tasks even easier, KB is easier to understand and deploy. If some parts of KB happen to be incorrect, Muf is able to trace the corresponding knowledge down to the text and allow user to fix it. The work was done on Czech drug la-bels, but we believe that Muf can be used also for different languages as well as different kinds of documents.", "venue": "K-CAP '07", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 2}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "66f5b5a674e90688af139fa477b0fd21c98f010c", "url": "https://www.semanticscholar.org/paper/66f5b5a674e90688af139fa477b0fd21c98f010c", "title": "Device Fabrication Knowledge Extraction from Materials Science Literature", "abstract": "Devices like solar cells, batteries etc. often comprise of a host of material types including organic, inorganic and hybrid materials. The fabrication procedures for these devices involve screening or designing the right set of materials and then subjecting them to a sequence of operations under very specific conditions. The performance characteristics of a device critically depend on the materials used in its fabrication, the specific operations carried out, their operating conditions and the specific sequence in which they are carried out. The space of potential materials, operations and operating conditions is vast, and selecting the right combination thereof to achieve the desired characteristics is a knowledge intensive activity. A large amount of such device fabrication knowledge is available in the form of publications, patents, company reports and so on. In this paper, we present a system that systematically extracts this knowledge from materials science literature. The extracted knowledge is represented as knowledge graphs conforming to an ontology that can be queried to make informed decisions in device fabrication procedures. The system first identifies the set of relevant paragraphs that contain fabrication knowledge. It then employs state of the art entity and relation extraction models to identify instances of operations, methods, materials, etc. and relations between them. The system then applies an unsupervised algorithm to identify sequences of operations representing fabrication procedures. We applied our system on solar cell fabrication knowledge extraction and achieved good performance. We believe our results provide much needed impetus for further work in this area.", "venue": "AAAI", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ke", "onto"], "mention_counts": {"ke": 3, "onto": 1, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 1, "kg": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "720c5c948c8bbddd4fcec312afeb25dd9d2f115d", "url": "https://www.semanticscholar.org/paper/720c5c948c8bbddd4fcec312afeb25dd9d2f115d", "title": "Utilizing Knowledge Graphs for Neural Machine Translation Augmentation", "abstract": "While neural networks have led to substantial progress in machine translation, their success depends heavily on large amounts of training data. However, parallel training corpora are not always readily available. Moreover, out-of-vocabulary words---mostly entities and terminological expressions---pose a difficult challenge to Neural Machine Translation systems. Recent efforts have tried to alleviate the data sparsity problem by augmenting the training data using different strategies, such as external knowledge injection. In this paper, we hypothesize that knowledge graphs enhance the semantic feature extraction of neural models, thus optimizing the translation of entities and terminological expressions in texts and consequently leading to better translation quality. We investigate two different strategies for incorporating knowledge graphs into neural models without modifying the neural network architectures. Additionally, we examine the effectiveness of our augmented models on domain-specific texts and ontologies. Our knowledge-graph-augmented neural translation model, dubbed KG-NMT, achieves significant and consistent improvements of +3 BLEU, METEOR and chrF3 on average on the newstest datasets between 2015 and 2018 for the WMT English-German translation task.", "venue": "K-CAP", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "mt", "kg", "mt", "kg", "onto", "mt"], "mention_counts": {"kg": 4, "onto": 1, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"kg": 4, "onto": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "03c95e7a1ca6c24d94292658cde8316189e33b6c", "url": "https://www.semanticscholar.org/paper/03c95e7a1ca6c24d94292658cde8316189e33b6c", "title": "Using natural language to integrate, evaluate, and optimize extracted knowledge bases", "abstract": "Web Information Extraction (WIE) systems extract billions of unique facts, but integrating the assertions into a coherent knowledge base and evaluating across different WIE techniques remains a challenge. We propose a framework that utilizes natural language to integrate and evaluate extracted knowledge bases (KBs). In the framework, KBs are integrated by exchanging probability distributions over natural language, and evaluated by how well the output distributions predict held-out text. We describe the advantages of the approach, and detail remaining research challenges.", "venue": "Conference on Automated Knowledge Base Construction", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "kg", "kg", "ke", "kg", "ke"], "mention_counts": {"kg": 3, "ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"kg": 3, "ke": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "1c27cb8364a7655b2e4e8aa799970a08f90dea61", "url": "https://www.semanticscholar.org/paper/1c27cb8364a7655b2e4e8aa799970a08f90dea61", "title": "Building a Large-Scale Knowledge Base for Machine Translation", "abstract": "Knowledge-based machine translation (KBMT) systems have achieved excellent results in constrained domains, but have not yet scaled up to newspaper text. The reason is that knowledge resources (lexicons, grammar rules, world models) must be painstakingly handcrafted from scratch. One of the hypotheses being tested in the PAN-GLOSS machine translation project is whether or not these resources can be semi-automatically acquired on a very large scale. \n \nThis paper focuses on the construction of a large ontology (or knowledge base, or world model) for supporting KBMT. It contains representations for some 70,000 commonly encountered objects, processes, qualities, and relations. The ontology was constructed by merging various online dictionaries, semantic networks, and bilingual resources, through semi-automatic methods. Some of these methods (e.g., conceptual matching of semantic taxonomies) are broadly applicable to problems of importing/exporting knowledge from one KB to another. Other methods (e.g., bilingual matching) allow a knowledge engineer to build up an index to a KB in a second language, such as Spanish or Japanese.", "venue": "AAAI Conference on Artificial Intelligence", "citationCount": 380, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "onto", "kg", "mt", "mt", "onto", "mt"], "mention_counts": {"kg": 3, "onto": 2, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"kg": 3, "onto": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "78a63938f856c6161fb7420c8c57cdd4ff87ee2c", "url": "https://www.semanticscholar.org/paper/78a63938f856c6161fb7420c8c57cdd4ff87ee2c", "title": "Ten ways of leveraging ontologies for natural language processing and its enterprise applications", "abstract": "In the last years, Artificial Intelligence and Deep Learning have matured from a facinating research area to real-word applications across multiple domains. Enterprises adopt data-driven approaches for various use cases. With the increased adoption, such issues as governance of the models, deployment, scalability, reusablity and maintenance are widely addressed on the engineering side, but not so much on the knowledge side. In this paper, we demonstrate 10 ways of leveraging ontology for Natural Language Processing. Specifically, we explore the usage of ontologies and related standards for labeling schema, configuration, providing lexical data, powering rule engine and automated generation of rules, as well as providing a standard output format. Additionally, we discuss three NLP-based applications: semantic search, question answering and natural language querying and show how they can benefit from ontology usage. The paper summarizes our experience of using ontology in a number of projects for medical, enterprise, financial, legal and security domains.", "venue": "SBD@SIGMOD", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 3, "onto": 5}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "f830a3e60d24801cc8c8685fdc6b1bb97e74055f", "url": "https://www.semanticscholar.org/paper/f830a3e60d24801cc8c8685fdc6b1bb97e74055f", "title": "Towards Natural Language Question Answering Over Earth Observation Linked Data Using Attention-Based Neural Machine Translation", "abstract": "With an increase in Geospatial Linked Open Data being adopted and published over the web, there is a need to develop intuitive interfaces and systems for seamless and efficient exploratory analysis of such rich heterogeneous multi-modal datasets. This work is geared towards improving the exploration process of Earth Observation (EO) Linked Data by developing a natural language interface to facilitate querying. Questions asked over Earth Observation Linked Data have an inherent spatio-temporal dimension and can be represented using GeoSPArql. This paper seeks to study and analyze the use of RNN-based neural machine translation with attention for transforming natural language questions into GeoSPArql queries. Specifically, it aims to assess the feasibility of a neural approach for identifying and mapping spatial predicates in natural language to GeoSPARQL's topology vocabulary extension including - Egenhofer and RCC8 relations. The queries can then be executed over a triple store to yield answers for the natural language questions. A dataset consisting of mappings from natural language questions to GeoSPArql queries over the Corine Land Cover(CLC) Linked Data has been created to train and validate the deep neural network. From our experiments, it is evident that neural machine translation with attention is a promising approach for the task of translating spatial predicates in natural language questions to GeoSPArql queries.", "venue": "IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "lod", "mt", "ld", "ld", "mt", "mt", "ld"], "mention_counts": {"ld": 4, "lod": 1, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"ld": 4, "lod": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "bd573fa8258324f6db09817240e7287e455af996", "url": "https://www.semanticscholar.org/paper/bd573fa8258324f6db09817240e7287e455af996", "title": "RTM at SemEval-2016 Task 1: Predicting Semantic Similarity with Referential Translation Machines and Related Statistics", "abstract": "We use referential translation machines (RTMs) for predicting the semantic similarity of text in both STS Core and Cross-lingual STS. RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource. RTMs become 14th out of 26 submissions in Cross-lingual STS. We also present rankings of various prediction tasks using the performance of RTM in terms of MRAER, a normalized relative absolute error metric. 1 Semantic Agreement We participated in Semantic Textual Similarity task at SemEval-2016 (Bethard et al., 2016) with RTMs. RTMs identify translation acts between any two data sets with respect to interpretants, data close to the task instances, effectively judging monolingual and bilingual similarity. We use RTMs for predicting the semantic similarity of text. Interpretants are used to derive features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of the acts of translation, which may ubiquitously be observed in communication. Semantic Web\u2019s dream is to allow machines to share, exploit, and understand knowledge on the web. As more and more shared conceptualizations of domains emerge, we get closer to this goal. Semantic textual similarity (STS) task (Agirre et al., 2016) at SemEval-2016 (Bethard et al., 2016) is about quantifying the degree of similarity between two given sentences S1 and S2 in the same language (English) in STS Core (STS English) or in different languages (English or Spanish) in Cross-lingual STS (STS Spanish), with a real number in [0, 5]. S1 and S2 may be constructed using different models and with different conceptualizations of the world or different ontologies and different vocabulary. Even if two instances are categorized as same, they may have different implications for commonsense reasoning (both albatros and penguin are a bird) (Bi\u00e7ici, 2002). The existence of a single ontology that can cover all the required conceptual information for reaching semantic understanding is questionable because it would presume an agreement among all ontology experts. Yet, semantic agreement using heterogeneous ontologies may not be possible as well since in the most extreme case, they would not use the same tokens. Therefore, semantic textual similarity is harder than the Chinese room thought experiment (Internet Encyclopedia of Philosophy, 2016) since we are not given any instructions about how to answer queries. Our goal is to quantify the level of semantic agreement between S1 and S2 and RTMs use interpretants, data close to the task instances for building prediction models for semantic similarity. 2 Referential Translation Machine Each RTM model is a data translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. RTMs are powerful enough to be applicable in different domains and tasks while achieving top performance in both", "venue": "*SEMEVAL", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "onto", "mt", "onto", "sw", "onto", "onto", "mt"], "mention_counts": {"sw": 1, "onto": 4, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.6605978084834118}, {"paperId": "7e134f5955b261b864e74c91eccae9732fc0a789", "url": "https://www.semanticscholar.org/paper/7e134f5955b261b864e74c91eccae9732fc0a789", "title": "Wikification and Beyond: The Challenges of Entity and Concept Grounding", "abstract": "Contextual disambiguation and grounding of concepts and entities in natural language are essential to progress in many natural language understanding tasks and fundamental to many applications. Wikification aims at automatically identifying concept mentions in text and linking them to referents in a knowledge base (KB) (e.g., Wikipedia). Consider the sentence, \"The Times report on Blumenthal (D) has the potential to fundamentally reshape the contest in the Nutmeg State.\". A Wikifier should identify the key entities and concepts and map them to an encyclopedic resource (e.g., \u201cD\u201d refers to Democratic Party, and \u201cthe Nutmeg State\u201d refers to Connecticut. Wikification benefits end-users and Natural Language Processing (NLP) systems. Readers can better comprehend Wikified documents as information about related topics is readily accessible. For systems, a Wikified document elucidates concepts and entities by grounding them in an encyclopedic resource or an ontology. Wikification output has improved NLP down-stream tasks, including coreference resolution, user interest discovery , recommendation and search. This task has received increased attention in recent years from the NLP and Data Mining communities, partly fostered by the U.S. NIST Text Analysis Conference Knowledge Base Population (KBP) track, and several versions of it has been studied. These include Wikifying all concept mentions in a single text document; Wikifying a cluster of co-referential named entity mentions that appear across documents (Entity Linking), and Wikifying a whole document to a single concept. Other works relate this task to coreference resolution within and across documents and in the context of multiple text genres. 2 Content Overview", "venue": "ACL", "citationCount": 26, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "nlp", "onto", "nlp", "kg", "nlp", "nlu"], "mention_counts": {"nlp": 4, "onto": 1, "kg": 2, "nlu": 1}, "nlp_mention_counts": {"nlp": 4, "nlu": 1}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "a0bedb248e49e8a1235bc83f13a4a28c05125dea", "url": "https://www.semanticscholar.org/paper/a0bedb248e49e8a1235bc83f13a4a28c05125dea", "title": "Creating a novel semantic video search engine through enrichment textual and temporal features of subtitled YouTube media fragments", "abstract": "Semantic video Annotation is an active research zone within the field of multimedia content understanding. With the stable increase of videos published on the famous video sharing platforms such as YouTube, more efforts are spent to automatically annotate these videos. In this paper, we propose a novel framework to annotating subtitled YouTube videos using both textual features such as all of portions extracted from web natural language processors in relation to subtitles, and temporal features such as the duration of the media fragments where particular entities are spotted. We implement SY-VSE (Subtitled YouTube Video Search Engine) as an efficient framework to cruising on the subtitled YouTube videos resident in the Linked Open Data (LOD) cloud. For realizing this purpose, we propose Unifier Module of Natural Language Processing (NLP) Tools Results (UM-NLPTR) for extracting main portions of the 10 NLP web tools from subtitles associated to YouTube videos in order to generate media fragments annotated with resources from the LOD cloud. Then, we propose Unifier Module of Popular API's Results (UM-PAR) containing the seven favorite web APIs to boost results of Named Entities (NE) obtained from UM-NLPTR. We will use dotNetRDF as a powerful and flexible API for working with Resource Description Framework (RDF) and SPARQL in .Net environments.", "venue": "International Conference on Computer and Knowledge Engineering", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "lod", "lod", "nlp", "nlp", "lod", "rdf", "rdf"], "mention_counts": {"nlp": 3, "lod": 3, "rdf": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"lod": 3, "rdf": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "037e37645ecbc87e1214a847e78b861b45541612", "url": "https://www.semanticscholar.org/paper/037e37645ecbc87e1214a847e78b861b45541612", "title": "Comparing the Template-Based Approach to GF: the case of Afrikaans", "abstract": "Ontologies are usually represented in OWL that is not easy to grasp by domain experts. A solution to bridge this gap is to use a controlled natural language or natural language generation (NLG), which allows the knowledge in the ontology to be rendered automatically into a natural language. Several approaches exist to realise this. We used both templates and the Grammatical Framework (GF) and examined the feasibility of each by developing NLG modules for a language that had none: Afrikaans. The template system requires manual translation of the ontology\u2019s vocabulary into Afrikaans, if not already done so, while the GF system can translate the terms automatically. Yet, the template system is found to produce more grammatically correct sentences and verbalises the ontology slightly faster than the GF system. The template-based approach seems easier to extend for future development.", "venue": "WebNLG", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlg", "nlg", "nlg", "onto", "onto", "onto"], "mention_counts": {"nlg": 3, "onto": 5}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "4390c695666f0c951c8c0c6ff60ac261c60ff922", "url": "https://www.semanticscholar.org/paper/4390c695666f0c951c8c0c6ff60ac261c60ff922", "title": "Survey on Defining Practices in Ontologies: Report Summary", "abstract": "Ontologies built using OBO Foundry principles are advised to include both formal (logical) definitions and natural language definitions. Depending on the effort, one or the other can be underrepresented. Possible explanations to this bottleneck are the high cost of producing wellwritten definitions; an insufficient understanding of the nature of natural language definitions or of logic; the lack of an operational theory of definitions; the lack of studies that evaluate usability and effectiveness of definitions in ontologies; a paucity of tools to help with definition authoring and checking.", "venue": "VDOS+DO@ICBO", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "nlp", "onto", "onto", "nlg", "nll", "onto", "nle"], "mention_counts": {"onto": 3, "nll": 1, "nlu": 1, "nlp": 1, "nlg": 1, "nle": 1}, "nlp_mention_counts": {"nll": 1, "nlu": 1, "nlp": 1, "nlg": 1, "nle": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "d9fe7500910d22d5dd98b4490f1bfb97a6a3d8a0", "url": "https://www.semanticscholar.org/paper/d9fe7500910d22d5dd98b4490f1bfb97a6a3d8a0", "title": "Generation of Multilingual Personalized Environmental Bulletins from an OWL-based Ontology", "abstract": "Natural Language Generation (NLG) from knowledge bases (KBs) has repeatedly been subject of research. However, most proposals tend to have in common that they start from KBs of limited size that either already contain linguistically-oriented knowledge structures or to whose structures different ways of realization are explicitly assigned. To avoid these limitations, we propose a three layer OWL-based ontology framework in which domain, domain communication and linguistic knowledge structures are clearly separated and show how a large scale instantiation of this framework in the environmental domain serves multilingual NLG.", "venue": "EnviroInfo", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Engineering"], "mentions": ["onto", "nlg", "onto", "nlg", "onto", "onto", "kg", "nlg"], "mention_counts": {"kg": 1, "nlg": 3, "onto": 4}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"kg": 1, "onto": 4}, "relevance_score": 0.6605978084834118}, {"paperId": "5096d7ac485371c980f06b4832642e1947696d35", "url": "https://www.semanticscholar.org/paper/5096d7ac485371c980f06b4832642e1947696d35", "title": "A cognitive approach to qualities for NLP", "abstract": "Unlike most current NLP knowledge bases, where the lexicalist approach prevails, FunGramKB is ontology-oriented, since the ontology plays a pivotal role between the lexical and the cognitive levels. The objective of this paper is to describe the semantic types assigned to qualities in FunGramKB ontology and how the cognitive approach adopted facilitates the structuring of the knowledge base as well as the reasoning in NLP systems.", "venue": "Proces. del Leng. Natural", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "kg", "nlp", "kg", "nlp"], "mention_counts": {"nlp": 3, "onto": 3, "kg": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 2, "onto": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "489cff534cf26da6112aec9f84e603abc8ecf46d", "url": "https://www.semanticscholar.org/paper/489cff534cf26da6112aec9f84e603abc8ecf46d", "title": "Analysis of natural language understanding technology based on Semantic Web ontology", "abstract": "The key technology of the semantic web are include: ontology, metadata including logic and reasoning and intelligent agents display. Ontology is a formal, explicit specification of conceptualization of the domain knowledge. This paper analyses problems of natural language understanding including: the expression of the target representation complexity, mapping type diversity, the difference of every element in the source of interaction between the degrees of it. This paper presents analysis of natural language understanding technology based on Semantic Web ontology. The experiment result demonstrates that treatment effect of the semantic ontology can improve the natural language understanding.", "venue": "ICM 2015", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "onto", "nlu", "nlu", "onto", "onto", "nlu", "onto", "nlu", "onto", "sw", "sw"], "mention_counts": {"sw": 3, "onto": 5, "nlu": 4}, "nlp_mention_counts": {"nlu": 4}, "ld_mention_counts": {"sw": 3, "onto": 5}, "relevance_score": 0.6546758600252723}, {"paperId": "08d9978e79b684e4e379ac03f2dfccefc37d0923", "url": "https://www.semanticscholar.org/paper/08d9978e79b684e4e379ac03f2dfccefc37d0923", "title": "Adapting Web information extraction knowledge via mining site-invariant and site-dependent features", "abstract": "We develop a novel framework that aims at automatically adapting previously learned information extraction knowledge from a source Web site to a new unseen target site in the same domain. Two kinds of features related to the text fragments from the Web documents are investigated. The first type of feature is called, a site-invariant feature. These features likely remain unchanged in Web pages from different sites in the same domain. The second type of feature is called a site-dependent feature. These features are different in the Web pages collected from different Web sites, while they are similar in the Web pages originating from the same site. In our framework, we derive the site-invariant features from previously learned extraction knowledge and the items previously collected or extracted from the source Web site. The derived site-invariant features will be exploited to automatically seek a new set of training examples in the new unseen target site. Both the site-dependent features and the site-invariant features of these automatically discovered training examples will be considered in the learning of new information extraction knowledge for the target site. We conducted extensive experiments on a set of real-world Web sites collected from three different domains to demonstrate the performance of our framework. For example, by just providing training examples from one online book catalog Web site, our approach can automatically extract information from ten different book catalog sites achieving an average precision and recall of 71.9% and 84.0% respectively without any further manual intervention.", "venue": "TOIT", "citationCount": 25, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ie", "ie", "ie", "ie", "ke", "ke"], "mention_counts": {"ke": 4, "ie": 4}, "nlp_mention_counts": {"ke": 4, "ie": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.6546758600252723}, {"paperId": "1d3356d7434d346cf849c9ef5e9761101be37ba9", "url": "https://www.semanticscholar.org/paper/1d3356d7434d346cf849c9ef5e9761101be37ba9", "title": "The LATO Knowledge Model for Automated Knowledge Extraction and Enrichment from Court Decisions Corpora", "abstract": "Knowledge extraction systems are strongly demanded in the legal domain, to provide legal actors like judges or lawyers with useful and relevant information to enforce a knowledge-based evaluation and judgement of new cases. In this paper, we present LATO-KM, a three-layer legal knowledge model where terms featuring legal knowledge, both law and case-law, are properly formalized as entities and relationships and they are implemented in the LATO ontology using SKOS. The LATO ontology constitutes the core component of CRIKE (CRIme Knowledge Extraction), a data-science approach and related tool environment conceived to support legal knowledge extraction and enrichment from a corpus of Court Decision documents.", "venue": "COUrT@CAiSE", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "skos", "ke", "onto", "onto", "ke", "kg"], "mention_counts": {"ke": 4, "skos": 1, "onto": 2, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "skos": 1, "onto": 2, "kg": 1}, "relevance_score": 0.6546758600252723}, {"paperId": "7279aa16c05b84abc0e2e47ff21c2e734351b257", "url": "https://www.semanticscholar.org/paper/7279aa16c05b84abc0e2e47ff21c2e734351b257", "title": "Using Semantics and NLP in Experimental Protocols", "abstract": "In this paper we present \u201cSMART Protocols\u201d, a semantic and NLP-based infrastructure for processing and enacting experimental protocols. Our contribution is twofold; on the one hand, SMART Protocols delivers a semantic layer that represents the knowledge encoded in experimental protocols. On the other hand, it builds the groundwork for making use of such semantics within an NLP framework. We emphasize on the semantic and NLP components, namely the SMART Protocols (SP) Ontology, the Sample Instrument Reagent Objective (SIRO) model and the text mining integrative architecture GATE. The SMART Protocols (SP) Ontology results from the analysis of over 300 experimental protocols in various domains \u2013molecular biology, cell and developmental biology and others. The gathered terminology is then evaluated, rules are improved accordingly and then a new iteration starts. The SIRO model defines an extended layer of metadata for experimental protocols; SIRO is also a Minimal Information (MI) model conceived in the same realm as the Patient Intervention Comparison Outcome (PICO) model that supports search, retrieval and classification purposes. The SIRO ontology development process includes NLP as well as domain expertise in the extraction of the vocabulary; domain experts extract an initial seed of terminology, then the process is automated by using gazetteers and extraction rules in JAPE. Both SIRO and the SP ontology are then used by our NLP engine, GATE. By combining comprehensive vocabularies with NLP rules and gazetteers we identify meaningful parts of speech in experimental protocols. Moreover, in cases for which SIRO is not available, our NLP automatically extracts it; also, searching for queries such as: \u201dWhat bacteria have been used in protocols for persister cells isolation\u201d", "venue": "SWAT4LS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "nlp", "nlp", "nlp", "onto", "nlp", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 8, "onto": 4}, "nlp_mention_counts": {"nlp": 8}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.6546758600252723}, {"paperId": "9f93af842016e9e08f5814b90bfac5c8fcd704b1", "url": "https://www.semanticscholar.org/paper/9f93af842016e9e08f5814b90bfac5c8fcd704b1", "title": "Healthcare-Event driven semantic knowledge extraction with hybrid data repository", "abstract": "In this paper, we introduce a Healthcare-Event (H-event) based knowledge extraction approach on a hybrid data repository. The repository collects (with individual user's permission) dynamic and large volume healthcare related data from various resources such as wearable sensors, social media Web APIs and our application itself. The proposed extraction approach relies on two data processing processes. One is the data integration process to dynamically retrieving the large data using public data service APIs. The first process also generates a set of big knowledge bases and stored in NoSQL storage. This paper will focus on the second extraction process that is the H-Event based ontological knowledge extraction for detecting and monitoring user's healthcare related situations, such as medical symptoms, treatments, conditions and daily activities from the NoSQL knowledge bases. The second process can be seen as post-processing step to detect more explicit healthcare knowledge about personalised health conditions and represent the knowledge using RDF formats in a semantic triple repository to enhance further data analytics.", "venue": "InTech", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "kg", "ke", "tp", "kg", "kg", "ke", "ke", "rdf"], "mention_counts": {"onto": 1, "ke": 3, "kg": 3, "tp": 1, "rdf": 1}, "nlp_mention_counts": {"ke": 3, "tp": 1}, "ld_mention_counts": {"kg": 3, "onto": 1, "ke": 3, "rdf": 1}, "relevance_score": 0.6546758600252723}, {"paperId": "37660114739e25002eb503b6630e53abe7d05c2b", "url": "https://www.semanticscholar.org/paper/37660114739e25002eb503b6630e53abe7d05c2b", "title": "Ceres: Harvesting Knowledge from the Semi-structured Web", "abstract": "Knowledge graphs have been used to support a wide range of applications and enhance search and QA for Google, Bing, Amazon Alexa, etc. However, we often miss long-tail knowledge, including unpopular entities, unpopular relations, and unpopular verticals. In this talk we describe our efforts in harvesting knowledge from semi-structured websites, which are often populated according to some templates using vast volume of data stored in underlying databases. We describe our Ceres system, which extracts knowledge from semi-structured web. AutoCeres is a ClosedIE system that extracts knowledge according to existing ontology. It improves the accuracy of fully automatic knowledge extraction from 60%+ of state-of-the-art to 90%+ on semi-structured data. OpenCeres is the first-ever OpenIE system on semi-structured data, that is able to identify new relations not readily included in existing ontologies. ZeroShotCeres goes further and enables extracting knowledge for completely new domains, where there is no seed knowledge for bootstrapping the extraction. Finally, we describe our other efforts in ontology alignment, entity linkage, graph mining, and QA, that allow us to best leverage the knowledge we extract for search and QA.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "onto", "ke", "ke", "onto", "onto", "kg"], "mention_counts": {"ke": 4, "onto": 3, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "onto": 3, "kg": 1}, "relevance_score": 0.6546758600252723}, {"paperId": "7299951eed1626904b3cfeeb4b279c7d4bd680a9", "url": "https://www.semanticscholar.org/paper/7299951eed1626904b3cfeeb4b279c7d4bd680a9", "title": "Automated knowledge derivation: Domain\u2010independent techniques for domain\u2010restricted text sources", "abstract": "This article provides a description of the major components of a system that builds and updates a knowledge base by extracting the knowledge from natural language text. the knowledge extraction is done in a domain\u2010independent manner and does not rely on particular vocabulary or grammar constructions. the only restriction is that the input text must be technical text from some specific problem domain. an important capability of the system is that it can bootstrap itself. That is, beginning with only a description of the types of object and relationships to be stored in the knowledge base, the system can start with an empty knowledge base and build the knowledge base as it processes the text. the knowledge extraction system's success in extracting knowledge from various input texts was evaluated using scoring metrics reported by Lehnert and Sundheim [AI Mag., 12(3), 81\u201394 (1991)]. the initial results indicate that the knowledge extraction mechanism is both effective and independent of a particular author's writing style or a particular domain. \u00a9 1995 John Wiley & Sons, Inc.", "venue": "Int. J. Intell. Syst.", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "kg", "ke", "ke", "kg", "ke", "kg"], "mention_counts": {"kg": 4, "ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"kg": 4, "ke": 4}, "relevance_score": 0.6546758600252723}, {"paperId": "4b8522529868a832e3a07e5a647e0b9cfe559d81", "url": "https://www.semanticscholar.org/paper/4b8522529868a832e3a07e5a647e0b9cfe559d81", "title": "Knowledge Extraction Based on Forgetting and Subontology Generation (DL Invited Talk Abstract)", "abstract": "This presentation will give an overview of our ongoing work in developing knowledge extraction methods for description logic based ontologies. Because the knowledge is not only given by the axioms stated in an ontology but also by the knowledge that can be inferred from these axioms, knowledge extraction is a challenging problem. Forgetting creates a compact and faithful representation of the stored knowledge over a user-specified signature by performing inferences on the symbols outside this signature. After an introduction of the idea of forgetting, an overview of our forgetting tools and some applications we have explored, I will discuss recent collaborative work with SNOMED International to create bespoke knowledge extraction software for the medical ontology SNOMED CT. The software creates a self-contained subontology containing definitions of a specified set of focus concepts which minimises the number of supporting symbols used and satisfies SNOMED modelling guidelines. Such subontologies make it easier to reuse and share content, assist with ontology analysis, and querying and classification is faster. The talk will give an overview of this research spanning several years, focussing on key ideas, findings, practical challenges encountered and current applications.", "venue": "Description Logics", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "onto", "onto", "onto", "ke", "onto", "ke"], "mention_counts": {"ke": 4, "onto": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "onto": 4}, "relevance_score": 0.6546758600252723}, {"paperId": "b049c2c2d3f492db34514a0e20c7849befcaef38", "url": "https://www.semanticscholar.org/paper/b049c2c2d3f492db34514a0e20c7849befcaef38", "title": "A Natural Language Processing Pipeline to Extract Phenotypic Data from Formal Taxonomic Descriptions with a Focus on Flagellate Plants", "abstract": "Assembling large-scale phenotypic datasets for evolutionary and biodiversity studies of plants can be extremely difficult and time consuming. New semi-automated Natural Language Processing (NLP) pipelines can extract phenotypic data from taxonomic descriptions, and their performance can be enhanced by incorporating information from ontologies, like the Plant Ontology (PO) and the Plant Trait Ontology (TO). These ontologies are powerful tools for comparing phenotypes across taxa for large-scale evolutionary and ecological analyses, but they are largely focused on terms associated with flowering plants. We describe a bottom-up approach to identify terms from flagellate plants (including bryophytes, lycophytes, ferns, and gymnosperms) that can be added to existing plant ontologies. We first parsed a large corpus of electronic taxonomic descriptions using the Explorer of Taxon Concepts tool (http://taxonconceptexplorer.org/) and identified flagellate plant specific terms that were missing from the existing ontologies. We extracted new structure and trait terms, and we are currently incorporating the missing structure terms to the PO and modifying the definitions of existing terms to expand their coverage to flagellate plants. We will incorporate trait terms to the TO in the near future. Keywords\u2014Natural Language Processing; Plant Ontology; Plant Trait Ontology; taxonomic descriptions; flagellate plants; phenotypic traits; matrices; phylogeny", "venue": "ICBO", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "nlp", "nlp", "nlp", "onto", "onto", "nlp", "onto", "onto"], "mention_counts": {"nlp": 4, "onto": 8}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 8}, "relevance_score": 0.6546758600252723}, {"paperId": "786dfe6c2ec095b0dca581ed1fd79b7836d6d07c", "url": "https://www.semanticscholar.org/paper/786dfe6c2ec095b0dca581ed1fd79b7836d6d07c", "title": "Automatic Ontology-Based Knowledge Extraction from Web Documents", "abstract": "To bring the Semantic Web to life and provide advanced knowledge services, we need efficient ways to access and extract knowledge from Web documents. Although Web page annotations could facilitate such knowledge gathering, annotations are rare and will probably never be rich or detailed enough to cover all the knowledge these documents contain. Manual annotation is impractical and unscalable, and automatic annotation tools remain largely undeveloped. Specialized knowledge services therefore require tools that can search and extract specific knowledge directly from unstructured text on the Web, guided by an ontology that details what type of knowledge to harvest. An ontology uses concepts and relations to classify domain knowledge. Other researchers have used ontologies to support knowledge extraction, but few have explored their full potential in this domain. The paper considers the Artequakt project which links a knowledge extraction tool with an ontology to achieve continuous knowledge support and guide information extraction. The extraction tool searches online documents and extracts knowledge that matches the given classification structure. It provides this knowledge in a machine-readable format that will be automatically maintained in a knowledge base (KB). Knowledge extraction is further enhanced using a lexicon-based term expansion mechanism that provides extended ontology terminology.", "venue": "IEEE Intelligent Systems", "citationCount": 498, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ke", "ie", "ke", "kg", "ke", "ke", "onto", "ke", "ke", "onto", "kg", "onto", "sw"], "mention_counts": {"onto": 6, "ke": 6, "sw": 1, "kg": 2, "ie": 1}, "nlp_mention_counts": {"ke": 6, "ie": 1}, "ld_mention_counts": {"ke": 6, "sw": 1, "onto": 6, "kg": 2}, "relevance_score": 0.6362851125425543}, {"paperId": "8f03235731160c8418671dec9865b0f10a5b20a9", "url": "https://www.semanticscholar.org/paper/8f03235731160c8418671dec9865b0f10a5b20a9", "title": "OntoNERdIE - Mapping and Linking Ontologies to Named Entity Recognition and Information Extraction Resources", "abstract": "Semantic Web and NLP We describe an implemented offline procedure that maps OWL/RDF-encoded ontologies with large, dynamically maintained instance data to named entity recognition (NER) and information extraction (IE) engine resources, preserving hierarchical concept information and links back to the ontology concepts and instances. The main motivations are (i) improving NER/IE precision and recall in closed domains, (ii) exploiting linguistic knowledge (context, inflection, anaphora) for identifying ontology instances in texts more robustly, (iii) giving full access to ontology instances and concepts in natural language processing results, e.g. for subsequent ontology queries, navigation or inference, (iv) avoiding duplication of work in development and maintenance of similar resources in independent places, namely lingware and ontologies. We show an application in hybrid deep-shallow natural language processing that is e.g. used for question analysis in closed domains. Further applications could be automatic hyperlinking or other innovative semantic-web related applications.", "venue": "LREC", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "nlp", "sw", "onto", "onto", "onto", "ie", "onto", "sw", "rdf", "onto", "nlp", "onto", "nlp"], "mention_counts": {"onto": 8, "nlp": 3, "sw": 2, "rdf": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 3, "ie": 2}, "ld_mention_counts": {"sw": 2, "onto": 8, "rdf": 1}, "relevance_score": 0.6234546105271034}, {"paperId": "b3f2d5f9482fcb617e21599acca6e03aa5e968d1", "url": "https://www.semanticscholar.org/paper/b3f2d5f9482fcb617e21599acca6e03aa5e968d1", "title": "Flexible Ontology Population from Text: The OwlExporter", "abstract": "Ontology population from text is becoming increasingly important for NLP applications. Ontologies in OWL format provide for a standardized means of modeling, querying, and reasoning over large knowledge bases. Populated from natural language texts, they offer significant advantages over traditional export formats, such as plain XML. The development of text analysis systems has been greatly facilitated by modern NLP frameworks, such as the General Architecture for Text Engineering (GATE). However, ontology population is not currently supported by a standard component. We developed a GATE resource called the OwlExporter that allows to easily map existing NLP analysis pipelines to OWL ontologies, thereby allowing language engineers to create ontology population systems without requiring extensive knowledge of ontology APIs. A particular feature of our approach is the concurrent population and linking of a domainand NLP-ontology, including NLP-specific features such as safe reasoning over coreference chains.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 60, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "onto", "onto", "onto", "kg", "nlp", "onto", "nlp", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 5, "onto": 10, "kg": 1}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"kg": 1, "onto": 10}, "relevance_score": 0.6234546105271034}, {"paperId": "00634c422cf163a9d8959d78fe2943ff5073e4ce", "url": "https://www.semanticscholar.org/paper/00634c422cf163a9d8959d78fe2943ff5073e4ce", "title": "Extracting Knowledge from Parliamentary Debates for Studying Political Culture and Language", "abstract": "This paper presents knowledge extraction and natural language processing methods used to enrich the knowledge graph of the plenary debates (textual transcripts of speeches) of the Parliament of Finland. This knowledge graph includes some 960 000 speeches (1907\u20132021) interlinked with a prosopographical knowledge graph about the politicians. A recent subset of the speeches was used to extract named entities and topical keywords for semantic searching and browsing the data and for data analysis. The process is based on linguistic analysis, named entity linking, and automatic subject indexing. The results were included into the ParliamentSampo knowledge graph in a SPARQL endpoint. This data can be used for studying parliamentary language and culture in Digital Humanities research and for developing applications, such as the ParliamentSampo portal.", "venue": "TEXT2KG/MK@ESWC", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "nlp", "kg", "kg", "kg", "ke"], "mention_counts": {"nlp": 1, "kg": 4, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"kg": 4, "ke": 2}, "relevance_score": 0.6160945466525044}, {"paperId": "2f8c0fa4312babeda1891f05245b00d217eb7b8e", "url": "https://www.semanticscholar.org/paper/2f8c0fa4312babeda1891f05245b00d217eb7b8e", "title": "Using Text Analytics for Health to Get Meaningful Insights from a Corpus of COVID Scientific Papers", "abstract": "Since the beginning of COVID pandemic, there have been around 700000 scientific papers published on the subject. A human researcher cannot possibly get acquainted with such a huge text corpus \u2014 and therefore developing AI-based tools to help navigating this corpus and deriving some useful insights from it is highly needed. In this paper, we will use Text Analytics for Health pre-trained service together with some cloud tools to extract some knowledge from scientific papers, gain insights, and build a tool to help researcher navigate the paper collection in a meaningful way. Code. The code for experiments described in this paper is available from http://github.com/CloudAdvocacyAzurePaperExplorationWorkshop. Introduction: Automatic Paper Analysis Automatic scientific paper analysis is fast growing area of studies, and due to recent improvements in NLP techniques is has been greatly improved in the recent years. In this paper, we will show you how to derive specific insights from COVID papers, such as changes in medical treatment over time, or joint treatment strategies using several medications. The main approach we will describe in this paper is to extract as much semi-structured information from text as possible, and then store it into some NoSQL database for further processing. Storing information in the database would allow us to make some very specific queries to answer some of the questions, as well as to provide visual exploration tool for medical expert for structured search and insight generation. Processing large volume of papers is done by running parallel sweep job on Azure Machie Learning cluster. The overall architecture of the proposed system is shown in Fig. 1: Fig.1: Architecture of a system to extract knowledge insights from a corpus of scientific papers. Note that this architecture is built on top of the platform components of Microsoft Azure, which allows us to delegate many complex issues (such as scalability) to the cloud provider. COVID Scientific Papers and CORD Dataset The idea to apply NLP methods to scientific literature seems quite natural and has been proposed in many different works [1,2,3]. First of all, scientific texts are already well-structured, they contain things like keywords, abstract, as well as well-defined terms. Thus, at the very beginning of COVID pandemic, a research challenge has been launched on Kaggle to analyze scientific papers on the subject. The dataset behind this competition is called CORD [4], and it contains constantly updated corpus of everything that is published on topics related to COVID. This dataset consists of the following parts: \u2022 Metadata file Metadata.csv contains most important information for all publications in one place. Each paper in this table has unique identifier cord_uid (which in fact does not happen to be completely unique, once you start working with the dataset). The information includes: Title of publication, Journal, Authors, Abstract, Date of publication, doi \u2022 Full-text papers in document_parses directory that contain structured text in JSON format, which greatly simplifies the analysis. \u2022 Pre-built Document Embeddings that maps cord_uids to float vectors that reflect some overall semantics of the paper. In this paper, we will focus on paper abstracts, because they contain the most important information from the paper. However, for full analysis of the dataset, it makes sense to use the same approach on full texts as well. Natural Language Processing Tasks In the recent years, there has been a huge progress in the field of Natural Language Processing, and very powerful neural network language models have been trained. In the area of NLP, the following tasks are typically considered: \u2022 Text classification / intent recognition \u2014 In this task, we need to classify a piece of text into a number of categories. This is a typical classification task. \u2022 Sentiment Analysis \u2014 We need to return a number that shows how positive or negative the text is. This is a typical regression task. \u2022 Named Entity Recognition (NER) \u2014 In NER, we need to extract named entities from text, and determine their type. For example, we may be looking for names of medicines, or diagnoses. Another task similar to NER is keyword extraction. \u2022 Text summarization \u2014 Here we want to be able to produce a short version of the original text, or to select the most important pieces of text. \u2022 Question Answering \u2014 In this task, we are given a piece of text and a question, and our goal is to find the exact answer to this question from text. \u2022 Open-Domain Question Answering (ODQA) \u2014 The main difference from previous task is that we are given a large corpus of text, and we need to find the answer to our question somewhere in the whole corpus. In [5] have described how we can use ODQA approach to automatically find answers to specific COVID questions. However, this approach does not provide insights into the text corpus. To make some insights from text, NER seems to be the most prominent technique to use. If we can find specific entities that are present in text, we could then perform semantically rich search in text that answers specific questions, as well as obtain data on co-occurrence of different entities, figuring out specific scenarios that interest us. To train NER model, as well as any other neural language model, we need a reasonably large dataset that is properly marked up. Finding those datasets is often not an easy task, and producing them for new problem domain often requires initial human effort to mark up the data. Pre-Trained Language Models and Text Analytics for Health Cognitive Service Luckily, modern transformer language models can be trained in semi-supervised manner using transfer learning. First, the base language model (for example, BERT [6]) is trained on a large corpus of text first, and then can be specialized to a specific task such as classification or NER on a smaller dataset. This transfer learning process can also contain additional step \u2014 further training of generic pretrained model on a domain-specific dataset. For example, in the area of medical science Microsoft Research has pre-trained a model called PubMedBERT [7], using texts from PubMed repository. This model can then be further adopted to different specific tasks, provided we have some specialized datasets available. However, training a model requires a lot of skills and computational power, in addition to a dataset. Microsoft (as well as some other large cloud vendors) also make some pre-trained models available through the REST API. Those services are called Cognitive Services, and one of those services for working with text is called Text Analytics [8]. It can do the following: \u2022 Keyword extraction and NER for some common entity types, such as people, organizations, dates/times, etc. \u2022 Sentiment analysis \u2022 Language Detection \u2022 Entity Linking, by automatically adding internet links to some most common entities. This also performs disambiguation, for example Mars can refer to both the planet or a chocolate bar, and correct link would be used depending on the context. For example, here is the result of analyzing one medical paper abstract by Text Analytics: As you can see, some specific entities (for example, HCQ, which is short for hydroxychloroquine) are not recognized at all. Recently, a special version of the service, called Text Analytics for Health [9] was released, which exposes pre-trained PubMedBERT model with some additional capabilities. Here is the result of extracting entities from the same piece of text using Text Analytics for Health: Text Analytics is a REST service, which can be called by using Text Analytics Python SDK in the following manner: poller = text_analytics_client.begin_analyze_healthcare_entities([txt]) res = list(poller.result()) print(res) In addition to just the list of entities, we also get the following: \u2022 Enity Mapping of entities to standard medical ontologies, such as UMLS [10]. \u2022 Relations between entities inside the text, such as TimeOfCondition, etc. \u2022 Negation, which indicated that an entity was used in negative context, for example COVID-19 diagnosis did not occur. Fig. 2: Results of entity extraction, linking and ontology mapping returned by Text Analytics for Health In addition to using Python SDK, we can also call Text Analytics using REST API directly. This is useful if you are using a programming language that does not have a corresponding SDK, or if you prefer to receive Text Analytics result in the JSON format for further storage or processing. In Python, this can be easily done using requests library: uri = f\"{endpoint}/text/analytics/v3.1/entities/ health/jobs?model-version=v3.1\" headers = { \"Ocp-Apim-Subscription-Key\" : key } resp = requests.post(uri,headers=headers,data=doc) res = resp.json() if res['status'] == 'succeeded': result = t['results'] else: result = None Resulting JSON file will look like this: {\"id\": \"jk62qn0z\", \"entities\": [ {\"offset\": 24, \"length\": 28, \"text\": \"coronavirus disease pandemic\", \"category\": \"Diagnosis\", \"confidenceScore\": 0.98, \"isNegated\": false}, {\"offset\": 54, \"length\": 8, \"text\": \"COVID-19\", \"category\": \"Diagnosis\", \"confidenceScore\": 1.0, \"isNegated\": false, \"links\": [ {\"dataSource\": \"UMLS\", \"id\": \"C5203670\"}, {\"dataSource\": \"ICD10CM\", \"id\": \"U07.1\"}, ... ]}, \"relations\": [ {\"relationType\": \"Abbreviation\", \"bidirectional\": true, \"source\": \"#/results/documents/2/entities/6\", \"target\": \"#/results/documents/2/entities/7\"}, ...], } In production code, one may want to incorporate a mechanism that will retry the operation when an error is returned by the service. Parallel Paper Processing using Azure Machine Learning Cluster Since the dataset currently contains ~700K paper abstracts, processing them sequentially through Text Analytics would be quite time-consuming. To run this code in parallel, we can use technologies such as Azure Batch or Azure Machine Learning [11]. Both allow you to create a cluster of ide", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "nlp", "onto", "ke", "nlp", "nlp"], "mention_counts": {"nlp": 5, "onto": 2, "ke": 1}, "nlp_mention_counts": {"nlp": 5, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.6160945466525044}, {"paperId": "1b6798695de27880009346c6c2023665139b0014", "url": "https://www.semanticscholar.org/paper/1b6798695de27880009346c6c2023665139b0014", "title": "Ontology-Based Question Answering over Corporate Structured Data", "abstract": "Ontology-based approach to the Natural Language Understanding (NLU) processing allows to improve questions answering quality in dialogue systems. We describe our NLU engine architecture and evaluate its implementation. The engine transforms user's input into the SPARQL SELECT, ASK or INSERT query to the knowledge graph provided by the ontology-based data virtualization platform. The transformation is based on the lexical level of the knowledge graph built according to the Ontolex ontology. The described approach can be applied for graph data population tasks and to the question answering systems implementation, including chat bots. We describe the dialogue engine for a chat bot which can keep the conversation context and ask clarifying questions, simulating some aspects of the human logical thinking. Our approach uses graph-based algorithms to avoid gathering datasets, required in the neural nets-based approaches, and provide better explainability of our models. Using question answering engine in conjunction with data virtualization layer over the corporate data sources allows extracting facts from the structured data to be used in conversation.", "venue": "2021 International Symposium on Knowledge, Ontology, and Theory (KNOTH)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "nlu", "onto", "onto", "nlu", "onto", "kg", "kg", "onto"], "mention_counts": {"kg": 2, "onto": 4, "nlu": 3}, "nlp_mention_counts": {"nlu": 3}, "ld_mention_counts": {"kg": 2, "onto": 4}, "relevance_score": 0.6160945466525044}, {"paperId": "5bc1bb36cb73796ed68535b1ee4e4ba50d4bf0f3", "url": "https://www.semanticscholar.org/paper/5bc1bb36cb73796ed68535b1ee4e4ba50d4bf0f3", "title": "Bridging the Gap Between Ontology and Lexicon via Class-Specific Association Rules Mined from a Loosely-Parallel Text-Data Corpus", "abstract": "There is a well-known lexical gap between content expressed in the form of natural language (NL) texts and content stored in an RDF knowledge base (KB). For tasks such as Information Extraction (IE), this gap needs to be bridged from NL to KB, so that facts extracted from text can be represented in RDF and can then be added to an RDF KB. For tasks such as Natural Language Generation, this gap needs to be bridged from KB to NL, so that facts stored in an RDF KB can be verbalized and read by humans. In this paper we propose LexExMachina, a new methodology that induces correspondences between lexical elements and KB elements by mining class-specific association rules. As an example of such an association rule, consider the rule that predicts that if the text about a person contains the token \"Greek\", then this person has the relation nationality to the entity Greece. Another rule predicts that if the text about a settlement contains the token \"Greek\", then this settlement has the relation country to the entity Greece. Such a rule can help in question answering, as it maps an adjective to the relevant KB terms, and it can help in information extraction from text. We propose and empirically investigate a set of 20 types of class-specific association rules together with different interestingness measures to rank them. We apply our method on a loosely-parallel text-data corpus that consists of data from DBpedia and texts from Wikipedia, and evaluate and provide empirical evidence for the utility of the rules for Question Answering.", "venue": "International Conference on Language, Data, and Knowledge", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "rdf", "nlg", "rdf", "rdf", "rdf", "onto", "kg"], "mention_counts": {"onto": 1, "nlg": 1, "kg": 1, "rdf": 4, "ie": 2}, "nlp_mention_counts": {"nlg": 1, "ie": 2}, "ld_mention_counts": {"kg": 1, "onto": 1, "rdf": 4}, "relevance_score": 0.6160945466525044}, {"paperId": "9f007722471e2855b2d87ef03be7072aeff3bf50", "url": "https://www.semanticscholar.org/paper/9f007722471e2855b2d87ef03be7072aeff3bf50", "title": "Populating a Domain Ontology from a Web Biographical Dictionary of Music - An Unsupervised Rule-based Method to Handle Brazilian Portuguese Texts", "abstract": "An increasing amount of information is available on the web and usually is expressed as text, representing unstructured or semi-structured data. Semantic information is implicit in these texts, since they are mainly intended for human consumption and interpretation. Since unstructured information is not easily handled automatically, an information extraction process has to be used to identify concepts and establish relations among them. Information extraction outcome can be represented as a domain ontology. Ontologies are an appropriate way to represent structured knowledge bases, enabling sharing, reuse and inference. In this paper, an information extraction process is used for populating a domain ontology. It targets Brazilian Portuguese texts from a biographical dictionary of music, which requires specific tools due to some language unique aspects. An unsupervised rule-based method is proposed. Through this process, latent concepts and relations expressed in natural language can be extracted and represented as an ontology, allowing new uses and visualizations of the content, such as semantically browsing and inferring new", "venue": "WEBIST", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "onto", "ie", "onto", "ie", "kg"], "mention_counts": {"kg": 1, "onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"kg": 1, "onto": 5}, "relevance_score": 0.6160945466525044}, {"paperId": "3eb09dfdb66aec5b4276f4368973b42b6821fc2e", "url": "https://www.semanticscholar.org/paper/3eb09dfdb66aec5b4276f4368973b42b6821fc2e", "title": "NERD meets NIF: Lifting NLP Extraction Results to the Linked Data Cloud", "abstract": "We have often heard that data is the new oil. In particular, extracting information from semi-structured textual documents on the Web is key to realize the Linked Data vision. Several attempts have been proposed to extract knowledge from textual documents, extracting named entities, classifying them according to pre-dened taxonomies and disambiguating them through URIs identifying real world entities. As a step towards interconnecting the Web of documents via those entities, dierent extractors have been proposed. Although they share the same main purpose (extracting named entity), they dier from numerous aspects such as their underlying dictionary or ability to disambiguate entities. We have developed NERD, an API and a front-end user interface powered by an ontology to unify various named entity extractors. The unied result output is serialized in RDF according to the NIF specication and published back on the Linked Data cloud. We evaluated NERD with a dataset composed of ve TED talk transcripts, a dataset composed of 1000 New York Times articles and a dataset composed of the 217 abstracts of the papers published at WWW 2011.", "venue": "LDOW", "citationCount": 80, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "ld", "ke", "nlp", "ie", "ld", "onto", "rdf"], "mention_counts": {"onto": 1, "ld": 3, "ke": 1, "nlp": 1, "rdf": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 1}, "ld_mention_counts": {"ld": 3, "ke": 1, "onto": 1, "rdf": 1}, "relevance_score": 0.6160945466525044}, {"paperId": "29b8164adfae1cdd5fd4af4ab43d0bed1764e9db", "url": "https://www.semanticscholar.org/paper/29b8164adfae1cdd5fd4af4ab43d0bed1764e9db", "title": "Unsupervised wrapper induction using linked data", "abstract": "This work explores the usage of Linked Data for Web scale Information Extraction and shows encouraging results on the task of Wrapper Induction. We propose a simple knowledge based method which is (i) highly flexible with respect to different domains and (ii) does not require any training material, but exploits Linked Data as background knowledge source to build essential learning resources. The major contribution of this work is a study of how Linked Data - an imprecise, redundant and large-scale knowledge resource - can be used to support Web scale Information Extraction in an effective and efficient way and identify the challenges involved. We show that, for domains that are covered, Linked Data serve as a powerful knowledge resource for Information Extraction. Experiments on a publicly available dataset demonstrate that, under certain conditions, this simple unsupervised approach can achieve competitive results against some complex state of the art that always depends on training data.", "venue": "International Conference on Knowledge Capture", "citationCount": 34, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ld", "ld", "ld", "ld", "ie", "ie", "kg", "ld"], "mention_counts": {"ld": 5, "kg": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"ld": 5, "kg": 1}, "relevance_score": 0.6160945466525044}, {"paperId": "6f9896545e7c1735ec291d163d3aa9eab8827546", "url": "https://www.semanticscholar.org/paper/6f9896545e7c1735ec291d163d3aa9eab8827546", "title": "ENRICHMENT AND POPULATION OF A GEOSPATIAL ONTOLOGY FOR SEMANTIC INFORMATION EXTRACTION", "abstract": "Abstract. The massive amount of user-generated content available today presents a new challenge for the geospatial domain and a great opportunity to delve into linguistic, semantic, and cognitive aspects of geographic information. Ontology-based information extraction is a new, prominent field in which a domain ontology guides the extraction process and the identification of pre-defined concepts, properties, and instances from natural language texts. The paper describes an approach for enriching and populating a geospatial ontology using both a top-down and a bottom-up approach in order to enable semantic information extraction. The top-down approach is applied in order to incorporate knowledge from existing ontologies. The bottom-up approach is applied in order to enrich and populate the geospatial ontology with semantic information (concepts, relations, and instances) extracted from domain-specific web content.\n", "venue": "The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "onto", "ie", "onto", "ie", "onto"], "mention_counts": {"onto": 6, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.6160945466525044}, {"paperId": "3e420b7b052fe1bc6d1c69585a5ad3f38c7f432d", "url": "https://www.semanticscholar.org/paper/3e420b7b052fe1bc6d1c69585a5ad3f38c7f432d", "title": "Hierarchical, perceptron-like learning for ontology-based information extraction", "abstract": "Recent work on ontology-based Information Extraction (IE) has tried to make use of knowledge from the target ontology in order to improve semantic annotation results. However, very few approaches exploit the ontology structure itself, and those that do so, have some limitations. This paper introduces a hierarchical learning approach for IE, which uses the target ontology as an essential part of the extraction process, by taking into account the relations between concepts. The approach is evaluated on the largest available semantically annotated corpus. The results demonstrate clearly the benefits of using knowledge from the ontology as input to the information extraction process. We also demonstrate the advantages of our approach over other state-of-the-art learning systems on a commonly used benchmark dataset.", "venue": "The Web Conference", "citationCount": 75, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "ie", "onto", "onto", "onto", "ie", "onto"], "mention_counts": {"onto": 6, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.6160945466525044}, {"paperId": "be04b5820056ea1216face453bb84c69af6deac0", "url": "https://www.semanticscholar.org/paper/be04b5820056ea1216face453bb84c69af6deac0", "title": "Exploiting ASP for Semantic Information Extraction", "abstract": "The paper describes HiLeX, a new ASP-based system for the extraction of information from unstructured documents. Unlike previous systems, which are mainly syntactic, HiLeX combines both semantic and syntactic knowledge for a powerful information extraction. In particular, the exploitation of background knowledge, stored in a domain ontology, allows to empower significantly the information extraction mechanisms. HiLeX is founded on a new two-dimensional representation of documents, and heavily exploits DLP\u2013 an extension of disjunctive logic programming for ontology representation and reasoning which has been recently implemented on top of DLV . The domain ontology is represented in DLP, and the extraction patterns are encoded by DLP reasoning modules, whose execution yields the actual extraction of information from the input document. HiLeX allows to extract information from both HTML and flat text documents.", "venue": "ASp", "citationCount": 34, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "ie", "ie", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 3, "ie": 6}, "nlp_mention_counts": {"ie": 6}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "49bf3c3215f536882b10b15940795eecaff6d96f", "url": "https://www.semanticscholar.org/paper/49bf3c3215f536882b10b15940795eecaff6d96f", "title": "Ontology-based approach to enhance medical web information extraction", "abstract": "\nPurpose\nThe purpose of this study is to propose a framework for extracting medical information from the Web using domain ontologies. Patient\u2013Doctor conversations have become prevalent on the Web. For instance, solutions like HealthTap or AskTheDoctors allow patients to ask doctors health-related questions. However, most online health-care consumers still struggle to express their questions efficiently due mainly to the expert/layman language and knowledge discrepancy. Extracting information from these layman descriptions, which typically lack expert terminology, is challenging. This hinders the efficiency of the underlying applications such as information retrieval. Herein, an ontology-driven approach is proposed, which aims at extracting information from such sparse descriptions using a meta-model.\n\n\nDesign/methodology/approach\nA meta-model is designed to bridge the gap between the vocabulary of the medical experts and the consumers of the health services. The meta-model is mapped with SNOMED-CT to access the comprehensive medical vocabulary, as well as with WordNet to improve the coverage of layman terms during information extraction. To assess the potential of the approach, an information extraction prototype based on syntactical patterns is implemented.\n\n\nFindings\nThe evaluation of the approach on the gold standard corpus defined in Task1 of ShARe CLEF 2013 showed promising results, an F-score of 0.79 for recognizing medical concepts in real-life medical documents.\n\n\nOriginality/value\nThe originality of the proposed approach lies in the way information is extracted. The context defined through a meta-model proved to be efficient for the task of information extraction, especially from layman descriptions.\n", "venue": "Int. J. Web Inf. Syst.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "ie", "ie", "onto", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 3, "ie": 6}, "nlp_mention_counts": {"ie": 6}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "4a1aa01243360319a6accd1b187bd741d6ccf66c", "url": "https://www.semanticscholar.org/paper/4a1aa01243360319a6accd1b187bd741d6ccf66c", "title": "Autonomous Decentralized Kernel Cache Architecture for Multi Ontology Based Information Extraction on Microsoft Windows", "abstract": "Ontology Based Information Extraction (OBIE) is being adopted in various domains in order to improve the system's precision and recall. Though use of multiple ontologies in different semantic based Information Extraction systems helps to improve the system extraction accuracy but the performance of system degrades significantly. This paper proposes autonomous decentralized kernel cache architecture to improve the query time for OBIE systems that utilize multiple ontologies. In order to minimize performance bottlenecks we propose caching most frequently used relations of the ontologies in the autonomous and decentralized kernel cache in order to reduce the query time against different semantic queries. Results show performance improvement for different queries made on YAGO2s and DBpedia ontologies using the proposed kernel based Onto-Cache.", "venue": "2017 IEEE 13th International Symposium on Autonomous Decentralized System (ISADS)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "onto", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 6, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.6160945466525044}, {"paperId": "a9428763a64ed8b88942370ee180d10943493f29", "url": "https://www.semanticscholar.org/paper/a9428763a64ed8b88942370ee180d10943493f29", "title": "Biological ontology enhancement with fuzzy relations: a text-mining framework", "abstract": "Domain ontology can help in information retrieval from documents. But ontology is a pre-defined structure with crisp concept descriptions and inter-concept relations. However, due to the dynamic nature of the document repository, ontology should be upgradeable with information extracted through text mining of documents in the domain. This also necessitates that concepts, their descriptions and inter-concept relations should be associated with a degree of fuzziness that will indicate the support for the extracted knowledge according to the currently available resources. Supports may be revised with more knowledge coming in future. This approach preserves the basic structured knowledge format for storing domain knowledge, but at the same time allows for update of information. In this paper, we have proposed a mechanism which initiates text mining with a set of ontological concepts, and thereafter extracts fuzzy relations through text mining. Membership values of relations are functions of frequency of co-occurrence of concepts and relations. We have worked on the GENIA corpus and shown how fuzzy relations can be further used for guided information extraction from MEDLINE documents.", "venue": "International Conference on Wirtschaftsinformatik", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "onto", "ke", "ie", "onto"], "mention_counts": {"ke": 1, "onto": 5, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 5}, "relevance_score": 0.6160945466525044}, {"paperId": "72beb9f1a95275bbf974c8f2ff05b1248169a263", "url": "https://www.semanticscholar.org/paper/72beb9f1a95275bbf974c8f2ff05b1248169a263", "title": "Event Extraction and Representation: A Case Study for the Portuguese Language", "abstract": "Text information extraction is an important natural language processing (NLP) task, which aims to automatically identify, extract, and represent information from text. In this context, event extraction plays a relevant role, allowing actions, agents, objects, places, and time periods to be identified and represented. The extracted information can be represented by specialized ontologies, supporting knowledge-based reasoning and inference processes. In this work, we will describe, in detail, our proposal for event extraction from Portuguese documents. The proposed approach is based on a pipeline of specialized natural language processing tools; namely, a part-of-speech tagger, a named entities recognizer, a dependency parser, semantic role labeling, and a knowledge extraction module. The architecture is language-independent, but its modules are language-dependent and can be built using adequate AI (i.e., rule-based or machine learning) methodologies. The developed system was evaluated with a corpus of Portuguese texts and the obtained results are presented and analysed. The current limitations and future work are discussed in detail.", "venue": "Inf.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ke", "nlp", "nlp", "nlp", "ie", "kg"], "mention_counts": {"onto": 1, "nlp": 3, "ke": 1, "kg": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 3, "ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 1, "kg": 1}, "relevance_score": 0.6160945466525044}, {"paperId": "7aec6f54b6f2949f263306e24200b0b43ffcdb83", "url": "https://www.semanticscholar.org/paper/7aec6f54b6f2949f263306e24200b0b43ffcdb83", "title": "The Cancerology ontology: Designed to support the search of evidence-based oncology from biomedical literatures", "abstract": "This work proposes a new ontology, called the Cancerology, where it faces a problem of unclear analysis in a biomedical text processing because existing ontologies such National Cancer Institute's Thesaurus and Ontology do not offer some information relating to domain specific variations in terms that can be provided by the domain expert. This ontology is experimented through a method of text classification with retrieving the relevant cervix cancer abstracts relating to clinical trials from PubMed. The experimental results show more effectiveness for increasing the accuracy. This demonstrates that the Cancerology may be also effective for other areas of text processing and analysis, especially in the particular domain of oncology literature such as intelligent search service, text mining, and knowledge extraction.", "venue": "2011 24th International Symposium on Computer-Based Medical Systems (CBMS)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "tp", "onto", "ke", "onto", "tp", "onto", "onto"], "mention_counts": {"ke": 1, "onto": 5, "tp": 2}, "nlp_mention_counts": {"ke": 1, "tp": 2}, "ld_mention_counts": {"ke": 1, "onto": 5}, "relevance_score": 0.6160945466525044}, {"paperId": "3fc27edf7ed52cddf56118586d5bf418cdcf1855", "url": "https://www.semanticscholar.org/paper/3fc27edf7ed52cddf56118586d5bf418cdcf1855", "title": "A Framework for Institutional Risk Identification using Knowledge Graphs and Automated News Profiling", "abstract": "Organizations around the world face an array of risks impacting their operations globally. It is imperative to have a robust risk identification process to detect and evaluate the impact of potential risks before they materialize. Given the nature of the task and the current requirements of deep subject matter expertise, most organizations utilize a heavily manual process. In our work, we develop an automated system that (a) continuously monitors global news, (b) is able to autonomously identify and characterize risks, (c) is able to determine the proximity of reaching triggers to determine the distance from the manifestation of the risk impact and (d) identifies organization\u2019s operational areas that may be most impacted by the risk. Other contributions also include: (a) a knowledge graph representation of risks and (b) relevant news matching to risks identified by the organization utilizing a neural embedding model to match the textual description of a given risk with multi-lingual news. Introduction & Related Work Global institutions are exposed to various types of risks, ranging from market risks related to the institution\u2019s core functions (CFI 2020) (BOE 2020) to operational, compliance, cyber-security, geopolitical and reputational risks (Matellio 2020). Risks in these areas are inherently hard to identify and quantify. Risk mitigation is also extremely challenging, which is why the runway provided by its identification and quantification is crucial. Unfortunately, the lack of proper risk assessment has led to the demise of several organizations once the risk manifested (De Haas and Van Horen 2012). In our work, we present a system for risk identification utilizing knowledge graphs for representing risk areas and a neural embedding model (Reimers and Gurevych 2019) for multi-lingual news matching tailored towards financial institutions. The formal definition of a risk and the study of methods for risk assessment and mitigation has a long history of academic research (Henley and Kumamoto 1981), (Covello and Mumpower 1985), (Rechard 1999), (Bedford, Cooke et al. 2001), (Thompson, Deisler Jr, and Schwing 2005) and (Zio 2009). Similarly, the use of natural language processing and Copyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. knowledge graphs for news recommendation has been studied extensively and is used widely in practice. The majority of the work has focused on developing news recommendation systems tailored towards users preferences. (Wang et al. 2020) describes a news recommendation system employed by a major financial ratings agency utilizing a neural embedding model developed by (Peters et al. 2018) for news contextual embeddings representation. Other approaches to news recommendations include (IJntema et al. 2010) which utilize externally developed ontologies to find news, collaborative filtering (Lu et al. 2015) and graph embeddings (Ren, Long, and Xu 2019). System Architecture The system proposed (Figure 1) consists of four main components starting with a given set of risks identified by domain experts and producing a list of relevant news for each risk. The input to the system is a repository of risk descriptions provided by the domain experts. The first component extracts a set of relevant entities from the textual description of the risk. The second component uses these entities to construct a knowledge graph. The third component searches for a set of keywords related to the risk and parses them from multiple news sources. The fourth component is a neural network model used to rank news events using contextual embeddings generated for headlines as well as the risk descriptors. In order to demonstrate the end-to-end workflow of our system, we created a set of artificial risks shown in table 1 which a financial institution would face inspired from the risk types defined in (Matellio 2020). Risk Information Extraction Given a textual description of the risk, the text is decomposed into (1) trigger (the root cause of the risk), (2) outcome (the impact of the given risk) and (3) exposure vessel (the entity/vessel the risk impacts). Several approaches were tested to decompose the text into the three categories above. One of these is based on a deep bi-LSTM neural network sequence prediction model developed by (Stanovsky et al. 2018) for supervised open information extraction. The model breaks a given sentence (in our case the risk text) into the relationships they express. In particular, the model extracts a list of propositions, each ar X iv :2 10 9. 09 10 3v 1 [ cs .A I] 1 9 Se p 20 21 (1) Cyber-attacks targeting the retail banking business causing a loss of customer data (2) US China trade war escalation affecting the corporate and investment banking business causing a decrease in revenues (3) Employee misconduct in the investment banking business causing a reputational damage (4) Technology infrastructure failure in the corporate and investment banking business causing a reputational damage and/or monetary loss Table 1: Examples of Institutional Risks", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "onto", "kg", "ie", "kg", "nlp", "kg", "ie"], "mention_counts": {"nlp": 1, "kg": 5, "onto": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 1, "ie": 2}, "ld_mention_counts": {"kg": 5, "onto": 1}, "relevance_score": 0.6160945466525044}, {"paperId": "31163b803bfeb79a3e7e5f5a7fdebdae3cc880e8", "url": "https://www.semanticscholar.org/paper/31163b803bfeb79a3e7e5f5a7fdebdae3cc880e8", "title": "Natural Language Processing of Textual Requirements", "abstract": "Natural language processing (NLP) is the application of automated parsing and machine learning techniques to ana lyze standard text. Applications of NLP to requirements engineering include extraction of ontologies from a requirements speci fication, and use of NLP to verify the consistency and/or completion of a requirements specification. This work-in-progress paper describes a new approach to the interpretation, organizati on and management of textual requirements through the use of application-specific ontologies and natural language proc essing. We also design and exercise a prototype software tool that implements the new framework on a simplified model of an aircraft. Keywords-Systems Engineering; Ontologies; Natural Language Processing; Requirements; Rule Checking.", "venue": "International Conference on Systems", "citationCount": 26, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "nlp", "nlp", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 6, "onto": 3}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "4d4d82e3b95a23a5d7ba711b20e79229eaf43c0d", "url": "https://www.semanticscholar.org/paper/4d4d82e3b95a23a5d7ba711b20e79229eaf43c0d", "title": "Bio-SODA: Enabling Natural Language Question Answering over Knowledge Graphs without Training Data", "abstract": "The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20% and by an even higher factor on more complex bioinformatics datasets.", "venue": "International Conference on Statistical and Scientific Database Management", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "kg", "ld", "kg", "kg", "nlp", "kg", "nlp", "nlp"], "mention_counts": {"ld": 1, "sw": 1, "nlp": 3, "kg": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"ld": 1, "sw": 1, "kg": 4}, "relevance_score": 0.6160945466525044}, {"paperId": "25fd8d3757b342534a1f569633fd62001bb16421", "url": "https://www.semanticscholar.org/paper/25fd8d3757b342534a1f569633fd62001bb16421", "title": "Exploiters-Based Knowledge Extraction in Object-Oriented Knowledge Representation", "abstract": "This paper contains the consideration of knowledge extraction mechanisms of such object-oriented knowledge representation models as frames, object-oriented programming and object-oriented dynamic networks. In addition, conception of universal exploiters within object-oriented dynamic networks is also discussed. The main result of the paper is introduction of new exploiters-based knowledge extraction approach, which provides generation of a finite set of new classes of objects, based on the basic set of classes. The methods for calculation of quantity of new classes, which can be obtained using proposed approach, and of quantity of types, which each of them describes, are proposed. Proof that basic set of classes, extended according to proposed approach, together with union exploiter create upper semilattice is given. The approach always allows generating of finitely defined set of new classes of objects for any object-oriented dynamic network. A quantity of these classes can be precisely calculated before the generation. It allows saving of only basic set of classes in the knowledge base.", "venue": "CS&P", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "kg", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "c6948c903421753b5ae5b4a14fffddb66890152c", "url": "https://www.semanticscholar.org/paper/c6948c903421753b5ae5b4a14fffddb66890152c", "title": "A Multi-intelligent Agent Architecture for Knowledge Extraction: Novel Approaches for Automatic Production Rules Extraction", "abstract": "In this paper, multi-intelligent agent architecture has been proposed for automatic knowledge extraction from its resources (domain experts and text documents). The extracted knowledge should be stored in a knowledge base to be used later by knowledge-based systems. This article aims to produce an effective knowledge base by cooperation between expert mining and text mining techniques. Firstly, we are constructing an Expert Mining Intelligent Agent (EMIA) able to interview with domain experts for mining problem solving knowledge as production rules in a specific diagnosis domain. It is also responsible for extracting the patterns or linguistic expressions and save it in a conceptual database. Secondly, we are constructing a Text Mining Intelligent Agent (TMIA) capable of extracting production rules from a text document corpus. The achievement of that extraction can be performed by a text document categorization based on a traditional term weighting scheme (TF-IDF) and using the Stanford parser to analyze and produce a parsing tree for each sentence in that document. Then, the TMIA looks for all causal words and takes them as separation words to generate patterns and sub-patterns based on the conceptual database. Finally, the TMIA stores those patterns and sub-patterns in a pre-formatted template and displays it to a domain expert for a modification process to construct accurate production rule.", "venue": "MUE 2014", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "kg", "ke", "ke", "ke"], "mention_counts": {"kg": 3, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 3, "ke": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "bfff31c2a5d6ddf2740244ed4ee41e1757cf32a2", "url": "https://www.semanticscholar.org/paper/bfff31c2a5d6ddf2740244ed4ee41e1757cf32a2", "title": "Extracting Descriptions of Location Relations from Implicit Textual Networks", "abstract": "For the retrieval of concise entity relation information from large collections or streams of documents, existing approaches can be grouped into the categories of (multi-document) summarization and knowledge extraction. The former tend to fall short for this task due to the involved amount of information that cannot be easily condensed, while knowledge extraction approaches are often pattern-based and too discriminative for exploratory purposes. For location relations in particular, this translates to a set of very short relationship descriptors that predominantly encode hierarchical or containment relations such as located in or capital of. As a result, available knowledge bases that are typically populated through knowledge extraction are limited to these discrete and typed relations. In contrast, the representation of document collections as implicit networks of entities, terms, and sentences has emerged as a way to encode a much wider range of entity relations and occurrences, which can be leveraged for filtering the relevant information and enabling subsequent interactive explorations. In this paper, we discuss the extraction of descriptive sentences for sets of entities from such implicit networks to support an interactive exploration, and apply them to the extraction of complex location relations that are not hierarchical or containment-based. We introduce and compare efficient ranking methods for sentence extraction that address this entity-centric search task by leveraging entity and term relations in implicit network representations of large document collections. Based on Wikipedia articles and Wikidata as a knowledge base, we demonstrate the extraction of novel location relations that are not contained in the knowledge base.", "venue": "GIR", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "kg", "kg", "ke", "ke"], "mention_counts": {"kg": 3, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 3, "ke": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "1f37f644793050108dc8367ed6960abe2874ea0d", "url": "https://www.semanticscholar.org/paper/1f37f644793050108dc8367ed6960abe2874ea0d", "title": "LYAM++ results for OAEI 2015", "abstract": "The paper presents a novel technique for aligning cross-lingual ontologies that does not rely on machine translation, but uses the large multilingual semantic network BabelNet as a source of background knowledge. In addition, our approach applies a novel orchestration of the components of the matching workflow. We present our results on the evaluation challenge Multifarm. 1 Presentation of the System In spite of the considerable advance that has been made in the field of on-tology matching recently, many questions remain open [1]. The current work addresses the challenge of using background knowledge with a focus on aligning cross-lingual ontologies, i.e., ontologies defined in different natural languages [2]. Indeed, considering multilingual and cross-lingual information is becoming more and more important, in view particularly of the growing number of web content-creating non-English users and the clear demand of cross-language in-teroperability. In the context of the web of data, it is important to propose procedures for linking vocabularies across natural languages, in order to foster the creation of a veritable global information network. The use of different natural languages in the concepts and relations labeling process is becoming an important source of ontology heterogeneity. The methods that have been proposed to deal with it most commonly rely on automatic translation of labels to a single target language [3] or apply machine learning techniques [2]. However, machine translation tolerates low precision levels and machine learning methods require large training corpus that is rarely available in an ontology matching scenario. An inherent problem of translation is that there is often a lack of exact one-to-one correspondence between the terms in different natural languages. 1.1 State, Purpose, General Statement We present LYAM++ (Yet Another Matcher-Light)[4], a fully automatic cross-lingual ontology matching system that does not rely on machine translation. Instead, we make use of the openly available general-purpose multilingual semantic network BabelNet 1 in order to recreate the missing semantic context 1 http://babelnet.org/", "venue": "Organizational Memories", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "onto", "onto", "onto", "mt", "mt", "onto", "onto"], "mention_counts": {"onto": 6, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.6160945466525044}, {"paperId": "f3ec7b83f58965faec4972b7678a4a6f930f1ab8", "url": "https://www.semanticscholar.org/paper/f3ec7b83f58965faec4972b7678a4a6f930f1ab8", "title": "Teanga: A Linked Data based platform for Natural Language Processing", "abstract": "In this paper, we describe Teanga, a linked data based platform for natural language processing (NLP). Teanga enables the use of many NLP services from a single interface, whether the need was to use a single service or multiple services in a pipeline. Teanga focuses on the problem of NLP services interoperability by using linked data to define the types of services input and output. Teanga\u2019s strengths include being easy to install and run, easy to use, able to run multiple NLP tasks from one interface and helping users to build a pipeline of tasks through a graphical user interface.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ld", "ld", "nlp", "nlp", "ld", "nlp", "nlp", "nlp"], "mention_counts": {"ld": 3, "nlp": 6}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"ld": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "8a6a9910e9a1a89eea1ed271bd178b337733eb30", "url": "https://www.semanticscholar.org/paper/8a6a9910e9a1a89eea1ed271bd178b337733eb30", "title": "Practical Approach to Knowledge-based Question Answering with Natural Language Understanding and Advanced Reasoning", "abstract": "The complexity of natural language and the open-domain nature of the World Wide Web have caused modem-day question answering systems to rely only on information retrieval techniques and shallow natural language processing tasks. This approach has brought about serious drawbacks namely restriction on the nature of question and response. This restriction constitutes the first problem addressed by this \nresearch. Through recent academic works, many researchers have begun to acknowledge the problem and agreed that the solution comes in the form of a new approach based on natural language understanding and reasoning in a knowledge-based environment. Due to the infancy stage of this new approach and practical consideration, the current practices vary greatly and are mostly based on only low-level natural language understanding, minimalist representation formalism and conventional reasoning approach without advanced features. As a result, not only were these systems found to be inadequate to solve the first problem but have also created the second problem, that is the limitation to scale across domains and to real-life natural language text. This research hypothesized that a practical approach in the form of a solution framework which combines full-discourse natural language understanding, powerful representation formalism capable of exploiting ontological information and reasoning approach with advanced features, will solve both the first and second problem without compromising practicality factors.The solution framework is implemented as a system called \"Natural Language Understanding and \nReasoning for Intelligence\" (NaLURI). More importantly, two evaluations and their results are presented to demonstrate that the inclusion of more demanding features into a question answering system will not only allow for a wider range of questions and better response quality, but does not affect the response time, hence approving the hypothesis of this research.", "venue": "ArXiv", "citationCount": 19, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlu", "nlp", "nlu", "kg", "nlu", "nlu", "nlu", "kg"], "mention_counts": {"nlp": 1, "onto": 1, "kg": 2, "nlu": 5}, "nlp_mention_counts": {"nlp": 1, "nlu": 5}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.6160945466525044}, {"paperId": "6c94c0968b2ef08f8ac5cae64874e0a8d1adc138", "url": "https://www.semanticscholar.org/paper/6c94c0968b2ef08f8ac5cae64874e0a8d1adc138", "title": "How to Prepare an API for Programming in Natural Language", "abstract": "Natural language interfaces are becoming more and more common but are extremely difficult to build, to maintain, and to port to new domains. NLCI, the Natural Language Command Interpreter, is an architecture for building and porting such interfaces quickly.\nNLCI accepts commands as plain English texts and translates the input sentences into sequences of API calls that implement the intended actions. At its core is an ontology that models the API that is to be used. Then a natural language understanding pipeline analyzes the English input and generates source code. The analyses are independent of a particular API; switching\nto a different API only requires provision of a new ontology.\nIn this demonstration we show how a developer can provide a natural language interface for his or her API by preparing an API ontology. We also show how NLCI analyzes the input text, how we evaluated its results, and how well it performs. As an example we use an API that steers a Lego EV3 robot.", "venue": "SEMANTiCS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "onto", "nlg", "nll", "onto", "nle", "onto", "nlu", "nlp"], "mention_counts": {"onto": 3, "nll": 1, "nlu": 2, "nlp": 1, "nlg": 1, "nle": 1}, "nlp_mention_counts": {"nll": 1, "nlu": 2, "nlp": 1, "nlg": 1, "nle": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "6f42de94eb1d897e031db991c135a7e385d1740e", "url": "https://www.semanticscholar.org/paper/6f42de94eb1d897e031db991c135a7e385d1740e", "title": "Review: Ontological Semantics by Sergei Nirenburg and Victor Raskin", "abstract": "The book under review presents one particular approach to natural language understanding with emphasis on automatic semantic analysis. \"Ontological Semantics\" is in fact a name for the \"ideal form\" of the system the authors propose together with the related theories and assumptions. The monograph is divided into two major parts: a theoretical Part I and a practical Part II. Being more a computational linguist than a formal semanticist I would like to focus my attention mainly on the second part describing the layout of a complete natural language processing system. Part I may be viewed as a (polemic) introduction to computational linguistics, semantics, formal ontology, partly even to the theory of science. The practical motivation of the authors is apparent in their argumention against the efforts to base the entire natural language understanding system on a single formalism which, being suitable for one task, makes it unnecessarily hard or impossible to deal with another. Instead, they argue for a concept of \"microtheories\" ? separable and replaceable modules corresponding to the respective tasks of natural language analysis (morphological and syntactic analysis, coreference, temporality, word sense desambiguation etc.) and communicating with each other in a well-defined manner. Part II actually describes such a system. First, the central notion of \"text meaning representation\" (TMR) is introduced. TMR is basically a set of propositions connected through text-level discourse relations. Additionally, there is an arbitrary number of modalities and references together with a single TMR time specification (contains the time specifications for the respective propositions) and a single style specification. These basic categories contain graph structures of complex values and pointers. In spite of the complexity of the concrete attributes this approach is in general very intuitive ? the relevant information is captured as a graph centered around the respective propositions augmenting any information necessary with attribute values and binary relations. The system relies heavily on the use of external knowledge, which is divided into four components: the ontology, the fact repository, the lexicon, and the onomasticon. The ontology represents a concept (type) hierarchy. The actual instances of a type (e.g. London of type City) are present in the fact repository. The language expressions (e.g. the word \"London\") representing a proper name are listed in the onomasticon and linked to the entities of the fact repository. The lexicon contains all other language material and is linked to the ontology. A separate chapter describes ways of \u2026", "venue": "Prague Bull. Math. Linguistics", "citationCount": 2, "fieldsOfStudy": ["Philosophy", "Computer Science"], "mentions": ["nlu", "nlp", "nlu", "onto", "onto", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 6, "nlu": 2}, "nlp_mention_counts": {"nlp": 1, "nlu": 2}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.6160945466525044}, {"paperId": "347926b02188fb6d9e6839ab027cc4a34394c73d", "url": "https://www.semanticscholar.org/paper/347926b02188fb6d9e6839ab027cc4a34394c73d", "title": "Linguistic Linked Open Data (LLOD) \u2013 Building the cloud", "abstract": "The last decades have seen an immense maturation of Natural Language Processing (NLP) and an increased interest to apply NLP techniques and resources to real-world applications in business and academia. This process has certainly been facilitated by the increased availability of language data in the internet age, and the subsequent paradigm shift to statistical approaches, but also it coincided with an increasing acceptance of empirical approaches in linguistics and related academic fields, including empirical approaches to typology (Greenberg, 1963), corpus linguistics (Francis and Kucera, 1979, Brown Corpus), and (computational) lexicography (Kucera, 1969), as well as the dawn of Digital Humanities (Busa, 1974). Given the complexity of language and the analysis of linguistic data on different levels, its investigation involves a broad band-width of formalisms and resources used to analyze, process and generate natural language. With the transition to empirical, data-driven research, the primary challenge in the field is thus to store, connect and exploit the wealth of language data available in all its heterogeneity. Interoperability of language resources has hence been an important issue addressed by the community since the late 1980s (Text Encoding Initiative, 1990), but still remains a problem that is solved only partially, i.e., on the level of specific sub-types of linguistic resources, such as lexical resources (Francopoulo et al., 2006) or annotated corpora (Ide and Suderman, 2007), respectively. A closely related challenge is information integration, i.e., how information from different sources can be retrieved and combined in an efficient way. Recently, both challenges have been addressed by means of Linked Data principles (Chiarcos et al., 2013a,b), eventually leading to the formation of a Linguistic Linked Open Data (LLOD) cloud (Chiarcos et al., 2012b). The talk describes its current state of development, it presents selected examples for main types of linguistic resources in the LLOD cloud, and objectives leading to the adaptation of Linked Data principles for any of these. Further, the talk elaborates on history and goals behind this effort, its relation to established standardization initiatives in the field, and on-going community activities conducted under the umbrella of the Open Linguistics Working Group (OWLG) of the Open Knowledge Foundation (Chiarcos et al., 2012a), an initiative of experts from various fields concerned with linguistic data, which works towards", "venue": "SWAIE@RANLP", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "nlp", "llod", "nlp", "llod", "lod", "ld", "llod", "ld", "llod", "nlp", "llod", "lod"], "mention_counts": {"ld": 2, "nlp": 3, "nlg": 1, "lod": 2, "llod": 5}, "nlp_mention_counts": {"nlp": 3, "nlg": 1}, "ld_mention_counts": {"ld": 2, "llod": 5, "lod": 2}, "relevance_score": 0.608623419919635}, {"paperId": "09a7c04ad3b8a664db4242892683e24d2c3b3a3d", "url": "https://www.semanticscholar.org/paper/09a7c04ad3b8a664db4242892683e24d2c3b3a3d", "title": "From Research Articles to Knowledge Graphs", "abstract": "Understanding and extracting knowledge contained in text and encoding it as linked data for the WEB is a highly complex task that poses several challenges, requiring expertise from different fields such as conceptual modeling, natural language processing and web technologies including web mining, linked data generation and publishing, etc. When it comes to the scholarly domain, the transformation of human readable research articles into machine comprehensible knowledge bases is considered of high importance and necessity today due to the explosion of scientific publications in every major discipline, that makes it increasingly difficult for experts to maintain an overview of their domain or relate ideas from different domains. This situation could be significantly alleviated by knowledge bases capable of supporting queries such as: find all papers that address a given problem; how was the problem solved; which methods are employed by whom in addressing particular tasks; etc. that currently cannot be addressed by commonly used search engines such as Google Scholar or Semantic Scholar. This tutorial addresses the above challenge by introducing the participants to methods required in order to model knowledge regarding a given domain, extract information from available texts using advanced machine learning techniques, associate it with other information mined from the web in order to infer new knowledge and republish everything as linked open data on the Web. To this end, we will use a specific use case \u2013 that of the scholarly domain, and will show how to model research processes, extract them from research articles, associate them with contextual information from article metadata and other linked repositories and create knowledge bases available as linked data. Our aim is to show how methodologies from different computer science fields, namely natural language processing, machine learning and conceptual modeling, can be combined with Web technologies in a single meaningful workflow.", "venue": "WWW", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "kg", "kg", "kg", "ld", "nlp", "ld", "ie", "lod", "ld", "ke"], "mention_counts": {"ld": 3, "nlp": 2, "lod": 1, "ke": 1, "kg": 4, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1, "ie": 1}, "ld_mention_counts": {"ld": 3, "kg": 4, "lod": 1, "ke": 1}, "relevance_score": 0.608623419919635}, {"paperId": "23a94bd104f4da5c7f536cf4c92c685230a1fae4", "url": "https://www.semanticscholar.org/paper/23a94bd104f4da5c7f536cf4c92c685230a1fae4", "title": "ROXXI: Reviving witness dOcuments to eXplore eXtracted Information", "abstract": "In recent years, there has been considerable research on information extraction and constructing RDF knowledge bases. In general, the goal is to extract all relevant information from a corpus of documents, store it into an ontology, and answer future queries based only on the created knowledge base. Thus, the original documents become dispensable. On the one hand, an ontology is a convenient and non-redundant structured source of information, based on which specific queries can be answered efficiently. On the other hand, many users doubt the correctness of facts and ontology subgraphs presented to them as query results without proof. Instead, users often wish to verify the obtained facts or subgraphs by reading about them in context, i.e., in a document relating the facts and providing background information. In this demo, we present ROXXI, a system operating on top of an existing knowledge base and reviving the abandoned witness documents. In doing so, it goes the opposite way of information extraction approaches -- starting with ontological facts and tracing their way back to the documents they were extracted from. ROXXI offers interfaces for expert users (SPARQL) as well as for non-experts (ontology browser) and provides a ranked list of documents each associated with a content snippet highlighting the queried facts in context. At the demonstration site, we will show the advantages of this novel approach towards document retrieval and illustrate the benefits of reviving the documents that information extraction approaches neglect.", "venue": "Proceedings of the VLDB Endowment", "citationCount": 16, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "rdf", "ie", "ie", "kg", "kg", "onto", "onto", "onto", "ie", "kg", "ie", "onto"], "mention_counts": {"kg": 3, "onto": 5, "rdf": 1, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"kg": 3, "onto": 5, "rdf": 1}, "relevance_score": 0.608623419919635}, {"paperId": "90265c0a6c44aea84ddb0a4bdf0cba0ec640cb60", "url": "https://www.semanticscholar.org/paper/90265c0a6c44aea84ddb0a4bdf0cba0ec640cb60", "title": "Using Semantic Web Technology to Support NLG. Case Study: OWL finds RAGS", "abstract": "The semantic web is a general vision for supporting knowledge-based processing across the WWW and its successors. As such, semantic web technology has potential to support the exchange and processing of complex NLG data. This paper discusses one particular approach to data sharing and exchange that was developed for NLG - the RAGS framework. This was developed independently of the semantic web. RAGS was relatively complex and involved a number of idiosyncratic features. However, we present a rational reconstruction of RAGS in terms of semantic web concepts, which yields a relatively simple approach that can exploit semantic web technology directly. Given that RAGS was motivated by the concerns of the NLG community, it is perhaps remarkable that its aspirations seem to fit so well with semantic web technology.", "venue": "INLG", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "sw", "sw", "nlg", "sw", "kg", "sw", "sw", "sw", "sw", "nlg", "nlg", "onto"], "mention_counts": {"sw": 7, "nlg": 4, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"sw": 7, "onto": 1, "kg": 1}, "relevance_score": 0.608623419919635}, {"paperId": "a789dad4ea1ea14e44db93d1e8d181ba92a8f78c", "url": "https://www.semanticscholar.org/paper/a789dad4ea1ea14e44db93d1e8d181ba92a8f78c", "title": "Towards an OWL-based framework for extracting information from clinical texts", "abstract": "In this paper, we present our ongoing work towards an OWL-based framework for extracting a variety of information (including patient history) from clinical texts. Our framework integrates a well-known natural language processing (NLP) system by converting its ontology and output logical form interpretation into the Web Ontology Language (OWL). The OWL-based Semantic Query-Enhanced Web Rule Language (SQWRL) is then used as a platform for authoring Semantic Web-aware rules for extracting information of interest from the OWL knowledge based created from parsing a clinical report. We also describe our ongoing work on using this system for extracting a timeline-based patient medical record from the history of present illness section of clinical texts.", "venue": "ACM International Conference on Bioinformatics, Computational Biology and Biomedicine", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "ie", "onto", "kg", "sw", "onto", "nlp", "onto", "nlp", "onto"], "mention_counts": {"onto": 7, "nlp": 2, "sw": 1, "kg": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"kg": 1, "sw": 1, "onto": 7}, "relevance_score": 0.608623419919635}, {"paperId": "dccc0fc798935c891aa5d8ea1337e73c346acd8f", "url": "https://www.semanticscholar.org/paper/dccc0fc798935c891aa5d8ea1337e73c346acd8f", "title": "Modeling UIMA type system using web ontology language: towards interoperability among UIMA-based NLP tools", "abstract": "With the recent development and adoption of NLP framework architectures, NLP modules/tools developed independently in the research community can be adopted as integrated applications. Development of wrappers and interfaces required to adopt NLP modules/tools, however, still requires huge amount of efforts. In this paper, we focus on one NLP framework architecture, UIMA (Unstructured Information Management Architecture), which defines annotations as types described in a type system and can achieve direct interoperability if a common type system is used. We explore the use of ontology to model UIMA types and argue existing ontology development or reasoning tools can be utilized to understand types (we use types and annotations interchangeably) from existing NLP systems developed under UIMA, define equivalent annotations in different NLP systems, and apply the practice in the ontology community to draw agreements on the definition of common NLP types, thereby achieving better interoperability among NLP modules/tools.", "venue": "MIXHS '12", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "nlp", "nlp", "nlp", "nlp", "nlp", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 9, "onto": 4}, "nlp_mention_counts": {"nlp": 9}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.608623419919635}, {"paperId": "9ebac0d205739c3d0cfe4ff273f7f54dc99e89b2", "url": "https://www.semanticscholar.org/paper/9ebac0d205739c3d0cfe4ff273f7f54dc99e89b2", "title": "A REVIEW OF WORD SENSE DISAMBIGUATION METHOD", "abstract": "Background: Word Sense Disambiguation (WSD) is known to have a detrimental effect on the precision of information retrieval systems, where WSD is the ability to identify the meanings of words in context. There is a challenge in inference-correct-sensing on ambiguous words. Through many years of research, there have been various solutions to WSD that have been proposed; they have been divided into supervised and knowledge-based unsupervised. Objective: The first objective of this study was to explore the state-of-art of the WSD method with a hybrid method using ontology concepts. Then, with the findings, we may understand which tools are available to build WSD components. The second objective was to determine which method would be the best in giving good performance results of WSD, by analysing how the methods were used to answer specific WSD questions, their production, and how their performance was analysed. Methods: A review of the literature was conducted relating to the performance of WSD research, which used a comparison method of information retrieval analysis. The study compared the types of methods used in case, and examined methods for tools production, tools training, and analysis of performance. Results: In total 12 papers were found that satisfied all 3 inclusion criteria, and there was an anchor paper assigned to be referred. We chose the knowledge-based unsupervised approach because it has fewer word sets constraints than the supervised approaches which require training data. Concept-based ontology will help WSD in finding the semantic words concept with respect to another concept around it. Conclusion: Many methods was explored and compared to determine the most suitable way to build a WSD model based on semantics between words in query texts that can be related to the knowledge concept by using ontological knowledge presentation.", "venue": "Journal of Information Systems and Technology Management", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "wsd", "onto", "wsd", "wsd", "wsd", "wsd", "onto", "wsd", "kg", "kg", "wsd", "wsd", "wsd", "wsd", "wsd", "onto"], "mention_counts": {"kg": 2, "wsd": 12, "onto": 3}, "nlp_mention_counts": {"wsd": 12}, "ld_mention_counts": {"kg": 2, "onto": 3}, "relevance_score": 0.587352245731331}, {"paperId": "1e629c7d5933581e8aa71a6dc54b758dbee211a6", "url": "https://www.semanticscholar.org/paper/1e629c7d5933581e8aa71a6dc54b758dbee211a6", "title": "Standards & best practice for multilingual computational lexicons: ISLE MILE and more\u201d", "abstract": "ISLE (International Standards for Language Engineering) is a transatlantic standards oriented initiative under the Human Language Technology (HLT) programme within the EU-US International Research Co-operation. It is a continuation of the European EAGLES (Expert Advisory Group for Language Engineering Standards) initiative, carried out through a number of subsequent projects funded by the European Commission (EC) since 1993. Within the multilingual computational lexicons Working Group, ISLE aims at: extending EAGLES work on lexical semantics, necessary to establish inter-language links; designing and proposing standards for multilingual lexicons; developing a prototype tool to implement lexicon guidelines and standards; creating exemplary EAGLESconformant sample lexicons and tagging exemplary corpora for validation purposes; and developing standardised evaluation procedures for lexicons. After a short introduction on the ISLE proposal for standards, the MILE (Multilingual ISLE Lexical Entry), we wil l focus the discussion on short and medium term requirements with respect to standards for multilingual lexicons and content encoding, in particular industrial requirements. We will stress the importance of reaching consensus on (li nguistic and non-linguistic) \u201ccontent\u201d, in addition to agreement on formats and encoding issues, and wil l define further steps necessary to converge on common priorities. Semantic Web standards and the needs of content processing technologies will be also addressed. 1. Goals of the Panel ISLE International Standards for Language Engineering) is a transatlantic standards oriented initiative under the Human Language Technology (HLT) programme within the EU-US International Research Cooperation. It is a continuation of the long standing European EAGLES (Expert Advisory Group for Language Engineering Standards) initiative, carried out through a number of subsequent projects funded by the European Commission (EC) since 1993. Within the multilingual computational lexicons Working Group (CLWG), ISLE aims at: extending EAGLES work on lexical semantics, necessary to establi sh inter-language links; designing and proposing standards for multilingual lexicons; developing a prototype tool to implement lexicon guidelines and standards; creating exemplary EAGLES-conformant sample lexicons and tagging exemplary corpora for validation purposes; and developing standardised evaluation procedures for lexicons. The CLWG is committed to the consensual definition of a standardized infrastructure to develop multilingual resources for HLT applications, with particular attention to the needs of Machine Translation and Crosslingual Information Retrieval systems. The Panel wil l include, in addition to ISLE members, developers and users of multilingual systems and of content management systems, and researchers interested in multilingual and content encoding standards. After a short introduction on the ISLE proposal for the MILE (Multilingual ISLE Lexical Entry) a general http://www.ilc.pi.cnr.it/EAGLES96/isle/ISLE_Home_Page.htm. schema for the encoding of multilingual lexical information to be intended as a meta-entry, acting as a common representational layer for multilingual lexical resources -, we will focus the discussion on short and medium term requirements with respect to standards for multilingual lexicons and content encoding, in particular industrial requirements. We will stress the importance of reaching consensus on (linguistic and non-linguistic) \u201ccontent\u201d, in addition to agreement on formats and encoding issues, and wil l try to define further steps necessary to converge on common priorities. Semantic Web standards and the needs of content processing technologies will be also addressed. 2. A few Issues for the Panel If we break the global problem of multilingual content technologies into small more manageable pieces, Linguistic Resources (LR) are certainly one of these pieces. Which is the relevance and impact of the availabilit y of (good, deep, knowledge intensive) resources (lexicons, ontologies, corpora) for high-quality cross-lingual/multilingual systems? It is obvious that different technologies/applications \u2013 and different approaches within the same application need different information types: e.g. the needs of CLIR or content access systems are quite different from MT systems. Do we have examples of reall y \u2018good\u2019 bil ingual/multilingual lexicons, at least for some applications? Which are the priority information types for different multilingual content management systems? Are we able to establi sh clear lexical/linguistic/knowledge requirements for different application types, or even component technologies? And to define steps to gradually reach consensus? Which is the respective role of e.g. annotated corpora, monolingual lexicons (with different information types), bimultilingual lexicons, ontologies, knowledge bases, etc? Can we aim at basic, general purpose bil ingual/multilingual lexicons, to be tuned, adapted to different applications? A key strategic question also for the funding agencies is: for which type of resources to invest? With respect to short vs. medium term results? Is there the need for robust systems, able to acquire/tune lexical/linguistic knowledge, to accompany static basic resources? in particular, systems able to acquire multilingual lexical/linguistic information? Do we have good sources of bi-/multil ingual information (machine readable dictionaries, corpora, ...)? And reliable methods for acquisition? Do we have to rely on parallel corpora? Or it is more advisable to aim at the use of \u2018comparable corpora\u2019, accompanied by robust technologies for annotation (at different levels: morphosyntactic, syntactic/functional, semantic, ...), and by a shared set of text annotation schemata? What is the relation between lexical standards and text annotation standards? In particular when we speak about \u201ccontent\u201d interoperability, is the field \u2018mature\u2019 enough to converge around agreed standards? Or is the market compelli ng us toward operational standards? Is the field of multilingual lexical resources ready to tackle the challenges set by the Semantic Web development? Knowledge management is critical. Is it an achievable goal to arrive at some commonly agreed text annotation protocol also for the semantic/conceptual level (in order to be able to automatically establi sh links among different languages)? A last but criti cal question: if we had real-size lexicons plus conceptual systems with very fine-grained semantic/conceptual information, would there be systems (non ad-hoc toy systems) able to use them? It seems sometimes that there is a loop, or a vicious circle, between i) lack of suitable, large-size and knowledge intensive, resources (lexicons, ontologies, corpora, with many different types of syntactic, semantic, conceptual information encoded), and ii) systems\u2019 abil ity to use them effectively. Should we define a strategy of research and development within which the two paths are pursued in parallel, closely interact with each other, and be gradually integrated?", "venue": "LREC", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "mt", "hlt", "onto", "sw", "sw", "hlt", "onto", "onto", "sw"], "mention_counts": {"onto": 3, "sw": 3, "mt": 1, "kg": 1, "hlt": 2}, "nlp_mention_counts": {"hlt": 2, "mt": 1}, "ld_mention_counts": {"kg": 1, "sw": 3, "onto": 3}, "relevance_score": 0.5715444760934602}, {"paperId": "3ec17263a68d673854b67cd99abc110cb6e483f4", "url": "https://www.semanticscholar.org/paper/3ec17263a68d673854b67cd99abc110cb6e483f4", "title": "An Interoperable Platform for Multi-Grain Text Annotation", "abstract": "In this paper, we describe an interoperable platform for creating annotated corpora in different languages and domains. It focuses on two most widely used for practical information processing tasks levels of linguistic annotations, - morphological and conceptual, that can be performed separately or combined. The platform consists of two main modules, a program shell and a knowledge base. The program shell is universal and features flexible settings that ensure its adaptation to multilingual corpora of various domains and different levels of annotation. It is provided with several interfaces for knowledge acquisition and annotation control. The annotation platform knowledge base includes language-independent and language-dependent linguistic information. The language-independent information is presented by multilingual domain ontology, while the core of the language-dependent component of the platform knowledge base includes unilingual onto-lexicons. The annotation process consists in the practical realization of ontological analysis. In performing the annotation task, the NLP techniques are used to automatically support, rather than completely replace human judgment. The platform is multifunctional, and in addition to corpora annotation, it can directly be used for different types of theoretical linguistic research, e.g., terminology analysis, cross-linguistic comparative studies, etc. The paper covers both, the platform design and its application in the frame of a real project on the conceptual annotation of the \"Terrorism\" domain corpora in the Russian, English and French languages. which both the rule-based NLP technique and/or quantitative measures can be applied. The paper covers the platform general design and its application for the conceptual annotation of the \"Terrorism\" domain corpora in English Russian and French. The potential of the developed interoperable platform as a research tool to define quantitative metrics for tag disambiguation is also demonstrated on the example of the conceptual-level annotation. The suggested quantitative metrics account for a) the frequency of the concept usage in unilingual corpora annotations and b) the variety of the unilingual lexical units mapped into a multilingual ontological concept. The specificity of the approach is that a) the unit of the ontological analysis is taken to be a multicomponent phrase rather than a single word and b) tag disambiguation can supported by the rule-based NLP technology through the fully functional platform tagger interpreter and/or by quantitative measures.", "venue": "Intelligent Memory Systems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "kg", "onto", "kg", "nlp", "onto", "onto", "kg", "nlp"], "mention_counts": {"nlp": 3, "onto": 4, "kg": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 3, "onto": 4}, "relevance_score": 0.5715444760934602}, {"paperId": "43668495fa67a6d0f5a2b3c7914264d5ac6eb333", "url": "https://www.semanticscholar.org/paper/43668495fa67a6d0f5a2b3c7914264d5ac6eb333", "title": "Ontologies, Knowledge Representation, and Machine Learning for Translational Research: Recent Contributions", "abstract": "Summary Objectives : To select, present, and summarize the most relevant papers published in 2018 and 2019 in the field of Ontologies and Knowledge Representation, with a particular focus on the intersection between Ontologies and Machine Learning. Methods : A comprehensive review of the medical informatics literature was performed to select the most interesting papers published in 2018 and 2019 and that document the utility of ontologies for computational analysis, including machine learning. Results : Fifteen articles were selected for inclusion in this survey paper. The chosen articles belong to three major themes: (i) the identification of phenotypic abnormalities in electronic health record (EHR) data using the Human Phenotype Ontology ; (ii) word and node embedding algorithms to supplement natural language processing (NLP) of EHRs and other medical texts; and (iii) hybrid ontology and NLP-based approaches to extracting structured and unstructured components of EHRs. Conclusion : Unprecedented amounts of clinically relevant data are now available for clinical and research use. Machine learning is increasingly being applied to these data sources for predictive analytics, precision medicine, and differential diagnosis. Ontologies have become an essential component of software pipelines designed to extract, code, and analyze clinical information by machine learning algorithms. The intersection of machine learning and semantics is proving to be an innovative space in clinical research.", "venue": "Yearbook of medical informatics", "citationCount": 6, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlp", "onto", "onto", "onto", "onto", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 3, "onto": 7}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.5715444760934602}, {"paperId": "9520f78f645893b688ee42f6773ed649f7ed5dac", "url": "https://www.semanticscholar.org/paper/9520f78f645893b688ee42f6773ed649f7ed5dac", "title": "Learning to Refine an Automatically Extracted Knowledge Base Using Markov Logic", "abstract": "A number of text mining and information extraction projects such as Text Runner and NELL seek to automatically build knowledge bases from the rapidly growing amount of information on the web. In order to scale to the size of the web, these projects often employ ad hoc heuristics to reason about uncertain and contradictory information rather than reasoning jointly about all candidate facts. In this paper, we present a Markov logic-based system for cleaning an extracted knowledge base. This allows a scalable system such as NELL to take advantage of joint probabilistic inference, or, conversely, allows Markov logic to be applied to a web scale problem. Our system uses only the ontological constraints and confidence values of the original system, along with human-labeled data if available. The labeled data can be used to calibrate the confidence scores from the original system or learn the effectiveness of individual extraction patterns. To achieve scalability, we introduce a neighborhood grounding method that only instantiates the part of the network most relevant to the given query. This allows us to partition the knowledge cleaning task into tractable pieces that can be solved individually. In experiments on NELL's knowledge base, we evaluate several variants of our approach and find that they improve both F1 and area under the precision-recall curve.", "venue": "2012 IEEE 12th International Conference on Data Mining", "citationCount": 62, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "kg", "ke", "kg", "kg", "kg", "ke"], "mention_counts": {"kg": 4, "onto": 1, "ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"kg": 4, "onto": 1, "ke": 2}, "relevance_score": 0.5715444760934602}, {"paperId": "f0be0064681941cf2185c1f3043d36e44f96a4b3", "url": "https://www.semanticscholar.org/paper/f0be0064681941cf2185c1f3043d36e44f96a4b3", "title": "Extraction of temporal facts and events from Wikipedia", "abstract": "Recently, large-scale knowledge bases have been constructed by automatically extracting relational facts from text. Unfortunately, most of the current knowledge bases focus on static facts and ignore the temporal dimension. However, the vast majority of facts are evolving with time or are valid only during a particular time period. Thus, time is a significant dimension that should be included in knowledge bases.\n In this paper, we introduce a complete information extraction framework that harvests temporal facts and events from semi-structured data and free text of Wikipedia articles to create a temporal ontology. First, we extend a temporal data representation model by making it aware of events. Second, we develop an information extraction method which harvests temporal facts and events from Wikipedia infoboxes, categories, lists, and article titles in order to build a temporal knowledge base. Third, we show how the system can use its extracted knowledge for further growing the knowledge base.\n We demonstrate the effectiveness of our proposed methods through several experiments. We extracted more than one million temporal facts with precision over 90% for extraction from semi-structured data and almost 70% for extraction from text.", "venue": "TempWeb '12", "citationCount": 51, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "kg", "ke", "kg", "ie", "onto", "kg", "kg", "kg"], "mention_counts": {"kg": 5, "onto": 1, "ke": 1, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"kg": 5, "onto": 1, "ke": 1}, "relevance_score": 0.5715444760934602}, {"paperId": "fd800ff14570c08ef07a8a1de70a4542df9bde59", "url": "https://www.semanticscholar.org/paper/fd800ff14570c08ef07a8a1de70a4542df9bde59", "title": "An assessment on domain ontology-based information extraction techniques", "abstract": "Domain ontology is used in information retrieval to retrieve more relevant information from a collection of unstructured information source. Information retrieval is becoming an important research area in the field of computer science. Information retrieval (IR) is generally concerned with the searching and retrieving of knowledge-based information from database. In this paper, we represent the various models and techniques for information retrieval. Domain ontology is a description of domain concepts with relation and properties to be used in knowledge engineering as a knowledge base. In this paper, various domain ontology-based information retrieval methods have been reviewed. A comparative analysis is made on all the available methods, which will allow the analyst to choose the suitable domain ontology-based information extraction method. There are various methods developed to make the information extraction more efficient. The methods have been classified as Boolean, vector space, semantic-based techniques and probabilistic. Semantic-based information retrieval can still be classified as semantic association, semantic similarity and semantic annotation. This assessment allows the developer to choose the best fit model for their requirement in an efficient way.", "venue": "Int. J. Serv. Technol. Manag.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ie", "kg", "ie", "onto", "onto", "kg", "ie"], "mention_counts": {"kg": 2, "onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"kg": 2, "onto": 5}, "relevance_score": 0.5715444760934602}, {"paperId": "57d9e792392f4384a20f69f59a03c1418f7bed06", "url": "https://www.semanticscholar.org/paper/57d9e792392f4384a20f69f59a03c1418f7bed06", "title": "Towards Semi Automatic Construction of a Lexical Ontology for Persian", "abstract": "Lexical ontologies and semantic lexicons are important resources in natural language processing. They are used in various tasks and applications, especially where semantic processing is evolved such as question answering, machine translation, text understanding, information retrieval and extraction, content management, text summarization, knowledge acquisition and semantic search engines. Although there are a number of semantic lexicons for English and some other languages, Persian lacks such a complete resource to be used in NLP works. In this paper we introduce an ongoing project on developing a lexical ontology for Persian called FarsNet. We exploited a hybrid semi-automatic approach to acquire lexical and conceptual knowledge from resources such as WordNet, bilingual dictionaries, mono-lingual corpora and morpho-syntactic and semantic templates. FarsNet is an ontology whose elements are lexicalized in Persian. It provides links between various types of words (cross POS relations) and also between words and their corresponding concepts in other ontologies (cross ontologies relations). FarsNet aggregates the power of WordNet on nouns, the power of FrameNet on verbs and the wide range of conceptual relations from ontology community.", "venue": "LREC", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "nlp", "onto", "onto", "nlp", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 7, "mt": 1}, "nlp_mention_counts": {"nlp": 2, "mt": 1}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.5715444760934602}, {"paperId": "a4f1f350823a2b2b023fac5b8ec446544c0bc19c", "url": "https://www.semanticscholar.org/paper/a4f1f350823a2b2b023fac5b8ec446544c0bc19c", "title": "Development of an Ontology for Aerospace Engine Components Degradation in Service", "abstract": "This paper presents the development of an ontology for component service degradation. In this paper, degradation mechanisms in gas turbine metallic components are used for a case study to explain how a taxonomy within an ontology can be validated. The validation method used in this paper uses an iterative process and sanity checks. Data extracted from on-demand textual information are filtered and grouped into classes of degradation mechanisms. Various concepts are systematically and hierarchically arranged for use in the service maintenance ontology. The allocation of the mechanisms to the AS-IS ontology presents a robust data collection hub. Data integrity is guaranteed when the TO-BE ontology is introduced to analyse processes relative to various failure events. The initial evaluation reveals improvement in the performance of the TO-BE domain ontology based on iterations and updates with recognised mechanisms. The information extracted and collected is required to improve service knowledge and performance feedback which are important for service engineers. Existing research areas such as natural language processing, knowledge management, and information extraction were also examined.", "venue": "International Conference on Knowledge Engineering and Ontology Development", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "nlp", "onto", "onto", "ie", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 7, "ie": 2}, "nlp_mention_counts": {"nlp": 1, "ie": 2}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.5715444760934602}, {"paperId": "6f569660807624f94b185aef1622759c9d42e02d", "url": "https://www.semanticscholar.org/paper/6f569660807624f94b185aef1622759c9d42e02d", "title": "Semantic association rule mining in text using domain ontology", "abstract": "Online news websites are now valuable archives for both current and old news regarding various issues, particularly those that relate to the political and historical contexts of a country. These news platforms have become an important medium for all forms of political activities such as branding, campaigns, and communication. Online newspapers make large volume of textual data available, which are rich in political and historical inferences that can be leveraged for national development. In this paper we report a procedure for ontology-based association rule mining for knowledge extraction from text. Ordinarily, association rule mining algorithms have the limitations of generating many non-interesting rules, huge number of discovered rules, and low algorithm performance. This research demonstrates a procedure for improving the performance of association rule mining in text mining by using domain ontology. To do this, a study context of Nigerian politics based on information extracted from a Nigerian online newspaper was selected, and a methodology that combined natural language processing methods, ontology-based keywords extraction, and the modified Generating Association Rules based on Weighting scheme (GARW) was applied. The result obtained from the study revealed that compared to non-ontology based association rule mining approaches, our procedure provides significant rule reduction in the number of generated rules, and produced rules which are more semantically related to the problem context. The study validates the capability of domain ontology to improve the performance of association rule mining algorithms, particularly when dealing with unstructured textual data.", "venue": "Int. J. Metadata Semant. Ontologies", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "onto", "ie", "onto", "onto", "onto", "onto", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "onto": 6, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 6}, "relevance_score": 0.5715444760934602}, {"paperId": "ebda79a0277823de82d12ef577c3e0e654579d40", "url": "https://www.semanticscholar.org/paper/ebda79a0277823de82d12ef577c3e0e654579d40", "title": "Extraction, Merging, and Monitoring of Company Data from Heterogeneous Sources", "abstract": "We describe the implementation of an enterprise monitoring system that builds on an ontology-based information extraction (OBIE) component applied to heterogeneous data sources. The OBIE component consists of several IE modules - each extracting on a regular temporal basis a specific fraction of company data from a given data source - and a merging tool, which is used to aggregate all the extracted information about a company. The full set of information about companies, which is to be extracted and merged by the OBIE component, is given in the schema of a domain ontology, which is guiding the information extraction process. The monitoring system, in case it detects changes in the extracted and merged information on a company with respect to the actual state of the knowledge base of the underlying ontology, ensures the update of the population of the ontology. As we are using an ontology extended with temporal information, the system is able to assign time intervals to any of the object instances. Additionally, detected changes can be communicated to end-users, who can validate and possibly correct the resulting updates in the knowledge base.", "venue": "LREC", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "onto", "onto", "kg", "ie", "ie", "ie", "onto", "onto"], "mention_counts": {"kg": 2, "onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"kg": 2, "onto": 5}, "relevance_score": 0.5715444760934602}, {"paperId": "75bccfd7aa7e32b7d0717de21892802342f31ae7", "url": "https://www.semanticscholar.org/paper/75bccfd7aa7e32b7d0717de21892802342f31ae7", "title": "Building Domain Ontologies from Text Analysis: An Application for Question Answering", "abstract": "In the field of information extraction and automatic question answering access to a domain ontology may be of great help. But the main problem is building such an ontology, a difficult and time consuming task. We propose an approach in which the domain ontology is learned from the linguistic analysis of a number of texts which represent the domain itself. NLP analysis is done with GETARUNS system. GETARUNS can build a Discourse Model and is able to assign a relevance score to each entity. From Discourse Model we extract best candidates to become concepts in the domain ontology. To arrange concepts in the correct hierarchy we use WordNet taxonomy. Once the domain ontology is built we reconsider the texts to extract information. In this phase the entities recognized at discourse level are used to create instances of the concepts. The predicate-argument structure of the verb is used to construct instance slots for concepts. Eventually, the question answering task is performed by translating the natural language question in a suitable form and use that to query the Discourse Model enriched by the ontology.", "venue": "NLUCS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ie", "onto", "onto", "onto", "nlp", "onto", "ie"], "mention_counts": {"nlp": 1, "onto": 7, "ie": 2}, "nlp_mention_counts": {"nlp": 1, "ie": 2}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.5715444760934602}, {"paperId": "8eabda075e8966770d869f0738189609fa5087f9", "url": "https://www.semanticscholar.org/paper/8eabda075e8966770d869f0738189609fa5087f9", "title": "Ontology-based intelligent information extraction system on the semantic Web", "abstract": "This paper presents an intelligent information extraction system based on ontology on the semantic Web. It describes the kernel architecture of the system, and introduces the formal representation of ontology in detail. It also describes the ontology mapping and ontology evolution process. Lastly, the paper shows the structure of the intelligent information extraction system.", "venue": "Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "sw", "onto", "onto", "onto", "ie", "ie", "sw"], "mention_counts": {"sw": 2, "onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"sw": 2, "onto": 5}, "relevance_score": 0.5715444760934602}, {"paperId": "15f8656f3d217530a36e15fee256647c7ce43ae8", "url": "https://www.semanticscholar.org/paper/15f8656f3d217530a36e15fee256647c7ce43ae8", "title": "Using Markov Logic to Refine an Automatically Extracted Knowledge Base", "abstract": "A number of information extraction (IE) projects such as NELL and TextRunner seek to build a usable knowledge base from the rapidly growing amount of information on the web. However, these solutions use heuristic approaches to reasoning rather than sound probabilistic inference. In this paper, we present a method based on Markov logic for cleaning an automatically extracted knowledge base using only the confidence values and ontological constraints of the original system. Our approach works by reasoning jointly over all candidate facts. To achieve scalability, we introduce a neighborhood grounding method that only instantiates the part of the network most relevant to the given query. This allows us to partition the knowledge cleaning task into tractable pieces that can be solved individually. In experiments on NELL\u2019s knowledge base, our method improves both F1 and AUC.", "venue": "StarAI@UAI", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ke", "kg", "kg", "kg", "ke", "kg"], "mention_counts": {"ke": 2, "onto": 1, "kg": 4, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "onto": 1, "kg": 4}, "relevance_score": 0.5715444760934602}, {"paperId": "3ee438c9013da33c81bedb5400b832ab3a5bd9a1", "url": "https://www.semanticscholar.org/paper/3ee438c9013da33c81bedb5400b832ab3a5bd9a1", "title": "A System Based on Ontology and Case-Based Reasoning to Support Distributed Teams", "abstract": "The intrinsic nature of distributed software development (DSD) brings new challenges, such as communication issues and sharing information efficiently. Software companies have a tendency to face these challenges using individual and isolated approaches, making difficult to spread good practices for the DSD community. In other contexts, concepts and techniques from Artificial Intelligence (AI) are frequently used in order to improve the functioning of systems and process. This work is based on the following AI concepts: ontologies, case-based reasoning (CBR) and natural language processing (NLP). We propose a system, based on ontology and case-based reasoning, that operates as follows: i) we use a tool for ontology storage, access and processing; and ii) an ontology-based CBR tool which aims to aid software companies by recommending techniques and best practices for minimizing or solving potential challenges that may be faced by DSD processes. The main results from this research are: i) a specific ontology for distributed software development teams; ii) a tool to facilitate the access and manipulation of the proposed ontology; and iii) a case based reasoning system that utilizes natural language processing. Initial results of the performed experiments indicate a success rate of 91.7% in the recommendation of solutions for potential problems coming from DSD processes.", "venue": "2015 12th International Conference on Information Technology - New Generations", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "onto", "nlp", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 3, "onto": 7}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.5715444760934602}, {"paperId": "8c5413e0ede944a0c4f72f93289cede4b5d9f052", "url": "https://www.semanticscholar.org/paper/8c5413e0ede944a0c4f72f93289cede4b5d9f052", "title": "Ontology Engineering and Knowledge Extraction for Cross-Lingual Retrieval", "abstract": "In this paper, we show that by integrating existing NLP techniques and Semantic Web tools in a novel way, we can provide a valuable contribution to the solution of the knowledge acquisition bottleneck problem. NLP techniques to create a domain ontology on the basis of an open domain corpus have been combined with Semantic Web tools. More specifically, Watson and Prompt have been employed to enhance the kick-o ontology while Cornetto, a lexical database for Dutch, has been adopted to establish a link between the concepts and their Dutch lexicalization. The lexicalized ontology constitutes the basis for the cross-language retrieval of learning objects within the LT4eL eLearning project.", "venue": "RANLP", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "ke", "onto", "onto", "sw", "nlp", "sw"], "mention_counts": {"nlp": 2, "sw": 2, "onto": 4, "ke": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"ke": 1, "sw": 2, "onto": 4}, "relevance_score": 0.5715444760934602}, {"paperId": "dfdc5cd1cc1ab42a6032761a852df2290b2be870", "url": "https://www.semanticscholar.org/paper/dfdc5cd1cc1ab42a6032761a852df2290b2be870", "title": "Mining Scholarly Data for Fine-Grained Knowledge Graph Construction", "abstract": "Knowledge graphs (KG) are large network of entities and relationships, tipically expressed as RDF triples, relevant to a specific domain or an organization. Scientific Knowledge Graphs (SKGs) focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. The next big challenge in this field regards the generation of SKGs that also contain a explicit representation of the knowledge presented in research publications. In this paper, we present a preliminary approach that uses a set of NLP and Deep Learning methods for extracting entities and relationships from research publications and then integrates them in a KG. More specifically, we i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, ii) describe an approach for integrating entities and relationships generated by these tools, iii) analyse an automatically generated Knowledge Graph including 10, 425 entities and 25, 655 relationships derived from 12, 007 publications in the field of Semantic Web, and iv) discuss how Deep Learning methods can be applied to overcome some limitations of the current techniques.", "venue": "DL4KG@ESWC", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "nlp", "rdf", "kg", "nlp", "sw", "ke", "kg"], "mention_counts": {"nlp": 2, "sw": 1, "ke": 1, "kg": 4, "rdf": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"kg": 4, "sw": 1, "ke": 1, "rdf": 1}, "relevance_score": 0.5715444760934602}, {"paperId": "0ebe28f05195c532d8e39260b4c5034e6d2da3d9", "url": "https://www.semanticscholar.org/paper/0ebe28f05195c532d8e39260b4c5034e6d2da3d9", "title": "Pangloss: A Knowledge-based Machine Assisted Translation Research Project - Site", "abstract": "are developing a Translator's Workstation to assist a user in the translation of newspaper articles in the area of finance (mergers and acquisitions) in one language (Spanish initially) into a second language (English). At its core is a multilingual, knowledge-based, interlingual, interactive , machine-assisted translation system consisting of a source language analysis component, an interactive aug-mentor, and a target language generation component. In the initial phase, the CRL's objectives are to develop tools for constructing lexical items and ontological entries automatically from on-line resources, to develop the initial Spanish analysis component, and, jointly with CMT and ISI, to establish the infrastructure for the three site project, develop the formats and initial content of the interlingua, the ontology, and the knowledge base, and to prepare design documents for the second phase versions of the analysis and generation components, the augmentor, and the translator's workstation. With respect to developing tools for extracting information from on-line resources, during the first year we are focusing on providing general classificatory information for use in constructing the lexicons of the Spanish analysis and English generation components. Here we are building on work on automatically constructing interlin-gual word sense specifications from Longman's which we are extending and adapting to language particular lexical entries. With respect to the Spanish analysis component , the objective is to modify and extend the Spanish component of the CRL's multilingual machine translation system both in terms of coverage and robustness. At this point, the three sites have established the central infrastructure for the Pangloss project. For the first year system, the formats of the interlingua, the ontology, and the knowledge base have been set and initial design of the interlingua for the second phase version is underway. As a preliminary to the work on extracting information from on-line resources, we continue to gather resources in the form of monolingual and bilingual dictionaries and monolingual and bilingual corpora. We have obtained Collins English Dictionary, Collins Bilingual Spanish-English Dictionary and are looking into a Spanish mono-lingual dictionary and Japanese monolingual and bilingual dictionaries. We have identified a source for Span-ish texts in the financial domain and are seeking further sources as well as sources of Japanese texts. The central resource to date, however, is the Longman Dictionary of Contemporary English and the information being extracted relates to word formation, syntactic category, syntactic subcategorization, and semantic selection restrictions. Procedures are currently being developed to provide \u2026", "venue": "HLT", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "kg", "onto", "kg", "kg", "kg", "mt", "onto", "ie"], "mention_counts": {"kg": 4, "onto": 3, "mt": 1, "ie": 2}, "nlp_mention_counts": {"mt": 1, "ie": 2}, "ld_mention_counts": {"kg": 4, "onto": 3}, "relevance_score": 0.5715444760934602}, {"paperId": "31b74274e6020fc132d64c98c1f458dc19bc9695", "url": "https://www.semanticscholar.org/paper/31b74274e6020fc132d64c98c1f458dc19bc9695", "title": "Biological Nomenclatures: A Source of Lexical Knowledge and Ambiguity", "abstract": "There has been increased work in developing automated systems that involve natural language processing (NLP) to recognize and extract genomic information from the literature. Recognition and identification of biological entities is a critical step in this process. NLP systems generally rely on nomenclatures and ontological specifications as resources for determining the names of the entities, assigning semantic categories that are consistent with the corresponding ontology, and assignment of identifiers that map to well-defined entities within a particular nomenclature. Although nomenclatures and ontologies are valuable for text processing systems, they were developed to aid researchers and are heterogeneous in structure and semantics. A uniform resource that is automatically generated from diverse resources, and that is designed for NLP purposes would be a useful tool for the field, and would further database interoperability. This paper presents work towards this goal. We have automatically created lexical resources from four model organism nomenclature systems (mouse, fly, worm, and yeast), and have studied performance of the resources within an existing NLP system, GENIES. Using nomenclatures is not straightforward because issues concerning ambiguity, synonymy, and name variations are quite challenging. In this paper we focus mainly on ambiguity. We determined that the number of ambiguous gene names within the individual nomenclatures, across the four nomenclatures, and with general English ranged from 0%-10.18%, 1.187%-20.30%, and 0%-2.49% respectively. When actually processing text, we found the rate of ambiguous occurrences (not counting ambiguities stemming from English words) to range from 2.4%-32.9% depending on the organisms considered.", "venue": "Pacific Symposium on Biocomputing", "citationCount": 99, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["tp", "onto", "onto", "nlp", "nlp", "nlp", "onto", "nlp", "tp", "nlp"], "mention_counts": {"nlp": 5, "onto": 3, "tp": 2}, "nlp_mention_counts": {"nlp": 5, "tp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.5715444760934602}, {"paperId": "784b8506e1848f661651f865a07ec88051da216b", "url": "https://www.semanticscholar.org/paper/784b8506e1848f661651f865a07ec88051da216b", "title": "Content selection as semantic-based ontology exploration", "abstract": "Natural Language (NL) based access to information contained in Knowledge Bases (KBs) has been tackled by approaches following different paradigms. One strand of research deals with the task of ontology-based data access and data exploration (Franconi et al., 2010; Franconi et al., 2011). This type of approach relies on two pillar components. The first one is an ontology describing the underlying domain with a set of reasoning based query construction operations. This component guides the lay user in the formulation of a KB query by proposing alternatives for query expansion. The second is a Natural Language Generation (NLG) system to hide the details of the formal query language to the user. Our ultimate goal is the automatic creation of a corpus of KB queries for development and evaluation of NLG systems. The task we address is the following. Given an ontology K, automatically select from K descriptions q which yield sensible user queries. The difficulty lies in the fact that ontologies often omit important disjointness axioms and adequate domain or range restrictions (Rector et al., 2004; Poveda-Villal\u00f3n et al., 2012). For instance, the toy ontology shown in Figure 1 licences the meaningless query in (1). This happens because there is no disjointness axiom between the Song and Rectangular concepts and/or because the domain of the marriedTo relation is not restricted to persons.", "venue": "WebNLG", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "kg", "nlg", "onto", "onto", "onto", "nlg", "onto", "nlg", "onto"], "mention_counts": {"kg": 1, "nlg": 3, "onto": 6}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"kg": 1, "onto": 6}, "relevance_score": 0.5715444760934602}, {"paperId": "36ddc7f293de2b705cdc716505586e4b12d14a2b", "url": "https://www.semanticscholar.org/paper/36ddc7f293de2b705cdc716505586e4b12d14a2b", "title": "Mining ontological knowledge using Nyaya framework", "abstract": "Ontology has become the buzzword of the knowledge and semantics community. The process of automatically constructing an ontology with completeness and reduced time has become the need of the hour. This paper presents the method for automatically constructing an ontology for any domain based on the Indian philosophical system, the Nyaya Sastra. Nyaya defines the whole world from atom to universe. This categorisation provided by Nyaya acts as the framework for extracting ontological relations from documents obtained from the web. With Nyaya and Natural Language Processing (NLP) techniques, an improved and enriched knowledge can be obtained from web documents. This paper explains the way of constructing an ontology (i.e., extracting taxonomical and nontaxonomical relations) using Nyaya in detail. The extracted knowledge includes concepts, relations and qualities pertaining to a concept. A Semi-Supervised Learning (SSL) technique for learning Nyaya categories is also explained.", "venue": "Int. J. Netw. Virtual Organisations", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ke", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "ke": 1, "onto": 6}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 6}, "relevance_score": 0.5715444760934602}, {"paperId": "ed6df6df195c7d426fad4a5d15f82776ad59fd03", "url": "https://www.semanticscholar.org/paper/ed6df6df195c7d426fad4a5d15f82776ad59fd03", "title": "Using Natural Language Processing Techniques to Provide Personalized Educational Materials for Chronic Disease Patients in China: Development and Assessment of a Knowledge-Based Health Recommender System", "abstract": "Background Health education emerged as an important intervention for improving the awareness and self-management abilities of chronic disease patients. The development of information technologies has changed the form of patient educational materials from traditional paper materials to electronic materials. To date, the amount of patient educational materials on the internet is tremendous, with variable quality, which makes it hard to identify the most valuable materials by individuals lacking medical backgrounds. Objective The aim of this study was to develop a health recommender system to provide appropriate educational materials for chronic disease patients in China and evaluate the effect of this system. Methods A knowledge-based recommender system was implemented using ontology and several natural language processing (NLP) techniques. The development process was divided into 3 stages. In stage 1, an ontology was constructed to describe patient characteristics contained in the data. In stage 2, an algorithm was designed and implemented to generate recommendations based on the ontology. Patient data and educational materials were mapped to the ontology and converted into vectors of the same length, and then recommendations were generated according to similarity between these vectors. In stage 3, the ontology and algorithm were incorporated into an mHealth system for practical use. Keyword extraction algorithms and pretrained word embeddings were used to preprocess educational materials. Three strategies were proposed to improve the performance of keyword extraction. System evaluation was based on a manually assembled test collection for 50 patients and 100 educational documents. Recommendation performance was assessed using the macro precision of top-ranked documents and the overall mean average precision (MAP). Results The constructed ontology contained 40 classes, 31 object properties, 67 data properties, and 32 individuals. A total of 80 SWRL rules were defined to implement the semantic logic of mapping patient original data to the ontology vector space. The recommender system was implemented as a separate Web service connected with patients' smartphones. According to the evaluation results, our system can achieve a macro precision up to 0.970 for the top 1 recommendation and an overall MAP score up to 0.628. Conclusions This study demonstrated that a knowledge-based health recommender system has the potential to accurately recommend educational materials to chronic disease patients. Traditional NLP techniques combined with improvement strategies for specific language and domain proved to be effective for improving system performance. One direction for future work is to explore the effect of such systems from the perspective of patients in a practical setting.", "venue": "JMIR Medical Informatics", "citationCount": 13, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["kg", "onto", "kg", "onto", "onto", "nlp", "nlp", "kg", "onto", "onto", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 4, "kg": 3, "onto": 7}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 3, "onto": 7}, "relevance_score": 0.5676040851861229}, {"paperId": "494550ad73407effb29db14022066e69dc711920", "url": "https://www.semanticscholar.org/paper/494550ad73407effb29db14022066e69dc711920", "title": "A Flexible Conversational Dialog System for MP3 Player", "abstract": "In recent years, an increasing number of new devices have found their way into the cars we drive. Speech-operated devices in particular provide a great service to drivers by minimizing distraction, so that they can keep their hands on the wheel and their eyes on the road. This presentation will demonstrate our latest development of an in-car dialog system for an MP3 player designed under a joint research effort from Bosch RTC, VW ERL, Stanford CSLI, and SRI STAR Lab funded by NIST ATP [Weng et al 2004] with this goal in mind. This project has developed a number of new technologies, some of which are already incorporated in the system. These include: end-pointing with prosodic cues, error identification and recovering strategies, flexible multi-threaded, multi-device dialog management, and content optimization and organization strategies. A number of important language phenomena are also covered in the system's functionality. For instance, one may use words relying on context, such as 'this,' 'that,' 'it,' and 'them,' to reference items mentioned in particular use contexts. Different types of verbal revision are also permitted by the system, providing a great convenience to its users. The system supports multi-threaded dialogs so that users can diverge to a different topic before the current one is finished and still come back to the first after the second topic is done. To lower the cognitive load on the drivers, the content optimization component organizes any information given to users based on ontological structures, and may also refine users' queries via various strategies. Domain knowledge is represented using OWL, a web ontology language recommended by W3C, which should greatly facilitate its portability to new domains.The spoken dialog system consists of a number of components (see Fig. 1 for details). Instead of the hub architecture employed by Communicator projects [Senef et al, 1998], it is developed in Java and uses a flexible event-based, message-oriented middleware. This allows for dynamic registration of new components. Among the component modules in Figure 1, we use the Nuance speech recognition engine with class-based ngrams and dynamic grammars, and the Nuance Vocalizer as the TTS engine. The Speech Enhancer removes noises and echo. The Prosody module will provide additional features to the Natural Language Understanding (NLU) and Dialogue Manager (DM) modules to improve their performance.The NLU module takes a sequence of recognized words and tags, performs a deep linguistic analysis with probabilistic models, and produces an XML-based semantic feature structure representation. Parallel to the deep analysis, a topic classifier assigns top n topics to the utterance, which are used in the cases where the dialog manager cannot make any sense of the parsed structure. The NLU module also supports dynamic updates of the knowledge base.The CSLI DM module mediates and manages interaction. It uses the dialogue-move approach to maintain dialogue context, which is then used to interpret incoming utterances (including fragments and revisions), resolve NPs, construct salient responses, track issues, etc. Dialogue states can also be used to bias SR expectation and improve SR performance, as has been performed in previous applications of the DM. Detailed descriptions of the DM can be found in [Lemon et al 2002; Mirkovic & Cavedon 2005].The Knowledge Manager (KM) controls access to knowledge base sources (such as domain knowledge and device information) and their updates. Domain knowledge is structured according to domain-dependent ontologies. The current KM makes use of OWL, a W3C standard, to represent the ontological relationships between domain entities. Protege (http://protege.stanford.edu), a domain-independent ontology tool, is used to maintain the ontology offline. In a typical interaction, the DM converts a user's query into a semantic frame (i.e. a set of semantic constraints) and sends this to the KM via the content optimizer.The Content Optimization module acts as an intermediary between the dialogue management module and the knowledge management module during the query process. It receives semantic frames from the DM, resolves possible ambiguities, and queries the KM. Depending on the items in the query result as well as the configurable properties, the module selects and performs an appropriate optimization strategy.Early evaluation shows that the system has a task completion rate of 80% on 11 tasks of MP3 player domain, ranging from playing requests to music database queries. Porting to a restaurant selection domain is currently under way.", "venue": "HLT", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "onto", "onto", "onto", "onto", "nlu", "nlu", "nlu", "onto", "nlu", "onto", "onto", "onto"], "mention_counts": {"kg": 2, "onto": 8, "nlu": 4}, "nlp_mention_counts": {"nlu": 4}, "ld_mention_counts": {"kg": 2, "onto": 8}, "relevance_score": 0.5676040851861229}, {"paperId": "6e818c3ccacb749214066c497ffe40ab948f1460", "url": "https://www.semanticscholar.org/paper/6e818c3ccacb749214066c497ffe40ab948f1460", "title": "Frequent Itemset Mining and Multi-Layer Network-Based Analysis of RDF Databases", "abstract": "Triplestores or resource description framework (RDF) stores are purpose-built databases used to organise, store and share data with context. Knowledge extraction from a large amount of interconnected data requires effective tools and methods to address the complexity and the underlying structure of semantic information. We propose a method that generates an interpretable multilayered network from an RDF database. The method utilises frequent itemset mining (FIM) of the subjects, predicates and the objects of the RDF data, and automatically extracts informative subsets of the database for the analysis. The results are used to form layers in an analysable multidimensional network. The methodology enables a consistent, transparent, multi-aspect-oriented knowledge extraction from the linked dataset. To demonstrate the usability and effectiveness of the methodology, we analyse how the science of sustainability and climate change are structured using the Microsoft Academic Knowledge Graph. In the case study, the FIM forms networks of disciplines to reveal the significant interdisciplinary science communities in sustainability and climate change. The constructed multilayer network then enables an analysis of the significant disciplines and interdisciplinary scientific areas. To demonstrate the proposed knowledge extraction process, we search for interdisciplinary science communities and then measure and rank their multidisciplinary effects. The analysis identifies discipline similarities, pinpointing the similarity between atmospheric science and meteorology as well as between geomorphology and oceanography. The results confirm that frequent itemset mining provides an informative sampled subsets of RDF databases which can be simultaneously analysed as layers of a multilayer network.", "venue": "Mathematics", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "rdf", "rdf", "rdf", "rdf", "rdf", "ie", "ke", "rdf", "ke"], "mention_counts": {"kg": 1, "ke": 3, "rdf": 6, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 3, "rdf": 6}, "relevance_score": 0.5676040851861229}, {"paperId": "7acf4700a44f34fae7dea9aa9bc1dc6bf5b0f02e", "url": "https://www.semanticscholar.org/paper/7acf4700a44f34fae7dea9aa9bc1dc6bf5b0f02e", "title": "Information Extraction as an Ontology Population Task and Its Application to Genic Interactions", "abstract": "Ontologies are a well-motivated formal representation to model knowledge needed to extract and encode data from text. Yet, their tight integration with Information Extraction (IE) systems is still a research issue, a fortiori with complex ones that go beyond hierarchies. In this paper, we introduce an original architecture where IE is specified by designing an ontology, and the extraction process is seen as an Ontology Population (OP) task. Concepts and relations of the ontology define a normalized text representation. As their abstraction level is irrelevant for text extraction, we introduced a Lexical Layer (LL) along with the ontology, i.e. relations and classes at an intermediate level of normalization between raw text and concepts. On the contrary to previous IE systems, the extraction process only involves normalizing the outputs of Natural Language Processing (NLP) modules with instances of the ontology and the LL. All the remaining reasoning is left to a query module, which uses the inference rules of the ontology to derive new instances by deduction. In this context, these inference rules subsume classical extraction rules or patterns by providing access to appropriate abstraction level and domain knowledge. To acquire those rules, we adopt an Ontology Learning (OL) perspective, and automatically acquire the inference rules with relational Machine Learning (ML). Our approach is validated on a genic interaction extraction task from a Bacillus subtilis bacterium text corpus. We reach a global recall of 89.3% and a precision of 89.6%, with high scores for the ten conceptual relations in the ontology.", "venue": "IEEE International Conference on Tools with Artificial Intelligence", "citationCount": 16, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "ie", "ie", "onto", "onto", "onto", "onto", "nlp", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 10, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"onto": 10}, "relevance_score": 0.5676040851861229}, {"paperId": "36af51f13b2c1c24f13c6b468a7113054f8c4327", "url": "https://www.semanticscholar.org/paper/36af51f13b2c1c24f13c6b468a7113054f8c4327", "title": "Ontology-Based Information Extraction for Knowledge Enrichment and Validation", "abstract": "Ontology is widely used as a mean to represent and share common concepts and knowledge from a particular domain or specialisation. As a knowledge representation, the knowledge within an ontology must be able to evolve along with the recent changes and updates within the community practice. In this paper, we propose a new Ontology-based Information Extraction (OBIE) system that extends existing systems in order to enrich and validate an ontology. Our model enables the ontology to find related recent knowledge in the domain from communities, by exploiting their underlying knowledge as keywords. The knowledge extraction process uses ontology-based and pattern-based information extraction technique. Not only the extracted knowledge enriches the ontology, it also validates contradictory instance-related statements within the ontology that is no longer relevant to recent practices. We determine a confidence value during the enrichment and validation process to ensure the stability of the enriched ontology. We implement the model and present a case study in herbal medicine domain. The result of the enrichment and validation process shows promising results. Moreover, we analyse how our proposed model contributes to the achievement of a richer and stable ontology.", "venue": "International Conference on Advanced Information Networking and Applications", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "onto", "onto", "onto", "onto", "onto", "onto", "ke", "onto", "onto", "onto", "ie", "ie", "ie", "onto"], "mention_counts": {"ke": 2, "onto": 11, "ie": 3}, "nlp_mention_counts": {"ke": 2, "ie": 3}, "ld_mention_counts": {"ke": 2, "onto": 11}, "relevance_score": 0.5550494160031108}, {"paperId": "0e5f4ce4c3ce2a208a5353825b1fd7223b89a50e", "url": "https://www.semanticscholar.org/paper/0e5f4ce4c3ce2a208a5353825b1fd7223b89a50e", "title": "ConTrOn: Continuously Trained Ontology based on Technical Data Sheets and Wikidata", "abstract": "In engineering projects involving various parts from global suppliers, one common task is to determine which parts are best suited for the project requirements. Information about specific parts' characteristics is published in so called data sheets. However, these data sheets are oftentimes only published in textual form, e.g., as a PDF. Hence, they have to be transformed into a machine-interpretable format. This transformation process still requires a lot of manual intervention and is prone to errors. Automated approaches make use of ontologies to capture the given domain and thus improve automated information extraction from the data sheets. However, ontologies rely solely on experiences and perspectives of their creators at the time of creation and cannot accumulate knowledge over time on their own. This paper presents ConTrOn -- Continuously Trained Ontology -- a system that automatically augments ontologies. ConTrOn tackles terminology problems by combining the knowledge extracted from data sheets with an ontology created by domain experts and external knowledge bases such as WordNet and Wikidata. To demonstrate how the enriched ontology can improve the information extraction process, we selected data sheets from spacecraft development as a use case. The evaluation results show that the amount of information extracted from data sheets based on ontologies is significantly increased after the ontology enrichment.", "venue": "ArXiv", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "ie", "onto", "ie", "ke", "onto", "ie", "onto", "kg", "onto", "onto"], "mention_counts": {"ke": 1, "onto": 9, "kg": 1, "ie": 3}, "nlp_mention_counts": {"ke": 1, "ie": 3}, "ld_mention_counts": {"ke": 1, "onto": 9, "kg": 1}, "relevance_score": 0.5311625932181889}, {"paperId": "1dcd743a7f370585d0b1faa6c790cd6a205b0e95", "url": "https://www.semanticscholar.org/paper/1dcd743a7f370585d0b1faa6c790cd6a205b0e95", "title": "NaturalOWL : Generating Texts from OWL Ontologies in Protege and in Second Life", "abstract": "NaturalOWL is an open-source natural language generation engine written in Java. It produces descriptions of individuals (e.g., items for sale, museum exhibits) and classes (e.g., types of exhibits) in English and Greek from OWL DL ontologies. The ontologies must have been annotated in RDF with linguistic and user modeling resources. We demonstrate a plug-in for Prot\u00e9g\u00e9 that can be used to produce these resources and to generate texts by invoking NaturalOWL. We also demonstrate how NaturalOWL can be used by robotic avatars in Second Life to describe the exhibits of virtual museums. NaturalOWL demonstrates the benefits of Natural Language Generation (NLG) on the Semantic Web. Organizations that need to publish information about objects, such as exhibits or products, can publish OWL ontologies instead of texts. NLG engines, embedded in browsers or Web servers, can then render the ontologies in multiple natural languages, whereas computer programs may access the ontologies directly.", "venue": "European Conference on Artificial Intelligence", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "sw", "onto", "onto", "nlg", "onto", "nlg", "onto", "onto", "nlg", "onto", "nlg", "rdf", "onto", "onto"], "mention_counts": {"sw": 1, "nlg": 4, "onto": 9, "rdf": 1}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"sw": 1, "onto": 9, "rdf": 1}, "relevance_score": 0.5311625932181889}, {"paperId": "3248121914962cf63116ed91cacede86dbb7cc94", "url": "https://www.semanticscholar.org/paper/3248121914962cf63116ed91cacede86dbb7cc94", "title": "Neural language models for the multilingual, transcultural, and multimodal Semantic Web", "abstract": "A vision of a truly multilingual Semantic Web has found strong support with the Linguistic Linked Open Data community. Standards, such as OntoLex-Lemon, highlight the importance of explicit linguistic modeling in relation to ontologies and knowledge graphs. Nevertheless, there is room for improvement in terms of automation, usability, and interoperability. Neural Language Models have achieved several breakthroughs and successes considerably beyond Natural Language Processing (NLP) tasks and recently also in terms of multimodal representations. Several paths naturally open up to port these successes to the Semantic Web, from automatically translating linguistic information associated with structured knowledge resources to multimodal question-answering with machine translation. Language is also an important vehicle for culture, an aspect that deserves considerably more attention. Building on existing approaches, this article envisions joint forces between Neural Language Models and Semantic Web technologies for multilingual, transcultural, and multimodal information access and presents open challenges and opportunities in this direction.", "venue": "Semantic Web", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "lod", "sw", "llod", "sw", "nlp", "sw", "mt", "sw", "nlp"], "mention_counts": {"onto": 1, "sw": 4, "lod": 1, "llod": 1, "nlp": 2, "mt": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2, "mt": 1}, "ld_mention_counts": {"onto": 1, "sw": 4, "lod": 1, "llod": 1, "kg": 1}, "relevance_score": 0.5294660559538056}, {"paperId": "875df92d3cd292eacb8e851cba945a2ead0846b3", "url": "https://www.semanticscholar.org/paper/875df92d3cd292eacb8e851cba945a2ead0846b3", "title": "Intelligent Learning for Knowledge Graph towards Geological Data", "abstract": "Knowledge graph (KG) as a popular semantic network has been widely used. It provides an effective way to describe semantic entities and their relationships by extending ontology in the entity level. This article focuses on the application of KG in the traditional geological field and proposes a novel method to construct KG. On the basis of natural language processing (NLP) and data mining (DM) algorithms, we analyze those key technologies for designing a KG towards geological data, including geological knowledge extraction and semantic association. Through this typical geological ontology extracting on a large number of geological documents and open linked data, the semantic interconnection is achieved, KG framework for geological data is designed, application system of KG towards geological data is constructed, and dynamic updating of the geological information is completed accordingly. Specifically, unsupervised intelligent learning method using linked open data is incorporated into the geological document preprocessing, which generates a geological domain vocabulary ultimately. Furthermore, some application cases in the KG system are provided to show the effectiveness and efficiency of our proposed intelligent learning approach for KG.", "venue": "Scientific Programming", "citationCount": 22, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "lod", "nlp", "ke", "kg", "nlp", "onto", "ld", "lod"], "mention_counts": {"onto": 2, "ld": 1, "nlp": 2, "lod": 2, "ke": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"onto": 2, "ld": 1, "ke": 1, "lod": 2, "kg": 2}, "relevance_score": 0.5294660559538056}, {"paperId": "c823521300431fc55649c2f290259d91b303afa1", "url": "https://www.semanticscholar.org/paper/c823521300431fc55649c2f290259d91b303afa1", "title": "Arabic-English Automatic Ontology Mapping Based on Machine Readable Dictionary", "abstract": "Ontologies are the backbone of the semantic web and allow software agents to interoperate effectively. An ontology is able to represent and to clarify concepts and inter-concept relationships and can be used as a framework to represent underlying domain concepts expressed in many different languages. One way to do this is by mapping Ontologies in different languages using an inter-lingual index. In this paper we present a new methodology for ontology mapping in different script human languages (Arabic/English). We identify the steps of extracting concepts on both ontologies and automatically mapping them based on Machine Readable Dictionary (MRD) and Word Sense Disambiguation (WSD) tools. The paper also discusses a unique tool that automatically extracts unmapped concepts and uses MRD and WSD to match them and create semantic bridges between the ontologies.", "venue": "2009 Mexican International Conference on Computer Science", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "onto", "onto", "wsd", "wsd", "onto", "wsd", "onto", "onto", "onto", "onto"], "mention_counts": {"sw": 1, "wsd": 3, "onto": 7}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"sw": 1, "onto": 7}, "relevance_score": 0.5294660559538056}, {"paperId": "03b144c391ab21049f9bcd7900e192cfe582ff29", "url": "https://www.semanticscholar.org/paper/03b144c391ab21049f9bcd7900e192cfe582ff29", "title": "Ontological Approach Based on Multi-Agent System for Indexing and Filtering Arabic Documents", "abstract": "In recent years, Automatic Natural Language Processing (ANLP) for Arabic language has received a great amount of attention for the development of several applications such as question answering, information retrieval and translation, etc. However, there are a few automated applications using Semantic Web technologies for retrieving Arabic-language documents despite the high demand and need for this content. In addition, the Arabic language presents serious challenges to researchers and developers of NLP applications. These challenges are due to the complexity of the morphological, syntactic and semantic characteristics specific to the Arabic text, which requires the use of semantic resources such as ontology. In our work, we propose a new approach based on ontology and multi-agent systems to index and filter Arabic documents. Our proposal is composed of five layers, each layer contains several agents: (1) Lexical Layer; (2) Syntactic Layer; (3) Semantic Layer; (4) Indexing Layer; and GUI/Interface Layer. Our Arabic ontology is manually constructed on the basis of schemes and their semantics meanings. We use also combination of Arabic WordNet contents and Arabic VerbNet in the process of constructing the ontology. We use the semantic similarity to find the relevant documents according to the user\u2019s queries. The aim of this paper is to study the effect of patterns in solving the problem of the semantic indexing system (SIS). The main objective is to improve the quality of the indexing process to ensure the accuracy of the information search of relevant documents based on us ers\u2019 multiword queries, and also to reduce indexing and search time. Indeed, our experiments are conducted on the basis of the combination of two Arab corpus: OSAC and SemEval. We compared our results Ontological Approach Based on Multi-Agent System for Indexing and Filtering Arabic Documents with Lucene index for the same data and, we found that our approach achieves much better results than the other. Subject Categories and Descriptors: [H.3.1 Content Analysis and Indexing]; [H.3.3 Information Search and Retrieval] ; [I.2.11 Distributed Artificial Intelligence]; Multiagent systems General Terms: Information Retrieval, Natural Language Processing, Arabic Ontology", "venue": "J. Digit. Inf. Manag.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "onto", "onto", "onto", "onto", "nlp", "sw", "onto"], "mention_counts": {"nlp": 3, "sw": 1, "onto": 7}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 1, "onto": 7}, "relevance_score": 0.5294660559538056}, {"paperId": "9e7a8b246757b9032e173f082ad616dfd64b655a", "url": "https://www.semanticscholar.org/paper/9e7a8b246757b9032e173f082ad616dfd64b655a", "title": "Revitalizing the Global Public Health Intelligence Network (GPHIN)", "abstract": "Objective:\u00a0 To rebuild the software that underpins the Global Public Health Intelligence Network using modern natural language processing techniques to support recent and future improvements in situational awareness capability. Introduction:\u00a0 The Global Public Health Intelligence Network is a non-traditional all-hazards multilingual surveillance system introduced in 1997 by the Government of Canada in collaboration with the World Health Organization. 1 \u00a0GPHIN software collects news articles, media releases, and incident reports and analyzes them for information about communicable diseases, natural disasters, product recalls, radiological events and other public health crises. Since 2016, the Public Health Agency of Canada (PHAC) and National Research Council Canada (NRC) have collaborated to replace GPHIN with a modular platform that incorporates modern natural language processing techniques to support more ambitious situational awareness goals. Methods:\u00a0 The updated GPHIN platform assembles several natural language processing tools to annotate incoming data in order to support situational awareness; broadly, GPHIN aims to extract knowledge from data. Data are collected in 10 languages and are machine translated to English. Several of the machine translation models use high performance neural networks. Language models are updated regularly and support external dictionaries for handling emerging domain-specific terms that might not yet exist in the parallel corpora used to train the models. All incoming documents are assigned a relevance score. Machine learning models estimate a score based on similarity to sets of known high-relevance and known low-relevance documents. PHAC analysts provide feedback on the scoring from time to time in the course of their work, and this feedback is used to periodically retrain scoring models. Documents are assigned keywords using two ontologies: an all-hazards multilingual taxonomy hand-compiled at PHAC, and the U.S. National Library of Medicine\u2019s Unified Medical Language System (UMLS). Categories are assigned probabilistically to incoming articles (e.g., human infectious diseases, animal infectious diseases, substance abuse, environmental hazards), largely using affinity scores that correspond to keywords. Dates and times are resolved to canonical forms, so that mentions like\u00a0 last Tuesday \u00a0get resolved to specific dates; this makes it possible to sequence data about a single event that are released at varying frequencies and with varying timeliness. Cities, states/provinces, and countries are identified in the documents, and gaps in the hierarchical geographic relationships are filled in. Locations are disambiguated based on collocations; the system distinguishes between and correctly resolves Ottawa, KS vs. Ottawa, ON, Canada, for example. Countries are displayed with their socio-economic population statistics (Gini coefficient, human development index, median age, and so on). The system attempts to detect and reconcile near-duplicate articles in order to handle instances where one article is released on a newswire and subsequently gets lightly edited and syndicated in dozens or hundreds of local papers; this improves the signal-to-noise ratio of the data in GPHIN for better productivity. Template-based reports (where the same document may get re-issued with a new number of cases but no other changes, for example) are still a challenge, but whitelisting tools reduce the false positive rate. The system provides tools for constructing arbitrarily detailed searches, tied to colour-coded maps and graphs that update on-the-fly, and offers short extractive summaries of each search result for easy filtering. GPHIN also generates topical knowledge graphs about sets of articles that seek to reveal surprising correlations in the data; for example, graphically reconciling and highlighting cases where several neighbouring countries all have reports of a mysterious disease and where a particular mosquito is mentioned. Next steps in the ongoing rejuvenation involve collating discrete articles and documents into narrative timelines that track an ongoing event: collecting all data about the spread of an infectious disease outbreak or perhaps the aftermath of an earthquake in the developing world. Our research is focussing on how to build line lists from such a stream of news articles about an event and how to detect important change points in the ongoing narrative. Results:\u00a0 The new GPHIN platform was launched in August 2016 in order to support syndromic surveillance activities for the Rio 2016 Olympics, and has been updated incrementally since then to offer further capabilities to professional users in 30 countries. Its modular construction supports current situational awareness activities as well as further research into advanced natural language processing techniques. Conclusions:\u00a0 We improved (and continue to improve) GPHIN with modern natural language processing techniques, including better translations, relevance scoring, categorization, near-duplicate detection, and improved data visualization tools, all towards the goal of more productive and more trustworthy situational awareness.", "venue": "Online Journal of Public Health Informatics", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "mt", "nlp", "nlp", "nlp", "nlp", "kg", "onto", "ke", "nlp"], "mention_counts": {"onto": 1, "ke": 1, "nlp": 5, "mt": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 5, "ke": 1, "mt": 2}, "ld_mention_counts": {"kg": 1, "onto": 1, "ke": 1}, "relevance_score": 0.5294660559538056}, {"paperId": "f65fb80bd2e193a07f19e36567c103ff9d61d356", "url": "https://www.semanticscholar.org/paper/f65fb80bd2e193a07f19e36567c103ff9d61d356", "title": "Natural Language Processing as a Foundation of the Semantic Web", "abstract": "The main argument of this paper is that Natural Language Processing (NLP) does, and will continue to, underlie the Semantic Web (SW), including its initial construction from unstructured sources like the World Wide Web (WWW), whether its advocates realise this or not. Chiefly, we argue, such NLP activity is the only way up to a defensible notion of meaning at conceptual levels (in the original SW diagram) based on lower level empirical computations over usage. Our aim is definitely not to claim logic-bad, NLP-good in any simple-minded way, but to argue that the SW will be a fascinating interaction of these two methodologies, again like the WWW (which has been basically a field for statistical NLP research) but with deeper content. Only NLP technologies (and chiefly information extraction) will be able to provide the requisite RDF knowledge stores for the SW from existing unstructured text databases in the WWW, and in the vast quantities needed. There is no alternative at this point, since a wholly or mostly hand-crafted SW is also unthinkable, as is a SW built from scratch and without reference to the WWW. We also assume that, whatever the limitations on current SW representational power we have drawn attention to here, the SW will continue to grow in a distributed manner so as to serve the needs of scientists, even if it is not perfect. The WWW has already shown how an imperfect artefact can become indispensable.", "venue": "Foundations and Trends\u00ae in Web Science", "citationCount": 43, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "nlp", "sw", "sw", "nlp", "nlp", "nlp", "ie", "rdf"], "mention_counts": {"nlp": 7, "sw": 2, "rdf": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 7, "ie": 1}, "ld_mention_counts": {"sw": 2, "rdf": 1}, "relevance_score": 0.5294660559538056}, {"paperId": "262efeac080867e71a4ba9d9fcddfd1bdd365988", "url": "https://www.semanticscholar.org/paper/262efeac080867e71a4ba9d9fcddfd1bdd365988", "title": "Data mining for building knowledge bases: techniques, architectures and applications", "abstract": "Abstract Data mining techniques for extracting knowledge from text have been applied extensively to applications including question answering, document summarisation, event extraction and trend monitoring. However, current methods have mainly been tested on small-scale customised data sets for specific purposes. The availability of large volumes of data and high-velocity data streams (such as social media feeds) motivates the need to automatically extract knowledge from such data sources and to generalise existing approaches to more practical applications. Recently, several architectures have been proposed for what we call knowledge mining: integrating data mining for knowledge extraction from unstructured text (possibly making use of a knowledge base), and at the same time, consistently incorporating this new information into the knowledge base. After describing a number of existing knowledge mining systems, we review the state-of-the-art literature on both current text mining methods (emphasising stream mining) and techniques for the construction and maintenance of knowledge bases. In particular, we focus on mining entities and relations from unstructured text data sources, entity disambiguation, entity linking and question answering. We conclude by highlighting general trends in knowledge mining research and identifying problems that require further research to enable more extensive use of knowledge bases.", "venue": "Knowledge engineering review (Print)", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "kg", "kg", "ke", "kg", "kg", "ke"], "mention_counts": {"ke": 3, "kg": 5}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 5}, "relevance_score": 0.5294660559538056}, {"paperId": "ee59fd54380a309e31cfe595d0205398305e25a5", "url": "https://www.semanticscholar.org/paper/ee59fd54380a309e31cfe595d0205398305e25a5", "title": "Combining RDF Graph Data and Embedding Models for an Augmented Knowledge Graph", "abstract": "Vector embedding models have recently become popular for encoding both structured and unstructured data. In the context of knowledge graphs such models often serve as additional evidence supporting various tasks related to the knowledge base population: e.g., information extraction or link prediction to expand the original dataset. However, the embedding models themselves are often not used directly alongside structured data: they merely serve as additional evidence for structured knowledge extraction. In the metaphactory knowledge graph management platform, we use federated hybrid SPARQL queries for combining explicit information stated in the graph, implicit information from the associated embedding models, and information extracted using vector embeddings in a transparent way for the end user. In this paper we show how we integrated RDF data with vector space models to construct an augmented knowledge graph to be used in customer applications.", "venue": "The Web Conference", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "kg", "rdf", "kg", "ie", "rdf", "kg", "kg", "kg", "ke"], "mention_counts": {"kg": 5, "ke": 1, "rdf": 2, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"kg": 5, "ke": 1, "rdf": 2}, "relevance_score": 0.5294660559538056}, {"paperId": "bd478853a2bf7a7cd51264a70c8bccfd4b22b8bc", "url": "https://www.semanticscholar.org/paper/bd478853a2bf7a7cd51264a70c8bccfd4b22b8bc", "title": "Partnering enhanced-NLP with semantic analysis in support of information extraction", "abstract": "Information extraction using Natural Language Processing (NLP) tools focuses on extracting explicitly stated information from textual material. This includes Named Entity Recognition (NER), which produces entities and some of the relationships that may exist among them. Intelligent analysis requires examining the entities in the context of the entire document. While some of the relationships among the recognized entities may be preserved during extraction, the overall context of a document may not be preserved. In order to perform intelligent analysis on the extracted information, we provide an ontology, which describes the domain of the extracted information, in addition to rules that govern the classification and interpretation of added elements. The ontology is at the core of an interactive system that assists analysts with the collection, extraction, organization, analysis and retrieval of information, with the topic of \"terrorism financing\" as a case study. User interaction provides valuable assistance in assigning meaning to extracted information. The system is designed as a set of tools to provide the user with the flexibility and power to ensure accurate inference. This case study demonstrates the information extraction features as well as the inference power that is supported by the ontology.", "venue": "ODiSE'10", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "ie", "onto", "nlp", "ie", "ie", "ie", "onto", "ie", "nlp"], "mention_counts": {"nlp": 3, "onto": 3, "ie": 5}, "nlp_mention_counts": {"nlp": 3, "ie": 5}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.5294660559538056}, {"paperId": "e63c2574d3f76312117b2be5e01cda4c25ab2ce3", "url": "https://www.semanticscholar.org/paper/e63c2574d3f76312117b2be5e01cda4c25ab2ce3", "title": "Extracting Knowledge from the Bible: A Comparison between the Old and the New Testament", "abstract": "The objective of this work is to present a comparison between the Old Testament and the New Testament in terms of knowledge extraction and ontology learning. It is a knows fact that these two books have many differences in term of size, practice of worship, prophecy and there is also a difference in the time period when they were written. By applying the ontology learning and knowledge extraction methods we were interested do see if these differences are revealed and what are the similarities among them. Ontology learning can be applied for the semantic analysis of text, in order to extract concepts, relations, which can be further used for automated summaries or critical comparison. Such activities are important in education as they can allow dynamic creation of content or analyses that can be further used in the educational process. Since ontology-learning methods require large corpus of unstructured data, we have chosen the Bible as source for the text. In this way, the new developed methods are validated, and they can be used successfully in other educational domains. The Bible is the religious text of Christians and Jews. The Bible contains a collection of scriptures that was written by many authors, at different time and locations. Computationally, the Bible contains semi-structured information due to its organized structure of scriptures and numbered chapters. We have used Text2Onto as the main tool in order to obtain the most relevant concepts from the New Testament and then from The Old Testament. After that we analyze the most relevant concepts and the range of similarity for each domain identified in the New Testament and in The Old Testament. We can mention that there are no studies reported in the literature using ontology extraction for this religious domain. Those methods can be employed for automatic generation of content that can be further used in the educational process.", "venue": "2019 International Conference on Automation, Computational and Technology Management (ICACTM)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "ke", "onto", "onto", "onto", "ke", "onto"], "mention_counts": {"ke": 3, "onto": 5}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 5}, "relevance_score": 0.5294660559538056}, {"paperId": "7210ae1cc8b14cc78c7665c47cb27ab933d36279", "url": "https://www.semanticscholar.org/paper/7210ae1cc8b14cc78c7665c47cb27ab933d36279", "title": "Knowledge Extraction of Cohort Characteristics in Research Publications", "abstract": "When healthcare providers review the results of a clinical trial study to understand its applicability to their practice, they typically analyze how well the characteristics of the study cohort correspond to those of the patients they see. We have previously created a study cohort ontology to standardize this information and make it accessible for knowledge-based decision support. The extraction of this information from research publications is challenging, however, given the wide variance in reporting cohort characteristics in a tabular representation. To address this issue, we have developed an ontology-enabled knowledge extraction pipeline for automatically constructing knowledge graphs from the cohort characteristics found in PDF-formatted research papers. We evaluated our approach using a training and test set of 41 research publications and found an overall accuracy of 83.3% in correctly assembling the knowledge graphs. Our research provides a promising approach for extracting knowledge more broadly from tabular information in research publications.", "venue": "AMIA", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "kg", "onto", "ke", "kg", "ke", "kg", "onto"], "mention_counts": {"ke": 3, "onto": 2, "kg": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 2, "kg": 3}, "relevance_score": 0.5294660559538056}, {"paperId": "80fe5612a705bf7a65012ba30fe9e52fbc364a91", "url": "https://www.semanticscholar.org/paper/80fe5612a705bf7a65012ba30fe9e52fbc364a91", "title": "Knowledge Extraction Guided by Ontologies - Database Marketing Application", "abstract": "The knowledge extraction in large databases has being known as a long term and interactive project. In spite of such complexity and different options for the knowledge achievement, there is a research opportunity that could be explored, throughout the ontologies support. Then this support may be used for knowledge sharing and reuse. This paper describes a research of an ontological approach for leveraging semantic content of ontologies to improve knowledge extraction in a oil company marketing databases. We attain to analyze how ontologies and knowledge discovery process may interoperate and present our efforts to propose a possible framework for a formal integration.", "venue": "International Conference on Enterprise Information Systems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "onto", "onto", "onto", "ke", "ke", "onto"], "mention_counts": {"ke": 3, "onto": 5}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 5}, "relevance_score": 0.5294660559538056}, {"paperId": "0e650e3df99fc8e8c129b81bfe51f58a98c31a6a", "url": "https://www.semanticscholar.org/paper/0e650e3df99fc8e8c129b81bfe51f58a98c31a6a", "title": "RDF Knowledge Graph Visualization From a Knowledge Extraction System", "abstract": "In this paper, we present a system to visualize RDF knowledge graphs. These graphs are obtained from a knowledge extraction system designed by GEOLSemantics. This extraction is performed using natural language processing and trigger detection. The user can visualize subgraphs by selecting some ontology features like concepts or individuals. The system is also multilingual, with the use of the annotated ontology in English, French, Arabic and Chinese.", "venue": "SumPre-HSWI@ESWC", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "kg", "kg", "rdf", "nlp", "ke", "rdf", "onto", "ke"], "mention_counts": {"onto": 2, "nlp": 1, "ke": 2, "kg": 2, "rdf": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"kg": 2, "onto": 2, "ke": 2, "rdf": 2}, "relevance_score": 0.5294660559538056}, {"paperId": "2a3f4add59abbd1d640cbd9cefd03f6c835c2746", "url": "https://www.semanticscholar.org/paper/2a3f4add59abbd1d640cbd9cefd03f6c835c2746", "title": "Natural Language Generation in the context of the Semantic Web", "abstract": "Natural Language Generation (NLG) is concerned with transforming given content input into a natural language out- put, given some communicative goal. Although this input can take various forms and representations, it is the semantic/conceptual representations that have always been considered as the \"natural\" starting ground for NLG. Therefore, it is natural that the Se- mantic Web (SW), with its machine-processable representation of information with explicitly defined semantics, has attracted the interest of NLG practitioners from early on. We attempt to provide an overview of the main paradigms of NLG from SW data, emphasizing how the Semantic Web provides opportunities for the NLG community to improve their state-of-the-art ap- proaches whilst bringing about challenges that need to be addressed before we can speak of a real symbiosis between NLG and the Semantic Web.", "venue": "Semantic Web", "citationCount": 69, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "nlg", "sw", "nlg", "nlg", "nlg", "nlg", "nlg", "nlg", "nlg"], "mention_counts": {"sw": 3, "nlg": 8}, "nlp_mention_counts": {"nlg": 8}, "ld_mention_counts": {"sw": 3}, "relevance_score": 0.5294660559538056}, {"paperId": "e010061f99bcdc008b93f1feaab4690c4cf0bc66", "url": "https://www.semanticscholar.org/paper/e010061f99bcdc008b93f1feaab4690c4cf0bc66", "title": "Extracting Structured Knowledge for Semantic Web by Mining Wikipedia", "abstract": "Since Wikipedia has become a huge scale database storing wide-range of human knowledge, it is a promising corpus for knowledge extraction. A considerable number of researches on Wikipedia mining have been conducted and the fact that Wikipedia is an invaluable corpus has been confirmed. Wikipedia's impressive characteristics are not limited to the scale, but also include the dense link structure, URI for word sense disambiguation, well structured Infoboxes, and the category tree. One of the popular approaches in Wikipedia Mining is to use Wikipedia's category tree as an ontology and a number of researchers proved that Wikipedia's categories are promising resources for ontology construction by showing significant results. In this work, we try to prove the capability of Wikipedia as a corpus for knowledge extraction and how it works in the Semantic Web environment. We show two achievements; Wikipedia Thesaurus, a huge scale association thesaurus by mining the Wikipedia's link structure, and Wikipedia Ontology, a Web ontology extracted by mining Wikipedia articles.", "venue": "International Workshop on the Semantic Web", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "sw", "onto", "onto", "onto", "wsd", "ke", "ke", "sw"], "mention_counts": {"sw": 2, "wsd": 1, "onto": 4, "ke": 2}, "nlp_mention_counts": {"ke": 2, "wsd": 1}, "ld_mention_counts": {"sw": 2, "onto": 4, "ke": 2}, "relevance_score": 0.5294660559538056}, {"paperId": "19cb6142ba2e2cc252c6440928c39a7057beb79b", "url": "https://www.semanticscholar.org/paper/19cb6142ba2e2cc252c6440928c39a7057beb79b", "title": "Causal knowledge extraction by natural language processing in material science: a case study in chemical vapor deposition", "abstract": "Scientific publications written in natural language still play a central role as our knowledge source. However, due to the flood of publications, the literature survey process has become a highly time-consuming and tangled process, especially for novices of the discipline. Therefore, tools supporting the literature-survey process may help the individual scientist to explore new useful domains. Natural language processing (NLP) is expected as one of the promising techniques to retrieve, abstract, and extract knowledge. In this contribution, NLP is firstly applied to the literature of chemical vapor deposition (CVD), which is a sub-discipline of materials science and is a complex and interdisciplinary field of research involving chemists, physicists, engineers, and materials scientists. Causal knowledge extraction from the literature is demonstrated using NLP.", "venue": "Data Sci. J.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "nlp", "nlp", "ke", "ke", "nlp", "nlp"], "mention_counts": {"nlp": 5, "ke": 3}, "nlp_mention_counts": {"nlp": 5, "ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.5294660559538056}, {"paperId": "7e5c9df1cfb8730295c80cbd8c94cba64baa7930", "url": "https://www.semanticscholar.org/paper/7e5c9df1cfb8730295c80cbd8c94cba64baa7930", "title": "Constituent vs Dependency Parsing-Based RDF Model Generation from Dengue Patients' Case Sheets", "abstract": "Electronic Health Record (EHR) systems in healthcare organisations are primarily maintained in isolation from each other that makes interoperability of unstructured(text) data stored in these EHR systems challenging in the healthcare domain. Similar information may be described using different terminologies by different applications that can be evaded by transforming the content into the Resource Description Framework (RDF) model that is interoperable amongst organisations. RDF requires a document\u2019s contents to be translated into a repository of triplets (subject, predicate, object) known as RDF statements. Natural Language Processing (NLP) techniques can help get actionable insights from these text data and create triplets for RDF model generation. This paper discusses two NLP-based approaches to generate the RDF models from unstructured patients\u2019 documents, namely dependency structure-based and constituent(phrase) structure-based parser. Models generated by both approaches are evaluated in two aspects: exhaustiveness of the represented knowledge and the model generation time. The precision measure is used to compute the models\u2019 exhaustiveness in terms of the number of facts that are transformed into RDF representations.", "venue": "J. Inf. Knowl. Manag.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "rdf", "nlp", "rdf", "rdf", "rdf", "nlp", "rdf", "nlp", "rdf", "rdf"], "mention_counts": {"nlp": 3, "rdf": 8}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"rdf": 8}, "relevance_score": 0.5294660559538056}, {"paperId": "7294b5fa7f85a565f51ea675e5cc55be3e395bf4", "url": "https://www.semanticscholar.org/paper/7294b5fa7f85a565f51ea675e5cc55be3e395bf4", "title": "Using a Natural Language Understanding System to Generate Semantic Web Content", "abstract": "We describe our research on automatically generating rich semantic annotations of text and making it available on the Semantic Web. In particular, we discuss the challenges involved in adapting the OntoSem natural language processing system for this purpose. OntoSem, an implementation of the theory of ontological semantics under continuous development for over fifteen years, uses a specia lly constructed NLP-oriented ontology and an ontologicalsemantic lexicon to translate English text into a custom ontology-motivated knowledge representation language, the language of text meaning representations (TMRs). OntoSem concentrates on a variety of ambiguity resolution tasks as well as processing unexpected input and reference. To adapt OntoSem\u2019s representation to the Semantic Web, we developed a translation system, OntoSem2OWL, between the TMR language into the Semantic Web language OWL. We next used OntoSem and OntoSem2OWL to support SemNews, an e xperimental web service that monitors RSS news sources, processes the summaries of the news stories and publishes a structured representation of the meaning of the text in the news story.", "venue": "International Journal on Semantic Web and Information Systems (IJSWIS)", "citationCount": 37, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "sw", "nlu", "nlp", "sw", "sw", "sw", "onto", "onto"], "mention_counts": {"nlp": 2, "sw": 4, "onto": 4, "nlu": 1}, "nlp_mention_counts": {"nlp": 2, "nlu": 1}, "ld_mention_counts": {"sw": 4, "onto": 4}, "relevance_score": 0.5294660559538056}, {"paperId": "8cf13b1f64b6f97af9466fe078390e543480bb10", "url": "https://www.semanticscholar.org/paper/8cf13b1f64b6f97af9466fe078390e543480bb10", "title": "Practical World Modeling for NLP Applications", "abstract": "Practical NLP applications requiring semantic and pragmatic analysis of texts necessitate the construction of a world model, an ontology, to support interpretation of text elements. Constraints on world model elements serve as heuristics on the cooccurrence of lexical and other meanings in the text, facilitating both natural language understanding and generation. Propositional meanings (defined in the lexicon in terms of links to the world model) trickle down to the text meaning representation as instances of world model entities. Our primary objective in world modeling is to support multilingual applications, so constructing a language-independent ontology is crucial. The word sense view of ontology building leads to proliferation of concepts whenever words in different languages do not \"line-up\" (see EDR 1990), while using a core set of \"primitives\" is limited for large-scale applications, if shades of meaning are to be captured. In our environment, concept acquisition is guided by examining cross-linguistic evidence and representational trade-offs. In other large-scale ontology projects, the separation of lexical from conceptual knowledge is not always clear, as in the Cyc project at MCC, a knowledge base containing millions of facts about the world (Lenat and Guha, 1990), or the KT system (Dahlgren, 1988), which classifies commonsense knowledge for English words. In the DIONYSUS project at CMU, the world model, the lexicon and the text meaning representation are closely interconnected, in terms of their content and format. World modeling is supported by the ONTOS system, which consists of a) a constraint language, b) an ontology, or set of general concepts, c) a set of domain models and d) an intelligent knowledge acquisition interface. The basic features of the ONTOS constraint language are as follows (see Carlson & Nirenburg, 1990, for details). A world model is a collection of frames. A frame is a named set of slots, interpreted as an ontological concept (voluntary-olfactory-event, geopolitical-entity). A slot represents an ontological property (temperature, caused-by) and consists of a named set of facets. A facet is a named set of fillers. Facets refer to the status of property values, e.g.: value actual values of property (e.g., for concept instances) default typical value of a property sem set of\"legal\" values; akin to selectional restrictions", "venue": "ANLP", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "kg", "onto", "onto", "onto", "onto", "onto", "onto", "nlu"], "mention_counts": {"nlp": 2, "onto": 7, "kg": 1, "nlu": 1}, "nlp_mention_counts": {"nlp": 2, "nlu": 1}, "ld_mention_counts": {"kg": 1, "onto": 7}, "relevance_score": 0.5294660559538056}, {"paperId": "79d23fab9c14796a0364f7e6eba451dc4e294ae6", "url": "https://www.semanticscholar.org/paper/79d23fab9c14796a0364f7e6eba451dc4e294ae6", "title": "OntoSem: an Ontology Semantic Representation Methodology for Biomedical Domain", "abstract": "Ontologies are essential description tools for biomedical concepts and entities, supporting biomedical fundamental research such as semantic similarity analysis, protein-protein interaction prediction and so on. An increasing amount of ontology-like domain knowledge is published in scientific publications, meanwhile, advanced natural language processing (NLP) techniques have been widespread to extract information from text resources automatically, both of which facilitate the exploration of the semantic representation of biomedical ontologies. We propose a novel distributional semantic representation methodology based on the combination of two pre-trained and domain-specific word embedding tools, the non-contextualized Word2Vec and the context-dependent NCBI-blueBERT, to enhance the encoding ability for biomedical ontologies. Furthermore, we utilize a randomly initialized bidirectional LSTM to project the obtained word vector sequence to a fixed-length sentence vector, facilitating a flexible and uniform way for the computation of downstream tasks. We evaluate our method in two categories of tasks: the similarity access of ontology terms, and the ontology annotationbased protein-protein interaction classification. Experimental results demonstrate that our method provides encouraging results compared to the baselines in all tests. Our approach offers promising opportunities for representing ontologies semantics and in turn characterizing entities including proteins in biomedical research.", "venue": "2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "ie", "onto", "onto", "onto", "onto", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 8, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"onto": 8}, "relevance_score": 0.5294660559538056}, {"paperId": "dc3931e1c9bdfd17d19d9a68b09cc3ed2aca7ee8", "url": "https://www.semanticscholar.org/paper/dc3931e1c9bdfd17d19d9a68b09cc3ed2aca7ee8", "title": "Semantic web and language resources for e-government: linguistically motivated data mining", "abstract": "Language Technologies (LT) perform well when they rely on a previous Language Resources (LR) development. Hence, in this paper we illustrate how to build an efficient data mining system based on a coherent formalization of natural language and on a lingware (in Machine-Readable Form) built on the universal concepts of \"lexical unit\", \"meaning unit\" and \"morphosyntactic context\". From a linguistic and semantic point of view we get more coherent results through a linguistically motivated data mining than a statistical approach. We developed LR for Natural Language Processing (NLP) applications, composed by electronic dictionaries made of terminological multiword-expressions (Machine-Readable Form) and by local grammars (in the form of finite-state automata and transducers -- FSA/FST). Both parts of this lingware were built and applied according to Lexicon-Grammar (LG) formalization principles and methods. The mentioned language resources form the basis to develop an Information Retrieval System for e-Government. This device is a Semantic Web (SW) application, which will automatically recognize a given set of frequently-asked questions (from here on, FAQs) on European Community Information, previously formalized as syntactic patterns inside local grammars.", "venue": "WIMS '11", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "sw", "sw"], "mention_counts": {"nlp": 2, "sw": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 2}, "relevance_score": 0.5}, {"paperId": "b993521c31b431ec68002e1481f3a93c9a17b676", "url": "https://www.semanticscholar.org/paper/b993521c31b431ec68002e1481f3a93c9a17b676", "title": "Learning Information Extraction Rules: An Inductive Logic Programming approach", "abstract": "The objective of this work is to learn information extraction rules by applying Inductive Logic Programming (ILP) techniques to natural language data. The approach is ontology-based, which means that the extraction rules conclude with specific ontology relations that characterise the meaning of sentences in the text. An existing ILP system, FOIL, is used to learn attribute-value relations. This enables instances of these relations to be identified in the text. In specific, we explore the linguistic preprocessing of the data, the use of background knowledge in the learning process, and the practical considerations of applying a supervised learning approach to rule induction, i.e. in terms of the human effort in creating the data set, and in the inherent biases in the use of small data sets.", "venue": "European Conference on Artificial Intelligence", "citationCount": 83, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "bfbfdca12ab6ac6b01e5dc0537b7e62f6531a6bc", "url": "https://www.semanticscholar.org/paper/bfbfdca12ab6ac6b01e5dc0537b7e62f6531a6bc", "title": "Knowledge management and human language: crossing the chasm", "abstract": "Purpose \u2013 Seeks to explore the gap that exists between knowledge management (KM) systems and the natural language materials that form almost all corporate data stores.Design/methodology/approach \u2013 A conceptual discussion and approach are taken using recent scientific results in the fields of the semantic web and ontology\u2010based information extraction.Findings \u2013 Provides a high\u2010level introduction to information extraction (IE) and descriptions of application scenarios for KM tools that exploit IE, a form of natural language analysis to link semantic web models with documents. The paper presents some examples of ontology\u2010based IE systems, one of which, KIM, is under development in the SEKT Project. KIM offers IE\u2010based facilities for metadata creation, storage and conceptual search. The system can be used by diverse applications for annotating and querying documents.Originality/value \u2013 Focuses on technologies and facilities that will become an important part of next\u2010generation KM applications.", "venue": "Journal of Knowledge Management", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "sw", "sw"], "mention_counts": {"sw": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"sw": 2}, "relevance_score": 0.5}, {"paperId": "372b804da190b94b566b0e6ae9bc768014e652d3", "url": "https://www.semanticscholar.org/paper/372b804da190b94b566b0e6ae9bc768014e652d3", "title": "Multilingual Content Processing", "abstract": "This contribution describes the consequences of a multilingual set-up, as used in internet information gathering, search and content processing, for the architecture and the different components of such a system. First the scenario is briefly outlined; then the existing technology components are reviewed, with the focus of the effects of multilingual content to such components. Finally, the linguistic resources are discussed which form the backbone of such a multifunctional and multilingual system. It is shown that adding multilinguality to an information system has massive consequences for the design of all system components and resources. 1 The Crosslingual Scenario Information Acquisition has become a major challenge in the internet age. The amount of information to be monitored has grown significantly, but time and resources to perform this task are still constrained. Tools like personalised electronic news clipping, automatic knowledge mining, or internet monitoring try to match the new requirements. As a result, a scenario needs to be designed which requires massive natural language support to meet its goals: \u2022 automatic classification of incoming texts is used as a first-level filter, to get rid of irrelevant material \u2022 key word and index term extraction allows for a finer-graded determination of the topic of a document. Links to some classification or ontology scheme take the analysis beyond pure text search into the field of knowledge mining and the semantic web. \u2022 Named entity recognition and fact extraction take care of the fact that people do not search strings but semantically meaningful objects, i.e. content. Profiles of interest look for content, not just string comparison. Such information systems (developed e.g. in the ECSENSUS project and described in (Thurmair, 2004)) face an additional challenge now, as multilinguality is a major obstacle for efficient information access. In globalised contexts, relevant information is available in a variety of languages, many of them not spoken by the information recipient. Queries, interest profile definitions etc. are stated in the interface language (i.e. the users\u2019 native language) but the documents and the information sources to be looked up are in different languages, the document languages. One could claim now that the extension consists only of adding two additional translation steps, one on the query side, and one on the answering side (to re-translate hits e.g. in Korean into the interface languages, or somehow make the content available for further use), and perhaps a language identification tool to determine the document language. However, this is not quite the case; indeed there are massive consequences on the single components and technologies used, if multilingual aspects should be considered. This refers to the design of such components themselves (section 2 below) as well as the design of the linguistic resources required (section 3). It is known that some technologies are restricted to, or give better results in, specialised domains. This fact corresponds to a difference in the targeted markets, i.e. there is a difference between the corporate market (large internationalised companies), and the general internet market. While the former can often be constrained to specific applications (like: automotive, banking), and therefore produce higher quality in linguistic support tools, the internet applications must cope with any request. Experience in Machine Translation but also Information Extraction shows that results can be better, and systems are more likely to be accepted by users, if they can be tailored to a given application or context; general purpose tools are much more difficult to design. 2 Challenges in crosslingual Scenarios A closer look into the scenario just described shows that the requirement of multilinguality influences the architecture and the design of all components of an information system; this section discusses some of them. 2.1 Techniques for Query Analysis and Expansion This section discusses the query part of a crosslingual scenario, and design adaptations on the search components. Definition of Search terms The standard practice of analysing a search request is to remove the stop words, stem the content words, create some link between them (e.g. Boolean operators) and send the query to the search backend. This is not simply feasible in a crosslingual environment. \u2022 conventional techniques to increase recall (like truncation or stemming) are not applicable because the resulting strings often cannot be translated: If integration is stemmed to integrthis is a nontranslatable string. Therefore they cannot be searched in the document languages, and so they decrease the overall. system quality. \u2022 It is known that most terms in special languages are multiword terms. Translating the single words very often leads to useless document language queries, and to completely irrelevant search results: Searching for en nuclear power plants (de Atomkraftwerk) fails if nuclear (->Kern-), power (->Macht) and plant (-> Pflanze) are translated in isolation.", "venue": "LREC", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "onto", "ie", "sw"], "mention_counts": {"sw": 1, "onto": 1, "mt": 1, "ie": 1}, "nlp_mention_counts": {"mt": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "12536e2f80abb355380ae74ed93512f309a3a051", "url": "https://www.semanticscholar.org/paper/12536e2f80abb355380ae74ed93512f309a3a051", "title": "Using Part-of-Speech Tagging for Building Networks of Terms in Legal Sphere", "abstract": "This paper considers an important formalization problem and building the terminological ontology of problem subject domains based on content-related text data. As an ontological model, we propose to use a linguistic network model of text representation, the so-called network of key terms. In this network, the nodes are keywords and phrases that appear in the text corpus, and the links between them are semantic-syntactic links between these terms in the text. Using systems of aggregation of thematic information flows from freely available information resources distributed in global computer networks, input sets of text data were prepared. In particular, this paper solves the important and urgent problem of computerized processing of legal information. The task of computerized processing of natural language texts lies at the intersection between linguistic theory and mathematical sciences. Therefore, a wider natural language processing based on Part-of-Speech tagging was used for extraction of the key terms. After the extraction, a statistical weighing of the formed words and phrases was performed. The horizontal visibility graph algorithm was used to build undirected links between key terms. This paper also considers a new method that allows determining the direction of links between terms and weighting these links in the undirected network of words and phrases. This method takes into account the parts of speech tagging and also obeys the principle of inclusion of a word or phrase in their corresponding extended phrases with more words. The approbation of the proposed method was carried out on the example of a freely available legal document \u00abUniversal Declaration of Human Rights\u00bb. After extracting the key terms from this legal document and determining the direction and weight of links between words or phrases using the proposed methods the directed weighted network of terms was built. The considered in this work method for building the terminological networks can be used, in particular, in systems for automatic text structuring and summarizing of legal information, or systems for detecting the duplicates and contradictions in normative legal documents. It will promote the formation and improvement of conceptual and terminological apparatus in the legal sphere and harmonize national and international law.", "venue": "COLINS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "aef67255ae73eb929fb7d40d25b06d38ee9d9b2c", "url": "https://www.semanticscholar.org/paper/aef67255ae73eb929fb7d40d25b06d38ee9d9b2c", "title": "An Ontology Model for Nursing Narratives with Natural Language Generation Technology", "abstract": "The purpose of this study was to develop an ontology model to generate nursing narratives as natural as human language from the entity-attribute-value triplets of a detailed clinical model using natural language generation technology. The model was based on the types of information and documentation time of the information along the nursing process. The typesof information are data characterizing the patient status, inferences made by the nurse from the patient data, and nursing actions selected by the nurse to change the patient status. This information was linked to the nursing process based on the time of documentation. We describe a case study illustrating the application of this model in an acute-care setting. The proposed model provides a strategy for designing an electronic nursing record system.", "venue": "Medinfo", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlg", "nlg", "onto"], "mention_counts": {"nlg": 2, "onto": 2}, "nlp_mention_counts": {"nlg": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "9a417545cb1b43080b20982b42ab1099708ea56b", "url": "https://www.semanticscholar.org/paper/9a417545cb1b43080b20982b42ab1099708ea56b", "title": "Using Very Large Scale Ontologies for Natural Language Generation", "abstract": "Ontology-based natural language generation can be a determinative factor for the digitalization in the publishing, media and content production industry. Based on the technology presented here, in the foreseeable future the amount of generated news will exceed that of news written by human authors. In future, publicly available data in the domains of weather, sports, finance, traffic, events or open data from sources like Wikipedia, dbpedia, YAGO etc. will be combined to create hyper-personalized news streams. Thousands of product descriptions for online shops can be generated as unique texts in many languages day by day.", "venue": "JOWO", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "onto", "nlg"], "mention_counts": {"nlg": 2, "onto": 2}, "nlp_mention_counts": {"nlg": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "b65569333ee168fee7117e49ab735ee31416a802", "url": "https://www.semanticscholar.org/paper/b65569333ee168fee7117e49ab735ee31416a802", "title": "Augmented Robotics Dialog System for Enhancing Human\u2013Robot Interaction", "abstract": "Augmented reality, augmented television and second screen are cutting edge technologies that provide end users extra and enhanced information related to certain events in real time. This enriched information helps users better understand such events, at the same time providing a more satisfactory experience. In the present paper, we apply this main idea to human\u2013robot interaction (HRI), to how users and robots interchange information. The ultimate goal of this paper is to improve the quality of HRI, developing a new dialog manager system that incorporates enriched information from the semantic web. This work presents the augmented robotic dialog system (ARDS), which uses natural language understanding mechanisms to provide two features: (i) a non-grammar multimodal input (verbal and/or written) text; and (ii) a contextualization of the information conveyed in the interaction. This contextualization is achieved by information enrichment techniques that link the extracted information from the dialog with extra information about the world available in semantic knowledge bases. This enriched or contextualized information (information enrichment, semantic enhancement or contextualized information are used interchangeably in the rest of this paper) offers many possibilities in terms of HRI. For instance, it can enhance the robot's pro-activeness during a human\u2013robot dialog (the enriched information can be used to propose new topics during the dialog, while ensuring a coherent interaction). Another possibility is to display additional multimedia content related to the enriched information on a visual device. This paper describes the ARDS and shows a proof of concept of its applications.", "venue": "Italian National Conference on Sensors", "citationCount": 22, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["sw", "nlu", "ie", "kg"], "mention_counts": {"sw": 1, "kg": 1, "nlu": 1, "ie": 1}, "nlp_mention_counts": {"nlu": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "kg": 1}, "relevance_score": 0.5}, {"paperId": "8e080117f6cc01fe6ea82472e3ea8686ef943e0f", "url": "https://www.semanticscholar.org/paper/8e080117f6cc01fe6ea82472e3ea8686ef943e0f", "title": "What Is This, Anyway: Automatic Hypernym Discovery", "abstract": "Can a system that \u201clearns from reading\u201d figure out on it\u2019s own the semantic classes of arbitrary noun phrases? This is essential for text understanding, given the limited coverage of proper nouns in lexical resources such as WordNet. Previous methods that use lexical patterns to discover hypernyms suffer from limited precision and recall. We present methods based on lexical patterns that find hypernyms of arbitrary noun phrases with high precision. This more than doubles the recall of proper noun hypernyms provided by WordNet at a modest cost to precision. We also present a novel method using a Hidden Markov Model (HMM) to extend recall further. Introduction and Motivation A goal of \u201cMachine Reading\u201d is an automatic system that extracts information from text and supports a wide range of inferencing capabilities (Etzioni, Banko, and Cafarella 2006). To enable such inferencing, an Information Extraction (IE) system must go beyond representing extracted entities as text strings and ontologize the text strings (Pennacchiotti and Pantel 2006; Soderland and Mandhani 2007). This involves semantically typing the text strings, grounding the string in real world entities where possible, and mapping the string to a concept in a formal taxonomy. In many cases, particularly for proper nouns, the text string is not found in an existing taxonomy and must be added on the fly as a child of an existing concept. This paper focuses on just one step in this ontologizing process: finding hypernyms for an arbitrary noun phrase (NP). This is a necessary first step in semantically typing the NP and mapping it to a node in a taxonomy. The problem that we address here is as follows. Given an NP e, find a set of NPs ci such that each ci is a hypernym of e in some sense as judged by a human. hypernymOf(e) = {c1, c2, ...ck} Note that we define hypernymy as a relation between surface forms and not \u201csynsets\u201d, as is done in WordNet. Copyright c \u00a9 2009, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Consider the example of an NP extracted from the Web, \u201cBorland Delphi\u201d. This term is not found in WordNet, but lexical patterns in a large corpus suggest that Borland Delphi may have the following hypernyms: hypernymOf(Borland Delphi) = development environment = software development system = software tool = language = technology The rest of this paper is organized as follows. We discuss previous attempts at solving this problem. The manually engineered WordNet thesaurus (Miller et al. 1990) has good coverage of common nouns, but contains relatively few entries for Proper Nouns. Several researchers have used methods based on lexical patterns to discover hypernyms (Hearst 1992; Roark and Charniak 1998; Caraballo 1999; Snow, Jurafsky, and Ng 2005), although each of these methods suffers from limited precision and recall. We then present a series of methods for finding hypernyms of arbitrary NPs: HYPERNYMFINDERfreq which is based on the frequency of Hearst pattern matches and HYPERNYMFINDERsvm that uses a Support Vector Machine (SVM) classifier to incorporate additional features. Each of these systems learns to find hypernyms from lexical patterns and corpus statistics. Since this is an inherently error-prone process, each version of HYPERNYMFINDER assigns a probability to each ci it finds. Finally, we present HYPERNYMFINDERhmm, which uses an HMM language model to extend the recall of HYPERNYMFINDER to cover NPs that do not match any of the lexical patterns used by the previous versions. Previous Work on Hypernym Discovery The WordNet thesaurus (Miller et al. 1990) is a highprecision lexical resource that has served as a baseline for hypernym discovery since the 1990\u2019s. On a test set of 953 Noun phrases randomly selected from our Web corpus, we found that only 17% of the proper nouns and 64% of the common nouns were covered by WordNet. Information is often spotty even for NPs that are found in WordNet. For example, WordNet has an entry for \u201cMel Gibson\u201d as an instance of the class \u201cactor\u201d, a subclass of \u201chuman\u201d, but does not indicate that he is a \u201cman\u201d. We would need to use other", "venue": "AAAI Spring Symposium: Learning by Reading and Learning to Read", "citationCount": 108, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "1a947077879b8ae6ef9cb7d9249519c3792f41c1", "url": "https://www.semanticscholar.org/paper/1a947077879b8ae6ef9cb7d9249519c3792f41c1", "title": "Spoken WordNet", "abstract": "WordNets have been used in a wide variety of applications, including in design and development of intelligent and human assisting systems. Although WordNet was initially developed as an online lexical database, (Miller, 1995 and Fellbaum, 1998) later developments have inspired using WordNet database as resources in NLP applications, Language Technology developments, and as sources of structured learned materials. This paper proposes, conceptualizes, designs, and develops a voice enabled information retrieval system, facilitating WordNet knowledge presentation in a spoken format, based on a spoken query. In practice, the work converts the WordNet resource into a structured voiced based knowledge extraction system, where a spoken query is processed in a pipeline, and then extracting the relevant WordNet resources, structuring through another process pipeline, and then presented in spoken format. Thus the system facilitates a speech interface to the existing WordNet and we named the system as \u201cSpoken WordNet\u201d. The system interacts with two interfaces, one designed and developed for Web, and the other as an App interface for smartphone. This is also a kind of restructuring the WordNet as a friendly version for visually challenged users. User can input query string in the form of spoken English sentence or word. Jaccard Similarity is calculated between the input sentence and the synset definitions. The one with highest similarity score is taken as the synset of interest among multiple available synsets. User is also prompted to choose a contextual synset, in case of ambiguities.", "venue": "GWC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "ke"], "mention_counts": {"nlp": 1, "kg": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.5}, {"paperId": "e03059f2795f2b8a4766a61e90f6e3537ac89932", "url": "https://www.semanticscholar.org/paper/e03059f2795f2b8a4766a61e90f6e3537ac89932", "title": "Natural technologies for knowledge work: information visualization and knowledge extraction", "abstract": "This paper \u2018looks\u201d into one of the most novel knowledge management technology products that has been brought to the market in the recent years. The authors describe two technologies, information visualization and knowledge extraction, for leveraging our natural abilities of vision, language and memory. They discuss a way for exploiting structure that is available in the information system in one case (traditionally called structured) and easily perceived by humans in the other (traditionally called unstructured). The two technologies focus on the two sides of this goal, respectively. They demonstrate the value of these technologies in supporting interaction with much larger amounts of information than was possible with previous graphical interfaces and in guiding access and use of the information and often for automating portions of the work.", "venue": "J. Knowl. Manag.", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "99a26a24325d0e84389372118c64b87aeebdda13", "url": "https://www.semanticscholar.org/paper/99a26a24325d0e84389372118c64b87aeebdda13", "title": "Automatic Extraction of Metrics from SLAs for Cloud Service Management", "abstract": "To effectively manage cloud based services, organizations need to continuously monitor the performance metrics listed in the Cloud service contracts. However, these legal documents, like Service Level Agreements (SLA) or privacy policy documents, are currently managed as plain text files meant principally for human consumption. Additionally, providers often define their own performance metrics for their services. These factors hinder the automation of SLA management and require manual effort to monitor the cloud service performance. We have significantly automated the process of extracting, managing and monitoring cloud SLA using natural language processing techniques and Semantic Web technologies. In this paper, we describe our technical approach and the ontology that we have developed to describe, manage, and reason about cloud SLAs. We also describe the prototype system that we have developed to automatically extract information from legal Terms of Service that are available on cloud provider websites.", "venue": "2016 IEEE International Conference on Cloud Engineering (IC2E)", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "sw", "nlp", "onto"], "mention_counts": {"nlp": 1, "sw": 1, "onto": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "f2d7fbfc02be0d326ae6c50aa83d92a741b376f4", "url": "https://www.semanticscholar.org/paper/f2d7fbfc02be0d326ae6c50aa83d92a741b376f4", "title": "Natural language processing resources: Using semantic web technologies", "abstract": "Natural language processing relies heavily on resources. Most common usage scenarios include using the resources for automated lexical tagging or named entity recognition. Also manually annotated language resources are used for benchmarking of new automated approaches. To allow any processing on a large scale and considering the complexity of natural language (words can have multiple meanings within the same general context) the resources have to be quite large. In this paper we focus on lexical resources in ontology form.", "venue": "Proceedings of the ITI 2012 34th International Conference on Information Technology Interfaces", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "nlp", "onto"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "b39fc33caf991adf34651059969c8ff843023b8d", "url": "https://www.semanticscholar.org/paper/b39fc33caf991adf34651059969c8ff843023b8d", "title": "Using Natural Language Generation Technology to Improve Information Flows in Intensive Care Units", "abstract": "In the drive to improve patient safety, patients in modern intensive care units are closely monitored with the generation of very large volumes of data. Unless the data are further processed, it is difficult for medical and nursing staff to assimilate what is important. It has been demonstrated that data summarization in natural language has the potential to improve clinical decision making; we have implemented and evaluated a prototype system which generates such textual summaries automatically. Our evaluation of the computer generated summaries showed that the decisions made by medical and nursing staff after reading the summaries were as good as those made after viewing the currently available graphical presentations with the same information content. Since our automatically generated textual summaries can be improved by including additional content and expert knowledge, they promise to enhance information exchange between the medical and nursing staff, particularly when integrated with the currently available graphical presentations. The main feature of this technology is that it brings together a diverse set of techniques such as medical signal analysis, knowledge based reasoning, medical ontology and natural language generation. In this paper we discuss the main components of our approach with a critical analysis of their strengths and limitations and present options for improvement to address these limitations.", "venue": "ECAI", "citationCount": 22, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "nlg", "kg", "onto"], "mention_counts": {"kg": 1, "nlg": 2, "onto": 1}, "nlp_mention_counts": {"nlg": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "cee354e82c5fb07b3c9dfd78420c564bcc248134", "url": "https://www.semanticscholar.org/paper/cee354e82c5fb07b3c9dfd78420c564bcc248134", "title": "The AIS Project: Boosting Information Extraction from Legal Documents by using Ontologies", "abstract": "In the legal field, it is a fact that a large number of documents are processed every day by management \n \ncompanies with the purpose of extracting data that they consider most relevant in order to be stored in their \n \nown databases. Despite technological advances, in many organizations, the task of examining these usually-extensive \n \ndocuments for extracting just a few essential data is still performed manually by people, which is \n \nexpensive, time-consuming, and subject to human errors. Moreover, legal documents usually follow several \n \nconventions in both structure and use of language, which, while not completely formal, can be exploited to \n \nboost information extraction. In this work, we present an approach to obtain relevant information out from \n \nthese legal documents based on the use of ontologies to capture and take advantage of such structure and \n \nlanguage conventions. We have implemented our approach in a framework that allows to address different \n \ntypes of documents with minimal effort. Within this framework, we have also regarded one frequent problem \n \nthat is found in this kind of documentation: the presence of overlapping elements, such as stamps or signatures, \n \nwhich greatly hinders the extraction work over scanned documents. Experimental results show promising \n \nresults, showing the feasibility of our approach.", "venue": "ICAART", "citationCount": 20, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "c6e42393100b7e5f6c8e873c1f696cc058777c1d", "url": "https://www.semanticscholar.org/paper/c6e42393100b7e5f6c8e873c1f696cc058777c1d", "title": "Generating grammars for natural language understanding from knowledge about actions and objects", "abstract": "Many applications in the fields of Service Robotics and Industrial Human-Robot Collaboration, require interaction with a human in a potentially unstructured environment. In many cases, a natural language interface can be helpful, but it requires powerful means of knowledge representation and processing, e.g., using ontologies and reasoning. In this paper we present a framework for the automatic generation of natural language grammars from ontological descriptions of robot tasks and interaction objects, and their use in a natural language interface. Robots can use it locally or even share this interface component through the RoboEarth framework in order to benefit from features such as referent grounding, ambiguity resolution, task identification, and task assignment.", "venue": "IEEE International Conference on Robotics and Biomimetics", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlg", "nlu"], "mention_counts": {"nlg": 1, "onto": 2, "nlu": 1}, "nlp_mention_counts": {"nlg": 1, "nlu": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "1b6c53d0c65baaf07f876e1ea11df2db43901756", "url": "https://www.semanticscholar.org/paper/1b6c53d0c65baaf07f876e1ea11df2db43901756", "title": "PrivOnto: A semantic framework for the analysis of privacy policies", "abstract": "Privacy policies are intended to inform users about the collection and use of their data by websites, mobile apps and other services or appliances they interact with. This also includes informing users about any choices they might have regarding such data practices. However, few users read these often long privacy policies; and those who do have difficulty understanding them, because they are written in convoluted and ambiguous language. A promising approach to help overcome this situation revolves around semi-automatically annotating policies, using combinations of crowdsourcing, machine learning and natural language processing. In this article, we introduce PrivOnto, a semantic framework to represent annotated privacy policies. PrivOnto relies on an ontology developed to represent issues identified as critical to users and/or legal experts. PrivOnto has been used to analyze a corpus of over 23,000 annotated data practices, extracted from 115 privacy policies of US-based companies. We introduce a collection of 57 SPARQL queries to extract information from the PrivOnto knowledge base, with the dual objective of (1) answering privacy questions of interest to users and (2) supporting researchers and regulators in the analysis of privacy policies at scale. We present an interactive online tool using PrivOnto to help users explore our corpus of 23,000 annotated data practices. Finally, we outline future research and open challenges in using semantic technologies for privacy policy analysis.", "venue": "Semantic Web", "citationCount": 70, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "ie", "nlp"], "mention_counts": {"nlp": 1, "kg": 1, "onto": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "267d024aecaf9604771fe0e0eca718cc86a8861c", "url": "https://www.semanticscholar.org/paper/267d024aecaf9604771fe0e0eca718cc86a8861c", "title": "Automated Grading System Using Natural Language Processing", "abstract": "Most of the articles which cover automated grading consider keyword matching to be a crucial aspect while grading answers. Even though these are important, it is human to forget several uncommon terms and instead replace them with words that have a similar meaning. In this paper, a solution to grading of papers of theory based subjects is obtained where in Automatic Paper Grading will be performed using Natural Language Processing. Machine learning techniques like Semantic Analysis will be adopted. As a single answer can be presented in a number of ways by different students, matching keywords is inefficient. That is why, using ontology, extraction of words and their synonyms related to the domain is done which makes the evaluation process holistic as presence of keywords, synonyms, the right word combination and coverage of concepts can now be checked. The above mentioned techniques will be implemented with Ontology and will be tested on common input data consisting of technical answers. The results will be analyzed and an unbiased, high accuracy automated grading system for a theory based subject will be obtained with very little error rate which is comparable to a differential human-to-human error rate. The algorithm is designed based on the responses collected during the survey conducted amongst teachers regarding their parameters when correcting papers manually.", "venue": "2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT)", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "17fa63699b938fe606fadbe22fea2bfbc12bae0b", "url": "https://www.semanticscholar.org/paper/17fa63699b938fe606fadbe22fea2bfbc12bae0b", "title": "Semantic Relations Between Nominals", "abstract": "People make sense of a text by identifying the semantic relations which connect the entities or concepts described by that text. A system which aspires to human-like performance must also be equipped to identify, and learn from, semantic relations in the texts it processes. Understanding even a simple sentence such as \"Opportunity and Curiosity find similar rocks on Mars\" requires recognizing relations (rocks are located on Mars, signalled by the word on) and drawing on already known relations (Opportunity and Curiosity are instances of the class of Mars rovers). A language-understanding system should be able to find such relations in documents and progressively build a knowledge base or even an ontology. Resources of this kind assist continuous learning and other advanced language-processing tasks such as text summarization, question answering and machine translation. The book discusses the recognition in text of semantic relations which capture interactions between base noun phrases. After a brief historical background, we introduce a range of relation inventories of varying granularity, which have been proposed by computational linguists. There is also variation in the scale at which systems operate, from snippets all the way to the whole Web, and in the techniques of recognizing relations in texts, from full supervision through weak or distant supervision to self-supervised or completely unsupervised methods. A discussion of supervised learning covers available datasets, feature sets which describe relation instances, and successful algorithms. An overview of weakly supervised and unsupervised learning zooms in on the acquisition of relations from large corpora with hardly any annotated data. We show how bootstrapping from seed examples or patterns scales up to very large text collections on the Web. We also present machine learning techniques in which data redundancy and variability lead to fast and reliable relation extraction. Table of Contents: Introduction / Relations between Nominals, Relations between Concepts / Extracting Semantic Relations with Supervision / Extracting Semantic Relations with Little or No Supervision / Conclusion", "venue": "Semantic Relations Between Nominals", "citationCount": 52, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "tp", "kg", "mt"], "mention_counts": {"kg": 1, "onto": 1, "tp": 1, "mt": 1}, "nlp_mention_counts": {"tp": 1, "mt": 1}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "8df0f060f4156be3f9775f2bf036fb7ac40d892f", "url": "https://www.semanticscholar.org/paper/8df0f060f4156be3f9775f2bf036fb7ac40d892f", "title": "Intelligent manufacturing applications at Ford Motor Company", "abstract": "There is a common misconception that the automobile industry is slow to adapt new technologies, such as artificial intelligence (AI), into the manufacturing sector. In reality, many of the earliest adaptations of AI were in the automotive sector where such diverse technologies as expert systems, neural networks, genetic algorithms, and fuzzy logic were among the first to be used. In this paper we present an overview of how AI and knowledge-based technologies are currently being applied at Ford Motor Company within the manufacturing arena. Some of the applications that is described include an AI-based approach for vehicle assembly process planning, an application of AI for ergonomics analysis and a system that utilizes machine translation to translate assembly build instructions for Ford assembly plants that do not use English as their primary language. We also discuss how specific technologies, such as natural language processing, controlled languages, and ontologies, can be used to effectively deal with different types of knowledge, both structured and unstructured, that are prevalent in the manufacturing environment. The automobile industry is among the most competitive in the entire world, and the use of advanced technologies, is essential in the ongoing struggle to prosper in the global marketplace.", "venue": "Annual Conference on the North American Fuzzy Information Processing Society", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "kg", "mt"], "mention_counts": {"nlp": 1, "onto": 1, "kg": 1, "mt": 1}, "nlp_mention_counts": {"nlp": 1, "mt": 1}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "da9d1195a3ac065d36b4f6cf57d6e82ee20364c3", "url": "https://www.semanticscholar.org/paper/da9d1195a3ac065d36b4f6cf57d6e82ee20364c3", "title": "Mining arguments from cancer documents using Natural Language Processing and ontologies", "abstract": "In the medical domain, the continuous stream of scientific research contains contradictory results supported by arguments and counter-arguments. As medical expertise occurs at different levels, part of the human agents have difficulties to face the huge amount of studies, but also to understand the reasons and pieces of evidences claimed by the proponents and the opponents of the debated topic. To better understand the supporting arguments for new findings related to current state of the art in the medical domain we need tools able to identify arguments in scientific papers. Our work here aims to fill the above technological gap. We rely on the well-known interleaving of domain knowledge with natural language processing. To formalise the existing medical knowledge, we rely on ontologies. To structure the argumentation model we use also the expressivity and reasoning capabilities of Description Logics. To perform argumentation mining we formalise various linguistic patterns in a rule-based language. We tested our solution against a corpus of scientific papers related to breast cancer. The run experiments show a F-measure between 0.71 and 0.86 for identifying conclusions of an argument and between 0.65 and 0.86 for identifying premises of an argument.", "venue": "2016 IEEE 12th International Conference on Intelligent Computer Communication and Processing (ICCP)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "1014d71f8f2225d2ff3529b86a7e2cfd6d84c0b1", "url": "https://www.semanticscholar.org/paper/1014d71f8f2225d2ff3529b86a7e2cfd6d84c0b1", "title": "Augmenting word embeddings through external knowledge-base for biomedical application", "abstract": "The technological advancements in biomedical domain has led to a tremendous growth of unstructured data; primarily a result of increased publication of findings. At the same time, a corresponding interest in the Natural Language Processing (NLP) community to develop scalable methodologies to exploit such massive unlabeled corpora for unsupervised language processing has resulted in new opportunities towards developing semantic sensitive models. Amongst them, the field of word embeddings has garnered significant attention due to its capability to understand implicit semantics. However such data driven models are largely agnostic of the rich explicit semantic knowledge available in the biomedical domain in the form of vocabularies and ontologies. This is problematic because it leads to a poor representation of words with little local context and its effect is acute in biomedical domain. In this paper, we propose a novel model (MeSH2Vec) that jointly exploits both contextual information and available explicit semantic knowledge to learn externally augmented word embeddings. Unlike existing approaches, the proposed methodology is more dexterous in its ability to handle relationships between indirectly related concepts. The 13% improvement in the correlation to experts, shown on experiments involving biomedical concept similarity and relatedness task validates the effectiveness of the proposed approach and demonstrates the importance of incorporating human curated knowledge in the process of generating word embeddings.", "venue": "2017 IEEE International Conference on Big Data (Big Data)", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "kg"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "9920a65c44d342be6b70f69ead9eea794f81ef4e", "url": "https://www.semanticscholar.org/paper/9920a65c44d342be6b70f69ead9eea794f81ef4e", "title": "Rulelog: Highly Expressive Semantic Rules with Scalable Deep Reasoning", "abstract": "In this half-day tutorial, we cover the fundamental concepts, key technologies, emerging applications, recent progress, and outstanding research issues in the area of Rulelog, a leading approach to fully semantic rule-based knowledge representation and reasoning (KRR). Rulelog matches well many of the requirements of cognitive computing. It combines deep logical/probabilistic reasoning tightly with natural language processing (NLP), and complements machine learning (ML). Rulelog interoperates and composes well with graph databases, relational databases, spreadsheets, XML, and expressively simpler rule/ontology systems and can orchestrate overall hybrid KRR. Developed mainly since 2005, Rulelog is much more expressively powerful than the previous state-of-the-art practical KRR approaches, yet is computationally affordable. It is fully semantic and has capable efficient implementations that leverage methods from logic programming and databases, including dependency-aware smart caching and a dynamic compilation stack architecture.", "venue": "RuleML+RR", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "kg": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "aed27f03bd98df316c63903cd7aad6f53ea008a2", "url": "https://www.semanticscholar.org/paper/aed27f03bd98df316c63903cd7aad6f53ea008a2", "title": "Distilling and exploring nuggets from a corpus", "abstract": "This paper describes a live and scalable system that automatically extracts information nuggets for entities/topics from a continuously updated corpus for effective exploration and analysis. A nugget is a piece of semantic information that (1) must be mapped semantically to the transitive closure of a pre-defined ontology, (2) is explicitly supported by text, and (3) has a natural language description that completely conveys its semantic to a user. Fig. 1 shows a type of nugget \"involvement in events\" for a person entity (Leon Panetta): each nugget has a short description (\"meeting\", \"news conference\") with a list of supporting passages.\n Our key contributions are (1) We extract nuggets and remove redundancy to produce a summary of salient information with supporting clusters of passages. (2) We present an entity/topic centric exploration interface that also allows users to navigate to other entities involved in a nugget. (3) We use the statistical NLP technologies developed over the years in the ACE ,GALE and TAC-KBP programs, including parsing, mention detection, within and cross document coreference resolution, relation detection and slot filler extraction. (4) Our system is flexible and easily adaptable across domains as demonstrated on two corpora: generic news and scientific papers. Search engines such as Google News and Scholar do not retrieve nuggets, and only remove redundancy at document level. News aggregation applications such as Evri categorize news articles based on the entities of topics but do not extract nuggets. Other systems extract richer information, but not all of it has clear semantics; e.g., Silobreaker presents results as \"the relationship between X and Y in the context of [keyphrase]\", leaving users with the task of interpreting the semantics as it is not tied to a clear ontology. In contrast we remove redundancy, summarize results and present nuggets that have clear semantics.", "venue": "SIGIR '12", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "ie"], "mention_counts": {"nlp": 1, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "81c426d3f3a60adc4c10e025c34b15bde583a5d2", "url": "https://www.semanticscholar.org/paper/81c426d3f3a60adc4c10e025c34b15bde583a5d2", "title": "A NLP Based Framework to Support Document Verification-as-a-Service", "abstract": "Many enterprise systems are document intensive that requires extensive manual verification in the form of maker and checker. However, a maker-checker based verification raises several challenges with respect to increase in cost and time of verification. Furthermore, any manual labor intensive verification is not free from human oversight and can lead to costly errors. Therefore, to alleviate the challenges arising out of human verification of document intensive systems, we propose a rule based framework that enables automatic verification of document based systems. The framework uses ontology based knowledge representation techniques along with appropriate natural language processing methods to extract operational rules from business documents and then use suitable reasoning engine for verification. The above framework is validated in the light of a real life case study namely International Trade that deals with several critical financial documents like Letter-of-Credit, Bill-of-Lading, Commercial Invoice etc.", "venue": "IEEE International Enterprise Distributed Object Computing Conference", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "kg", "onto"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "a13898ce31dd87fbcdd1aa0b5767e2000cef61e3", "url": "https://www.semanticscholar.org/paper/a13898ce31dd87fbcdd1aa0b5767e2000cef61e3", "title": "Improving Online Clinical Trial Search Efficiency Using Natural Language Processing and Biomedical Ontology Mapping Approach", "abstract": "Summary form only given. With the increasing demand for healthcare, computer technology and the internet are playing a more important role for patients, practitioners, and researchers. Oftentimes the process of seeking or providing care does not start in a waiting room or in a doctor's office, but online. Because of this, special attention must be concentrated in increasing the efficiency of online search engines so that the rest of the care process may also run smoothly. This research proposes a solution to improve the search efficiency for patients using natural language processing and SNOMED mapping techniques. In this research, clinical trials are extracted from ClinicalTrials.gov and n-gram method is applied to process the clinical trial contents. The processed terms are then mapped to SNOMED terms and a covariance matrix is formulated with the Jaccard similarity coefficient measuring similarity between a pair of clinical trials. Based on the similarity measures, the most relevant clinical trials are extracted for the searcher's needs. In the end, a comparative study is conducted to prove the enhancement of the search efficiency. In conclusion, the combination of n-gram model and SNOMED terminology mapping to process the clinical trial contents is proved to improve the efficiency for the online search of the clinical trials. Future research with clinical trials will use multiple methods such as ontological and statistical approaches to improve the precision and recall of the search results. Another interesting next step may be to explore clustering by analyzing the correlation structure of the clinical trial contents.", "venue": "2013 IEEE International Conference on Healthcare Informatics", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "c8880a2ed2f1ab222f2ca738388ebc208e000afe", "url": "https://www.semanticscholar.org/paper/c8880a2ed2f1ab222f2ca738388ebc208e000afe", "title": "An Integrated Approach to Biomedical Term Identification Systems", "abstract": "In this paper a novel architecture to build biomedical term identification systems is presented. The architecture combines several sources of information and knowledge bases to provide practical and exploration-enabled biomedical term identification systems. We have implemented a system to evidence the convenience of the different modules considered in the architecture. Our system includes medical term identification, retrieval of specialized literature and semantic concept browsing from medical ontologies. By applying several Natural Language Processing (NLP) technologies, we have developed a prototype that offers an easy interface for helping to understand biomedical specialized terminology present in Spanish medical texts. The result is a system that performs term identification of medical concepts over any textual document written in Spanish. It is possible to perform a sub-concept selection using the previously identified terms to accomplish a fine-tune retrieval process over resources like SciELO, Google Scholar and MedLine. Moreover, the system generates a conceptual graph which semantically relates all the terms found in the text. In order to evaluate our proposal on medical term identification, we present the results obtained by our system using the MANTRA corpus and compare its performance with the Freeling-Med tool.", "venue": "Applied Sciences", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "kg"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "33f65a63d25d3d106f04e57583752cc4efef1991", "url": "https://www.semanticscholar.org/paper/33f65a63d25d3d106f04e57583752cc4efef1991", "title": "Making Semantic Annotation on Patient Data of Depression", "abstract": "Patient data, more exactly, electronic medical records (EMR), usually contain a lot of free texts. Those unstructured medical data cannot be easily understood by computers. In addition, EMR data have a strong privacy, which hinders the sharing and use of medical data and makes it impossible to conduct more in-depth medical research. This paper presents a method of the realization of semantic EMR by making semantic annotations on free texts in medical records. We will show how to use Natural Language Processing (NLP) tools to create semantic annotation with well-known biomedical terminologies/ontologies such as the Unified Medical Language System (UMLS). Moreover, we will describe how to make the semantic annotations on a set of virtual patient data for depression, which are generated by using the Advanced Patient Data Generator (APDG), a knowledge-based patient data generator. In short, our goal is to use semantic technology to improve the sharing and utilization of medical data and the interoperability among systems.", "venue": "ICMHI '18", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "kg": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "4db8d5adffcbf144e26530c4b0e3d3fedc902f38", "url": "https://www.semanticscholar.org/paper/4db8d5adffcbf144e26530c4b0e3d3fedc902f38", "title": "Demonstrations Track of the 25th International Joint Conference on Artificial Intelligence", "abstract": "There is a valuable body of research in Artificial Intelligence as witnessed by the large number of contributions to different journals and scientific conferences. This work, continued unceasingly for various decades, has given rise to a significant number of applications in fields other than Artificial Intelligence (e.g., medicine, communications, security, etc.) and, also, to assist in real-life situations \u2013 e.g., robots, unmanned vehicles (either terrestrial or aerial), autonomous assistant agents, recommender systems, etc. Indeed, the interest in applications of Artificial Intelligence has recently reached unprecedented peaks from the general public. IJCAI is the International Joint Conference on Artificial Intelligence, the main international gathering of researchers in AI, and a reference conference in the world in all fields of Artificial Intelligence. The 25th International Joint Conference on Artificial Intelligence was held in New York City (United States) from July 9 to July 15, 2016, and it offered also a wide variety of satellite events. The Demonstration Track was held for the first time and it has been re-edited in consecutive editions of the conference since then. The Demonstrations Track provides a unique opportunity in this regard. Its primary goals are: first, to provide a good framework for exchanging ideas between theory and practice; secondly, to showcase the applicability of different AI technologies to real-world problems. The Demonstration Track received a record number of 91 submissions on different topics from fields such as: Computer Vision, Genetic algorithms, Knowledgebased systems, Learning algorithms, Natural Language Processing, Planning and Robotics. All submissions accepted to the Demonstration Track were invited to submit a larger version of their research to AI Communications which underwent a thorough reviewing process. This track in AI Communications gives space to research work based on the best works accepted in the Demonstrations Track of the 25th IJCAI conference explicitly submitted for the Demonstrations Track, with various applications in Planning, Knowledge Engineering, Natural Language Processing, Semantic Web, Ontologies, Data Mining, Neural Networks and Deep Learning.", "venue": "AI Commun.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "sw", "nlp"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "3045f7509e6f6d34218149bab7d562d22f5380e9", "url": "https://www.semanticscholar.org/paper/3045f7509e6f6d34218149bab7d562d22f5380e9", "title": "Filling the Gap between Web 2.0 Technologies and Natural Language Processing Pipelines with Semantic Web", "abstract": "As Web 2.0 websites are now widespread and natural language proposes mature processing methods, using the later to enrich the former opens new possibilities to improve interactions. This paper will show a way of combining natural language processing tools with semantic web to enhance tagging and mashup functionalities of Web 2.0 websites.", "venue": "2009 Third International Conference on Advances in Semantic Processing", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "sw", "sw"], "mention_counts": {"nlp": 2, "sw": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 2}, "relevance_score": 0.5}, {"paperId": "77cd86140049837fbae141cf14843db5164a1c34", "url": "https://www.semanticscholar.org/paper/77cd86140049837fbae141cf14843db5164a1c34", "title": "Leveraging the semantic web and natural language processing to enhance drug-mechanism knowledge in drug product labels", "abstract": "Multiple studies indicate that drug-drug interactions are a significant source of preventable adverse drug events. Factors contributing to the occurrence of preventable ADEs resulting from DDIs include a lack of knowledge of the patient's concurrent medications and inaccurate or inadequate knowledge of interactions by health care providers. FDA-approved drug product labeling is a major source of information intended to help clinicians prescribe drugs in a safe and effective manner. Unfortunately, drug product labeling has been identified as often lagging behind emerging drug knowledge; especially when it has been several years since a drug has been released to the market. In this paper we report on a novel approach that explores employing Semantic Web technology and natural language processing to identify drug mechanism information that may update or expand upon statements present in product labeling.", "venue": "International Health Informatics Symposium", "citationCount": 4, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "sw", "sw", "nlp"], "mention_counts": {"nlp": 2, "sw": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 2}, "relevance_score": 0.5}, {"paperId": "51778d89e40c06d78c299957a2b028fa31a88985", "url": "https://www.semanticscholar.org/paper/51778d89e40c06d78c299957a2b028fa31a88985", "title": "CORVIDAE: Coreference Resolution Visual Development & Analysis Environment", "abstract": "Communication whether in verbal or written form is part of our daily life. Hence, we as humans have developed a set of skills that enable us to follow a discourse and extract important information from a text quite easily. For a machine, however language understanding is a quite challenging problem and considered to be AI-complete, i.e. a machine must reach human level intelligence in order to solve this task. Recent developments, especially those forming the semantic web, offer new ways of incorporating world knowledge into natural language processing methods, while at the same time progress made in the latter will help pushing the dream of the global knowledge graph closer to reality. In this paper we present CORVIDAE (Coreference Resolution Visual Development & Analysis Environment) a tool for NLP developers to analyse and eventually improve coreference resolution algorithms specially designed for those that interact with world knowledge.", "venue": "SEMANTiCS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "kg", "sw"], "mention_counts": {"nlp": 2, "sw": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "sw": 1}, "relevance_score": 0.5}, {"paperId": "e70daebfe3e57acf6ee61123bdef9cd111ec8535", "url": "https://www.semanticscholar.org/paper/e70daebfe3e57acf6ee61123bdef9cd111ec8535", "title": "DIR \u2014 A semantic information resource for healthcare datasets", "abstract": "It is important for data scientists to have a good understanding of the availability of relevant datasets as well as the content, structure, and existing analyses of these datasets. While a number of efforts are underway to integrate the large amount and variety of datasets, there is a lack of information resources that focus on specific learning needs of some targeted audiences. To address this gap, we have been developing a semantic Dataset Information Resource (DIR) framework to specifically address the challenges of entry-level data scientists in learning to identify, understand, and analyze major datasets with an initial focus on healthcare. The DIR does not contain actual data from the datasets but aims to provide comprehensive knowledge about the datasets and their analyses. The framework leverages Semantic Web technologies and the W3C Dataset Description Standard for knowledge integration and representation and includes natural language processing (NLP)-based methods to enable knowledge extraction and question answering. The prototype DIR implementation includes four major components\u2014dataset metadata and related knowledge, search modules, question answering for frequently-asked questions, and blogs. And the DIR currently includes information on three commonly-used large and complex healthcare datasets: HCUP, MarketScan, and MIMIC. Initial usage evaluation based on health informatics students is encouraging. Further development is underway.", "venue": "2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "ke"], "mention_counts": {"nlp": 1, "sw": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"sw": 1, "ke": 1}, "relevance_score": 0.5}, {"paperId": "6bebf15de5ee64ba6bb2fe321c889193f310f623", "url": "https://www.semanticscholar.org/paper/6bebf15de5ee64ba6bb2fe321c889193f310f623", "title": "Structuring Mined Knowledge for the Support of Hypothesis Generation in Molecular Biology", "abstract": "Hypothesis generation in the life sciences is an empirical process in which obtaining and structuring knowledge from literature plays a significant role. Text mining and Information Extraction techniques are seen as key for programmatically accessing the knowledge captured in the form of free text. We describe progress towards an application that supports the task of generating a hypothesis about biomolecular mechanisms using Semantic Web technologies and a workflow to carry out text mining in a service-oriented architecture. The output is a semantic model with putative biological relationships that have been extracted from literature, with each relationship linked to the corresponding evidence. We present preliminary data that extends a model for chromatin (de)condensation. The methodology can be used to bootstrap the process of human-guided construction of semantically rich biological models using the results of knowledge extraction processes.", "venue": "SWAT4LS", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Biology"], "mentions": ["ke", "sw", "ie"], "mention_counts": {"ke": 1, "sw": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "sw": 1}, "relevance_score": 0.5}, {"paperId": "5ab007c2c44cad72b87da9ce2a90f89287abf67e", "url": "https://www.semanticscholar.org/paper/5ab007c2c44cad72b87da9ce2a90f89287abf67e", "title": "CLEOPATRA\u201922: 3rd International Workshop on Cross-lingual Event-centric Open Analytics", "abstract": "The 3rd International Workshop on Cross-lingual Event-centric Open Analytics (CLEOPATRA 2022) is co-located with The Web Conference (WWW) and held on the 25th of April, 20221. Modern society faces an unprecedented number of events that impact countries, communities and economies around the globe across language, country and community borders. Recent examples include sudden or unexpected events such as terrorist attacks and political shake-ups such as Brexit, events related to the ongoing COVID-19 pandemic, as well as longer ongoing and evolving topics such as the migration crisis in Europe that regularly spawn events of global importance affecting local communities. These developments result in a vast amount of event-centric, multilingual information available from heterogeneous sources on the Web, on the Web of Data, within Knowledge Graphs, in social media, inside Web archives and in news sources. Such event-centric information differs across sources, languages and communities, potentially reflecting community-specific aspects, opinions, sentiments and bias. The theme of the workshop includes a variety of interdisciplinary challenges related to analysis, interaction with and interpretation of vast amounts of event-centric textual, semantic and visual information in multiple languages originating from different communities. The goal of the interdisciplinary CLEOPATRA workshop is to bring together researchers and practitioners from the fields of Semantic Web, the Web, NLP, IR, Human Computation, Visual Analytics and Digital Humanities to discuss and evaluate methods and solutions for effective and efficient analytics of event-centric multilingual information spread across heterogeneous sources. This will support the delivery of analytical results in ways meaningful to users, helping them to cross language barriers and better understand event representations and their context in other languages. The workshop features advanced methods for extracting event-centric information, multi-modal geo-localisation, cross-lingual sentiment detection and causality detection, NLP tools for under-resourced languages and applications of these methods to digital humanities research. Furthermore, the workshop introduces selected tools developed by researchers working in the CLEOPATRA ITN - a Marie Sk\u0142odowska-Curie Innovative Training Network. We would like to take this opportunity to sincerely thank the authors and presenters for their inspiring contributions to the workshop. Our sincere thanks are due to the program committee members for reviewing the submissions and ensuring the high quality of our workshop program. We also thank Manolis Koubarakis for his keynote talk in the workshop. We are also very grateful to the organisers of The Web Conference 2022, and particularly the Workshops Chairs, Nathalie Hernandez and Preslav Nakov, for their support with the workshop organisation.", "venue": "WWW", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "nlp", "sw"], "mention_counts": {"nlp": 2, "sw": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "sw": 1}, "relevance_score": 0.5}, {"paperId": "d8105f355273cd55c2682968cd98a2d05a057868", "url": "https://www.semanticscholar.org/paper/d8105f355273cd55c2682968cd98a2d05a057868", "title": "Cloud-based Semantic Integration and Knowledge Discovery Systems in Precision Medicine", "abstract": "We have developed a cloud-based (AWS and IBM SoftLayer) knowledge environment for scalable semantic mining of scientific literature and PTM integrative knowledge discovery in precision medicine, building upon our novel natural language processing (NLP) technologies and bioinformatics infrastructure. We provided semantic integration of full-scale PubMed mining results from disparate text mining tools, along with kinase-substrate data from iPTMnet, and PTM proteoforms and their relations from Protein Ontology (PRO). We shared the digital objects of those applications in multiple interoperable formats and have registered them in bioCADDIE using CEDAR. We experimented with multiple system setups using operating system, programming language, web server, or database server that best fits each application. We evaluated the cost effectiveness of cloud computing by only paying for what we use and readily experimenting with additional services. A web portal is available for accessing our cloud-based knowledge environment at https://proteininformationresource.org/cloud/.", "venue": "BCB", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "kg", "nlp"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "896984465931d6582a7ff98803a809c70ffbb211", "url": "https://www.semanticscholar.org/paper/896984465931d6582a7ff98803a809c70ffbb211", "title": "Automatic Information Extraction from Texts with Inference and Linguistic Knowledge Acquisition Rules", "abstract": "In this paper we present a novel methodology for automatic information extraction from natural language texts, based on the integration of linguistic rules, multiple ontologies and inference resources, integrated with an abstraction layer for linguistic annotation and data representation. The methodology allows ontology population with instances of events. The main contribution presented is related to the exploration of the flexibility of linguistic rules and domain knowledge representation, through their manipulation and integration by a reasoning system. The results from the case study indicate that the proposed approach is effective for the legal domain.", "venue": "2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "1871e67467e25855ec86fba55af88b6032d56f50", "url": "https://www.semanticscholar.org/paper/1871e67467e25855ec86fba55af88b6032d56f50", "title": "Unsupervised Information Extraction using BabelNet and DBpedia", "abstract": "Using linked data in real world applications is a hot topic in the field of Information Retrieval. In this paper we leveraged two valuable knowledge bases in the task of information extraction. BabelNet is used to automatically recognize and disambiguate concepts in a piece of unstructured text. After extracting all possible concepts, DBpedia is leveraged to reason about the type of each concept using SPARQL.", "venue": "#MSM", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ie", "ld", "ie"], "mention_counts": {"ld": 1, "kg": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"ld": 1, "kg": 1}, "relevance_score": 0.5}, {"paperId": "297b655b77254952b9aa4278833909e545bffb00", "url": "https://www.semanticscholar.org/paper/297b655b77254952b9aa4278833909e545bffb00", "title": "MuLLinG: MultiLevel Linguistic Graphs for Knowledge Extraction", "abstract": "MuLLinG is a model for knowledge extraction (especially lexical extraction from corpora), based on multilevel graphs. Its aim is to allow large-scale data acquisition, by making it easy to realize automatically, and simple to configure by linguists with limited knowledge in computer programming. In MuLLinG, each new level represents the information in a different manner (more and more abstract). We also introduce several associated operators, written to be as generic as possible. They are independent of what nodes and edges represent, and of the task to achieve. Consequently, they allow the description of a complex extraction process as a succession of simple graph manipulations. Finally, we present an experiment of collocation extraction using MuLLinG model.", "venue": "TextGraphs@ACL", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "7e4520391453fb7f4f2655217ec46fb1ef24f7ef", "url": "https://www.semanticscholar.org/paper/7e4520391453fb7f4f2655217ec46fb1ef24f7ef", "title": "Extracting Medical Knowledge from Crowdsourced Question Answering Website", "abstract": "The medical crowdsourced question answering (Q&A) websites are booming in recent years, and an increasingly large amount of patients and doctors are involved. The valuable information from these medical crowdsourced Q&A websites can benefit patients, doctors and the society. One key to unleash the power of these Q&A websites is to extract medical knowledge from the noisy question-answer pairs and filter out unrelated or even incorrect information. Facing the daunting scale of information generated on medical Q&A websites everyday, it is unrealistic to fulfill this task via supervised method due to the expensive annotation cost. In this paper, we propose a Medical Knowledge Extraction (MKE) system that can automatically provide high-quality knowledge triples extracted from the noisy question-answer pairs, and at the same time, estimate expertise for the doctors who give answers on these Q&A websites. The MKE system is built upon a truth discovery framework, where we jointly estimate trustworthiness of answers and doctor expertise from the data without any supervision. We further tackle three unique challenges in the medical knowledge extraction task, namely representation of noisy input, multiple linked truths, and the long-tail phenomenon in the data. The MKE system is applied to real-world datasets crawled from xywy.com, one of the most popular medical crowdsourced Q&A websites. Both quantitative evaluation and case studies demonstrate that the proposed MKE system can successfully provide useful medical knowledge and accurate doctor expertise. We further demonstrate a real-world application, Ask A Doctor, which can automatically give patients suggestions to their questions.", "venue": "IEEE Transactions on Big Data", "citationCount": 18, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "f1208833dbf0b24d31d0d0c4d0a028f75db75b95", "url": "https://www.semanticscholar.org/paper/f1208833dbf0b24d31d0d0c4d0a028f75db75b95", "title": "TRRD: Technology for extraction, storage, and use of knowledge about the structural-functional organization of the transcriptional regulatory regions in the eukaryotic genes", "abstract": "We have developed the Transcription Regulatory Regions Database (TRRD, http://www.bionet.nsc.ru/trrd/) designed for storage of data on the structural-functional organization of transcriptional regulatory regions of the eukaryotic genes and their expression patterns. TRRD contains experimentally supported data only. Knowledge extraction from scientific literature, storage and further application of the results are all stepwise, conforming to the Data Mining technology: i) knowledge extraction from scientific publications; ii) preprocessing (data cleaning, syntactic and semantic analysis; iii) data transformation; iv) application for prediction; v) interpretation of the obtained knowledge to resolve the timely issues in bioinformatics. TRRD contains a compilation of data on 2 344 genes, their 14 407 expression patterns, 3 490 regulatory units, and 10 135 transcription factor binding sites. TRRD is filled in by manual annotation of scientific publications. The information incorporated into TRRD is the result of annotations of 7 609 scientific papers. Sequence Retrieval System (SRS) is the main tool for search and navigation in TRRD. A large number of indexed fields in its SRS version allow the user to generate queries both within and between libraries. TRRD has thesauruses and search systems that provide additional options for data access. TRRD is currently linked to 20 worldwide information resources, including EMBL/GeneBank, Ensembl, EPD, SWISS-PROT, TRANSFAC, GDB, GeneCards, MGD, RGD, GO. The links serve as a framework for integration in a distributed network environment.", "venue": "Intell. Data Anal.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "5a5a14d2dcf30ea9fa0eae1f2fb76a1ac58d883e", "url": "https://www.semanticscholar.org/paper/5a5a14d2dcf30ea9fa0eae1f2fb76a1ac58d883e", "title": "Interactive genetic fuzzy rule selection through evolutionary multiobjective optimization with user preference", "abstract": "The design of fuzzy rule-based classifiers from numerical data can be regarded as one of data mining approaches. The main characteristic is its knowledge representation. Antecedent fuzzy sets of each fuzzy if-then rule linguistically represent a fuzzy region in the pattern space. When a user's main concern is knowledge extraction rather than classifier design, we have to consider two conflicting objectives: accuracy maximization and interpretability maximization. The number of correctly classified training patterns is often used for accuracy measure. On the other hand, interpretability is very subjective and hardly defined without the user's involvement. In this paper, we incorporate user's preference information into multiobjective genetic fuzzy rule selection. We propose a preference function composed of satisfaction level functions on six criteria: average confidence, average coverage, the number of used attributes, the maximum number of used granularities, classification accuracy, and the number of rules. Since it is hard to directly define each satisfaction level function beforehand, it is interactively modified by the user during the evolution. The preference function is handled as one of objective functions in multiobjective genetic fuzzy rule selection. The effectiveness of the proposed method is examined through a case study for knowledge extraction from the Pima Indian Diabetes data.", "venue": "2009 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making(MCDM)", "citationCount": 7, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "732475fb884646b434955242fbc8190970723c64", "url": "https://www.semanticscholar.org/paper/732475fb884646b434955242fbc8190970723c64", "title": "Integrated approach for designing medical decision support systems with knowledge extracted from clinical databases by statistical methods.", "abstract": "In clinical research data is often studied by a particular method without previous analysis of quality or semantic contents which could link clinical database and data analytical (e.g. statistical) procedures. In order to avoid bias caused by this situation, we propose that the analysis of medical data should be divided into two main steps. In the first one we concentrate on conducting the quality, semantic and structure analyses. In the second step our aim is to build an appropriate dictionary of data analysis methods for further knowledge extraction. Methods like robust statistical techniques, procedures for mixed continuous and discrete data, fuzzy linguistic approach, machine learning and neural networks can be included. The results may be evaluated both using test samples and applying other relevant data-analytical techniques to the particular problem under the study.", "venue": "Proceedings. Symposium on Computer Applications in Medical Care", "citationCount": 3, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "209c33234f438d72d0c41c38d05cafdd89d15277", "url": "https://www.semanticscholar.org/paper/209c33234f438d72d0c41c38d05cafdd89d15277", "title": "Exploring the inference role in automatic information extraction from texts", "abstract": "In this paper we present a novel methodology for automatic information extraction from natural language texts, based on the integration of linguistic rules, multiple ontologies and inference resources, integrated with an abstraction layer for linguistic annotation and data representation. The SAURON system was developed to implement and integrate the methodology phases. The knowledge domain of legal realm has been used for the case study scenario through a corpus collected from the State Superior Court website in Brazil. The main contribution presented is related to the exploration of the flexibility of linguistic rules and domain knowledge representation, through their manipulation and integration by a reasoning system. Therefore, it is possible to the system to continuously interact with linguistic and domain experts in order to improve the set of linguistic rules or the ontology components. The results from the case study indicate that the proposed approach is effective for the legal domain.", "venue": "SWAIE@RANLP", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "f15840a41ce2f410e8345b6b13975b04127acc96", "url": "https://www.semanticscholar.org/paper/f15840a41ce2f410e8345b6b13975b04127acc96", "title": "Information Retrieval", "abstract": "Lectures: Monday 14-16 in E-523 Course materials: http://isweb.uni-koblenz.de (teaching) Examination: Oral exam at the end of the semester Outline Motivation and Overview Text processing and analysis Link Analysis and Authority Ranking information Top-K Query Processing and Indexing search Advanced IR Models Multimedia Retrieval Automatic Classification Clustering and Graph Mining Peer-to-Peer Technologies information Information Extraction organization Data Warehouses and OLAP Ontologies and Semantic Web", "venue": "The Practical Handbook of Internet Computing", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["tp", "sw", "ie", "onto"], "mention_counts": {"sw": 1, "onto": 1, "tp": 1, "ie": 1}, "nlp_mention_counts": {"tp": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "3c12a16ac70ae1b1ea40430d9c212ccd1404a5bd", "url": "https://www.semanticscholar.org/paper/3c12a16ac70ae1b1ea40430d9c212ccd1404a5bd", "title": "Time aware knowledge extraction to analyze nanosafety cluster scientific activities", "abstract": "With the rapid development ot biomedical sciences, a growing amount of papers reporting new scientific findings are published and indexed in different unstructured biomedical data sources. In order to really appreciate and effectively benefit from the availability of this amount of data there is an urgent need to support the deployment of intelligent information services, such as: temporal trends and group detection, expert finding, review experts, link prediction, and so on. This need is even more stressed if we analyze dissemination activity of emerging scientific communities that are working on specific research topics in the field of biomedical science. Motivated by the fact that nanotechnologies are one of the key enabling technologies nowadays, in this paper we instantiate and contextualize the Time Aware Knowledge Extraction (TAKE) methodology, introduced in previous work, as a tool to analyze the activities of the nano-safety scientific community coordinated by the EU NanoSafety Cluster (NSC). This methodology enables us to extract timed association rules. To validate and give evidence of the goodness of these rules, a summary of the so obtained results of the analysis is provided identifying distinguishing features, and detecting emerging collaboration among the NSC's Working Groups and their members over the timeline.", "venue": "2016 IEEE Congress on Evolutionary Computation (CEC)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "eb46896c040ec0b242fdf431e044c92f1131c7e8", "url": "https://www.semanticscholar.org/paper/eb46896c040ec0b242fdf431e044c92f1131c7e8", "title": "Extraction of expanded entity phrases", "abstract": "This research is part of a larger integrated approach for extraction of information of interest from free text and the visualization of semantic relatedness between phrases of interest. This paper defines a new structure which is a key component, the expanded entity phrase (EPx). This paper also presents an approach for extracting EPx's from free text. The structure of the EPx's facilitates quantitative comparison with other EPx's. A combination of part of speech-based template matching and ontology-driven NLP provides an effective technique for extracting complex entity structures that cross clause boundaries. This approach also uses ontology-based inferences to lay the ground work for linking EPx's for semantic relatedness assessments involving different named entities not explicitly stated in the text. The real world data used in this research were derived from a collection of law enforcement email messages submitted by hundreds of investigators seeking information or posting information about crimes, incidents, requests, and announcements. Performance data on the approaches used for extracting EPx's and links from this data are presented.", "venue": "Proceedings of 2011 IEEE International Conference on Intelligence and Security Informatics", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "nlp", "onto"], "mention_counts": {"nlp": 1, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "3e85393c8329a5918a404c23bcd928e04aeea404", "url": "https://www.semanticscholar.org/paper/3e85393c8329a5918a404c23bcd928e04aeea404", "title": "Applied Harmonic Analysis and Data Processing", "abstract": "Massive data sets have their own architecture. Each data source has an inherent structure, which we should attempt to detect in order to utilize it for applications, such as denoising, clustering, anomaly detection, knowledge extraction, or classification. Harmonic analysis revolves around creating new structures for decomposition, rearrangement and reconstruction of operators and functions\u2014in other words inventing and exploring new architectures for information and inference. Two previous very successful workshops on applied harmonic analysis and sparse approximation have taken place in 2012 and in 2015. This workshop was the an evolution and continuation of these workshops and intended to bring together world leading experts in applied harmonic analysis, data analysis, optimization, statistics, and machine learning to report on recent developments, and to foster new developments and collaborations. Mathematics Subject Classification (2010): 42-XX, 65Txx, 94Axx, 65K05, 15A52. Introduction by the Organisers The workshop Applied Harmonic Analysis and Data Processing was organized by Ingrid Daubechies (Durham), Gitta Kutyniok (Berlin), Holger Rauhut (Aachen) and Thomas Strohmer (Davis). This meeting was attended by 49 participants from three continents. Advances in technology and the ever-growing role of digital sensors and computers in science have led to an exponential growth in the amount and complexity of data we collect. Uncertainty, scale, non-stationarity, noise, and heterogeneity are fundamental issues impeding progress at all phases of the pipeline that creates knowledge from data. This means that the amount of new 724 Oberwolfach Report 14/2018 mathematical challenges arising from the need of data analysis and information processing is enormous, with their solution requiring fundamentally new ideas and approaches, with significant consequences in the practical applications. Applied Harmonic Analysis provides a range of techniques towards the problem of efficiently representing, analyzing, compressing, and processing with \u201cBig Data\u201d. Massive data sets have their own architecture. Each data source has an inherent structure, which we should attempt to detect in order to utilize it for applications, such as denoising, clustering, anomaly detection, knowledge extraction, recovery, etc. Harmonic analysis revolves around creating new structures for decomposition, rearrangement and reconstruction of operators and functions\u2014in other words inventing and exploring new architectures for information and inference. Indeed, in the last three decades Applied Harmonic Analysis has been at the center of many significant new ideas and methods crucial in a wide range of signal and image processing applications, and in the analysis and processing of large data sets. For example, compressive sensing, sparse approximations and models, geometric multiscale analysis and diffusion geometry represent some quite recent important breakthroughs. Several new directions have emerged on the heels of compressive sensing: Lowrank matrix recovery aims at recovering a matrix with small rank from incomplete data. In particular, matrix completion recovers the matrix from only a small fraction of its entries. Since low-rank structures arise in numerous applications, one can expect an enormous impact. However, much of the theory so far deals with linear measurements, while in practice we often also face non-linear measurements, for instance in situations where only signal intensity can be obtained. Despite recent breakthroughs in the area of phase retrieval, many challenging mathematical problems problems remain open in these areas. Inverse problems arising in connection with massive, complex data sets pose tremendous challenges and require new mathematical tools. Numerous deep questions arise. How can we utilize ideas of sparsity and minimal information complexity in this context? Is there a unified view of such measures that would include sparsity, lowrankness, and others (such as low-entropy), as special cases? This may lead to a new theory that considers an abstract notion of simplicity in general inverse problems. An important emerging topic in this context is the design efficient non-convex algorithms with provable convergence guarantees. One of the most exciting developments in machine learning in the past five years is the advent of deep learning, which is a special form of a neural network. Deep neural networks, and in particular convolutional networks have recently achieved state-of-the-art results on several complex computer vision and speech recognition tasks. However, until now deep learning acts very much like a black box, since algorithms are often based on ad hoc rules without theoretical foundation, the learned representations lack intepretability; we do not really understand why certain deep networks succeed and and we do not know how to modify them for those cases where they fail. Thus, developing a mathematical foundation for deep Applied Harmonic Analysis and Data Processing 725 learning is an important and rather challening task in data science, and one part of this workshop was dedicated to this topic. This workshop was a concerted effort to bring together researchers with various backgrounds, including harmonic analysis, optimization, probabibility theory, group theory, approximation theory, computer science, machine learning, and electrical engineering. The workshop featured 27 talks, thereof several longer overview talks. Moreover, a session of short presentations of 3 minutes took place on Monday, which we call the 3 Minutes of Fame (following Andy Warhols concept of 15 minutes of fame). This session has meanwhile become a tradition and has proven to be an efficient vehicle to ensure that every participant had the possibility to advertise her resarch. At the same time it is very entertaining for the audience. Almost all of the attendees participated, ranging from PhD students to renowned professors, contributing to the success of this session. Some highlights of the program included: \u2022 Advanced sampling theory: One of the problems that link harmonic analysis with data processing is the sampling problem. The main theoretical issue is how the stability of sampling and recovery is related to the number or density of samples. Related issues are the questions of localization, non-uniform sampling, and last not least suitable numerical algorithms. Karlheinz Gr\u00f6chenig presented a range of compelling results using tools from shift-invariant spaces and totally positive functions. Albert Cohen discussed function approximation from sampling in high dimensions using optimal weighted least squares approximation. Felix Krahmer talked about \u201cunlimited sampling\u201d, a mathematical framework for sampling that can overcome limitations in current analog-to-digital converters. \u2022 Nonlinear inverse problems: In many applications we can only acquire nonlinear measurements of the function of interest. Phase retrieval is but the most prominent example. Several talks were dedicated to nonlinear inverse problems. Babak Hassibi and Rima Alaifari both presented recent progress in the solution of the phase retrieval problem, while Yuxin Chen and Justin Romberg highlighted exciting progress in convex and nonconvex optimization for certain nonlinear problems. \u2022 Emerging theory of Deep Learning: Despite the huge practical successes of Deep Learning in recent years, the mathematical understanding of deep learning is in its infancy. Several talks aimed at to remedy this situation. Philipp Grohs demonstrated how to avoid the curse of dimensionality when solving Kolmogorov equation in high dimensions by means of deep learning. Mahdi Soltanolkoltabi and Remi Gribonval were among several speakers who presented theoretical progress towards understanding some of the heuristics behind neural networks. The organizers would like to take the opportunity to thank MFO for providing support and a very inspiring environment for the workshop. The magic of the place and the pleasant atmosphere contributed greatly to the success of the workshop. 726 Oberwolfach Report 14/2018 Acknowledgement: The MFO and the workshop organizers would like to thank the National Science Foundation for supporting the participation of junior researchers in the workshop by the grant DMS-1641185, \u201cUS Junior Oberwolfach Fellows\u201d. Applied Harmonic Analysis and Data Processing 727 Workshop: Applied Harmonic Analysis and Data Processing", "venue": "Oberwolfach Reports", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "7ecbc94c07da2f67763fb885c2344132c8da059b", "url": "https://www.semanticscholar.org/paper/7ecbc94c07da2f67763fb885c2344132c8da059b", "title": "PaloPro: a platform for knowledge extraction from big social data and the news", "abstract": "PaloPro is a platform that aggregates textual content from social media and news sites in different languages, analyses them using a series of text mining algorithms and provides advanced analytics to journalists and social media marketers. The platform capitalises on the abundance of social media sources and the information they provide for persons, products and events. In order to handle huge amounts of multilingual data that are collected continuously, we have adopted language independent techniques at all levels and from an engineering point of view, we have designed a system that takes advantage of parallel distributed computing technologies and cloud infrastructure. Different systems handle data aggregation, data processing and knowledge extraction and others deal with the integration and visualisation of knowledge. In this paper, we focus on two important text mining tasks, named entity recognition from texts and sentiment analysis to extract the sentiment associated with the corresponding identified entities.", "venue": "Int. J. Big Data Intell.", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "df0cbceff7f1e82af6273080bed34e90e2057a42", "url": "https://www.semanticscholar.org/paper/df0cbceff7f1e82af6273080bed34e90e2057a42", "title": "Grammar and Dictionary based Named-entity Linking for Knowledge Extraction of Evidence-based Dietary Recommendations", "abstract": "In order to help people to follow the new knowledge about healthy diet that comes rapidly each day with the new published scientific reports, a grammar and dictionary based named-entity linking method is presented that can be used for knowledge extraction of evidence-based dietary recommendations. The method consists of two phases. The first one is a mix of entity detection and determination of a set of candidates for each entity, and the second one is a candidate selection. We evaluate our method using a corpus from dietary recommendations presented in one sentence provided by the World Health Organization and the U.S. National Library of Medicine. The corpus consists of 50 dietary recommendations and 10 sentences that are not related with dietary recommendations. For 47 out of 50 dietary recommendations the proposed method extract all the useful knowledge, and for remaining 3 only the information for one entity is missing. Due to the 10 sentences that are not dietary recommendation the method does not extract any entities, as expected.", "venue": "International Conference on Knowledge Discovery and Information Retrieval", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "77a958861a8692b977dbffc39df8b436b9ec70df", "url": "https://www.semanticscholar.org/paper/77a958861a8692b977dbffc39df8b436b9ec70df", "title": "Fulfilling information needs of patients in online health communities.", "abstract": "BACKGROUND\nOnline health communities (OHCs) experience difficulties in utilising patient reported posts to fulfil the information needs of online patients concerning health related issues.\n\n\nOBJECTIVES\nWe aim to propose a comprehensive method that leverages medical domain knowledge to extract useful information from posts to fulfil information needs of online patients.\n\n\nMETHODS\nA knowledge representation framework based on authoritative knowledge sources in the medical field for the OHC is proposed. On the basis of the framework, a health related information extraction process for analysing the posts in the OHC is proposed. Then, knowledge support rate (KSR) and effective information rate (EIR) are introduced as metrics to evaluate changes in knowledge extracted from the knowledge sources in terms of fulfilling the information needs of patients in the OHC.\n\n\nRESULTS\nOn the basis of a data set with 372\u00a0343 posts in an OHC, experimental results indicate that our method effectively extracts relevant knowledge for online patients. Moreover, KSR and EIR are feasible metrics of changes in knowledge in terms of fulfilling the information needs.\n\n\nCONCLUSIONS\nThe OHCs effectively fulfil the information needs of patients by utilising authoritative domain knowledge in the medical field. Knowledge based services for online patients facilitate an intelligent OHC in the future.", "venue": "Health Information and Libraries Journal", "citationCount": 7, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["ke", "kg", "ie"], "mention_counts": {"ke": 1, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.5}, {"paperId": "166a9f13ff56343e9f5c68d88638775d57893fb0", "url": "https://www.semanticscholar.org/paper/166a9f13ff56343e9f5c68d88638775d57893fb0", "title": "Learning Models for Concept Extraction From Images With Drug Labels for a Unified Knowledge Base Utilizing NLP and IoT Tasks", "abstract": "The evolution of humankind is through the exchange of information and extraction of knowledge from available information. The process of exchange of the information differs by the probability of the medium through which the information is exchanged. The Internet of things (IoT) contains millions of devices with sensors simultaneously transferring real time information to devices as rapid streams of data that need to be processed on the go. This leads to the need for development of effective and efficient approaches for segregating data based on class, relatedness, and differences in the information. The extraction of text from images is performed through tesseract irrespective of the language. SCIBERT models to extract scientific information and evaluating on a suite of tasks specially in classifying drugs based on free data (tweets, images, etc.). The images and text-based semantic similarity analysis provide similar drugs grouped together by composition or manufacturer.", "venue": "International Journal of Information Technology and Web Engineering", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "nlp"], "mention_counts": {"nlp": 1, "kg": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.5}, {"paperId": "7ec401783e1e34286293e161e7148e509dc3e005", "url": "https://www.semanticscholar.org/paper/7ec401783e1e34286293e161e7148e509dc3e005", "title": "Prediction in OLAP Data Cubes", "abstract": "Online analytical processing (OLAP) provides tools to explore data cubes in order to extract the interesting information, it refers to techniques used to query, visualise and synthesise the multidimensional data. Nevertheless OLAP is limited on visualisation, structuring and exploring manually the data cubes. On the other side, data mining allows algorithms that offer automatic knowledge extraction, such as classification, explanation and prediction algorithms. However, OLAP is not capable of explaining and predicting events from existing data; therefore, it is possible to make a more efficient online analysis by coupling data mining and OLAP to allow the user to assist in this new task of knowledge extraction. In this paper, we will carry on within works achieved in this theme and we suggest to extend the abilities of OLAP to prediction (enhancing the OLAP abilities and techniques by introducing a predictive model based on a data mining algorithms). The model is calculated on the aggregated data, and prediction is done on detailed missing data. Our approach is based on regression trees and neural networks; it consists to predict facts having a missed measures value in the data cubes. The user will have in his disposition, a new platform called PredCube, that offers the possibility to query, visualise and synthesise the multidimensional data, and also to predict missing values in the data cube using three data mining methods, and evaluate the quality of the prediction by comparing the average error and the execution time given by each one.", "venue": "Journal of Information & Knowledge Management", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "f2f1d3bc8d0791100a39b414c5b818b024241ce4", "url": "https://www.semanticscholar.org/paper/f2f1d3bc8d0791100a39b414c5b818b024241ce4", "title": "Urban Knowledge Extraction, Representation and Reasoning as a Bridge from Data City towards Smart City", "abstract": "Urban Data management represents a major challenge in the field of Smart Cities. Its understanding is essential for the development of better smart services, which are a persistent demand in urban policies. From all the sources of data available, those that involve a collective processing of urban information (by the citizens or other collectives) deliver in fact, useful insights into social perception. Such is the case, for example, of data collected from mobile networks. Prior to the design of sociotechnical artifacts in cities, it seems important to extract the qualitative, quantitative opinions, sentiment, feedbacks present in these data. In this paper we present three solutions for mining these contents through Knowledge Extraction methods, as a previous step to the prospection of new smart services.", "venue": "2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "094e86fc71a735ee28dca7547b33d4746a567a27", "url": "https://www.semanticscholar.org/paper/094e86fc71a735ee28dca7547b33d4746a567a27", "title": "Space-Time Summarization of Multisensor Time Series. Case of Missing Data", "abstract": "A wide variety of application domains have to deal with incomplete data sets. In particular, data from sensors networks are often incomplete due to factors like partial system failures or bad conditions of measurements. With such incomplete massive spatio-temporal data sets, it becomes practically hard to manipulate data and to extract knowledge. In this paper, we use the so-called Space-Time Principal Component Analysis (STPCA) as a tool for propose a representation of the data set without missing values in a reduced dimension on which we can apply data mining and knowledge extraction algorithms. The effectiveness of the proposed method is demonstrated on real vehicle traffic data set containing about 15 million of measurements with rate of incompleteness of order 20% and more. Experiments show a really good behavior and strong robustness of the method to compute a representation of the data, summarize them and keep the inherent information.", "venue": "Seventh IEEE International Conference on Data Mining Workshops (ICDMW 2007)", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "8d996204359e2d13a71252676aed0a8f4e4e0b31", "url": "https://www.semanticscholar.org/paper/8d996204359e2d13a71252676aed0a8f4e4e0b31", "title": "A novel approach for extraction and representation of main data from web pages to Android application", "abstract": "World Wide Web is the largest and richest information repository available today. The enormous growth of web makes large scale analysis difficult. An automated method is needed for analyzing and extracting knowledge from the web, along with samples of the knowledge that can be extracted, without user intervention. Data mining techniques are applied to extract knowledge from web data. Web data is text, image, records, hyperlinks, tags et. Rich Site Summary (RSS) is a way to get briefed items on a website as the web site gets updated, and these are called feeds. A feed is often a series of headlines and brief summaries of all the articles published on a web page. Instead of having to visit numerous web sites to get weather, sports, latest gossip, or latest political debates etc, just go to the mobile application and see it combined and aggregated into a single window with a good user interface. An application is developed for the popular mobile platform Android.", "venue": "2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "67af52ebf8ac224c39e5dcad9d998a7e69865340", "url": "https://www.semanticscholar.org/paper/67af52ebf8ac224c39e5dcad9d998a7e69865340", "title": "Quality not Quantity! A Qualitative Evaluation and Proposal for Understanding the Depth of Audience \u201cKnowledge\u201d Post Data Extraction", "abstract": "Knowledge is defined as\u2026the result of machine extracted patterns; humans making sense of their environment; information generated and aggregated from software services or as the lowest form of human cognition. Different perspectives, different domains, but one concept. Information scientists are often concerned with retrieving knowledge from data sources and sharing that knowledge with concerned stakeholders; with such differing views on what qualifies as knowledge a cross-domain approach might prove beneficial. This work is a qualitative assessment of the layers of knowledge intended to bridge the gap between the analyst and their intended or unintended audiences. It examines the benefit of abstracting concepts used in the education discipline to justify including a post-evaluation stage to the Knowledge Discovered through Databases (KDD) framework. It also intends to promote awareness of the various human cognitive capacities and provide a useful approach for communicating and evaluating machine-extracted knowledge that supports higher order thinking.", "venue": "2020 IEEE 21st International Conference on Information Reuse and Integration for Data Science (IRI)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "d9b2109332e993fbf6ca68d4dccf44fe3c28f6a3", "url": "https://www.semanticscholar.org/paper/d9b2109332e993fbf6ca68d4dccf44fe3c28f6a3", "title": "Conceptualization of the tabular representation of knowledge", "abstract": "Tabular representation is used as a basis for knowledge extraction. The structure of knowledge is built from a generalized concept to data structures that are used in applied software. The resulting formalizations allow for control of information in the form of natural language texts (regulatory documents), databases and spreadsheets of automated transport systems. The knowledge extracted from the tabular representation serves as the basis for decision-making and data mining.", "venue": "International Conference on Computer Science and Information Technologies", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "6dac26eba5149ba14fb34d8c142541154b4c16aa", "url": "https://www.semanticscholar.org/paper/6dac26eba5149ba14fb34d8c142541154b4c16aa", "title": "A Transformative Concept: From Data Being Passive Objects to Data Being Active Subjects", "abstract": "The exploitation of potential societal benefits of Earth observations is hampered by users having to engage in often tedious processes to discover data and extract information and knowledge. A concept is introduced for a transition from the current perception of data as passive objects (DPO) to a new perception of data as active subjects (DAS). This transition would greatly increase data usage and exploitation, and support the extraction of knowledge from data products. Enabling the data subjects to actively reach out to potential users would revolutionize data dissemination and sharing and facilitate collaboration in user communities. The three core elements of the transformative DAS concept are: (1) \u201cintelligent semantic data agents\u201d (ISDAs) that have the capabilities to communicate with their human and digital environment. Each ISDA provides a voice to the data product it represents. It has comprehensive knowledge of the represented product including quality, uncertainties, access conditions, previous uses, user feedbacks, etc., and it can engage in transactions with users. (2) A knowledge base that constructs extensive graphs presenting a comprehensive picture of communities of people, applications, models, tools, and resources and provides tools for the analysis of these graphs. (3) An interaction platform that links the ISDAs to the human environment and facilitates transaction including discovery of products, access to products and derived knowledge, modifications and use of products, and the exchange of feedback on the usage. This platform documents the transactions in a secure way maintaining full provenance.", "venue": "International Conference on Data Technologies and Applications", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "kg", "ke"], "mention_counts": {"kg": 1, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.5}, {"paperId": "2d334c37c9e9840d8a032fa48d8c9bce8fffc197", "url": "https://www.semanticscholar.org/paper/2d334c37c9e9840d8a032fa48d8c9bce8fffc197", "title": "Sketch+ for Visual and Correlation-Based Exploratory Data Analysis: A Case Study with COVID-19 Databases", "abstract": "The amount of data daily generated by different sources grows exponentially and brings new challenges to the information technology experts. The recorded data usually include heterogeneous attribute types, such as the traditional date, numerical, textual, and categorical information, as well as complex ones, such as images, videos, and multidimensional data. Simply posing similarity queries over such records can underestimate the semantics and potential usefulness of particular attributes. In this context, the Exploratory Data Analysis (EDA) technology is well-suited to understand data and perform knowledge extraction and visualization of existing patterns. In this paper, we propose Sketch+ , a technique and a corresponding supporting tool to compare electronic health records (provided by hospitals) by similarity, supporting correlation-based exploratory analysis over attributes of different types and allowing data preprocessing tasks for visualization and knowledge extraction. Sketch+ computes partial and overall data correlation considering distance spaces induced by the attributes. It employs both ANOVA and association rules with lift correlations to study relationships between variables, allowing extensive data analysis. Among the tools provided, a pixel-oriented one drives the analysts to observe visual correlations among dates, categorical and numerical attributes. As a running case study, we employed three open databases of COVID-19 cases, showing that specialists can benefit from the inference modules of Sketch+ to analyze electronic records. The study highlights how Sketch+ can be employed to spot strong correlations among tuples and attributes, with statistically significant results. The exploratory analysis has been shown to be an essential complement for similarity search tasks, identifying and evaluating patterns from heterogeneous attributes.", "venue": "J. Inf. Data Manag.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "852fcf4dd65fe2534e426bf3fbe76dd64c95a52f", "url": "https://www.semanticscholar.org/paper/852fcf4dd65fe2534e426bf3fbe76dd64c95a52f", "title": "Towards a Training-Oriented Adaptive Decision Guidance and Support System", "abstract": "Information systems today have become incredibly complex and span multiple organizational networks, database and applications servers and on to the external Internet cloud resources. Consequently strategic approaches are needed to troubleshoot system failures by first identifying the component causing the failure, and thereby, further investigating the cause of the failure to solve the problem. Information regarding past troubleshooting strategies can be used to provide guidance for solving similar problems. We present a framework, DSDAware (Decision Support for Database Administrators using Warehouse-as-a-service) for developing a Decision Guidance and Support System (DGSS). The framework dynamically extracts knowledge from various correlated data sources containing systems related data and from the problem solving procedures of the human experts. The knowledge is used in a strategic problem solving approach to train new administrators by guiding them through the troubleshooting process using an interactive interface, and to offer a decision support service to the Web community. Our work specifically focuses on z/OS Mainframe DB2 database (DB) problems where the inherent complexity of the system makes troubleshooting a challenging task. The diminishing population of mainframe DB administrators (DBA) asserts the need for a DGSS for the new DBAs. The research applies text and data mining techniques for knowledge extraction, a rule-based system for knowledge representation and problem categorization, and a case-based system for providing decision support.", "venue": "2012 IEEE 28th International Conference on Data Engineering Workshops", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "bdbbc063489be3db1d533fcb0ef18152cf30b374", "url": "https://www.semanticscholar.org/paper/bdbbc063489be3db1d533fcb0ef18152cf30b374", "title": "Information extraction and integration from heterogeneous, distributed, autonomous information sources - a federated ontology-driven query-centric approach", "abstract": "This paper motivates and describes the data integration component of INDUS (intelligent data understanding system) environment for data-driven information extraction and integration from heterogeneous, distributed, autonomous information sources. The design of INDUS is motivated by the requirements of applications such as scientific discovery, in which it is desirable for users to be able to access, flexibly interpret, and analyze data from diverse sources from different perspectives in different contexts. INDUS implements a federated, query-centric approach to data integration using user-specified ontologies.", "venue": "Proceedings Fifth IEEE Workshop on Mobile Computing Systems and Applications", "citationCount": 81, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "1b4f25939fc75f59028a222e85dd251561f36f90", "url": "https://www.semanticscholar.org/paper/1b4f25939fc75f59028a222e85dd251561f36f90", "title": "Information Extraction from Twitter Using DBpedia Ontology: Indonesia Tourism Places", "abstract": "As the popular microblogs of social media, Twitter provides a number of short messages of interesting information, like the mention of entities to promote local cultures and tourism places in order for others to know and visit the places. However, the entities mentioned within the tweet may be interpreted differently and vary depending on the context of the tweet. To find the appropriate context of the entities, we need the system that can perform semantic annotation for every entity, especially for the tourism places. In this paper, we provide Named Entity Recognition used to define the candidate entities of the places. Furthermore, by using DBpedia Ontology as a semantic annotation resources we construct the information extraction from the context and semantic of the entities to collect the URI of Wikipedia page of the place. The experiment result showed the system was capable to extract the mentioned place and annotated the URI of Wikipedia with high accuracy which defined by 0.95 of Precision, 0.79 of Recall, and 0.87 of F-Measure.", "venue": "2019 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "0b3f7261ab77b07310fe26ca6e00e1e0369e985d", "url": "https://www.semanticscholar.org/paper/0b3f7261ab77b07310fe26ca6e00e1e0369e985d", "title": "Prior Knowledge for Predictive Modeling: The Case of Acute Aquatic Toxicity", "abstract": "Early assessment of the potential impact of chemicals on health and the environment requires toxicological properties of the molecules. Predictive modeling is often used to estimate the property values in silico from pre-existing experimental data, which is often scarce and uncertain. One of the ways to advance the predictive modeling procedure might be the use of knowledge existing in the field. Scientific publications contain a vast amount of knowledge. However, the amount of manual work required to process the enormous volumes of information gathered in scientific articles might hinder its utilization. This work explores the opportunity of semiautomated knowledge extraction from scientific papers and investigates a few potential ways of its use for predictive modeling. The knowledge extraction and predictive modeling are applied to the field of acute aquatic toxicity. Acute aquatic toxicity is an important parameter of the safety assessment of chemicals. The extensive amount of diverse information existing in the field makes acute aquatic toxicity an attractive area for investigation of knowledge use for predictive modeling. The work demonstrates that the knowledge collection and classification procedure could be useful in hybrid modeling studies concerning the model and predictor selection, addressing data gaps, and evaluation of models\u2019 performance.", "venue": "Journal of Chemical Information and Modeling", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "2c67b238299fc33aba2ecab2622eeedc7c64f5e2", "url": "https://www.semanticscholar.org/paper/2c67b238299fc33aba2ecab2622eeedc7c64f5e2", "title": "AeroDAML: Applying Information Extraction to Generate DAML Annotations from Web Pages", "abstract": "The DARPA Agent Markup Language (DAML) is an emerging knowledge representation for the Semantic Web. DAML can encode the semantics of a document for use by agents on the web. However, DAML annotation of documents and web pages is a tedious and time consuming task. AeroDAML is a knowledge markup tool that applies natural language information extraction techniques to automatically generate DAML annotations from web pages. AeroDAML links most proper nouns and common relationships with classes and properties in DAML ontologies. This paper discusses the design of AeroDAML including linguistic and practical issues related to semantic annotation.", "venue": "Semannot@K-CAP 2001", "citationCount": 167, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "sw"], "mention_counts": {"sw": 1, "onto": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "87906ba455ad9379c238dd306594d22d8161fa7c", "url": "https://www.semanticscholar.org/paper/87906ba455ad9379c238dd306594d22d8161fa7c", "title": "Towards Semantic Understanding -- An Approach Based on Information Extraction Ontologies", "abstract": "Information is ubiquitous, and we are flooded with more than we can process. Somehow, we must rely less on visual processing, point-and-click navigation, and manual decision making and more on computer sifting and organization of information and automated negotiation and decision making. A resolution of these problems requires software with semantic understanding---a grand challenge of our time.More particularly, we must solve problems of automated interoperability, integration, and knowledge sharing, and we must build information agents and process agents that we can trust to give us the information we want and need and to negotiate on our behalf in harmony with our beliefs and goals.This paper proffers the use of information-extraction ontologies as an approach that may lead to semantic understanding.", "venue": "Australasian Database Conference", "citationCount": 126, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "ec59845da9ee7efd33787b23f2f7c4be532da00e", "url": "https://www.semanticscholar.org/paper/ec59845da9ee7efd33787b23f2f7c4be532da00e", "title": "Inside YAGO2s: a transparent information extraction architecture", "abstract": "YAGO [9, 6] is one of the largest public ontologies constructed by information extraction. In a recent refactoring called YAGO2s, the system has been given a modular and completely transparent architecture. In this demo, users can see how more than 30 individual modules of YAGO work in parallel to extract facts, to check facts for their correctness, to deduce facts, and to merge facts from different sources. A GUI allows users to play with different input files, to trace the provenance of individual facts to their sources, to change deduction rules, and to run individual extractors. Users can see step by step how the extractors work together to combine the individual facts to the coherent whole of the YAGO ontology.", "venue": "The Web Conference", "citationCount": 70, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "89ef240277f7b6215ceb92464e164f0d7d28762e", "url": "https://www.semanticscholar.org/paper/89ef240277f7b6215ceb92464e164f0d7d28762e", "title": "Integrating Syntactic and Semantic Analysis into the Open Information Extraction Paradigm", "abstract": "In this paper we present an approach aimed at enriching the Open Information Extraction paradigm with semantic relation ontologization by integrating syntactic and semantic features into its workflow. To achieve this goal, we combine deep syntactic analysis and distributional semantics using a shortest path kernel method and soft clustering. The output of our system is a set of automatically discovered and ontologized semantic relations.", "venue": "International Joint Conference on Artificial Intelligence", "citationCount": 47, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "e5a986f30211ca454c33de5cdfd0b8d1b11e3da8", "url": "https://www.semanticscholar.org/paper/e5a986f30211ca454c33de5cdfd0b8d1b11e3da8", "title": "BOEMIE: Reasoning-based Information Extraction", "abstract": "This paper presents a novel approach for exploiting an ontology in an ontology-based information extraction system, which substitutes part of the extraction process with reasoning, guided by a set of automatically acquired rules.", "venue": "NLPAR@LPNMR", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "b726e48ebb2a667d8fa6fd7745ab38f6b9dcb241", "url": "https://www.semanticscholar.org/paper/b726e48ebb2a667d8fa6fd7745ab38f6b9dcb241", "title": "Hybrid Ontology-Based Information Extraction for Automated Text Grading", "abstract": "Although automatic text grading systems have reached an accuracy level comparable to human grading, with successful commercial and research implementations (e.g., Latent Semantic Analysis), these systems can provide limited feedback about which statements of the text are incorrect and why they are incorrect. In the present work, we propose the use of a hybrid Ontology-based Information Extraction (OBIE) system to identify both correct and incorrect statements by combining extraction rules and machine learning based information extractors. Experiments show that given 77 student answers to a Cell Biology final exam question, our hybrid system can identify both correct and incorrect statements with high precision and recall measures.", "venue": "2013 12th International Conference on Machine Learning and Applications", "citationCount": 22, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "fb10c8f39c1abb8193bbf866fad6e94275715668", "url": "https://www.semanticscholar.org/paper/fb10c8f39c1abb8193bbf866fad6e94275715668", "title": "Linguistic information extraction for job ads (SIRE project)", "abstract": "As a text, each job advertisement expresses rich information about the occupation at hand, such as competence needs (i.e. required degrees, field knowledge, task expertise or technical skills). To facilitate the access to this information, the SIRE project conducted a corpus based study of how to articulate HR expert ontologies with modern semi-supervised information extraction techniques. An adaptive semantic labeling framework is developed through a parallel work on retrieval rules and on latent semantic lexicons of terms and jargon phrases. In its operational stage, our prototype will collect online job ads and index their content into detailed RDF triples compatible with applications ranging from enhanced job search to automated labor-market analysis.", "venue": "RIAO Conference", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "onto", "ie", "ie"], "mention_counts": {"onto": 1, "rdf": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 1, "rdf": 1}, "relevance_score": 0.5}, {"paperId": "53c5557a79400e20cff82d19bbd717ee12c14771", "url": "https://www.semanticscholar.org/paper/53c5557a79400e20cff82d19bbd717ee12c14771", "title": "Multimedia information extraction from HTML product catalogues", "abstract": "We describe a demo application of information extraction from company websites, focusing on bicycle product offers. A statistical approach (Hidden Markov Models) is used in combination with different ways of image classification, including latent semantic analysis of image collections. Ontological knowledge is used to group the extracted items into structured objects. The results are stored in an RDF repository and made available for structured search.", "venue": "DATESO", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "rdf", "ie", "ie"], "mention_counts": {"onto": 1, "rdf": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 1, "rdf": 1}, "relevance_score": 0.5}, {"paperId": "e97220eba545d1bf04229e606ceed78f5b267f32", "url": "https://www.semanticscholar.org/paper/e97220eba545d1bf04229e606ceed78f5b267f32", "title": "Deriving a multi-domain information extraction system from a rough ontology", "abstract": "This paper presents a multi-domain information extraction system. In order to decrease the time spent on the elaboration of resources for the IE system and guide the end-user in a new domain, we suggest to use a machine learning system that helps defining new templates and associated resources. This knowledge is automatically derived from the text collection, in interaction with the end-user to rapidly develop a local ontology giving an accurate image of the content of the text. The system is finally evaluated using classical indicators.", "venue": "IJCAI", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "3e39c81ae2e588af694f0b6052176797b0ca10ca", "url": "https://www.semanticscholar.org/paper/3e39c81ae2e588af694f0b6052176797b0ca10ca", "title": "An Ontology-Based Information Extraction (OBIE) Framework for Analyzing Initial Public Offering (IPO) Prospectus", "abstract": "With the large amounts of information associated with the Initial Public Offering (IPO) process, an intelligent tool is needed for assisting the decision-making activities for both the investors and the underwriters. Even though a large body of related studies exists in extant literature, minimum attention has been devoted to the aspect of understanding hidden semantics within the informative contents of IPO prospectus. In this paper, we present a framework for processing the textual content of IPO prospectus based on an emerging technique named Ontology Based Information Extraction (OBIE). Preliminary results indicates that the framework is capable of meeting the design requirements identified. Moreover, lessons learned during the design and implementation span technical and organizational considerations and can serve as guidance for future research and development in related areas.", "venue": "2014 47th Hawaii International Conference on System Sciences", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "e22342e5ebb04a0c20f49a4c13fd1a54f90aaf9f", "url": "https://www.semanticscholar.org/paper/e22342e5ebb04a0c20f49a4c13fd1a54f90aaf9f", "title": "Ontology-based information extraction for subject-focussed automatic essay evaluation", "abstract": "Automatic essay evaluation (AEE) systems are designed to assist a teacher in the task of classroom assessment in order to alleviate the demands of manual subject evaluation. However, although numerous AEE systems are available, most of these systems do not use elaborate domain knowledge for evaluation, which limits their ability to give informative feedback to students and also their ability to constructively grade a student based on a particular domain of study. This paper is aimed at improving on the achievements of previous studies by providing a subject-focussed evaluation system that considers the domain knowledge while scoring and provides informative feedback to its user. The study employs a combination of techniques such as system design and modelling using Unified Modelling Language (UML), information extraction, ontology development, data management, and semantic matching in order to develop a prototype subject-focussed AEE system. The developed system was evaluated to determine its level of performance and usability. The result of the usability evaluation showed that the system has an overall mean rating of 4.17 out of maximum of 5, which indicates \u2018good usability\u2019. In terms of performance, the assessment done by the system was also found to have sufficiently high correlation with those done by domain experts, in addition to providing appropriate feedback to the user.", "venue": "2017 International Conference on Computing Networking and Informatics (ICCNI)", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "54df9a3f12fa4810a94a7a5929d0cc7a672b13c4", "url": "https://www.semanticscholar.org/paper/54df9a3f12fa4810a94a7a5929d0cc7a672b13c4", "title": "Information to Wisdom: Commonsense Knowledge Extraction and Compilation", "abstract": "Commonsense knowledge is a foundational cornerstone of artificial intelligence applications. Whereas information extraction and knowledge base construction for instance-oriented assertions, such as Brad Pitt's birth date, or Angelina Jolie's movie awards, has received much attention, commonsense knowledge on general concepts (politicians, bicycles, printers) and activities (eating pizza, fixing printers) has only been tackled recently. In this tutorial we present state-of-the-art methodologies towards the compilation and consolidation of such commonsense knowledge (CSK). We cover text-extraction-based, multi-modal and Transformer-based techniques, with special focus on the issues of web search and ranking, as of relevance to the WSDM community.", "venue": "WSDM", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "kg"], "mention_counts": {"ke": 1, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.5}, {"paperId": "6ebb8240f1f7af088b3f8f0b50178e08faa0d02d", "url": "https://www.semanticscholar.org/paper/6ebb8240f1f7af088b3f8f0b50178e08faa0d02d", "title": "Towards Ontology-based Information Extraction and Annotation of Paper Documents for Personalized Knowledge Acquisition", "abstract": "Despite the advent of electronic personal information management (PIM) tools, knowledge workers are still heavily using paper-based information sources. But up to now, even in sophisticated tools for PIM such as the Semantic Desktop, the knowledge workers\u2019 paper world is still neglected. Thus, electronic archiving of a web page for later reference is much easier than taking care of an interesting article in a magazine\u2014whose copy might set dust on the user\u2019s shelf and will long be forgotten when it would be helpful for a specific task. This paper presents how to use document analysis, ontology-based information extraction, and annotation techniques for personal knowledge acquisition in order to bridge the gap between the user\u2019s paper world and his personal knowledge space in the Semantic Desktop. A recent prototype shows the feasibility of the approach.", "venue": "Wissensmanagement", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "8bf44d2e0e50250b2c9a6be7b5c809b5bd5391db", "url": "https://www.semanticscholar.org/paper/8bf44d2e0e50250b2c9a6be7b5c809b5bd5391db", "title": "Arabic collocations extraction using Gate", "abstract": "Information extraction (IE) from corpora is texts analysis in order to extract structured information such as Named Entities (NE) which may be names of person, organization, address, date, location etc. \u2026 GATE is a software toolkit written in Java from 1995 and widely used worldwide by many communities (scientists, companies, teachers, students) for natural language processing. We have experimented Gate for extracting terms by writing new Jape rules (Java Annotation Pattern Engine) and used them on a tagged corpus developed at Leeds University. These terms will be used in the texts-based ontologies building. In our case this ontology will be incorporated into a search engine to expand queries on the Web, in the specified domain.", "venue": "2010 International Conference on Machine and Web Intelligence", "citationCount": 33, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "nlp", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "8f187bb4e3b1737b68fafe03bedf7dc6081d4661", "url": "https://www.semanticscholar.org/paper/8f187bb4e3b1737b68fafe03bedf7dc6081d4661", "title": "Recycling an Information Extraction System to Automatically Produce Semantic Annotations for the Web", "abstract": "This paper is intended to show how an Information extraction system can be recycled to produce RDF schemas for the semantic web. We show that this kind of systems has to respect operational constraints like the fact that the information produced must be highly relevant (high precision, possibly low recall). We conclude in reconsidering some tasks like Question Answering (Q/A): the production of explicit structured data on the web will lead a better relevance of information retrieval engines.", "venue": "SAAKM@ECAI", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "sw", "ie", "rdf"], "mention_counts": {"sw": 1, "rdf": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"sw": 1, "rdf": 1}, "relevance_score": 0.5}, {"paperId": "ca4a945b4b1a4c799abd71731c286440f81af997", "url": "https://www.semanticscholar.org/paper/ca4a945b4b1a4c799abd71731c286440f81af997", "title": "Advanced Semantics for Commonsense Knowledge Extraction", "abstract": "Commonsense knowledge (CSK) about concepts and their properties is useful for AI applications such as robust chatbots. Prior works like ConceptNet, TupleKB and others compiled large CSK collections, but are restricted in their expressiveness to subject-predicate-object (SPO) triples with simple concepts for S and monolithic strings for P and O. Also, these projects have either prioritized precision or recall, but hardly reconcile these complementary goals. This paper presents a methodology, called Ascent, to automatically build a large-scale knowledge base (KB) of CSK assertions, with advanced expressiveness and both better precision and recall than prior works. Ascent goes beyond triples by capturing composite concepts with subgroups and aspects, and by refining assertions with semantic facets. The latter are important to express temporal and spatial validity of assertions and further qualifiers. Ascent combines open information extraction with judicious cleaning using language models. Intrinsic evaluation shows the superior size and quality of the Ascent KB, and an extrinsic evaluation for QA-support tasks underlines the benefits of Ascent. A web interface, data and code can be found at https://www.mpi-inf.mpg.de/ascent.", "venue": "The Web Conference", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ie"], "mention_counts": {"kg": 1, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.5}, {"paperId": "446eb703027c7f12a9d3a9cd9095a296057982f5", "url": "https://www.semanticscholar.org/paper/446eb703027c7f12a9d3a9cd9095a296057982f5", "title": "S-CREAM: Semiautomatic CREAtion of Metadata", "abstract": "Richly interlinked, machine-understandable data constitute the basis for the Semantic Web. We provide a framework, SCREAM, that allows for creation of metadata and is trainable for a specific domain. Annotating web documents is one of the major techniques for creating metadata on the web. The implementation of S-CREAM, OntoMat supports now the semi-automatic annotation of web pages. This semi-automatic annotation is based on the information extraction component Amilcare. OntoMat extract with the help of Amilcare knowledge structure from web pages through the use of knowledge extraction rules. These rules are the result of a learningcycle based on already annotated pages.", "venue": "SAAKM@ECAI", "citationCount": 60, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ie", "ke"], "mention_counts": {"sw": 1, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "ke": 1}, "relevance_score": 0.5}, {"paperId": "6c622c5353ae00b00cad8ed83656ed5edf8962ea", "url": "https://www.semanticscholar.org/paper/6c622c5353ae00b00cad8ed83656ed5edf8962ea", "title": "Ontologizing concept maps using graph theory", "abstract": "Given the new challenges of open and unsupervised information extraction, there is a need to identify important and relevant knowledge structures (concepts and relationships) in the vast amount of extracted data and to filter the noise that results from unsupervised information extraction. This is generally referred to as the ontologization task. This paper uses measures from graph theory to identify these key elements such as Page Rank, Betweenness, and Degree. We also propose a combination of metrics for ranking concepts and relationships. Our approach shows effective results in terms of precision compared to other standard measures for weighting concepts and relationships such as TF*IDF or frequency of co-occurrences.", "venue": "ACM Symposium on Applied Computing", "citationCount": 20, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "59b08d00d2ee7a4ed8e6b35e6e472a2fd1a2ec0d", "url": "https://www.semanticscholar.org/paper/59b08d00d2ee7a4ed8e6b35e6e472a2fd1a2ec0d", "title": "Ontology-Assisted Database Integration to Support Natural Language Processing and Biomedical Data-mining", "abstract": "Summary Successful biomedical data mining and information extraction require a complete picture of biological phenomena such as genes, biological processes, and diseases; as these exist on different levels of granularity. To realize this goal, several freely available heterogeneous databases as well as proprietary structured datasets have to be integrated into a single global customizable scheme. We will present a tool to integrate different biological data sources by mapping them to a proprietary biomedical ontology that has been developed for the purposes of making computers understand medical natural language.", "venue": "Journal of Integrative Bioinformatics", "citationCount": 30, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "ie", "onto"], "mention_counts": {"nlp": 1, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "5725c74190f39383e33eacb90a21d010ab8402ed", "url": "https://www.semanticscholar.org/paper/5725c74190f39383e33eacb90a21d010ab8402ed", "title": "Boosting Information Extraction through Semantic Technologies: The KIDs use case at CONSOB", "abstract": "In this paper we report on the initial results of a project concerning the integration of Semantic Technologies with Information Extraction (IE) techniques, jointly carried out by Sapienza University of Rome and CONSOB (Commissione Nazionale per la Societ\u00e0 e la Borsa), the Italian public authority responsible for regulating the securities market. The use case. In the EU, the creators of financial products (a.k.a. financial manufacturers) are obliged by law3 to make information related to so-called PRIIPs (Packaged Retail Investment and Insurance-based Investments Products) publicly available. The NCAs (National Competent Authorities) have supervisory duties on such products, so that they can be safely placed on the respective national markets. The legislation requires information about PRIIPs to be communicated to NCAs through documents called KIDs (Key Information Documents). In the practice, this means that features to be checked are cast into text reports, typically formatted as pdf files, and extracting structured data from them (to bootstrap control activities), is actually in charge to the authority (In Italy, CONSOB). Due to the massive amount of documents to be analyzed (e.g., \u223c700.000 KIDs received by CONSOB in 2019, more than 1 million in 2020), this process cannot be carried out manually, but still it is only partially automated to date. Objectives. Our main aim is thus to develop a solution to streamline the extraction process and reduce as much as possible (ideally eliminate) the need of manual intervention, still guaranteeing very high accuracy. At the same time, such solution should return a data structure providing a due account of the semantics of the business domain and suited for rich and highly informative post-extraction analysis. Solution. Given the previously highlighted requirements, the proposed solution aims at constructing a Knowledge Graph (KG), whose intensional component (expressed in OWL) is designed with the help of domain experts, and whose extensional level is automatically created from KIDs through a rule-based IE mechanism. The choice of structuring the extracted data as a KG not only facilitates the integration with other corporate and external data, enabling rich analysis and management at an abstract, conceptual level, but also allows for properly formalizing the conceptual distinction between PRIIPs and KIDs describing them, and the continuous updates which KIDs are subjected to.", "venue": "SEMWEB", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "ie", "ie"], "mention_counts": {"kg": 1, "onto": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "72b80b82ecb1c32f9a67bd2a039f4346f7c86321", "url": "https://www.semanticscholar.org/paper/72b80b82ecb1c32f9a67bd2a039f4346f7c86321", "title": "Automatic test knowledge extraction from VHDL (ATKET)", "abstract": "The authors describe ATKET (automatic test knowledge extraction tool), which synthesizes test knowledge using structural and behavioral information available in the very high-speed IC description language (VHDL) description of a design. A VHDL analyzer produces an intermediate representation of the information contained in a VHDL design. ATKET interfaces to this intermediate representation to access structural and behavioral information in the design and stores it in suitable data structures. A convenient representation called the module operation tree (MOT) is used to capture the behavior of modules in the design. Information stored in the MOT along with structural information describing connections between modules in the design is used to generate test knowledge. Results obtained from ATKET for a circuit which was difficult to test are presented.<<ETX>>", "venue": "[1992] Proceedings 29th ACM/IEEE Design Automation Conference", "citationCount": 69, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "1cb1f7bd719109eb0a34ef5e82391411cbf4600f", "url": "https://www.semanticscholar.org/paper/1cb1f7bd719109eb0a34ef5e82391411cbf4600f", "title": "TRENCADIS - A WSRF Grid MiddleWare for Managing DICOM Structured Reporting Objects", "abstract": "The adoption of the digital processing of medical data, especially on radiology, has leaded to the availability of millions of records (images and reports). However, this information is mainly used at patient level, being the extraction of information, organised according to administrative criteria, which make the extraction of knowledge difficult. Moreover, legal constraints make the direct integration of information systems complex or even impossible. On the other side, the widespread of the DICOM format has leaded to the inclusion of other information different from just radiological images. The possibility of coding radiology reports in a structured form, adding semantic information about the data contained in the DICOM objects, eases the process of structuring images according to content. DICOM Structured Reporting (DICOM-SR) is a specification of tags and sections to code and integrate radiology reports, with seamless references to findings and regions of interests of the associated images, movies, waveforms, signals, etc. The work presented in this paper aims at developing of a framework to efficiently and securely share medical images and radiology reports, as well as to provide high throughput processing services. This system is based on a previously developed architecture in the framework of the TRENCADIS project, and uses other components such as the security system and the Grid processing service developed in previous activities. The work presented here introduces a semantic structuring and an ontology framework, to organise medical images considering standard terminology and disease coding formats (SNOMED, ICD9, LOINC..).", "venue": "HealthGrid", "citationCount": 10, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ie", "ke", "onto"], "mention_counts": {"ke": 1, "onto": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "d26648dcfa2cfb61bf72b91525054d5da84a96b9", "url": "https://www.semanticscholar.org/paper/d26648dcfa2cfb61bf72b91525054d5da84a96b9", "title": "Architecture of a Question-Answering System for a specific repository of documents", "abstract": "This paper describes the architecture for a Question-Answering System for a repository of documents that are labeled according to a standard. This architecture defines the basic structure into three modules to ensure a recovery process of the appropriate response. The first module performs analysis operations and representation of the question entered by the user in natural language. The second module allows for the operations concerning the search for documents; and finally the third module performs the extraction of the response. For the analysis of a question, Natural Language Processing (NLP) techniques are used in order for the question to be represented in a logical manner. Also, other components are used such as ontology, taxonomy, and knowledge bases that could process the question, place it in a context, and determine the type of question. An information retrieval module that interacts with the document repository, it is used to obtain documents. These documents are analyzed to determine the paragraphs that could be considered as a response. The aim of the model allows for an answer to a question that is asked in natural language of a document repository. This proposal will establish an appropriate mechanism to enable components of the representation of information based on a natural language text and later retrieval", "venue": "International Conference on Software Technology and Engineering", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "kg", "onto"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "5fda635bddc93525eded757ff32c5bb15eef4778", "url": "https://www.semanticscholar.org/paper/5fda635bddc93525eded757ff32c5bb15eef4778", "title": "Ontology-Based Extraction of Kazakh Language Word Combinations in Natural Language Processing", "abstract": "This article provides an ontological model of nominative word combinations in the Kazakh language. It is necessary for creation of the automated templates for search of nominative word combinations of the Kazakh language in text corpora. The presented model expands the theory of applied linguistics in the field of extracting information from the text during corpus studies. The results will be used in semantic searches, Q&A systems and in the development of software applications for obtaining knowledge, as well as for training and evaluation of knowledge on the syntax of the Kazakh language in the system of e-learning.", "venue": "DATA", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "nlp", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "6109afdd6c072d7879ab793334abf2c4535d4a3a", "url": "https://www.semanticscholar.org/paper/6109afdd6c072d7879ab793334abf2c4535d4a3a", "title": "Extracci\u00f3n de respuestas a partir de ontolog\u00edas utilizando patrones estructurales en SQWRL", "abstract": "Question-answering is a challenge for the Information Extraction (EI ) task. The use of ontologies reduces the complexity of this task. However, the challenges are presented in the processing of the user request through a query and how it is converted into a formal language for information extraction from ontologies. This paper proposes a method", "venue": "Res. Comput. Sci.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "ie"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "91f95031b044c55ba2004952cd8d70641eac3118", "url": "https://www.semanticscholar.org/paper/91f95031b044c55ba2004952cd8d70641eac3118", "title": "Use of Natural Language Processing for Precise Retrieval of Key Elements of Health IT Evaluation Studies.", "abstract": "Having precise information about health IT evaluation studies is important for evidence-based decisions in medical informatics. In a former feasibility study, we used a faceted search based on ontological modeling of key elements of studies to retrieve precisely described health IT evaluation studies. However, extracting the key elements manually for the modeling of the ontology was time and resource-intensive. We now aimed at applying natural language processing to substitute manual data extraction by automatic data extraction. Four methods (Named Entity Recognition, Bag-of-Words, Term-Frequency-Inverse-Document-Frequency, and Latent Dirichlet Allocation Topic Modeling were applied to 24 health IT evaluation studies. We evaluated which of these methods was best suited for extracting key elements of each study. As gold standard, we used results from manual extraction. As a result, Named Entity Recognition is promising but needs to be adapted to the existing study context. After the adaption, key elements of studies could be collected in a more feasible, time- and resource-saving way.", "venue": "Studies in health technology and informatics", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "9f56e9ce12f311904fd0eebac1fc515c94143425", "url": "https://www.semanticscholar.org/paper/9f56e9ce12f311904fd0eebac1fc515c94143425", "title": "Study on the Intelligent Selection Model of Fuzzy Semantic Optimal Solution in the Process of Translation Using English Corpus", "abstract": "In order to improve the accuracy and reasonableness of using English corpus for translation, a method of using English corpus to perform translation tasks based on fuzzy semantic optimal solution intelligent selection and inspired computing for wireless networks is proposed. The information extraction model using English corpus for translation is constructed, and the fuzzy semantic keyword feature directivity model of English corpus translation is established. Fuzzy semantic ontology feature registration method is used to calculate the fuzzy semantic intelligence optimal solution vector in English translation. The semantic fuzzy feature matching and adaptive subject word registration are realized in English translation. The fuzzy link relation of semantic ontology is established, and the fuzzy semantic optimal solution is obtained. The accuracy of machine translation in English corpus is improved. The experimental results show that the fuzzy semantic optimal solution has better registration performance and the feature matching degree of the subject words is higher, which improves the reasonableness and accuracy of translation in English corpus. At the same time, it provides a new idea for intelligent computation and recognition of wireless network.", "venue": "Wireless Communications and Mobile Computing", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "mt"], "mention_counts": {"onto": 2, "mt": 1, "ie": 1}, "nlp_mention_counts": {"mt": 1, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "fde586d1082864698000500d2cbf6a285439f9ad", "url": "https://www.semanticscholar.org/paper/fde586d1082864698000500d2cbf6a285439f9ad", "title": "Searching Semantic Resources for Complex Selectional Restictions to Support Lexical Acquisition", "abstract": "Natural language processing systems are increasingly using ontologies and other large-scale semantic resources to support Verb Sense Disambiguation (VSD) and other applications. One of the ways in which these resources can be used is to identify the selectional restrictions on verb arguments needed for sense distinction. However, manually navigating such resources can be difficult and inefficient due to their size and complexity. In this paper, we present a process for automatically searching through an ontology to determine appropriate concepts for expressing selectional restrictions on verb sense. The goal of this research is to semi-automate the development of a semantically rich lexicon to support high-precision information extraction.", "venue": "2009 Third International Conference on Advances in Semantic Processing", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "ie", "onto"], "mention_counts": {"nlp": 1, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "a34adb88eadfa4f4533e65ee6b7b13cf56525609", "url": "https://www.semanticscholar.org/paper/a34adb88eadfa4f4533e65ee6b7b13cf56525609", "title": "Using an Ontological Representation of Chemotherapy Toxicities for Guiding Information Extraction and Integration from EHRs", "abstract": "INTRODUCTION\nChemotherapies against cancers are often interrupted due to severe drug toxicities, reducing treatment opportunities. For this reason, the detection of toxicities and their severity from EHRs is of importance for many downstream applications. However toxicity information is dispersed in various sources in the EHRs, making its extraction challenging.\n\n\nMETHODS\nWe introduce OntoTox, an ontology designed to represent chemotherapy toxicities, its attributes and provenance. We illustrated the interest of OntoTox by integrating toxicities and grading information extracted from three heterogeneous sources: EHR questionnaires, semi-structured tables, and free-text.\n\n\nRESULTS\nWe instantiated 53,510, 2,366 and 54,420 toxicities from questionnaires, tables and free-text respectively, and compared the complementarity and redundancy of the three sources.\n\n\nDISCUSSION\nWe illustrated with this preliminary study the potential of OntoTox to guide the integration of multiple sources, and identified that the three sources are only moderately overlapping, stressing the need for a common representation.", "venue": "Medinfo", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "ie", "onto", "ie"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "4a3044608e745d6d4a1dbac2ae5b4fa953ea7b78", "url": "https://www.semanticscholar.org/paper/4a3044608e745d6d4a1dbac2ae5b4fa953ea7b78", "title": "Generating knowledge networks from phenotypic descriptions", "abstract": "Several computing systems rely on information about living beings, such as Identification Keys \u2014 artifacts created by biologists to identify specimens following a flow of questions about their observable characters (phenotype). These questions are described in a free-text format, e.g., \u201cbig and black eye\u201d. Free-texts hamper the automatic information interpretation by machines, limiting their ability to perform search and comparison of terms, as well as integration tasks. This paper proposes a method to extract phenotypic information from natural language texts from biology legacy information systems, transforming them in an Entity-Quality formalism \u2014 a format to represent each phenotype character (Entity) and its state (Quality). Our approach aligns automatically recognized. Entities and Qualities with domain concepts described in ontologies. It adopts existing Natural Language Processing techniques, adding an extra original step, which exploits intrinsic characteristics of phenotypic descriptions and of the organizational structure of Identification Keys. The approach was validated over the FishBase data. We conducted extensive experiments based on a manually annotated Gold Standard set to assess the precision and applicability of the proposed extraction method. The obtained results reveal the feasibility of our technique, its benefits and possibilities of scientific studies using the extracted knowledge network.", "venue": "2016 IEEE 12th International Conference on e-Science (e-Science)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "ke"], "mention_counts": {"nlp": 1, "onto": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "6624e2c653b34228cb55f8d00d18f47a5edd0983", "url": "https://www.semanticscholar.org/paper/6624e2c653b34228cb55f8d00d18f47a5edd0983", "title": "DSAT: Ontology-based Information Extraction on Technical Data Sheets", "abstract": "Current engineering design processes oftentimes involve transferring information from manufacturer-provided data sheets into domainspecific design tools. While most data sheets are provided only as PDF files, this remains a tedious and manual task. This paper presents the Data Sheets Annotation Tool (DSAT), which assists engineers in gathering the information required in the design process. Using an OntologyBased Information Extraction (OBIE) method, the properties of components are extracted from data sheets and subsequently presented in an integrated, web-based interface. Engineers can now review and correct these automatic annotations, before exporting them for further use. In the demonstration, we employ a real-world use case rooted in modelbased space-system engineering. We show how the automated process can extract relevant component-attributes from technical data sheets and how users can redact the results. We further highlight the impact of content and quality of the underlying ontologies.", "venue": "SEMWEB", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto"], "mention_counts": {"onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "40fa8d5ac46a33e4b30258abeb706b2fb8b26fb4", "url": "https://www.semanticscholar.org/paper/40fa8d5ac46a33e4b30258abeb706b2fb8b26fb4", "title": "Application-specific Semantic Information Extraction from Unstructured Data", "abstract": "\u2014 With a rapid growth of information available on the internet there is also growing demand in applications that can process data from different sources accessing information that is only needed for its particular use. Due to the fact that information on the internet is mostly unstructured and thus cannot be processed automatically many tasks which require information extraction are still performed manually. Thereby research of methods for selectively structuring and semantically classifying of unstructured text data is playing an increasingly significant role. In this paper, based on a research project at University of Applied Sciences Augsburg, a method for extracting application-specific information is discussed on a concrete practical example. Beside the presented algorithms this paper intents to provide the reader with knowledge about semantic application development and thus focuses on applied research in this area. Consequently, methods from fields such as information retrieval, Semantic Web and ontologies are brought together and are applied in the example application in order to investigate applicability and relevance in practice.", "venue": "IEEE GSC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "sw", "onto"], "mention_counts": {"sw": 1, "onto": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "f05833878f0fc2caa03fcd1cd9269cd1eb7bbc93", "url": "https://www.semanticscholar.org/paper/f05833878f0fc2caa03fcd1cd9269cd1eb7bbc93", "title": "Semantic Foraging in Defined Contexts", "abstract": "An experimental prototype system was created and used to investigate how information relevant to analyst queries, and constrained by a contextual model, can be found over a large information space. Agents employing the ant model sift through documents quickly using a transductive support machine classifier and return those meeting a classifier which is constantly refined through feedback from semantic information extraction to a knowledge base. An ontology-informed extraction is performed on returned documents; an objective function then evaluates how well each document fulfilled the queries and this information is used to create a new classifier for each query. In numerous trials on a static corpus, recall and precision of the classifiers was consistently above 92%. Semantic results have not been quantified but appear highly promising.", "venue": "2008 10th IEEE Conference on E-Commerce Technology and the Fifth IEEE Conference on Enterprise Computing, E-Commerce and E-Services", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "kg", "ie"], "mention_counts": {"kg": 1, "onto": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "9849e269e4041094c8fefaf53b0780d9224b8fae", "url": "https://www.semanticscholar.org/paper/9849e269e4041094c8fefaf53b0780d9224b8fae", "title": "The COVID-19 Open Research Dataset - Abstract", "abstract": "The COVID-19 Open Research Dataset (CORD-19) is a growing corpus of scientific articles on COVID-19 and related historical research on other coronavirus outbreaks and epidemics. Since its release on March 13th, 2020, CORD-19 has grown to include over 45K articles (of which over 30K have machine-readable structured full text) and has been widely adopted for various text mining and information retrieval applications. In this talk, we discuss the curation of CORD-19 and promising avenues of research conducted over the corpus. We hope this resource will bring together the computing community, clinical experts, and policy makers in the search for effective treatments and management policies for COVID-19. Biographies: Lucy Lu Wang is a Young Investigator at the Allen Institute for AI, where she is part of the Semantic Scholar Research team. She works on increasing access to and understanding of scientific text, with a focus on applications in bioNLP, biomedical ontology, and the science of science. Her work on gender trends in publishing and supplement interaction detection has been featured in Geekwire, Gizmodo, Axios, VentureBeat, and the New York Times. She completed her PhD at the University of Washington in Biomedical and Health Informatics. Kyle Lo is a research scientist at the Allen Institute for AI on the Semantic Scholar Research team, where he works on NLP for scientific text with emphasis on knowledge extraction, summarization, and discourse. His recent work has been in domain adaptation of language models for improved scientific text mining and AI-assistive tools for improving research workflows. His work on programatically identifying sex bias in clinical trial participation published in JAMA was featured in Quartz. He has an MS in Statistics from the University of Washington. Copyright \u00a9 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).", "venue": "SIIRH@ECIR", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "ke"], "mention_counts": {"nlp": 1, "onto": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "454987e9d5c89af26e4b4de4805aee1932eaf56b", "url": "https://www.semanticscholar.org/paper/454987e9d5c89af26e4b4de4805aee1932eaf56b", "title": "Multimedia data integration and processing for E-Government", "abstract": "Knowledge management has become a challenge for almost all E-government based applications where one of the main problem is the efficient management of great amounts of data. In order to efficiently access the information embedded in very large document repositories, techniques for semantic document management are required. They ensure improvement for a large and intense process of dematerialization and aim at eliminating or at least reducing, the amount of paper documents. In this work, we present a novel model of digital documents for the improvement of the dematerialization effectiveness. This model represents the starting point for an information system that is able to manage the document streams in an efficient way. It takes into account E-government applications needs like the compliance with the laws and regulations in force and the adaptability to evolving technologies. At the best of our knowledge, the proposed model is one of the first attempts to give a single and unified characterization for the management of multimedia documents, pertaining to a bureaucratic domain as the E-government one, on which semantic procedures are used for the transformation of non structured documents (pertaining to specialized domain) into structured data, suitable for automatic processing. Furthermore, an architecture for the management of documents life cycle is proposed, which provides advanced functionalities for semantic processing, such as giving formal structure to document informative content, information extraction, semantic retrieval, indexing, storage, presentation, together with long-term preservation. Keywords-Ontology Learning, Ontology Population, Natural Languages Processing", "venue": "Distributed Multimedia Systems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "ie"], "mention_counts": {"nlp": 1, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "eb585e8397f45d3254aa3bae974b6de86b02d31a", "url": "https://www.semanticscholar.org/paper/eb585e8397f45d3254aa3bae974b6de86b02d31a", "title": "Computationally Efficient Context-Free Named Entity Disambiguation with Wikipedia", "abstract": "The induction of the semantics of unstructured text corpora is a crucial task for modern natural language processing and artificial intelligence applications. The Named Entity Disambiguation task comprises the extraction of Named Entities and their linking to an appropriate representation from a concept ontology based on the available information. This work introduces novel methodologies, leveraging domain knowledge extraction from Wikipedia in a simple yet highly effective approach. In addition, we introduce a fuzzy logic model with a strong focus on computational efficiency. We also present a new measure, decisive in both methods for the entity linking selection and the quantification of the confidence of the produced entity links, namely the relative commonness measure. The experimental results of our approach on established datasets revealed state-of-the-art accuracy and run-time performance in the domain of fast, context-free Wikification, by relying on an offline pre-processing stage on the corpus of Wikipedia. The methods introduced can be leveraged as stand-alone NED methodologies, propitious for applications on mobile devices, or in the context of vastly reducing the complexity of deep neural network approaches as a first context-free layer.", "venue": "Inf.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "7068f37ae7c6eaccf97434c8e45785c31fc2cfbd", "url": "https://www.semanticscholar.org/paper/7068f37ae7c6eaccf97434c8e45785c31fc2cfbd", "title": "From information to knowledge: introducing WebStract's knowledge engineering approach", "abstract": "Information overload is a problem because of the overwhelming volume of data that has become accessible through the Internet or other mass communication media. It is difficult for users to sift through this data and to locate useful knowledge because large amounts of unorganized, raw data are confusing to search engines as well as people. WebStract is an experimental tool to assist in the qualification, organization and distribution of information. It offers semi-automated mechanisms to transform raw electronic data into domain knowledge and it provides multiple views for easier user consumption. After introducing WebStract, the paper focuses on the three stage transformation process that is used to transform raw data into domain knowledge. The three stages are knowledge extraction, knowledge elucidation and knowledge presentation. Knowledge extraction retrieves information from electronic documents (e.g., accessible through the WWW) and analyzes it for useful syntactical patterns that are stored in a database. Knowledge elucidation analyzes the syntactical patterns, in a semi-automated fashion, to produce a prioritized, hierarchical summary of the original documents. A fuzzy filtering mechanism allows retrieval of the stored knowledge and the resulting organized summary is presented using the familiar \"book\" metaphor. Each book can provide several viewpoints for users to review the information. A main application of WebStract is the support of problem based learning in on-line course delivery. WebStract is currently in its third generation of development.", "venue": "Engineering Solutions for the Next Millennium. 1999 IEEE Canadian Conference on Electrical and Computer Engineering (Cat. No.99TH8411)", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "37b0550f3cced00326219dfea9057a06b1436a53", "url": "https://www.semanticscholar.org/paper/37b0550f3cced00326219dfea9057a06b1436a53", "title": "Semantic annotation of semi-structured documents", "abstract": "The present paper proposes a novel method for semantic annotation of semi-structured documents using GATE (General Architecture for Text Engineering), one of the most famous and powerful annotation tools. The problem with GATE is that it is designed to annotate plain text and perform some natural language processing (NLP). Hence, when semi-structured documents are loaded, it ignores the markup or formatting information and works with text. But, depending on the document loading options (ldquomarkup awarerdquo or not) it either annotates the whole document including markup or takes just text destroying the original document structure. This behavior is unacceptable for annotating and saving annotation information into original documents which belong to popular formats (such as Microsoft Word, Excel, etc.). The proposed solution in the present paper allows saving annotations in original documents avoiding the destruction of the document contents and formatting information. The proposed method is essentially important for semantically enriching semi-structured documents (especially Microsoft Word and Excel) because it allows relating the information in these documents, without disturbing the original information, with ontological information, like ontology instances, rather than to the whole document.", "venue": "2008 Canadian Conference on Electrical and Computer Engineering", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "76e8e7a954bcd69a0801444ac44e5c538961b4be", "url": "https://www.semanticscholar.org/paper/76e8e7a954bcd69a0801444ac44e5c538961b4be", "title": "Enabling Technologies for Smart Construction Engineering: a Review", "abstract": "Data-driven knowledge extraction is becoming one the greatest source of information to study and manage complex systems. Today, new challenges can be provided thanks to Information and Communication Technologies and IoT paradigm that are allowing big mass of data to be stored, processed and analyzed. In this paper an insight on data driven knowledge extraction potentiality in the field of smart construction engineering is given. In particular, the main technologies for supporting the decision-management phase in Architecture, Engineering and Construction Industry (AEC Industry) are analysed. To this aim, such technologies are classified in four fields and the main contributions presented in the related literature are singled out and discussed. Finally, some examples of the current applications in the AEC industry are presented and the gap for future development in this field is enlightened.", "venue": "2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "ee21e7a1acf03738d0f7a0b87261f6cb31ab77d8", "url": "https://www.semanticscholar.org/paper/ee21e7a1acf03738d0f7a0b87261f6cb31ab77d8", "title": "Survey and Proposal of a Method for Business Rules Identification in Legacy Systems Source Code and Execution Logs", "abstract": "Computer systems implement business processes from different organizations. Among the currently operating computer systems, much of it is classified as legacy system. Typically, legacy systems are complex applications that are still active, due to the high cost of modernization and a high degree of criticality. In recent years, were published several works addressing the importance of legacy systems modernization, emphasizing the extraction of the business process model implemented in these systems. Within this context, a key step is to extract knowledge from source code and / or systems execution logs, aiming to use this information in reverse engineering processes. In this work are presented and analyzed methods based on source code manipulation and system\u2019s execution logs mining, which can be used to extract knowledge from legacy systems, prioritizing business rules identification. A comparison between the two different approaches is presented, as well as their positive and negative characteristics. Our results include a list of desired features and a proposal of a method for legacy systems reverse engineering and business rules identification.", "venue": "ICEIS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "bea0699e153d66a96f125313311c119ccad09db9", "url": "https://www.semanticscholar.org/paper/bea0699e153d66a96f125313311c119ccad09db9", "title": "Formalization of Unstructured Content to Semantic Form", "abstract": "This paper will present an approach for knowledge extraction from unstructured content. Unstructured content refers to information syndicated online, with little structure. The information extracted from these content sources is semantically tagged and stored in the form of semantic graphs. The approach is upgraded with the transformation of semantic graphs to a relational database. The goal of the content acquirement is to build a repository of formal knowledge, similar to DBpedia, for content in Slovene language. The formalized content will use the database as the permanent storage and would be provided in various formats suited for machine processing (RDF dump, API, SPARQL endpoint).", "venue": "ADBIS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "rdf", "ie"], "mention_counts": {"ke": 1, "rdf": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "rdf": 1}, "relevance_score": 0.5}, {"paperId": "6558e7560956613164ba002876f261ea338119a5", "url": "https://www.semanticscholar.org/paper/6558e7560956613164ba002876f261ea338119a5", "title": "Knowledge Channels. Bringing the Knowledge on the Web to Software Agents", "abstract": "In this paper, we present a new framework to extract knowl- edge from today's non-semantic web. It associates semantics with the information extracted, which improves agent interoperability; it can also deal with changes to the structure of a web page, which improves adapt- ability; furthermore, it achieves to delegate the knowledge extraction procedure to specialist agents, easing software development and promot- ing software reuse and maintainability.", "venue": "CAiSE Short Paper Proceedings", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "sw", "ie"], "mention_counts": {"ke": 1, "sw": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "sw": 1}, "relevance_score": 0.5}, {"paperId": "ff2fb9f54e818f7bd3f532fe6c75694307198110", "url": "https://www.semanticscholar.org/paper/ff2fb9f54e818f7bd3f532fe6c75694307198110", "title": "Automatic Argumentation Detection and its Role in Law and the Semantic Web", "abstract": "The automatic detection of arguments in text regards a relatively new area at the intersection of Natural Language Processing, Information Retrieval and Legal Information Systems. This paper presents some fundamental issues when processing texts that contain argumentation. Furthermore, our research bridges different areas, including the legal field and the Semantic Web, where argumentation detection and reconstruction could be beneficial. Finally, it analyses several methodologies to accomplish this task, providing results from different experiments done over several kinds of texts, specially legal reports.", "venue": "Law, Ontologies and the Semantic Web", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlp", "sw", "tp"], "mention_counts": {"nlp": 1, "sw": 2, "tp": 1}, "nlp_mention_counts": {"nlp": 1, "tp": 1}, "ld_mention_counts": {"sw": 2}, "relevance_score": 0.5}, {"paperId": "153a4c7272ebf4883746d2629f98749a857c8018", "url": "https://www.semanticscholar.org/paper/153a4c7272ebf4883746d2629f98749a857c8018", "title": "TCN: Table Convolutional Network for Web Table Interpretation", "abstract": "Information extraction from semi-structured webpages provides valuable long-tailed facts for augmenting knowledge graph. Relational Web tables are a critical component containing additional entities and attributes of rich and diverse knowledge. However, extracting knowledge from relational tables is challenging because of sparse contextual information. Existing work linearize table cells and heavily rely on modifying deep language models such as BERT which only captures related cells information in the same table. In this work, we propose a novel relational table representation learning approach considering both the intra- and inter-table contextual information. On one hand, the proposed Table Convolutional Network model employs the attention mechanism to adaptively focus on the most informative intra-table cells of the same row or column; and, on the other hand, it aggregates inter-table contextual information from various types of implicit connections between cells across different tables. Specifically, we propose three novel aggregation modules for (i) cells of the same value, (ii) cells of the same schema position, and (iii) cells linked to the same page topic. We further devise a supervised multi-task training objective for jointly predicting column type and pairwise column relation, as well as a table cell recovery objective for pre-training. Experiments on real Web table datasets demonstrate our method can outperform competitive baselines by of F1 for column type prediction and by of F1 for pairwise column relation prediction.", "venue": "The Web Conference", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ie"], "mention_counts": {"kg": 1, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.5}, {"paperId": "102168d53405c3a4640f2e45056eff4ca70a4f6f", "url": "https://www.semanticscholar.org/paper/102168d53405c3a4640f2e45056eff4ca70a4f6f", "title": "TUCUXI: The InTelligent Hunter Agent for Concept Understanding and LeXical ChaIning", "abstract": "In this paper we present TUCUXI, an intelligent hunter agent that replaces traditional keywords-based queries on the Web with a user-provided domain ontology, where meanings to be searched are not ambiguous. TUCUXI judges the relevance of the retrieved pages by matching the domain ontology against a simplified, but semantically rich, document representation (Map of Meanings). The Map of Meanings extraction involves the Lexical Chaining technique, from the Natural Language Processing (NLP) research field.", "venue": "IEEE/WIC/ACM International Conference on Web Intelligence (WI'04)", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "342fa559ca76142e5dded350f081f6481acd1d27", "url": "https://www.semanticscholar.org/paper/342fa559ca76142e5dded350f081f6481acd1d27", "title": "Research on the similarity between nodes with hypernymy/hyponymy relations based on IC and taxonomical structure", "abstract": "The similarity method has an important effect on some tasks of natural language processing, such as information retrieval, automatic translation and named entity recognition. Hypernymy/hyponymy relations are widespread in semantic webs and knowledge graphs, so computing the similarity of hypernymy/hyponymy is a key issue in the text processing field. All measures of both feature-based and IC-based methods have obvious deficiencies. The feature-based method estimated the similarity by the depth of the node, and the IC-based method computed the similarity by the position of the deepest common parent. The deficiency of the feature-based method and IC-based method is that they include one parameter, so the performance is slightly inaccurate and unstable. To address this deficiency, our paper proposed a hybrid method that computes the similarity of hypernymy/hyponymy by a hybrid parameter (dhype(lch)) that implies two parameters: depth of the node and position of the deepest common parent. Compared with several similarity methods, the proposed method achieved better performance in terms of accuracy rate, Pearson correlation coefficient and artificial fitting effect.", "venue": "\u02dcThe \u0153international Arab journal of information technology", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "tp", "kg", "sw"], "mention_counts": {"sw": 1, "nlp": 1, "kg": 1, "tp": 1}, "nlp_mention_counts": {"nlp": 1, "tp": 1}, "ld_mention_counts": {"kg": 1, "sw": 1}, "relevance_score": 0.5}, {"paperId": "26725234bf65b28ad0090ea5ed54d2357d161656", "url": "https://www.semanticscholar.org/paper/26725234bf65b28ad0090ea5ed54d2357d161656", "title": "Supporting Patent Mining by using Ontology-based Semantic Annotations", "abstract": "Semantic web approach seems interesting for supporting content mining of millions of patents accessible through the Web. In this paper, we describe our approach for generating semantic annotations on patents, by relying on the structure and on a semantic representation of patent documents. We use both the structure of the patent documents and their textual contents processed by Natural Language Processing (NLP) tools. This method, primarily aimed at helping biologists use patent information can be generalized to all kinds of domains or of structured documents.", "venue": "International Conference on Wirtschaftsinformatik", "citationCount": 44, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "onto", "nlp"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "064f0232579365bf45c71a371f90bd62fb95e537", "url": "https://www.semanticscholar.org/paper/064f0232579365bf45c71a371f90bd62fb95e537", "title": "Generation of Silver Standard Concept Annotations from Biomedical Texts with Special Relevance to Phenotypes", "abstract": "Electronic health records and scientific articles possess differing linguistic characteristics that may impact the performance of natural language processing tools developed for one or the other. In this paper, we investigate the performance of four extant concept recognition tools: the clinical Text Analysis and Knowledge Extraction System (cTAKES), the National Center for Biomedical Ontology (NCBO) Annotator, the Biomedical Concept Annotation System (BeCAS) and MetaMap. Each of the four concept recognition systems is applied to four different corpora: the i2b2 corpus of clinical documents, a PubMed corpus of Medline abstracts, a clinical trails corpus and the ShARe/CLEF corpus. In addition, we assess the individual system performances with respect to one gold standard annotation set, available for the ShARe/CLEF corpus. Furthermore, we built a silver standard annotation set from the individual systems\u2019 output and assess the quality as well as the contribution of individual systems to the quality of the silver standard. Our results demonstrate that mainly the NCBO annotator and cTAKES contribute to the silver standard corpora (F1-measures in the range of 21% to 74%) and their quality (best F1-measure of 33%), independent from the type of text investigated. While BeCAS and MetaMap can contribute to the precision of silver standard annotations (precision of up to 42%), the F1-measure drops when combined with NCBO Annotator and cTAKES due to a low recall. In conclusion, the performances of individual systems need to be improved independently from the text types, and the leveraging strategies to best take advantage of individual systems\u2019 annotations need to be revised. The textual content of the PubMed corpus, accession numbers for the clinical trials corpus, and assigned annotations of the four concept recognition systems as well as the generated silver standard annotation sets are available from http://purl.org/phenotype/resources. The textual content of the ShARe/CLEF (https://sites.google.com/site/shareclefehealth/data) and i2b2 (https://i2b2.org/NLP/DataSets/) corpora needs to be requested with the individual corpus providers.", "venue": "PLoS ONE", "citationCount": 20, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "onto", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "81aeb52ae60880649c2e6f18f0c40b85d02083b2", "url": "https://www.semanticscholar.org/paper/81aeb52ae60880649c2e6f18f0c40b85d02083b2", "title": "Exploring big data in small forms: A multi-layered knowledge extraction of social networks", "abstract": "Big data poses great challenges for social network analysts in both the data volume and the latent dimensions hidden in the unstructured data. In this paper, we propose a comprehensive knowledge extraction approach for social networks to guide latent dimensions analysis. An improved hypergraph model of social behaviors was then proposed for conveniently conducting multi-faceted analytics in relationships inherent to social media. A real life case study based on Twitter's data was also presented to illustrate the multi-dimensional relations between users based on the categories they co-join and the tweets they co-spread with three orthogonal dimensions of affect analyzed simultaneously, i.e. valence, activation, and intention.", "venue": "2013 IEEE International Conference on Big Data", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "588d6b039f1d51a674d02fdaf8215f4abdcc9cb0", "url": "https://www.semanticscholar.org/paper/588d6b039f1d51a674d02fdaf8215f4abdcc9cb0", "title": "A Rough Set Theory Approach for Rule Generation and Validation Using RSES", "abstract": "Despite significant progress in e-learning technology over previous years, in view of huge sizes of data and databases, efficient knowledge extraction techniques are still required to make e-learning effective tool for delivery of learning. Rough set theory approach provides an effective technique for extraction of knowledge out of massive data. In order to provide effective support to learners, it is essential to know individual style of learning for each learner. For determining learning style of each learner, one is required to extract essentials of style of learning from a large number of parameters including academic background, profession, time available etc. In such scenario, rough theory proves a useful tool. In this paper, a rough set theory approach is proposed for determining learning styles of learners efficiently, so that based on the style, a learner may be provided learning support on the basis of requirement of the learner. These is achieved by eliminating redundant and ambiguous data and by generating reduct set, core set and rules from the given data. The results of this study are validated through RSES software by using same rough set analysis.", "venue": "Int. J. Rough Sets Data Anal.", "citationCount": 30, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "23ac589b331011f50625832ee2e9242b0392b5cc", "url": "https://www.semanticscholar.org/paper/23ac589b331011f50625832ee2e9242b0392b5cc", "title": "Weighted SVMBoost based Hybrid Rule Extraction Methods for Software Defect Prediction", "abstract": "The software testing efforts and costs are mitigated by appropriate automatic defect prediction models. So far, many automatic software defect prediction (SDP) models were developed using machine learning methods. However, it is difficult for the end users to comprehend the knowledge extracted from these models. Further, the SDP data is of unbalanced in nature, which hampers the model performance. To address these problems, this paper presents a hybrid weighted SVMBoost-based rule extraction model such as WSVMBoost and Decision Tree, WSVMBoost and Ripper, and WSVMBoost and Bayesian Network for SDP problems. The extraction of the rules from the opaque SVMBoost is carried out in two phases: (i) knowledge extraction, (ii) rule extraction. The experimental results on four NASA MDP datasets have shown that the WSVMBoost and Decision tree hybrid yielded better performance than the other hybrids and WSVM.", "venue": "Int. J. Rough Sets Data Anal.", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "7d9a25560d2b15e3b24df7c04d009169ba7de8ac", "url": "https://www.semanticscholar.org/paper/7d9a25560d2b15e3b24df7c04d009169ba7de8ac", "title": "Optimal knowledge extraction technique based on hybridisation of improved artificial bee colony algorithm and cuckoo search algorithm", "abstract": "We present a framework that we are currently developing, that allows one to extract knowledge from the knowledge discovery in database (KDD) dataset. Data mining is a very active and space growing research area. Knowledge discovery in databases (KDD) is very useful in scientific domains. In simple terms, association rule mining is one of the most well-known methods for such knowledge discovery. Initially, database are divided into training and testing for the aid of fuzzy generating the rules using fuzzy rules generation the set of rules are generated from the given dataset. From the generated rules, we are extracting the significant rules by using the improved artificial bee colony algorithm and cuckoo search algorithm (IABCCS). After extracting optimal knowledge from the dataset via rules, the data will be classified using fuzzy classifier with the aid of this finally we will classify the attack and normal.", "venue": "Int. J. Bus. Intell. Data Min.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "634c8db392b1b7502e12a0d73d4508c1e3182894", "url": "https://www.semanticscholar.org/paper/634c8db392b1b7502e12a0d73d4508c1e3182894", "title": "Improving Customer Relationship Management using Machine Learning techniques: A Tunisian Case Study", "abstract": "In the CRM domain, predicting interesting customer profiles relies on studying their different characteristics and behavior. Data mining tools provide functions and algorithms that allow such knowledge extraction. We investigate a research study of data mining concepts and an application of its different functions for a Tunisian hotel case study. The proposed data mining application results in successful knowledge extraction that will be very precious to enhance the loyalty of the hotel customers.", "venue": "2020 International Multi-Conference on: \u201cOrganization of Knowledge and Advanced Technologies\u201d (OCTA)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "02405f0342e9532cbf238972a2828c6d2914b6b7", "url": "https://www.semanticscholar.org/paper/02405f0342e9532cbf238972a2828c6d2914b6b7", "title": "Towards Unsupervised Knowledge Extraction", "abstract": "Integration of symbolic and sub-symbolic approaches is rapidly emerging as an Artificial Intelligence (AI) paradigm. This paper presents a proof-of-concept approach towards an unsupervised learning method, based on Restricted Boltzmann Machines (RBMs), for extracting semantic associations among prominent entities within data. Validation of the approach is performed in two datasets that connect language and vision, namely Visual Genome and GQA. A methodology to formally structure the extracted knowledge for subsequent use through reasoning engines is also offered.", "venue": "AAAI Spring Symposium: Combining Machine Learning with Knowledge Engineering", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "4cb3caa4d20d9bbd77047b890d5c74f6869ea2f0", "url": "https://www.semanticscholar.org/paper/4cb3caa4d20d9bbd77047b890d5c74f6869ea2f0", "title": "Data Ecosystems for Scientific Experiments: Managing Combustion Experiments and Simulation Analyses in Chemical Engineering", "abstract": "The development of scientific predictive models has been of great interest over the decades. A scientific model is capable of forecasting domain outcomes without the necessity of performing expensive experiments. In particular, in combustion kinetics, the model can help improving the combustion facilities and the fuel efficiency reducing the pollutants. At the same time, the amount of available scientific data has increased and helped speeding up the continuous cycle of model improvement and validation. This has also opened new opportunities for leveraging a large amount of data to support knowledge extraction. However, experiments are affected by several data quality problems since they are a collection of information over several decades of research, each characterized by different representation formats and reasons of uncertainty. In this context, it is necessary to develop an automatic data ecosystem capable of integrating heterogeneous information sources while maintaining a quality repository. We present an innovative approach to data quality management from the chemical engineering domain, based on an available prototype of a scientific framework, SciExpeM, which has been significantly extended. We identified a new methodology from the model development research process that systematically extracts knowledge from the experimental data and the predictive model. In the paper, we show how our general framework could support the model development process, and save precious research time also in other experimental domains with similar characteristics, i.e., managing numerical data from experiments.", "venue": "Frontiers in Big Data", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "bd3cc154df12888fa148d38f171b79b7afc84601", "url": "https://www.semanticscholar.org/paper/bd3cc154df12888fa148d38f171b79b7afc84601", "title": "Extraction and Application of Cognitive Related Semantic Relationships", "abstract": "Unstructured knowledge extraction is the process of recognizing and storing valuable knowledge from the natural language texts. However, few tools are available to automatically extract knowledge concepts and their relations from the text books, especially for those in Chinese. This paper proposed a method to implement the \u2018example of\u2019 and \u2018part of\u2019 semantic relations' and their related entities' extracting from the digital textbooks in Chinese. The experimental data shows that the extraction of the both relations and the entities can achieve a rather high accuracy and satisfied results comparing with the previous studies.", "venue": "International Conference on Semantics, Knowledge and Grid", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "a89a4fa152ba632d4e3756adf3eda2fc31e4ad8e", "url": "https://www.semanticscholar.org/paper/a89a4fa152ba632d4e3756adf3eda2fc31e4ad8e", "title": "Privacy-Preserving Deep Learning Models for Law Big Data Feature Learning", "abstract": "Nowadays, a massive number of data, referred as big data, are being collected from social networks and Internet of Things (IoT), which are of tremendous value. Many deep learning-based methods made great progress in the extraction of knowledge of those data. However, the knowledge extraction of the law data poses vast challenges on the deep learning, since the law data usually contain the privacy information. In addition, the amount of law data of an institution is not large enough to well train a deep model. To solve these challenges, some privacy-preserving deep learning are proposed to capture knowledge of privacy data. In this paper, we review the emerging topics of deep learning for the feature learning of the privacy data. Then, we discuss the problems and the future trend in deep learning for privacy-preserving feature learning on law data.", "venue": "2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "b333b4f237b2ab21ef9817db2f45fbe6881ffef9", "url": "https://www.semanticscholar.org/paper/b333b4f237b2ab21ef9817db2f45fbe6881ffef9", "title": "Mining data from simulation of beer production", "abstract": "Data mining is a methodology for the extraction of knowledge from data, especially, knowledge relating to a problem that we want to solve. Data mining from simulation outputs is performed in this paper, it focuses on techniques for extracting knowledge from simulation outputs for beer production and optimizing devices and labors with certain target. We first set up one simulation model for beer production process and construct optimization objective. Then we set up one data mining model based on witness miner. The mining results show that the model is able to fund important information affecting target, make manager diagnose the bottlenecks of the beer production process, and help manager to make decisions rapidly under uncertainty.", "venue": "International Conference on Natural Language Processing and Knowledge Engineering", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "838fb86e9a77a1bf7337071798062129337b58a4", "url": "https://www.semanticscholar.org/paper/838fb86e9a77a1bf7337071798062129337b58a4", "title": "Knowledge Extraction with No Observable Data", "abstract": "Knowledge distillation is to transfer the knowledge of a large neural network into a smaller one and has been shown to be effective especially when the amount of training data is limited or the size of the student model is very small. To transfer the knowledge, it is essential to observe the data that have been used to train the network since its knowledge is concentrated on a narrow manifold rather than the whole input space. However, the data are not accessible in many cases due to the privacy or confidentiality issues in medical, industrial, and military domains. To the best of our knowledge, there has been no approach that distills the knowledge of a neural network when no data are observable. In this work, we propose KegNet (Knowledge Extraction with Generative Networks), a novel approach to extract the knowledge of a trained deep neural network and to generate artificial data points that replace the missing training data in knowledge distillation. Experiments show that KegNet outperforms all baselines for data-free knowledge distillation. We provide the source code of our paper in https://github.com/snudatalab/KegNet.", "venue": "Neural Information Processing Systems", "citationCount": 51, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "b3c41d5b0ad862118e27c0b6f2317c9a7f50d175", "url": "https://www.semanticscholar.org/paper/b3c41d5b0ad862118e27c0b6f2317c9a7f50d175", "title": "Automatic knowledge extraction from documents", "abstract": "Access to a large amount of knowledge is critical for success at answering open-domain questions for DeepQA systems such as IBM Watson\u2122. Formal representation of knowledge has the advantage of being easy to reason with, but acquisition of structured knowledge in open domains from unstructured data is often difficult and expensive. Our central hypothesis is that shallow syntactic knowledge and its implied semantics can be easily acquired and can be used in many areas of a question-answering system. We take a two-stage approach to extract the syntactic knowledge and implied semantics. First, shallow knowledge from large collections of documents is automatically extracted. Second, additional semantics are inferred from aggregate statistics of the automatically extracted shallow knowledge. In this paper, we describe in detail what kind of shallow knowledge is extracted, how it is automatically done from a large corpus, and how additional semantics are inferred from aggregate statistics. We also briefly discuss the various ways extracted knowledge is used throughout the IBM DeepQA system.", "venue": "IBM Journal of Research and Development", "citationCount": 127, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "dc65e97a5f82add0946a4d95766a5bc6ac0ac48f", "url": "https://www.semanticscholar.org/paper/dc65e97a5f82add0946a4d95766a5bc6ac0ac48f", "title": "A Hybrid Data Mining Approach for Knowledge Extraction and Classification in Medical Databases", "abstract": "This paper presents a novel hybrid data mining approach for knowledge extraction and classification in medical databases. The approach combines self organizing map, k-means and naive Bayes with a neural network based classifier. The idea is to cluster all data in soft clusters using neural and statistical clustering and fuse them using serial and parallel fusion in conjunction with a neural classifier. The approach has been implemented and tested on a benchmark medical database. The preliminary experiments are very promising.", "venue": "Seventh International Conference on Intelligent Systems Design and Applications (ISDA 2007)", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "e7afd26565e4c92abf634fc74b49d0138c95b425", "url": "https://www.semanticscholar.org/paper/e7afd26565e4c92abf634fc74b49d0138c95b425", "title": "CAMUR: Knowledge extraction from RNA-seq cancer data through equivalent classification rules", "abstract": "Abstract Motivation: Nowadays, knowledge extraction methods from Next Generation Sequencing data are highly requested. In this work, we focus on RNA-seq gene expression analysis and specifically on case\u2013control studies with rule-based supervised classification algorithms that build a model able to discriminate cases from controls. State of the art algorithms compute a single classification model that contains few features (genes). On the contrary, our goal is to elicit a higher amount of knowledge by computing many classification models, and therefore to identify most of the genes related to the predicted class. Results: We propose CAMUR, a new method that extracts multiple and equivalent classification models. CAMUR iteratively computes a rule-based classification model, calculates the power set of the genes present in the rules, iteratively eliminates those combinations from the data set, and performs again the classification procedure until a stopping criterion is verified. CAMUR includes an ad-hoc knowledge repository (database) and a querying tool. We analyze three different types of RNA-seq data sets (Breast, Head and Neck, and Stomach Cancer) from The Cancer Genome Atlas (TCGA) and we validate CAMUR and its models also on non-TCGA data. Our experimental results show the efficacy of CAMUR: we obtain several reliable equivalent classification models, from which the most frequent genes, their relationships, and the relation with a particular cancer are deduced. Availability and implementation: dmb.iasi.cnr.it/camur.php Contact: emanuel@iasi.cnr.it Supplementary information: Supplementary data are available at Bioinformatics online.", "venue": "Bioinform.", "citationCount": 31, "fieldsOfStudy": ["Computer Science", "Medicine", "Biology"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "5c5b2b96891356a159cb9898a4dd48a76e75b877", "url": "https://www.semanticscholar.org/paper/5c5b2b96891356a159cb9898a4dd48a76e75b877", "title": "Unsupervised Feature Construction and Knowledge Extraction from Genome-Wide Assays of Breast Cancer with Denoising Autoencoders", "abstract": "Big data bring new opportunities for methods that efficiently summarize and automatically extract knowledge from such compendia. While both supervised learning algorithms and unsupervised clustering algorithms have been successfully applied to biological data, they are either dependent on known biology or limited to discerning the most significant signals in the data. Here we present denoising autoencoders (DAs), which employ a data-defined learning objective independent of known biology, as a method to identify and extract complex patterns from genomic data. We evaluate the performance of DAs by applying them to a large collection of breast cancer gene expression data. Results show that DAs successfully construct features that contain both clinical and molecular information. There are features that represent tumor or normal samples, estrogen receptor (ER) status, and molecular subtypes. Features constructed by the autoencoder generalize to an independent dataset collected using a distinct experimental platform. By integrating data from ENCODE for feature interpretation, we discover a feature representing ER status through association with key transcription factors in breast cancer. We also identify a feature highly predictive of patient survival and it is enriched by FOXM1 signaling pathway. The features constructed by DAs are often bimodally distributed with one peak near zero and another near one, which facilitates discretization. In summary, we demonstrate that DAs effectively extract key biological principles from gene expression data and summarize them into constructed features with convenient properties.", "venue": "Pacific Symposium on Biocomputing", "citationCount": 127, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "92cb804e5065020d4b292384f4e9a5b75f5a11a0", "url": "https://www.semanticscholar.org/paper/92cb804e5065020d4b292384f4e9a5b75f5a11a0", "title": "Machine Learning for Knowledge Extraction from PHR Big Data", "abstract": "Cloud computing, Internet of things (IOT) and NoSQL database technologies can support a new generation of cloud-based PHR services that contain heterogeneous (unstructured, semi-structured and structured) patient data (health, social and lifestyle) from various sources, including automatically transmitted data from Internet connected devices of patient living space (e.g. medical devices connected to patients at home care). The patient data stored in such PHR systems constitute big data whose analysis with the use of appropriate machine learning algorithms is expected to improve diagnosis and treatment accuracy, to cut healthcare costs and, hence, to improve the overall quality and efficiency of healthcare provided. This paper describes a health data analytics engine which uses machine learning algorithms for analyzing cloud based PHR big health data towards knowledge extraction to support better healthcare delivery as regards disease diagnosis and prognosis. This engine comprises of the data preparation, the model generation and the data analysis modules and runs on the cloud taking advantage from the map/reduce paradigm provided by Apache Hadoop.", "venue": "International Conference on Informatics, Management and Technology in Healthcare", "citationCount": 11, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "c8a86d3e039996793bffeb61d56075c6de1e1f62", "url": "https://www.semanticscholar.org/paper/c8a86d3e039996793bffeb61d56075c6de1e1f62", "title": "Categorization of interestingness measures for knowledge extraction", "abstract": "Finding interesting association rules is an important and active research field in data mining. The algorithms of the Apriori family are based on two rule extraction measures, support and confidence. Although these two measures have the virtue of being algorithmically fast, they generate a prohibitive number of rules most of which are redundant and irrelevant. It is therefore necessary to use further measures which filter uninteresting rules. Many synthesis studies were then realized on the interestingness measures according to several points of view. Different reported studies have been carried out to identify \"good\" properties of rule extraction measures and these properties have been assessed on 61 measures. The purpose of this paper is twofold. First to extend the number of the measures and properties to be studied, in addition to the formalization of the properties proposed in the literature. Second, in the light of this formal study, to categorize the studied measures. This paper leads then to identify categories of measures in order to help the users to efficiently select an appropriate measure by choosing one or more measure(s) during the knowledge extraction process. The properties evaluation on the 61 measures has enabled us to identify 7 classes of measures, classes that we obtained using two different clustering techniques.", "venue": "ArXiv", "citationCount": 8, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "1ecc4544fb5ebe516d64859faf5058b23c9ad448", "url": "https://www.semanticscholar.org/paper/1ecc4544fb5ebe516d64859faf5058b23c9ad448", "title": "Knowledge Extraction and Improved Data Fusion for Sales Prediction in Local Agricultural Markets\u2020", "abstract": "In this paper, a monitoring system of agricultural production is modeled as a Data Fusion System (data from local fairs and meteorological data). The proposal considers the particular information of sales in agricultural markets for knowledge extraction about the associations among them. This association knowledge is employed to improve predictions of sales using a spatial prediction technique, as shown with data collected from local markets of the Andean region of Ecuador. The commercial activity in these markets uses Alternative Marketing Circuits (CIALCO). This market platform establishes a direct relationship between producer and consumer prices and promotes direct commercial interaction among family groups. The problem is presented first as a general fusion problem with a network of spatially distributed heterogeneous data sources, and is then applied to the prediction of products sales based on association rules mined in available sales data. First, transactional data is used as the base to extract the best association rules between products sold in different local markets, knowledge that allows the system to gain a significant improvement in prediction accuracy in the spatial region considered.", "venue": "Italian National Conference on Sensors", "citationCount": 10, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "8e573cb5f0a944779f4c834c42ce702be7ec1e32", "url": "https://www.semanticscholar.org/paper/8e573cb5f0a944779f4c834c42ce702be7ec1e32", "title": "Datamining and Islamic knowledge extraction: alhadith as a knowledge resource", "abstract": "Qur'an, AL-Sunnah and Islamic traditional books are the rich resources for Muslims that used as the sole authoritative source of knowledge, wisdom and law. The challenge for computer scientists is to extract and represent these knowledge, wisdom and law in computer systems, this knowledge is directed or underlying, therefore, to build an intelligent systems which can answer any question with knowledge from Quran, Al-Sunnah and other Islamic books, special techniques for mining data must be used to deal with this issue, which can help society, both Muslim and non-Muslim, to understand and appreciate the Islamic religion, this paper attempts to understand how the new techniques in data mining can extract Islamic knowledge from its resources, and represent these knowledge in meaningful for the user. Moreover, this study concentrates on Hadith as knowledge resource, and proposes approach to classify Hadith to its categories using supervised learning classification. The finding of this study shows that there are several ways to extract knowledge from Hadith depending on the goal of the knowledge.", "venue": "Proceeding of the 3rd International Conference on Information and Communication Technology for the Moslem World (ICT4M) 2010", "citationCount": 17, "fieldsOfStudy": ["Computer Science", "Political Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "3ba734fe5dab3df35c0e375a4ce6b0b4cf2b43f1", "url": "https://www.semanticscholar.org/paper/3ba734fe5dab3df35c0e375a4ce6b0b4cf2b43f1", "title": "Analysis of Subway Passenger Flow for a Smarter City: Knowledge Extraction From Seoul Metro\u2019s \u2018Untraceable\u2019 Big Data", "abstract": "Timely and efficient analysis of big data collected from various gateways installed in a smart city is an intractable problem and requires immediate priority. Given the stochastic and massive nature of big data, the existing literature often relies on artificial intelligence techniques based on information theory. As a new approach, this paper presents a knowledge extraction method based on an analysis of Seoul Metro\u2019s \u2019untraceable\u2019 ridership big data. Without identification information, the untraceable ridership data only shows the hourly accumulation of station entry and exit information. To reconstruct the missing information in the data set, this study proposes a fluid dynamics model and adopts a heuristic genetic algorithm based on optimization theory as the problem solver. The result of our model presents the distribution of the elapsed time defined on an hourly basis taken until a passenger returns to the station they departed from. To validate our model, we acquired subway ridership data with passengers\u2019 identification with permission from Seoul Metro. This paper presents two novel aspects of subway ridership, namely the dependency on departure time and the discrepancy between weekend and weekday traffic. Our analytical approach contributes to solving the problem of extracting hidden knowledge from big collection of data missing critical information, e.g., constantly and autonomously gathered data fragments from numerous gateways in smart cities.", "venue": "IEEE Access", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "ce0b8b6fca7dc089548cc2e9aaac3bae82bb19da", "url": "https://www.semanticscholar.org/paper/ce0b8b6fca7dc089548cc2e9aaac3bae82bb19da", "title": "Making machine learning models interpretable", "abstract": "Data of different levels of complexity and of ever growing diversity of characteristics are the raw materials that machine learning practitioners try to model using their wide palette of methods and tools. The obtained models are meant to be a synthetic representation of the available, observed data that captures some of their intrinsic regularities or patterns. Therefore, the use of machine learning techniques for data analysis can be understood as a problem of pattern recognition or, more informally, of knowledge discovery and data mining. There exists a gap, though, between data modeling and knowledge extraction. Models, de- pending on the machine learning techniques employed, can be described in diverse ways but, in order to consider that some knowledge has been achieved from their description, we must take into account the human cog- nitive factor that any knowledge extraction process entails. These models as such can be rendered powerless unless they can be interpreted ,a nd the process of human interpretation follows rules that go well beyond techni- cal prowess. For this reason, interpretability is a paramount quality that machine learning methods should aim to achieve if they are to be applied in practice. This paper is a brief introduction to the special session on interpretable models in machine learning, organized as part of the 20 th European Symposium on Artificial Neural Networks, Computational In- telligence and Machine Learning. It includes a discussion on the several works accepted for the session, with an overview of the context of wider research on interpretability of machine learning models.", "venue": "The European Symposium on Artificial Neural Networks", "citationCount": 286, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "a53f0a2cbf7d2e31a4219fe2e98aff2ef4483f92", "url": "https://www.semanticscholar.org/paper/a53f0a2cbf7d2e31a4219fe2e98aff2ef4483f92", "title": "Overview of knowledge extraction techniques in five question-answering systems", "abstract": "Knowledge extraction is a complex process allowing the identification of previously unknown structures and potentially useful original information from big masses of data. Among its domains of application, we find the question answering systems. This article aims to provide an overview of five Q/A systems : IntelliServe, LaSIE, Quantum, SBLD and Watson. IntelliServe and Watson are commercial and the others are dedicated to the field of research. Each of these systems uses special techniques for the treatment of requests as well as for the extraction of the response.", "venue": "2014 9th International Conference on Intelligent Systems: Theories and Applications (SITA-14)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "163b198c1032fd66141a5dd728fd73b97d40b96a", "url": "https://www.semanticscholar.org/paper/163b198c1032fd66141a5dd728fd73b97d40b96a", "title": "Automatic Fashion Knowledge Extraction from Social Media", "abstract": "Fashion knowledge plays a pivotal role in helping people in their dressing. In this paper, we present a novel system to automatically harvest fashion knowledge from social media. It unifies three tasks of occasion, person and clothing discovery from multiple modalities of images, texts and metadata. A contextualized fashion concept learning model is applied to leverage the rich contextual information for improving the fashion concept learning performance. At the same time, to counter the label noise within training data, we employ a weak label modeling method to further boost the performance. We build a website to demonstrate the quality of fashion knowledge extracted by our system.", "venue": "ACM Multimedia", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "65621f9f8b500f7e9f53cdeced4c36ab00a64f3d", "url": "https://www.semanticscholar.org/paper/65621f9f8b500f7e9f53cdeced4c36ab00a64f3d", "title": "A risk assessment modeling technique based on knowledge extraction and information diffusion with support specification", "abstract": "For the difficulties of lacking (even no) samples and cases in actual risk evaluation and support decision-making, it was much trouble for risk assessment modeling by general statistic methods and data mining technique. A kind of idea and technique of knowledge extraction and risk assessment was presented based on support specification, and the basic operation steps and modeling route were expounded. Based on an example of aviation weather supporting, a risk evaluation model of weather influencing aircraft taking off and landing safety was established, and the corresponding evaluation experiments were carried out.", "venue": "Int. J. Gen. Syst.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "4ba2701a3a286567a81c6d9dbeaf939cb1047182", "url": "https://www.semanticscholar.org/paper/4ba2701a3a286567a81c6d9dbeaf939cb1047182", "title": "A Simulation Knowledge Extraction-based Decision Support System for the Healthcare Emergency Department", "abstract": "Nowadays, healthcare systems services have become a serious concern for many countries across the world. Due to its complexity and Variability the Emergency Department ED is considered the most critical unit of the hospital and the healthcare systems in general. Increasing the patient satisfaction, reducing as much as possible the patient's waiting time and the patient's length of stay, and optimizing the resources utilization are the overriding preoccupation for any ED manager. To support the performance enhancement in the ED, simulation studies have profusely been involved. In this paper the authors describe a decision support system based on the combination of a simulation and a temporal knowledge extraction model for the operation improvement of the emergency department in the public hospital Lakhdar Bouzidi in Bordj Bou Arreridj Algeria. Their methodology points out how agent-based modeling simulation can benefit from data mining analysis methods to provide a powerful decision support system that can help managers to improve the functioning of the ED.", "venue": "Int. J. Heal. Inf. Syst. Informatics", "citationCount": 6, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "9ca39a86cdfc9cdeefa2f5c564405c61cff1aea0", "url": "https://www.semanticscholar.org/paper/9ca39a86cdfc9cdeefa2f5c564405c61cff1aea0", "title": "Embodied knowledge extraction from human motion using singular value decomposition", "abstract": "Embodied knowledge is the knowledge remembered by the human body and reflected by the dexterity in the motion of the body. In this paper, we propose a new method using singular value decomposition for extracting embodied knowledge from the time-series data of the motion which is measured with various sensors such as an accelerometer, a motion capture system and a force sensor. We compose a matrix from the the time-series data and use the left singular vectors of the matrix as the patterns of the motion and the singular values as a scalar, by which each corresponding left singular vector affects the matrix. Two experiments were conducted to testify the method. One is a gesture recognition experiment in which we categorize gesture motions by two kinds of models with the indexes of similarity and estimation using left singular vectors. The other is an ambulation evaluation experiment in which we distinguished the levels of walking disability using a 3D hyperplane constructed by the singular values. Finally we discuss the characteristic and significance of the embodied knowledge extraction using singular value decomposition proposed in this paper.", "venue": "IEEE International Conference on Fuzzy Systems", "citationCount": 4, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "38cf780ceb28518179d33b0ebaff7d69a1954444", "url": "https://www.semanticscholar.org/paper/38cf780ceb28518179d33b0ebaff7d69a1954444", "title": "Computer-aided Knowledge Extraction and Management for Decision Supporting Processes", "abstract": "This publication presents the essence of the process of obtaining knowledge for the purpose of managing data in strategic decision-taking processes. A method of analysing selected datasets will be described by reference to the semantic analysis and interpretation of data. A new class of systems supporting strategic information management processes \u2013 Understanding Based Management Financial Leverage Ratios Support Systems (UBMFLRSS) \u2013 has been chosen for the analysis. These systems are designed for cognitively analysing financial leverage ratios (financial debt ratios) and reasoning about sources financing the company's assets and the proportion of external capital based on analyses of short-term and long-term liabilities, as well as about the effectiveness of expenditure and the interest paid. Based on the semantic analysis of the value of leverage ratios, it is possible to assess the current standing of the enterprise and its future situation by indicating the direction of change that should be made. Keywords-cognitive financial systems; knowledge extraction; data mining; semantic interpretation", "venue": "International Conference on Networking and Services", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "0ce4fffce27792b3479e31b8a45df141aa532cf7", "url": "https://www.semanticscholar.org/paper/0ce4fffce27792b3479e31b8a45df141aa532cf7", "title": "Action knowledge extraction from Web text", "abstract": "Action knowledge is an important type of behavioral knowledge and of vital importance to many applications in social computing, especially in behavior modeling, analysis and prediction. In this paper, we present a computational method to action knowledge extraction from online media. Our approach is based on mutual bootstrapping and combined with knowledge reasoning. Compared with the related work, our approach can acquire more types of action knowledge, and needs much less human labor. We evaluate the performance of our method using the Web textual data from security informatics domain. The experimental results show the effectiveness of our proposed method.", "venue": "2013 IEEE International Conference on Intelligence and Security Informatics", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "ec56f848358c05460bd110cbc5412958813b1082", "url": "https://www.semanticscholar.org/paper/ec56f848358c05460bd110cbc5412958813b1082", "title": "Cleavage knowledge extraction in HIV-1 protease using hidden Markov model", "abstract": "Inactive HIV is a poly protein precursor. This protein chain has to be cleaved at 9 specific positions to produce individual functional mature proteins responsible for making up a new active virus. Cleavage knowledge extraction in HIV protease will assist in designing effective inhibitors used in the treatment of AIDS. Although much progress has been made in sequencing the viral protease, little progress has been made in understanding the specificity. Several machine learning techniques have been used in understanding the specificity of HIV-1 protease with the highest prediction rate being 92%. In this paper the hidden Markov model is used for analyzing the specificity of HIV-1 protease. The objective is to learn the lock and key mechanism of the protease and protein precursor using the hidden Markov model (HMM) from a set of experimental observations. A good self consistency rate of 96% and recognition accuracy of 95.24% on unseen data is achieved. The HIV protease specificity to cleave between a phenylalanine and tyrosine or proline is also validated by our experiments indicating that the HMM is successful in learning the complex lock and key rule between protease and precursor protein. Used with other techniques, HMM can be used as an effective tool for designing new drugs.", "venue": "Proceedings of 2005 International Conference on Intelligent Sensing and Information Processing, 2005.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "047919e95924ac77062c0313b0e84c35895f3f7c", "url": "https://www.semanticscholar.org/paper/047919e95924ac77062c0313b0e84c35895f3f7c", "title": "Data mining toolkit for extraction of knowledge from LMS", "abstract": "Today, information technology (IT) is an active part of education. Its main impact is in the administration of learning management systems (LMS). The support provided by IT in LMS has generated greater dexterity in the evaluation of the quality of education. The evaluation process usually includes the use of tools applied to online analytical processing (OLAP). The application of OLAP allows the consultation of large amounts of data. Data mining algorithms can be applied to the data collected to perform a pattern analysis. The potential use of these tools has resulted in their specialization, both in the presentation and in the algorithmic techniques, allowing the possibility of educational data mining (EDM). EDM seeks to enhance or customize education within LMS by classifying groups of students in terms of similar characteristics that require specific resources. The ease of use and extensive information about some of the EDM tools has caused many educational institutions to consider them for their own use. However, these institutions often make errors in data management. Errors in the use of data mean that the improvements in LMS are inadequate. The work described in this paper provides a guide on the use of applied methodology in the process of knowledge extraction (KDD). It also enumerates some of the tools that can be used for each step of the process.", "venue": "ICETC 2017", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "64924fc0d5e6f03e7c980062293344dcc4baa65d", "url": "https://www.semanticscholar.org/paper/64924fc0d5e6f03e7c980062293344dcc4baa65d", "title": "A Data-Driven Approach For Design Knowledge Extraction of Synchronous Reluctance Machines Using Multi-Physical Analysis", "abstract": "This paper provides electromagnetic, structural, and acoustic design guidelines for synchronous reluctance machines using a data-driven approach. A design space of different rotor geometries is created and simulated using a finite element package to evaluate the self- and mutual-correlation of different physical performances, such as average torque, torque ripple, iron power loss, saliency ratio, sound pressure level, and mechanical stress caused by centrifugal and magnetic forces. Then, a statistical analysis is conducted to extract knowledge for relating the design and objective spaces with each other. It is demonstrated that not all the objectives must be incorporated into the design process since some of them are non-conflicting. Hence, a motor designer can numerically evaluate which design variables should be changed and by how much in order to fulfill the design specifications. Lastly, multiple designs are selected based on different requirements. Useful guidelines for selecting the appropriate motor speed and voltage ratings are proposed while considering structurally-reliable designs.", "venue": "International Conference on Electrical Machines", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "bba57c53ab9b600f71d888601ed0aa03812c8199", "url": "https://www.semanticscholar.org/paper/bba57c53ab9b600f71d888601ed0aa03812c8199", "title": "MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding", "abstract": "Recently, there has been an increasing interest in building question answering (QA) models that reason across multiple modalities, such as text and images. However, QA using images is often limited to just picking the answer from a pre-defined set of options. In addition, images in the real world, especially in news, have objects that are co-referential to the text, with complementary information from both modalities. In this paper, we present a new QA evaluation benchmark with 1,384 questions over news articles that require cross-media grounding of objects in images onto text. Specifically, the task involves multi-hop questions that require reasoning over image-caption pairs to identify the grounded visual object being referred to and then predicting a span from the news body text to answer the question. In addition, we introduce a novel multimedia data augmentation framework, based on cross-media knowledge extraction and synthetic question-answer generation, to automatically augment data that can provide weak supervision for this task. We evaluate both pipeline-based and end-to-end pretraining-based multimedia QA models on our benchmark, and show that they achieve promising performance, while considerably lagging behind human performance hence leaving large room for future work on this challenging new task.", "venue": "AAAI Conference on Artificial Intelligence", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "3acef85d20abb59f2bda90b2a0dda05dcf214379", "url": "https://www.semanticscholar.org/paper/3acef85d20abb59f2bda90b2a0dda05dcf214379", "title": "An expert system for land cover classification", "abstract": "A framework to represent a broad class of problems in the analysis of remote sensing imagery is proposed, and an inference engine to tackle such problems is derived. A simple model for spectral knowledge representation is used along with a method for quantification of knowledge through an evidential approach. An automatic knowledge extraction technique is also proposed to gather knowledge from training samples. The techniques of knowledge extraction, representation and inferencing have been used to do a land cover analysis on two data sets, and the results are compared with contemporary digital techniques. It is found that the proposed approach has the advantages of avoiding commission errors, and can incorporate non-spectral and collateral knowledge, while its accuracy using only spectral knowledge is comparable with standard digital methods. >", "venue": "IEEE Trans. Geosci. Remote. Sens.", "citationCount": 36, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "831eff1679be344e480cf3cefd13e4c8d8560d3a", "url": "https://www.semanticscholar.org/paper/831eff1679be344e480cf3cefd13e4c8d8560d3a", "title": "Managing the knowledge contained in electronic documents: a clustering method for text mining", "abstract": "The huge amount of unstructured data available on the Web and the intranets creates an information overloading problem. So, managing the knowledge contained in the textual documents is an important problem of Knowledge Management. Knowledge Extraction from collections of data is possible by Knowledge Discovery in Database (KDD), an interactive and iterative process focused on the exploration of data to discover new and interesting patterns within them. The fundamental phase of KDD process is Data Mining if data are in structured form and Text Mining when they are unstructured. This paper describes a prototype of a vertical corporate portal that implements a KDD process for knowledge extraction from unstructured data contained in textual documents. Text mining is realized through a clustering method that produces a partition of a set of documents on the basis of their contents characterized through the frequency of the words.", "venue": "12th International Workshop on Database and Expert Systems Applications", "citationCount": 36, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "6f8ef076c4af9b4b9f05265a30b15c9c5abf1231", "url": "https://www.semanticscholar.org/paper/6f8ef076c4af9b4b9f05265a30b15c9c5abf1231", "title": "Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective", "abstract": "Knowledge Extraction (KE) which aims to extract structural information from unstructured texts of-ten suffers from data scarcity and emerging unseen types, i.e. , low-resource scenarios. Many neural approaches on low-resource KE have been widely investigated and achieved impressive performance. In this paper, we present a literature review towards KE in low-resource scenarios, and systematically categorize existing works into three paradigms: (1) exploiting higher -resource data, (2) exploiting stronger models, and (3) exploiting data and models together . In addition, we describe promising applications and outline some potential directions for future research. We hope that our survey can help both the academic and industrial community to better understand this \ufb01eld, inspire more ideas and boost broader applications.", "venue": "ArXiv", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "c66fa6586ebc5616afa7181f22b2e39424280554", "url": "https://www.semanticscholar.org/paper/c66fa6586ebc5616afa7181f22b2e39424280554", "title": "Construction of an Attribute-Value Representation for Semi-structured Medical Findings Knowledge Extraction", "abstract": "Data Mining is a process related to analysis, understanding and knowledge extraction from databases. In order to perform this process it is usually necessary to represent the data in the so called attribute-value format. This work proposes an extension of a methodology which supports, through a semi-automatic process, the construction of a table in the attribute-value format from information contained in medical findings which are described in natural language (Portuguese). A case study in which the methodology has been applied to a collection of Upper Digestive Endoscopies\u2019 medical findings is presented. Results show the suitability of our proposal.", "venue": "CLEI Electron. J.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "96159be6e5d7cb43d0c023b4cd97018682a9042f", "url": "https://www.semanticscholar.org/paper/96159be6e5d7cb43d0c023b4cd97018682a9042f", "title": "Effective image and video mining: an overview of model-based approaches", "abstract": "This paper is dedicated to revisiting image and video mining techniques from the viewpoint of image modeling approaches, which constitute the theoretical basis for these techniques. The most important areas belonging to image or video mining are: image knowledge extraction, content-based image retrieval, video retrieval, video sequence analysis, change detection, model learning, as well as object recognition. Traditionally, these areas have been developed independently, and hence have not benefited from some common sense approaches which provide potentially optimal and time-efficient solutions. Two different types of input data for knowledge extraction from an image collection or video sequences are considered: original image or symbolic (model) description of the image. Several basic models are described briefly and compared with each other in order to find effective solutions for the image and video mining problems. They include feature-based models and object-related structural models for the representation of spatial and temporal entities (objects, scenes or events).", "venue": "MDM '05", "citationCount": 22, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "cb58d6df36fcffcf2ee0c7a2f6f6e620edc6efbb", "url": "https://www.semanticscholar.org/paper/cb58d6df36fcffcf2ee0c7a2f6f6e620edc6efbb", "title": "Knowledge Extraction from Geographical Databases for Land Use Data Production", "abstract": "Our study focuses on the task of land use evolution in urban environment which is fundamental in revealing the territorial planning. It refers crucially to the use of spatial data mining tools due to their high potential in handling with spatial data characteristics. The results of our knowledge discovery process are spatial and spatiotemporal association rules referring to the land use and its evolution. Three proposals based on different knowledge extraction techniques are detailed. The first approach aims to extract spatiotemporal association rules by introducing time into the attributes. The second approach forecasts the extracted rules at different dates. The third approach is devoted to the mining of spatiotemporal association rules. This proposal looks for rules that relate properties of reference objects with properties of other spatial relevant objects. The extracted patterns are relationships involving the spatial objects during time periods. To prove the applicability of each approach, experimentations are conducted on real world data. The obtained results are promising.", "venue": "Environmental Information Systems", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "b3f7362a266fade538fc7c5f7f6c5bdf5e20feab", "url": "https://www.semanticscholar.org/paper/b3f7362a266fade538fc7c5f7f6c5bdf5e20feab", "title": "A survey on data analysis on large-Scale wireless networks: online stream processing, trends, and challenges", "abstract": "In this paper we focus on knowledge extraction from large-scale wireless networks through stream processing. We present the primary methods for sampling, data collection, and monitoring of wireless networks and we characterize knowledge extraction as a machine learning problem on big data stream processing. We show the main trends in big data stream processing frameworks. Additionally, we explore the data preprocessing, feature engineering, and the machine learning algorithms applied to the scenario of wireless network analytics. We address challenges and present research projects in wireless network monitoring and stream processing. Finally, future perspectives, such as deep learning and reinforcement learning in stream processing, are anticipated.", "venue": "Journal of Internet Services and Applications", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "506f93148ab4565efd6cf66c49ca4640d1d701f0", "url": "https://www.semanticscholar.org/paper/506f93148ab4565efd6cf66c49ca4640d1d701f0", "title": "Exploring Personal Knowledge Extraction from Conversations with CHARM", "abstract": "Incorporating users' personal facts enhances the quality of many downstream services. Automated extraction of such personal knowledge has recently received considerable attention. However, often the operation of extraction models is not exposed to the user, making predictions inexplicable. In this work we present a web demonstration platform showcasing a recent personal knowledge extraction model, CHARM, which provides information on how the prediction was made and which data was decisive for it. Our demonstration explores two potential sources of input data: conversational transcripts and social media submissions.", "venue": "WSDM", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "e9b1fea6920b886c2d009c6cdb6fd4d3f22cd3aa", "url": "https://www.semanticscholar.org/paper/e9b1fea6920b886c2d009c6cdb6fd4d3f22cd3aa", "title": "Cross-Media Knowledge Extraction in the Car Manufacturing Industry", "abstract": "In this paper, we present a novel framework for machine learning-based cross-media knowledge extraction. The framework is specifically designed to handle documents composed of three types of media -- text, images and raw data -- and to exploit the evidence for an extracted fact from across the media. We validate the framework by applying it in the design and development of cross-media extraction systems in the context of two real-world use cases in the car manufacturing industry. Moreover, we show that in these use cases the cross-media approach effectively improves system extraction accuracy.", "venue": "2009 21st IEEE International Conference on Tools with Artificial Intelligence", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "068e0e73931f0538e5deed01510935babacab366", "url": "https://www.semanticscholar.org/paper/068e0e73931f0538e5deed01510935babacab366", "title": "Data representation for diagnostic neural networks", "abstract": "A paradigm for diagnostic neural network systems that emphasizes informative data representation and encoding and uses generic preprocessing techniques to extract knowledge from database records is discussed. The proposed diagnostic system differs from other approaches to automatic knowledge extraction in the following ways: by emphasizing the importance of intelligent encoding and preprocessing of raw data, rather than classifications; by demonstrating the importance of making a clear distinction between diagnostic and classification tasks; and by providing a generic, uniform representation for data records comprising interdependent, heterogeneous features. The correlation matrix memory (CMM), a linear system with a single-layer of input-output connections, that is used as the neural network system's classifier is described. The limitations of the learning system are discussed.<<ETX>>", "venue": "IEEE Expert", "citationCount": 42, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "5ad86bee4de14020249a46dc8790bc67f1ab9738", "url": "https://www.semanticscholar.org/paper/5ad86bee4de14020249a46dc8790bc67f1ab9738", "title": "PFARM guest talk: \u201cProactivity = observation + analysis + knowledge extraction + action planning?\u201d", "abstract": "The core of proactive system management is the exploitation of the built-in intelligence of computer infrastructures in order to implement self-\u2217 properties for the assurance of a guaranteed quality of service even in the case of faults by reacting to them prior they affect services. The creation of a proper system control policy and implementation is always highly challenging. Modern private and public infrastructures are extremely large; accordingly, fault recovery relies to an increasing extent on rough granular reconfiguration. This way, traditional, discrete representation based fine granular fault handling mechanisms need to be complemented with efficient system level policies using continuous quantitative system models. A continuous observation-learning-policy improvement process is needed to build up and maintain efficient control policies. The most efficient way is the granular refinement of an initial control policy by observing the system behavior under an initial control policy. Subsequently, the improvement of the quality of the control is based on the processing of the basic information collected in the form of logs. Note, that this roundtrip is the only way to cope with the typically rapidly evolving and changing application environments. The presentation will provide an overview on the relation of data acquisition, log processing, modern signal processing and artificial intelligence for knowledge extraction needed to a proper, empirical model based system monitoring and control policy. The basic approaches and tools will be illustrated by a complete example workflow motivated by the recent industry sponsored research projects at BME on large scale infrastructure control.", "venue": "DSN 2011", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "f5d3cd277cb217d7eb9a8128458f63614e043df6", "url": "https://www.semanticscholar.org/paper/f5d3cd277cb217d7eb9a8128458f63614e043df6", "title": "Deep Learning Approaches for Fashion Knowledge Extraction From Social Media: A Review", "abstract": "Fashion knowledge encourages people to properly dress and faces not only physiological necessity of users, but also the requirement of social practices and activities. It usually includes three jointly related aspects of: occasion, person and clothing. Nowadays, social media platforms allow users to interact with each other online to share opinions and information. The use of social media sites such as Instagram has already spread to almost every fashion brand and been evaluated as business take-off tools. With the heightened use of social media as a means of marketing communication for fashion brands, it has become necessary to empirically analyse and extract fashion knowledge from them. Thus, social brands are investing on them. In this way, they can understand the consumer\u2019s preferences. This change is also having a significant impact on social media data analysis. To solve this issue, the Deep learning (DL) methods are proven to be effective solutions due to their automatic learning capability. However, little systematic work currently exists on how researchers have applied DL for analysing fashion knowledge from social media data. Hence, this contribution outlines DL-based techniques for social media data related to fashion domain. In this study, a review of the dataset within the fashion world and the DL methods applied on, it is presented to help out new researchers interested in this subject. In particular, five different tasks will be considered: Object Detection, that includes Clothes Landmark Detection, Clothes Parsing and Product Retrieval, Fashion Classification, Clothes Generation, Automatic Fashion Knowledge Extraction and Clothes Recommendation. Therefore, the purpose of this paper is to underline the multiple applications within the fashion world using deep learning techniques. However, this review does not cover all the methods used: in fact, only Deep Learning methods have been analyzed. This choice was made since, given the huge amount of fashion social media data that has been collected, Deep Learning methods achieve the best performance both in terms of accuracy and time. Limitations point towards unexplored areas for future investigations, serving as useful guidelines for future research directions.", "venue": "IEEE Access", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "1dd29c45d3c69d071ec50e0d165ccd304bb270a1", "url": "https://www.semanticscholar.org/paper/1dd29c45d3c69d071ec50e0d165ccd304bb270a1", "title": "Sentimental Content Analysis and Knowledge Extraction from News Articles", "abstract": "In web era, since technology has revolutionized mankind life, plenty of data and information are published on the Internet each day. For instance, news agencies publish news on their websites all over the world. These raw data could be an important resource for knowledge extraction. These shared data contain emotions (i.e., positive, neutral or negative) toward various topics; therefore, sentimental content extraction could be a beneficial task in many aspects. Extracting the sentiment of news illustrates highly valuable information about the events over a period of time, the viewpoint of a media or news agency to these events. In this paper an attempt is made to propose an approach for news analysis and extracting useful knowledge from them. Firstly, we attempt to extract a noise robust sentiment of news documents; therefore, the news associated to six countries: United State, United Kingdom, Germany, Canada, France and Australia in 5 different news categories: Politics, Sports, Business, Entertainment and Technology are downloaded. In this paper we compare the condition of different countries in each 5 news topics based on the extracted sentiments and emotional contents in news documents. Moreover, we propose an approach to reduce the bulky news data to extract the hottest topics and news titles as a knowledge. Eventually, we generate a word model to map each word to a fixed-size vector by Word2Vec in order to understand the relations between words in our collected news database.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "2cf4c438128cafcab586647cfb25569a11a32fb4", "url": "https://www.semanticscholar.org/paper/2cf4c438128cafcab586647cfb25569a11a32fb4", "title": "Knowledge Extraction from Healthcare Data Using User-Adaptable Keywords-Based Query Language", "abstract": "Nowadays, the volume of the information gathered by any organization increases more and more rapidly. It is essential to be able to use this information efficiently for it to benefit the operation of the organization. There is no point of gathering the information if it is not converted into knowledge. The knowledge extraction process becomes the backbone of any successful organization. Moreover, the extraction of the knowledge must be quick and efficient, so that the newly-obtained knowledge can be put in use at once. The problem addressed in this paper is how to allow the domain expert to extract the knowledge from their information systems themselves without involving the third party in the form of an IT specialist. This goal is of utmost importance for the domain experts, e.g. hospital managers and physicians, because they need to make decisions based on the available knowledge and to do it rapidly and efficiently. We propose a system in this paper that allows formulating queries in the natural language and that also adapts to the specifics of the user. Our experiments show that such kind of querying could provide an improvement in the decision-making process of healthcare professionals.", "venue": "ICISDM", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "04222f4199ac4bd6750825c6bdf21a07a7278592", "url": "https://www.semanticscholar.org/paper/04222f4199ac4bd6750825c6bdf21a07a7278592", "title": "A Big-Data platform for Medical Knowledge Extraction from Electronic Health Records: Automatic Assignment of ICD-9 Codes", "abstract": "In this paper, we present a big data plarform for knowledge categorization in Electronic Health Records and examine its application to automatic assignment of ICD-9 codes. Our platform relies on reusable, adaptable components that can perform knowledge extraction at a large scale. For the ICD-9 automatic assignment, we build and validate our approach using data from the MIMIC II Clinical Database that contains over 20,000 discharge summaries. We show that our platform can achieve state of the art performance in this dataset and that the classification results improve with more data. Overall, in the first level of the ICD-9 hierarchy our algorithm achieves an average precision of 79.7% for an average recall of 70.2%.", "venue": "Petra", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "44426ff636d4fe03a958b18bb3401cb0f6b9057c", "url": "https://www.semanticscholar.org/paper/44426ff636d4fe03a958b18bb3401cb0f6b9057c", "title": "DENAS: automated rule generation by knowledge extraction from neural networks", "abstract": "Deep neural networks (DNNs) have been widely applied in the software development process to automatically learn patterns from massive data. However, many applications still make decisions based on rules that are manually crafted and verified by domain experts due to safety or security concerns. In this paper, we aim to close the gap between DNNs and rule-based systems by automating the rule generation process via extracting knowledge from well-trained DNNs. Existing techniques with similar purposes either rely on specific DNNs input instances or use inherently unstable random sampling of the input space. Therefore, these approaches either limit the exploration area to a local decision-space of the DNNs or fail to converge to a consistent set of rules. The resulting rules thus lack representativeness and stability. In this paper, we address the two aforementioned shortcomings by discovering a global property of the DNNs and use it to remodel the DNNs decision-boundary. We name this property as the activation probability, and show that this property is stable. With this insight, we propose an approach named DENAS including a novel rule-generation algorithm. Our proposed algorithm approximates the non-linear decision boundary of DNNs by iteratively superimposing a linearized optimization function. We evaluate the representativeness, stability, and accuracy of DENAS against five state-of-the-art techniques (LEMNA, Gradient, IG, DeepTaylor, and DTExtract) on three software engineering and security applications: Binary analysis, PDF malware detection, and Android malware detection. Our results show that DENAS can generate more representative rules consistently in a more stable manner over other approaches. We further offer case studies that demonstrate the applications of DENAS such as debugging faults in the DNNs and generating signatures that can detect zero-day malware.", "venue": "ESEC/SIGSOFT FSE", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "8907b014993485748c2fb9184feacb83408092f3", "url": "https://www.semanticscholar.org/paper/8907b014993485748c2fb9184feacb83408092f3", "title": "A Data-Driven Approach for Design Knowledge Extraction of Synchronous Reluctance Machines Using Multi-Physical Analysis", "abstract": "Using a data-driven approach, this paper provides electromagnetic, structural, acoustic, and thermal guidelines for designing synchronous reluctance machines. Design spaces of different rotor geometries are created and simulated using finite element packages to evaluate the self- and mutual-correlation of different physical performances, such as average torque, torque ripple, efficiency, power factor, saliency ratio, sound pressure level, mechanical stress, and average winding temperature. Then, a statistical analysis is conducted to extract knowledge and guidelines for relating the design and objective spaces. It is demonstrated that not all the objectives must be incorporated into the design process since some of them are non-conflicting. Hence, a motor designer can numerically evaluate which design variables should be changed and by how much in order to fulfill the design specifications. Multiple designs are selected based on different multi-physical requirements. Useful guidelines for selecting the appropriate motor speed and voltage ratings are proposed while considering structurally reliable designs.", "venue": "IEEE transactions on industry applications", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "2bfcd3a09ce0314258df1e3b3c85d699fa6b86e0", "url": "https://www.semanticscholar.org/paper/2bfcd3a09ce0314258df1e3b3c85d699fa6b86e0", "title": "A Novel Fuzzy Rough Granular Neural Network for Classification", "abstract": "A novel fuzzy rough granular neural network (NFRGNN) based on the multilayer perceptron using backpropagation algorithm is described for fuzzy classification of patterns. We provide a development strategy of knowledge extraction from data using fuzzy rough set theoretic techniques. Extracted knowledge is then encoded into the network in the form of initial weights. The granular input vector is presented to the network while the target vector is provided in terms of membership values and zeros. The superiority of NFRGNN is demonstrated on several real life data sets.", "venue": "International Journal of Computational Intelligence Systems", "citationCount": 16, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "29d9c5df1b4ef8b70b98f47045a98f321bb2565a", "url": "https://www.semanticscholar.org/paper/29d9c5df1b4ef8b70b98f47045a98f321bb2565a", "title": "Editorial for special issue on Big Data and knowledge extraction for Cyber-Physical Systems 2016", "abstract": "The cyber-physical system (CPS) has been coming into our view and will be applied in our daily life and business process management. The emerging CPS must be robust and responsive for its implementation in coordinated, distributed, and connected ways. It is expected that future CPS will far exceed today\u2019s systems on a variety of characteristics, for example, capability, adaptability, resiliency, safety, security, and usability. Big Data are large, complex, or rapidly generated data sets that cannot be processed by traditional technologies. Every day, the entities comprising decision makers, managers, engineers, scientists, and citizens are faced with a multitude of constantly flowing data streams coming from different sources in different formats. Making sense of these volumes of Big Data requires cutting-edge tools that can analyze and extract useful knowledge from vast and diverse data streams. The wonderful living of humans and high efficiency of business rely mostly on how to use the Big Data intelligently and correctly and how to retrieve useful knowledge from the massive data\u2014then, it would be possible to seamlessly integrate the virtual world and the physical world. How to integrate and analyze the data? How to retrieve knowledge from Big Data? How to share knowledge among smart things? How to ensure security and protect privacy? These are some of the questions in the long list of challenges that are needed to be addressed in future CPS. This Special Issue solicits high-quality contributions with consolidated and thoroughly evaluated research in the area of Big Data and knowledge extraction for CPS that are worthy of archival publication in the journal. It is intended to (1) provide a summary of research that advances knowledge acquisition and utilization from Big Data for CPS and (2) serve as a comprehensive collection of the current state-of-the-art technologies within the context. Due to the great effort made in publicity, we received 37 submissions from both academia and industry in the relevant fields. Following a strict review process, we accepted 22 papers for this Special Issue. Each of the papers was peer-reviewed by at least two experts in the field. In the following, we provide a brief introduction to each paper. The paper \u2018\u2018Parallel Irregular Fusion Estimation Based on Nonlinear Filter for Indoor RFID Tracking System,\u2019\u2019 authored by Xue-Bo Jin, Chao Dou, Ting-li Su, Xiao-fen Lian, and Yan shi, proposes a real-time indoor RFID tracking system which employs an irregular estimation strategy with parallel structure, where the dynamic model update and state fusion estimation are synchronously processed. Based on the extended Kalman filter (EKF) and unscented Kalman filter (UKF), two nonlinear estimation methods are designed. The tracking performance is evaluated, and the simulation results show that the UKF method achieves lower covariance in indoor RFID tracking, while the EKF one has less calculation time. To efficiently track multi-level packages, the paper \u2018\u2018Multi-level Package Identification Scheme Based on RFID Code and Related Package Message\u2019\u2019 by Yanghua Gao, Zhihua Zhang, Huanwen Wang, and Hailiang Lu proposes an RFID code scheme. With the investigation of the relation among multi-level packages, the subordinate relation of different level RFID codes can be established by recording and storing the inclusion relation between codes in database. Furthermore, to verify the feasibility of the proposed code scheme, it is applied to the fast-moving consumer goods management which involves three-level packages. In the paper \u2018\u2018On Energy-balanced Backpressure Routing Mechanisms for Stochastic Energy Harvesting Wireless Sensor Networks\u2019\u2019 by Zheng Liu, Xinyu Yang, Peng Zhao, and Wei Yu, the unpredictablility of the harvestable energy is considered, and the stochastic Lyapunov optimization framework is employed to jointly manage energy and make routing decisions. The purpose is to mitigate the energy imbalance problem. Two distributed online policies are developed, which are as follows: (1) Energy-balanced Backpressure Routing Algorithm (EBRA) for lossless networks and (2) Enhanced Energy-balanced Backpressure Routing Algorithm (EEBRA) for time-varying wireless", "venue": "Int. J. Distributed Sens. Networks", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "a3a98bf62f4a577b721b5a68d047fc6f65ad8e0b", "url": "https://www.semanticscholar.org/paper/a3a98bf62f4a577b721b5a68d047fc6f65ad8e0b", "title": "Positing the problem: enhancing classification of extremist web content through textual analysis", "abstract": "Webpages with terrorist and extremist content are key factors in the recruitment and radicalization of disaffected young adults who may then engage in terrorist activities at home or fight alongside terrorist groups abroad. This paper reports on advances in techniques for classifying data collected by the Terrorism and Extremism Network Extractor (TENE) webcrawler, a custom-written program that browses the World Wide Web, collecting vast amounts of data, retrieving the pages it visits, analyzing them, and recursively following the links out of those pages. The textual content is subjected to enhanced classification through software analysis, using the Posit textual analysis toolset, generating a detailed frequency analysis of the syntax, including multi-word units and associated part-of-speech components. Results are then deployed in a knowledge extraction process using knowledge extraction algorithms, e.g., from the WEKA system. Indications are that the use of the data enrichment through application of Posit analysis affords a greater degree of match between automatic and manual classification than previously attained. Furthermore, the incorporation and deployment of these technologies promises to provide public safety officials with techniques that can help to detect terrorist webpages, gauge the intensity of their content, discriminate between webpages that do or do not require a concerted response, and take appropriate action where warranted.", "venue": "2016 IEEE International Conference on Cybercrime and Computer Forensic (ICCCF)", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "b423da99f52e8ebc8e7a39be5a5a540abf302154", "url": "https://www.semanticscholar.org/paper/b423da99f52e8ebc8e7a39be5a5a540abf302154", "title": "Data, information, knowledge, understanding: computing up the meaning hierarchy", "abstract": "This paper discusses computational approaches to computing meaning, arguing that the following four steps are necessary intermediates between the appearance of words in natural language, in whatever medium, and the meaning we want to compute from natural language: pattern detection, information structure creation, knowledge extraction, and application to understanding. It presents computational models for each step as well, in the form of algorithms that detect repetitive or persistent patterns in otherwise unrestricted natural language text, construct information structures from those patterns, extract knowledge from those information structures, and apply the knowledge to problems of understanding situation descriptions obtained in natural language.", "venue": "SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "57a4c3b58f996656744a01c8e2f1a5826f5f6e58", "url": "https://www.semanticscholar.org/paper/57a4c3b58f996656744a01c8e2f1a5826f5f6e58", "title": "An algorithm for knowledge extraction", "abstract": "An algorithm with an embedded genetic algorithm is presented in this paper, for attribute reduction and knowledge extraction. Due to the high dimension of text data, the classification precision is applied as the threshold of an evaluation index for the iterative process of the algorithm, and is also utilized as the fitness function of the embedded genetic algorithm. Further, the rules from the reduced attribute set generated by the algorithm are given. Finally, the validity and accuracy of the algorithm are verified on the data set.", "venue": "International Conferences on Computers, Information Processing and Advanced Education", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "d21c882d16092ec5e0e88654856ebb77a4ffbaf5", "url": "https://www.semanticscholar.org/paper/d21c882d16092ec5e0e88654856ebb77a4ffbaf5", "title": "CAVE-SOM: Immersive visual data mining using 3D Self-Organizing Maps", "abstract": "Data mining techniques are becoming indispensable as the amount and complexity of available data is rapidly growing. Visual data mining techniques attempt to include a human observer in the loop and leverage human perception for knowledge extraction. This is commonly allowed by performing a dimensionality reduction into a visually easy-to-perceive 2D space, which might result in significant loss of important spatial and topological information. To address this issue, this paper presents the design and implementation of a unique 3D visual data mining framework - CAVE-SOM. The CAVE-SOM system couples the Self-Organizing Map (SOM) algorithm with the immersive Cave Automated Virtual Environment (CAVE). The main advantages of the CAVE-SOM system are: i) utilizing a 3D SOM to perform dimensionality reduction of large multi-dimensional datasets, ii) immersive visualization of the trained 3D SOM, iii) ability to explore and interact with the multi-dimensional data in an intuitive and natural way. The CAVE-SOM system uses multiple visualization modes to guide the visual data mining process, for instance the data histograms, U-matrix, connections, separations, uniqueness and the input space view. The implemented CAVE-SOM framework was validated on several benchmark problems and then successfully applied to analysis of wind-power generation data. The knowledge extracted using the CAVE-SOM system can be used for further informed decision making and machine learning.", "venue": "The 2011 International Joint Conference on Neural Networks", "citationCount": 29, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "3fb055822bb7d2d1417a91626cc845614c80f775", "url": "https://www.semanticscholar.org/paper/3fb055822bb7d2d1417a91626cc845614c80f775", "title": "Novel Architecture for supporting medical decision making of different data types based on Fuzzy Cognitive Map Framework", "abstract": "Medical problems involve different types of variables and data, which have to be processed, analyzed and synthesized in order to reach a decision and/or conclude to a diagnosis. Usually, information and data set are both symbolic and numeric but most of the well-known data analysis methods deal with only one kind of data. Even when fuzzy approaches are considered, which are not depended on the scales of variables, usually only numeric data is considered. The medical decision support methods usually are accessed in only one type of available data. Thus, sophisticated methods have been proposed such as integrated hybrid learning approaches to process symbolic and numeric data for the decision support tasks. Fuzzy cognitive maps (FCM) is an efficient modelling method, which is based on human knowledge and experience and it can handle with uncertainty and it is constructed by extracted knowledge in the form of fuzzy rules. The FCM model can be enhanced if a fuzzy rule base (IF-THEN rules) is available. This rule base could be derived by a number of machine learning and knowledge extraction methods. Here it is introduced a hybrid attempt to handle situations with different types of available medical and /or clinical data and with difficulty to handle them for decision support tasks using soft computing techniques.", "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society", "citationCount": 30, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "862dd8a5d06aec313e123d36c04d73a3bcab2927", "url": "https://www.semanticscholar.org/paper/862dd8a5d06aec313e123d36c04d73a3bcab2927", "title": "An ambient intelligence architecture for extracting knowledge from distributed sensors", "abstract": "Precisely monitoring the environmental conditions is an essential requirement for AmI projects, but the wealth of data generated by the sensing equipment may easily overwhelm the modules devoted to higher-level reasoning, clogging them with irrelevant details. The present work proposes a new approach to knowledge extraction from raw data that addresses this issue at different levels of abstraction. Wireless sensor networks are used as the pervasive sensory tool, and their computational capabilities are exploited to remotely perform preliminary data processing. A central intelligent unit subsequently extracts higher-level concepts represented in a geometrical space and carries on symbolic reasoning based on them. The same tiered architecture is replicated in order to provide further levels of abstraction.", "venue": "ICIS", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "8e81d6ab8c9b49eb2cb350e8ced6980cdfb9540a", "url": "https://www.semanticscholar.org/paper/8e81d6ab8c9b49eb2cb350e8ced6980cdfb9540a", "title": "GEO2Enrichr: browser extension and server app to extract gene sets from GEO and analyze them for biological functions", "abstract": "MOTIVATION\nIdentification of differentially expressed genes is an important step in extracting knowledge from gene expression profiling studies. The raw expression data from microarray and other high-throughput technologies is deposited into the Gene Expression Omnibus (GEO) and served as Simple Omnibus Format in Text (SOFT) files. However, to extract and analyze differentially expressed genes from GEO requires significant computational skills.\n\n\nRESULTS\nHere we introduce GEO2Enrichr, a browser extension for extracting differentially expressed gene sets from GEO and analyzing those sets with Enrichr, an independent gene set enrichment analysis tool containing over 70\u2009000 annotated gene sets organized into 75 gene-set libraries. GEO2Enrichr adds JavaScript code to GEO web-pages; this code scrapes user selected accession numbers and metadata, and then, with one click, users can submit this information to a web-server application that downloads the SOFT files, parses, cleans and normalizes the data, identifies the differentially expressed genes, and then pipes the resulting gene lists to Enrichr for downstream functional analysis. GEO2Enrichr opens a new avenue for adding functionality to major bioinformatics resources such GEO by integrating tools and resources without the need for a plug-in architecture. Importantly, GEO2Enrichr helps researchers to quickly explore hypotheses with little technical overhead, lowering the barrier of entry for biologists by automating data processing steps needed for knowledge extraction from the major repository GEO.\n\n\nAVAILABILITY AND IMPLEMENTATION\nGEO2Enrichr is an open source tool, freely available for installation as browser extensions at the Chrome Web Store and FireFox Add-ons. Documentation and a browser independent web application can be found at http://amp.pharm.mssm.edu/g2e/.\n\n\nCONTACT\navi.maayan@mssm.edu.", "venue": "Bioinform.", "citationCount": 40, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "52e38281269f05a49ee97cf950be02b7ef5380bb", "url": "https://www.semanticscholar.org/paper/52e38281269f05a49ee97cf950be02b7ef5380bb", "title": "Clustering-Based Approaches for Symbolic Knowledge Extraction", "abstract": ". Opaque models belonging to the machine learning world are ever more exploited in the most di\ufb00erent application areas. These models, acting as black boxes (BB) from the human perspective, cannot be entirely trusted if the application is critical unless there exists a method to extract symbolic and human-readable knowledge out of them. In this paper we analyse a recurrent design adopted by symbolic knowledge extractors for BB regressors\u2014that is, the creation of rules associated with hypercubic input space regions. We argue that this kind of partitioning may lead to suboptimal solutions when the data set at hand is high-dimensional or does not satisfy symmetric constraints. We then propose a (deep) clustering-based approach to be performed before symbolic knowledge extraction to achieve better performance with data sets of any kind.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "bc5281012ac26132b2e049c6a27e2a1cd7919ff7", "url": "https://www.semanticscholar.org/paper/bc5281012ac26132b2e049c6a27e2a1cd7919ff7", "title": "Wikidata-lite for Knowledge Extraction and Exploration", "abstract": "\u2014Wikidata is the largest collaborative general knowl- edge graph supported by a worldwide community. It includes many helpful topics for knowledge exploration and data science applications. However, due to the enormous size of Wikidata, it is challenging to retrieve a large amount of data with millions of results, make complex queries requiring large aggregation operations, or access too many statement references. This paper introduces our preliminary works on Wikidata-lite, a toolkit to build a database of\ufb02ine for knowledge extraction and exploration, e.g., retrieving item information, statements, provenances, or searching entities by their keywords, attributes. Wikidata-lite has high performance and memory ef\ufb01ciency, much faster than the of\ufb01cial Wikidata SPARQL endpoint for big queries. The Wikidata-lite repository is available at https://github.com/phucty/ wikidb.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "267b245abc80534b94f79d5b8593708af7b58ca5", "url": "https://www.semanticscholar.org/paper/267b245abc80534b94f79d5b8593708af7b58ca5", "title": "A multi-omics graph database for data integration and knowledge extraction", "abstract": "Major recent advances in sequencing technologies have created new opportunities for studying the complex microbiome domain. However, microbial communities have many unknown roles and unclear impacts on their host environment. The increased availability of microbial omics data associated with heterogeneous metadata has the potential to revolutionize microbiome research. This study proposes a novel data-integration model and a practical pipeline to explore microbial community functions with the integration of omics data. Three case studies were employed to highlight the advanced abilities and applications of our graph database model. Furthermore, we show that a variety of information can be queried against our model and easily extracted using the proposed analysis pipeline. Our findings suggest that the proposed model is highly queryable and provides a critical analytical platform to extract useful knowledge from multi-omics data. We show that such knowledge extraction can lead to new discoveries, particularly when utilizing all available datasets.", "venue": "BCB", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "01ae11b5dfc2ffdf0c054d67f6c3dee2826059d3", "url": "https://www.semanticscholar.org/paper/01ae11b5dfc2ffdf0c054d67f6c3dee2826059d3", "title": "Big Data Goes Small: Real-Time Spectrum-Driven Embedded Wireless Networking Through Deep Learning in the RF Loop", "abstract": "The explosion of 5G networks and the Internet of Things will result in an exceptionally crowded RF environment, where techniques such as spectrum sharing and dynamic spectrum access will become essential components of the wireless communication process. In this vision, wireless devices must be able to (i) learn to autonomously extract knowledge from the spectrum on-the-fly; and (ii) react in real time to the inferred spectrum knowledge by appropriately changing communication parameters, including frequency band, symbol modulation, coding rate, among others. Traditional CPU-based machine learning suffers from high latency, and requires application-specific and computationally-intensive feature extraction/selection algorithms. Conversely, deep learning allows the analysis of massive amount of unprocessed spectrum data without ad-hoc feature extraction. So far, deep learning has been used for offline wireless spectrum analysis only. Therefore, additional research is needed to design systems that bring deep learning algorithms directly on the device\u2019s hardware and tightly intertwined with the RF components to enable real-time spectrum-driven decision-making at the physical layer. In this paper, we present RFLearn, the first system enabling spectrum knowledge extraction from unprocessed I/Q samples by deep learning directly in the RF loop. RFLearn provides (i) a complete hardware/software architecture where the CPU, radio transceiver and learning/actuation circuits are tightly connected for maximum performance; and (ii) a learning circuit design framework where the latency vs. hardware resource consumption trade-off can explored. We implement and evaluate the performance of RFLearn on custom software-defined radio built on a system-on-chip (SoC) ZYNQ-7000 device mounting AD9361 radio transceivers and VERT2450 antennas. We showcase the capabilities of RFLearn by applying it to solving the fundamental problems of modulation and OFDM parameter recognition. Experimental results reveal that RFLearn decreases latency and power by about 17x and 15x with respect to a software-based solution, with a comparatively low hardware resource consumption.", "venue": "IEEE Conference on Computer Communications", "citationCount": 32, "fieldsOfStudy": ["Computer Science", "Engineering"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "8e99822fc42f7ddec4b6863c52ebed6b9102a1da", "url": "https://www.semanticscholar.org/paper/8e99822fc42f7ddec4b6863c52ebed6b9102a1da", "title": "Artificial Intelligence\u2013Driven Asset Optimizer", "abstract": "\n Currently, as oil and gas companies continue to face risk of volatility in oil prices, production optimization and maintenance play a critical role in driving operational excellence for the industry while maintaining good profit margins. E&P companies must maintain a focus on reducing unit cost/barrel. This can be achieved by reducing operating costs, increasing production, and reducing downtime. We propose a recommendation engine driven by artificial intelligence (AI) that seamlessly integrates subsurface information and production characteristics for knowledge extraction needed to optimize production operations across conventional and unconventional assets. We used a three-phase approach to designing and building an advisory system that ingests data, learns patterns, and feeds these learnings from the data into different functional workflows necessary for improving the efficiency and effectiveness of production operations. The system uses these mechanisms of knowledge extraction, statistical learning, and contextual adaptation as it evolves into an autonomous asset optimization system that can proactively recommend actions for effective decision making to lower the unit cost/barrel.", "venue": "Day 1 Mon, September 24, 2018", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "908e5ce5a3456e12b3dd04445311022c1fa6a1d3", "url": "https://www.semanticscholar.org/paper/908e5ce5a3456e12b3dd04445311022c1fa6a1d3", "title": "Enhancing enterprise knowledge processes via cross-media extraction", "abstract": "In large organizations the resources needed to solve challenging problems are typically dispersed over systems within and beyond the organization, and also in different media. However, there is still the need, in knowledge environments, for extraction methods able to combine evidence for a fact from across different media. In many cases the whole is more than the sum of its parts: only when considering the different media simultaneously can enough evidence be obtained to derive facts otherwise inaccessible to the knowledge worker via traditional methods that work on each single medium separately. In this paper, we present a cross-media knowledge extraction framework specifically designed to handle large volumes of documents composed of three types of media text, images and raw data and to exploit the evidence across the media. Our goal is to improve the quality and depth of automatically extracted knowledge.", "venue": "K-CAP '07", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "af01b57a7b52b40c74f2e7ece65d5ca4f50e4894", "url": "https://www.semanticscholar.org/paper/af01b57a7b52b40c74f2e7ece65d5ca4f50e4894", "title": "Automating knowledge capture in the aerospace domain", "abstract": "We present an approach to automating knowledge extraction in the aerospace engineering domain which has had a fundamental impact on the way engineers manage their collective knowledge built with years of experience. Even though obtaining labelled data in this domain is hard due to the high cost of domain experts' time, the application of the machine learning-based technology was successful, yielding results comparable to the state-of-the-art. Moreover, we present a comparison between several machine learning approaches in extracting knowledge from reports about jet engines. We show that the application of a semi-supervised approach does not provide a significant increase in accuracy so as to justify its adoption due to its much higher computational cost, but that the application of a large-scale approach considerably reduces both training and testing time while keeping accuracy comparable to the standard supervised approach, making it a good choice for this class of application scenarios.", "venue": "K-CAP '09", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "b20e9610aff8311163d6a8f106351993debd7d38", "url": "https://www.semanticscholar.org/paper/b20e9610aff8311163d6a8f106351993debd7d38", "title": "MMKE: A Multi-Model Knowledge Extraction System from Unstructured Texts", "abstract": "In this work, we present a Multi-Model Knowledge Extraction (MMKE) System which consists of two unstructured text extraction models (RelationSO model and SubjectRO model) based on a multi-task learning framework. Instead of recognizing entity first and then predicting relationships between entity pairs in previous works, MMKE detects subject and corresponding relationships before extracting objects to cope with the diverse object-type problem, overlapping problem and non-predefined relation problem. Our system accepts unstructured text as input, from which it automatically extracts triplets knowledge (subject, relation, object). More importantly, we incorporate a number of user-friendly extraction functionalities, such as multi-format uploading, one-click extractions, knowledge editing and graphical displays. The demonstration video is available at this link: https://youtu.be/HtOPJrGhSxk.", "venue": "AAAI", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "1be90ef6e690aa7857fc16d28394365d69ab02fa", "url": "https://www.semanticscholar.org/paper/1be90ef6e690aa7857fc16d28394365d69ab02fa", "title": "Knowledge extracted from recurrent deep belief network for real time deterministic control", "abstract": "Recently, the market on deep learning including not only software but also hardware is developing rapidly. Big data is collected through IoT devices and the industry world will analyze them to improve their manufacturing process. Deep Learning has the hierarchical network architecture to represent the complicated features of input patterns. Although deep learning can show the high capability of classification, prediction, and so on, the implementation on GPU devices are required. We may meet the trade-off between the higher precision by deep learning and the higher cost with GPU devices. We can success the knowledge extraction from the trained deep learning with high classification capability. The knowledge that can realize faster inference of pre-trained deep network is extracted as IF-THEN rules from the network signal flow given input data. Some experiment results with benchmark tests for time series data sets showed the effectiveness of our proposed method related to the computational speed.", "venue": "2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "e78fee7e661f1e4c2d4f794817ff1b42ac536780", "url": "https://www.semanticscholar.org/paper/e78fee7e661f1e4c2d4f794817ff1b42ac536780", "title": "Automatic and Explainable Labeling of Medical Event Logs With Autoencoding", "abstract": "Process mining is a suitable method for knowledge extraction from patient pathways. Structured in event logs, medical events are complex, often described using various medical codes. An efficient labeling of these events before applying process mining analysis is challenging. This paper presents an innovative methodology to handle the complexity of events in medical event logs. Based on autoencoding, accurate labels are created by clustering similar events in latent space. Moreover, the explanation of created labels is provided by the decoding of its corresponding events. Tested on synthetic events, the method is able to find hidden clusters on sparse binary data, as well as accurately explain created labels. A case study on real healthcare data is performed. Results confirm the suitability of the method to extract knowledge from complex event logs representing patient pathways.", "venue": "IEEE journal of biomedical and health informatics", "citationCount": 8, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "59c8b74f4529ec3fdf4bf0e654998c8a14b44adb", "url": "https://www.semanticscholar.org/paper/59c8b74f4529ec3fdf4bf0e654998c8a14b44adb", "title": "Clustering of spatial data for knowledge extraction", "abstract": "Spatial Data Infrastructures (SDI) are repositories of large volumes of data, documented through standardized metadata. Data mining is one of the main techniques used to extract knowledge from large amounts of data, because of its versatility. The purpose of this article is to use clustering techniques and data mining to extract relationships and knowledge from metadata in SDI. For this reason, knowledge discovery techniques, clustering, text mining and data mining algorithms were used. In order to demonstrate the effectiveness of the proposed method, a case study was implemented to evaluate the performance of data mining techniques in this type of database. The results showed that the data mining process and clustering techniques guided to the classification proposed method for extracting relations and knowledge from a group of metadata extracted from within the database.", "venue": "2016 11th Iberian Conference on Information Systems and Technologies (CISTI)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "29f14d7123396c329ccfe181683f3762454116c1", "url": "https://www.semanticscholar.org/paper/29f14d7123396c329ccfe181683f3762454116c1", "title": "Myths and Challenges in Knowledge Extraction and Big Data Analysis on Human-Generated Content from Web and Social Media Sources", "abstract": "Whatever people produce on digital media can be a relevant source of knowledge and behavioural analysis. This is the subject of interest of a wide part of the new discipline known as Web Science. However, special care must be exercised when setting up studies on this kind of sources. Indeed, these studies rarely satisfy the established scientific method guidelines, because of the nature and size of the data, as well as because of the bias and scarce generalizability of results. This paper identifies some of the most crucial challenges that need to be addressed when tackling knowledge extraction and data analysis out of observational studies on human-generated content.", "venue": "KDWeb", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "175761b24c69f62cc837fdc9549595d9a714e423", "url": "https://www.semanticscholar.org/paper/175761b24c69f62cc837fdc9549595d9a714e423", "title": "GELA: A Software Tool for the Analysis of Gene Expression Data", "abstract": "Leveraging advances in transcriptome profiling technologies (RNA-seq), biomedical scientists are collecting ever-increasing gene expression profiles data with low cost and high throughput. Therefore, automatic knowledge extraction methods are becoming essential to manage them. In this work, we present GELA (Gene Expression Logic Analyzer), a novel pipeline able to perform a knowledge discovery process in gene expression profiles data of RNA-seq. Firstly, we introduce the RNA-seq technologies, then, we illustrate our gene expression profiles data analysis method (including normalization, clustering, and classification), and finally, we test our knowledge extraction algorithm on the public RNA-seq data sets of Breast Cancer and Stomach Cancer, and on the public microarray data sets of Psoriasis and Multiple Sclerosis, obtaining in both cases promising results.", "venue": "International Conference on Database and Expert Systems Applications", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "c01cf48e31d79f6f24b8d78bda48496354a08d7b", "url": "https://www.semanticscholar.org/paper/c01cf48e31d79f6f24b8d78bda48496354a08d7b", "title": "How 802.1x Enhances Knowledge Extraction from Large Scale Campus WiFi Deployment", "abstract": "In recent years, the world has witnessed how internet connectivity is exponentially growing in cities around the world. Universitas Islam Indonesia (UII) as one of biggest private universities in Indonesia is also seeing the similar trend like the rest of the world. With more than 700 high density access points and roughly 30,000 users, most of internet connectivity in campus is provided from WiFi access. After 802.1x WiFi authentication-method deployment, UII saw an opportunity to utilise WiFi metadata as a source of business intelligence. Previously, many business processes or managerial decisions in the university were decided by some hidden assumptions and approximations. These assumptions and approximations sometimes created sub-optimal managerial decisions. To improve the strategic decision, we proposed an evidence-based management based on WiFi data. We utilise this data to extract spatial knowledge, movement behaviour, seamless attendance record, and traffic analysis for marketing purpose. The results show promising result where many of university decision is helped by the result given from the knowledge extraction system. Managements can act faster as information is elicited from tacit knowledge within WiFi metada in real time and more accurate.", "venue": "KMIS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "83abac37cf7c8b1cf376ff960ad93cb24addabdb", "url": "https://www.semanticscholar.org/paper/83abac37cf7c8b1cf376ff960ad93cb24addabdb", "title": "Detecting Events in Streaming Multimedia with Big Data Techniques", "abstract": "The massive amount of multimedia information currently available through the Internet demands efficient techniques to extract knowledge from Big Data. In this work, we propose an architecture to capture, process, analyse and visualize data coming from multiple streaming multimedia TV stations and radio stations. For that, we rely on the Hadoop framework available within the IBM InfoSphere BigInsights platform. We create a workflow to automate the different stages that range from Automatic Speech Recognition using open-source tools to visualization by means of the R framework. We emphasize techniques such as diarization and the optimization of the number of Hadoop nodes, provisioned from Cloud infrastructures, to deliver enhanced performance. The results show that it is possible to automate knowledge extraction from multimedia data running on virtualized infrastructures by means of Big Data techniques.", "venue": "2016 24th Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP)", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "e3001cb4582bf002adb1c0956e5e789f13698d40", "url": "https://www.semanticscholar.org/paper/e3001cb4582bf002adb1c0956e5e789f13698d40", "title": "Adaptive processing of Earth Observation data on Cloud infrastructures based on workflow description", "abstract": "The analysis of Earth Observation data is a challenging task due to the variety, velocity and volume of incoming data from various sources. As storing all the raw data is almost impossible, knowledge extraction would be a recommended approach in reducing data size without losing valuable information. For describing the complex processing required to extract knowledge we propose a flexible solution based on workflows and an adaptive execution platform. The main focus of this paper is the Executor component that is oriented on scalability and isolation from the virtual resources management that can be dedicated to a specific cloud infrastructure.", "venue": "2015 IEEE International Conference on Intelligent Computer Communication and Processing (ICCP)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "a28f9da8e8642f6e0ee2a47377a0f7e3724222df", "url": "https://www.semanticscholar.org/paper/a28f9da8e8642f6e0ee2a47377a0f7e3724222df", "title": "Characterization of Diseases Based on Phenotypic Information Through Knowledge Extraction using Public Sources", "abstract": "Despite the huge findings made by the study of the behaviour of diseases, there are currently many non-cure or non-treatment diseases and only some of their symptoms can be beaten. Understanding how the diseases behave implies a complex analysis that together with the new technologies provide researchers with more calculation and observational capabilities, as well as novel approaches that allow us to observe how the diseases behave and relate in different environments with distinct factors. Current research aims to find new ways of characterizing the diseases based on phenotypic manifestations using knowledge extraction techniques from public sources. With the characterization of the diseases, a better understanding about the diseases and how similar they are can be achieved, leading for example to find new drugs that can be applied to different diseases. In order to carry out the present research we have made use of our own dataset of symptoms and diseases developed using an approach that allows us to generate phenotypic knowledge from the extraction of medical information from several data sources.", "venue": "CBMS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "b10241bd403fcc252fa7a9878b7df13aced7bfc7", "url": "https://www.semanticscholar.org/paper/b10241bd403fcc252fa7a9878b7df13aced7bfc7", "title": "Publishing Anonymized Set-Valued Data via Disassociation towards Analysis", "abstract": "Data publishing is a challenging task for privacy preservation constraints. To ensure privacy, many anonymization techniques have been proposed. They differ in terms of the mathematical properties they verify and in terms of the functional objectives expected. Disassociation is one of the techniques that aim at anonymizing of set-valued datasets (e.g., discrete locations, search and shopping items) while guaranteeing the confidentiality property known as k m -anonymity. Disassociation separates the items of an itemset in vertical chunks to create ambiguity in the original associations. In a previous work, we defined a new ant-based clustering algorithm for the disassociation technique to preserve some items associated together, called utility rules, throughout the anonymization process, for accurate analysis. In this paper, we examine the disassociated dataset in terms of knowledge extraction. To make data analysis easy on top of the anonymized dataset, we define neighbor datasets or in other terms datasets that are the result of a probabilistic re-association process. To assess the neighborhood notion set-valued datasets are formalized into trees and a tree edit distance (TED) is directly applied between these neighbors. Finally, we prove the faithfulness of the neighbors to knowledge extraction for future analysis, in the experiments.", "venue": "Future Internet", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "6aff5cc0a6e1bcd88d96004d8b6a3fbf56e10847", "url": "https://www.semanticscholar.org/paper/6aff5cc0a6e1bcd88d96004d8b6a3fbf56e10847", "title": "Knowledge management and XML: derivation of synthetic views over semi-structured data", "abstract": "One of the effects of the expansion of the World Wide Web is theproduction of a huge amount of data, differentiated for type,available to a large number of different users. Furthermore, theconstant progress of computer hardware technology in the past threedecades has led to the availability of powerful computers, datacollection equipments, and storage media; this technology providesa great boost to the database and information industry by allowingtransaction management, information retrieval, and data analysisover massive amounts of heterogeneous data. Moreover, the explosionof Internet increases the availability of data in differentformats: structured (e.g. relational), semistructured (e.g. HTML,XML) and unstructured (e.g. plain text, audio/video) data [2].Thus, new data management systems, able to take advantage of theseheterogeneous data, are emerging and will play a vital role in theinformation industry. Thus, heterogeneous database systems emergeand play a vital role in the information industry.\nKnowledge Management is concerned with the technological,economic and organizational aspects related to (i) thecreation, distribution, diversification and sharing of knowledge incomplex organizations and to (ii) the management ofinformative flows, processes and interactions with externalKnowledge [8].\nFigure 1 summarizes the steps (each represented on a differentlevel of the pyramid) through which knowledge is typicallyextracted from basic data. The first three levels regard themanagement of explicit knowledge (i.e. codified, structuredor semistructured and completely available). In particular,starting from the bottom, the first level is concerned with storingand exchanging \"factual\" knowledge, essentially corresponding tobasic data. Technologies used here comprise Databases [17],Data Repositories, Archive Sharing tools and the emergingExtensible Markup Language (XML) [18].\nThe second level regards \"conceptual knowledge\" modeling, i.e.the definition of concepts and relationships among them. Suchknowledge is typically represented by means of diagram-basedformalisms for both information and related processes [9]. TheUnified Modeling Language (UML) is currently one of the mostpromising modeling languages, oriented towards thespecification,implementation and documentation of complex softwaresystems, but also used for modeling company processes not strictlyrelated to the software.\nThe third level is concerned with organization and integrationof information represented according to heterogeneous formalisms.Techniques used here are essentially those concerning DataWarehousing (DW) [10]. Data warehouses are integratedrepositories of data extracted from multiple heterogeneous sources,organized under a unified schema and at a single site, in order tofacilitate management and decision making. Data Warehousingtechnologies include data cleaning, data integration, and OnlineAnalytical Processing (OLAP), i.e. analysis techniques based onaggregation and summarization.\nThe highest level regards Knowledge Discovery, i.e. theuncovering of new, implicit and potentially usefulknowledge from large amounts of data. The core phase of knowledgediscovery is Data Mining [10], an interactive, iterative,multi-step process, comprising in particular pattern searching andeventual refinements on the basis of domain experts' knowledge.\nIn the context of explicit knowledge management, the ExtensibleMarkup Language takes naturally place. XML is a language forsemistructured data [1, 5] of the World Wide Web Consortium(W3C) [13] which is designed to allow marking, transferring andreusing information by means of a standard method of definition ofthe documents structure and format. Its metalanguage features havebeen used in knowledge management typically for (i) thesemi-automatic production of documents, (ii) the reuse ofsemistructured information and its integration in heterogeneoussystems, (iii) the creation of knowledge maps for theorganization and sharing of information.\nThe increasing quantity of available semistructured data and theuse of XML for their description and exchange discovers newreaserch themes related to management and knowledge extraction overXML data. In this scenario, our proposal consists of a system forthe syntesization of XML documents that attempts to extracttheir semantics and to derive synthetic versions of them by meansof a multidimensional interpretation [10]. In the contest ofKnowledge Management, data synthesization can be regarded as a newway for knowledge extraction, by discovering and aggregating(useful) core information and by neglecting (useless) details.", "venue": "SIAP", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "a74ddf34190684ac9540333d91cdf37c77f63770", "url": "https://www.semanticscholar.org/paper/a74ddf34190684ac9540333d91cdf37c77f63770", "title": "A First Approach on Big Data Missing Values Imputation", "abstract": "Albeit most techniques and algorithms assume that the data is accurate, measurements in our analogic world are far from being perfect. Since our capabilities of storing and processing data are growing everyday, these imperfections will accumulate, generating poorer decisions and hindering any knowledge extraction process carried out over the raw data. One of the most disturbing imperfections is the presence of missing values. Many inductive algorithms assume that the data is complete, thus if they face missing data they will not work properly or the quality of the knowledge extracted will be poorer. At this point there is no sophisticated missing values treatment implemented in any major Big Data framework. In this contribution, we present two novel imputation methods based on clustering that achieve better results than simply removing the faulty examples or filling-in the missing values with the mean that can be easily ported to Spark\u2019s MLlib.", "venue": "IoTBDS", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "58e4e98d6586dab426fd1d54b48d9497374a6c4b", "url": "https://www.semanticscholar.org/paper/58e4e98d6586dab426fd1d54b48d9497374a6c4b", "title": "MetaInsight: Automatic Discovery of Structured Knowledge for Exploratory Data Analysis", "abstract": "Automatic Exploratory Data Analysis (EDA) focuses on automatically discovering pieces of knowledge in the form of interesting data patterns. However, the conveyed knowledge by these suggested data patterns are disjointed or lack organization. Therefore, it is difficult for users to gain structured knowledge, and as the number of suggested patterns grows, these stand-alone patterns are less likely to motive users to conduct follow-up analysis, which hinders it from being effectively utilized to facilitate EDA. In this paper, we propose MetaInsight, a structured representation of knowledge extracted from multi-dimensional data aiming to facilitate EDA automatically and effectively. Specifically, we propose a novel formulation of basic data pattern to capture essential characteristics of raw data distribution to achieve knowledge extraction. Then based on the mined Homogeneous Data Patterns (HDP) and inter-pattern similarity, MetaInsight is identified by categorizing basic data patterns (within an HDP) into commonness(es) and exceptions thus achieving structured knowledge representation. The commonness(es) and exceptions concretize the knowledge obtained by induction and validation processes which are two typical analysis mechanisms conducted in EDA. We propose a novel scoring function to quantify the usefulness of MetaInsight, an effective and efficient mining procedure and a ranking algorithm to automatically discover high-quality MetaInsights from multi-dimensional data. We demonstrate the effectiveness and efficiency of MetaInsights (w.r.t. facilitating EDA) through evaluation on real-world datasets and user studies on both expert users and non-expert users.", "venue": "SIGMOD Conference", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "d398e5192ae2575265ae03270d977b5117e8d128", "url": "https://www.semanticscholar.org/paper/d398e5192ae2575265ae03270d977b5117e8d128", "title": "From data to wisdom: A new multi-layer prototype for Big Data management process", "abstract": "Actually, massive amount of data is created every day due to the proliferation of free tools such as blogs, social media networks, large-scale e-commerce, websites, etc. to create and to share information. Big Data management and integration has become the core of modern science. It brings benefits for scientific disciplines, business areas by creating a radical shift in the way of managing data. However, existing works lack the flexibility to deal with diverse and changing data sources and the scalability to cope with large streaming data. They just focus on one part of the whole process of big data management (e.g. Storage, Integration, Processing or Knowledge Extraction). For the first time in the literature, this paper proposes a complete novel multi-layer prototype for Big Data management in order to process all the issues of Big Data management from data level to the extracted knowledge exploitation, by introducing semantic technologies to bridge the gap of Big Data Integration by adding flexibility, scalability and richness.", "venue": "International Conference on Intelligent Systems Design and Applications", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "c52c997c903ffaca4e1ef0ceb82cdcf31aed2f5f", "url": "https://www.semanticscholar.org/paper/c52c997c903ffaca4e1ef0ceb82cdcf31aed2f5f", "title": "Knowledge extraction from time series of electric energy demand using temporal data mining", "abstract": "Planning activities are very important in the energy sector, where the utilities are seeking information that may assist in decisions regarding expansion needs and resource management, improving the quality of their services. This paper presents a methodology based on mining tools and representation of time series, in order to extract knowledge from series of electricity demand in various substations connected to an energy provider. To represent this knowledge, the language proposed by M\u00f6rchen (2005) called Time Series Knowledge Representation (TSKR) is used. It was conducted a case study using time series of energy demand for 8 substations interconnected by a ring system, which feeds the metropolitan area of Goiania-GO (Brazil), provided by CELG (Companhia Energ\u00e9tica de Goi\u00e1s), responsible for the service of power distribution in the state of Goi\u00e1s (Brazil). Using the proposed methodology, three levels of knowledge that describe the behavior of the studied system were extracted, representing clearly the system dynamics, thus becoming a tool to assist planning activities.", "venue": "2017 IEEE Latin American Conference on Computational Intelligence (LA-CCI)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "0bc83a9bf8e87d32df7a2ad105e2d4bce5ca0c3d", "url": "https://www.semanticscholar.org/paper/0bc83a9bf8e87d32df7a2ad105e2d4bce5ca0c3d", "title": "Ethics-aware Data Governance (Vision Paper)", "abstract": "The number of datasets available to legal practitioners, policy makers, scientists, and many other categories of citizens is growing at an unprecedented rate. Ethics-aware data processing has become a pressing need, considering that data are often used within critical decision processes (e.g., staff evaluation, college admission, criminal sentencing). The goal of this paper is to propose a vision for the injection of ethical principles (fairness, non-discrimination, transparency, data protection, diversity, and human interpretability of results) into the data analysis lifecycle (source selection, data integration, and knowledge extraction) so as to make them first-class requirements. In our vision, a comprehensive checklist of ethical desiderata for data protection and processing needs to be developed, along with methods and techniques to ensure and verify that these ethically motivated requirements and related legal norms are fulfilled throughout the data selection and exploration processes. Ethical requirements can then be enforced at all the steps of knowledge extraction through a unified data modeling and analysis methodology relying on appropriate conceptual and technical tools.", "venue": "SEBD", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Business"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "568adbd6b3236e438626198943643f8de080982a", "url": "https://www.semanticscholar.org/paper/568adbd6b3236e438626198943643f8de080982a", "title": "Knowledge Acquisition for the Air Combat Based on GWO", "abstract": "The problem of knowledge acquisition for the air combat of fighter is studied, and a way to acquire knowledge from massive flight parameters data is put forward. First, expert system of air combat is built; then for flight maneuver knowledge extraction, a method based on grey wolf optimization (GWO) is proposed, in order to obtain simple and effective knowledge, and design a new evaluation function of the GWO algorithm. Finally, the simulation results of horizontal right turn and single plane loop verify that the knowledge extraction method of flight maneuver is effective and feasible.", "venue": "Journal of Physics: Conference Series", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Physics"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "c21d7c8bc649a79962ce778dbc7968093eb6d843", "url": "https://www.semanticscholar.org/paper/c21d7c8bc649a79962ce778dbc7968093eb6d843", "title": "A Data Driven Knowledge Acquisition Method and Its Application in Power System Dynamic Stability Assessment", "abstract": "In this paper, a novel data driven knowledge extraction scheme is proposed and applied to realize power system stability estimation since power system stability assessment can be treated as a typical classification problem (stable/unstable). The strategy is composed of three cascading layers, including the feature selection for choosing an optimal subset from candidate inputs, pattern discovery layer for identifying the latent structure of samples in the selected feature space, and the decision tree layer for generating the self-contained production rules based on the pattern discovery results. Application results on IEEE test system show its merits as a knowledge extraction method, thereby the proposed approach can be widely used in other engineering domains.", "venue": "2008 Seventh International Conference on Machine Learning and Applications", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "5ee60d9fc7214b24fd8199ad3d602823039d2f00", "url": "https://www.semanticscholar.org/paper/5ee60d9fc7214b24fd8199ad3d602823039d2f00", "title": "Concept of operations for knowledge discovery from Big Data across enterprise data warehouses", "abstract": "The success of data-driven business in government, science, and private industry is driving the need for seamless integration of intra and inter-enterprise data sources to extract knowledge nuggets in the form of correlations, trends, patterns and behaviors previously not discovered due to physical and logical separation of datasets. Today, as volume, velocity, variety and complexity of enterprise data keeps increasing, the next generation analysts are facing several challenges in the knowledge extraction process. Towards addressing these challenges, data-driven organizations that rely on the success of their analysts have to make investment decisions for sustainable data/information systems and knowledge discovery. Options that organizations are considering are newer storage/analysis architectures, better analysis machines, redesigned analysis algorithms, collaborative knowledge management tools, and query builders amongst many others. In this paper, we present a concept of operations for enabling knowledge discovery that data-driven organizations can leverage towards making their investment decisions. We base our recommendations on the experience gained from integrating multi-agency enterprise data warehouses at the Oak Ridge National Laboratory to design the foundation of future knowledge nurturing data-system architectures.", "venue": "Defense, Security, and Sensing", "citationCount": 5, "fieldsOfStudy": ["Computer Science", "Engineering"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "bceee9faa877c6cdf367a999d51aae0e2428f5a7", "url": "https://www.semanticscholar.org/paper/bceee9faa877c6cdf367a999d51aae0e2428f5a7", "title": "Towards a molecules production from DNA sequences based on clustering by 3D cellular automata approach and n-grams technique", "abstract": "Knowledge extraction from genomic data is important activity for the biologist. In order to extract the underlying biological knowledge, we based on the generic framework of Knowledge extraction from data. In this paper, we transformed DNA sequences into texts, the texts are indexed by TF-IDF and n-grams approach. Secondly, we grouped the similar DNA sequences by clustering; we applied bio-inspired method 3D cellular automata. Then we analyze the clustering results by the transformation of each DNA sequences into amino acids sequences according the standard genetic code, we concluded that, the clusters help the biologist to select DNA sequences can produce a kind of medicament (molecule) and their various derivatives (low concentration).", "venue": "2015 IEEE/ACS 12th International Conference of Computer Systems and Applications (AICCSA)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "9a2e495f18c03dd8eecef2366d387b043233962f", "url": "https://www.semanticscholar.org/paper/9a2e495f18c03dd8eecef2366d387b043233962f", "title": "Building an Artificial Intelligence Laboratory Based on Real World Data: The Experience of Gemelli Generator", "abstract": "The problem of transforming Real World Data into Real World Evidence is becoming increasingly important in the frameworks of Digital Health and Personalized Medicine, especially with the availability of modern algorithms of Artificial Intelligence high computing power, and large storage facilities.Even where Real World Data are well maintained in a hospital data warehouse and are made available for research purposes, many aspects need to be addressed to build an effective architecture enabling researchers to extract knowledge from data.We describe the first year of activity at Gemelli Generator RWD, the challenges we faced and the solutions we put in place to build a Real World Data laboratory at the service of patients and health researchers. Three classes of services are available today: retrospective analysis of existing patient data for descriptive and clustering purposes; automation of knowledge extraction, ranging from text mining, patient selection for trials, to generation of new research hypotheses; and finally the creation of Decision Support Systems, with the integration of data from the hospital data warehouse, apps, and Internet of Things.", "venue": "Frontiers of Computer Science", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "13895299a16d4f81ef9c08b12bdb5cddbac466c3", "url": "https://www.semanticscholar.org/paper/13895299a16d4f81ef9c08b12bdb5cddbac466c3", "title": "Educational data mining perspectives within university big data environment", "abstract": "All organizations are working nowadays in a very dynamic and strongly competitive environment. In order to survive and remain competitive, they need to take timely, adequate and informed decisions that are based not only on intuition and past experience. The main challenges for data analysis are related with the specific characteristics of \u201cbig data\u201d and the availability of suitable analytical tools for knowledge extraction that would support the processes of taking strategic management decisions. While \u201cbig data\u201d are already widely available and used in business, there are only rare cases of utilizing \u201cbig data\u201d in the educational sector. The main purpose of this paper is to focus on the challenges related to the analytical processing of \u201cbig data\u201d generated and stored at higher education institutions. The paper discusses the unique opportunities that Big Data analysis could give for the educational sector development and the improvements that could scale from a single school, to governmental directions and satisfaction of the labor market. However, big data analytics confronts universities with great challenges as well, related to finding appropriate methods and tools for extracting knowledge and patterns from extremely rich and complex data sets, and integrating the insights into a coherent vision for strategic management decisions.", "venue": "2017 International Conference on Engineering, Technology and Innovation (ICE/ITMC)", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "03aac3b63ec7cc86771559c5b8e42942af270d1f", "url": "https://www.semanticscholar.org/paper/03aac3b63ec7cc86771559c5b8e42942af270d1f", "title": "Computational Customer Behavior Modeling for Knowledge Management with an Automatic Categorization Using Retail Service's Datasets", "abstract": "In the retail service, knowledge management with point of sales (POS) data mining is integral to maintaining and improving productivity. The present paper describes a method of computational customer behavior modeling based on real datasets, and we demonstrate some knowledge extractions from the model. The model is constructed by Bayesian network based on a large-scale POS dataset that incorporates customer identification information and questionnaire responses. In addition, we employ an automatic categorization using probabilistic latent semantic indexing (PLSI), because an appropriate categorization of customers and items is needed for construction of a useful model in real services. We identify a number of categories with regard to customer behavior, and demonstrate the efficacy of our knowledge extraction approach for effective service provision and knowledge management.", "venue": "2010 IEEE 7th International Conference on E-Business Engineering", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "78b7c529b4b0a95a28d8ddccb4408971cff38474", "url": "https://www.semanticscholar.org/paper/78b7c529b4b0a95a28d8ddccb4408971cff38474", "title": "A stacked generalization method for disease progression prediction", "abstract": "One of the most important challenges that face researchers in the medical field is the variety and the extraordinary mass of data types that can be generated even for a single patient. Machine learning models are commonly used to extract knowledge from medical data. However, most of these models have been built based on only a single data type, ignoring learning valuable information existing in other data types. In this paper, we seek to enhance the efficiency of machine learning models in knowledge extraction. We apply data integration via ensemble stacked supervised generalization method. We propose a framework which adopts feature selection techniques to improve the stacked models in order to achieve a better performance than state-of-the-art data integration models. We also consider getting information from multiple data sources using multiple machine learning models to explore data effectively as each data type has a suitable machine learning model. The proposed predictive model is applied to the METABRIC dataset with survival prediction of breast cancer at 2000 days of diagnosis. Our ensemble stacked generalization method achieves 80% prediction accuracy which represents a high-performance index relative to the other known ensemble methods. Also, we demonstrate that some enhanced feature selection methods are able to select relevant features that are close to the features selected by a biological expert from breast cancer pathways.", "venue": "International Computer Engineering Conference", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "aa3a06843ff1607d7b05324eb1bdfb7c5ba5ae5d", "url": "https://www.semanticscholar.org/paper/aa3a06843ff1607d7b05324eb1bdfb7c5ba5ae5d", "title": "The materials data ecosystem: Materials data science and its role in data-driven materials discovery", "abstract": "Since its launch in 2011, the Materials Genome Initiative (MGI) has drawn the attention of researchers from academia, government, and industry worldwide. As one of the three tools of the MGI, the use of materials data, for the first time, has emerged as an extremely significant approach in materials discovery. Data science has been applied in different disciplines as an interdisciplinary field to extract knowledge from data. The concept of materials data science has been utilized to demonstrate its application in materials science. To explore its potential as an active research branch in the big data era, a three-tier system has been put forward to define the infrastructure for the classification, curation and knowledge extraction of materials data.", "venue": "Chinese Physics B", "citationCount": 2, "fieldsOfStudy": ["Physics", "Computer Science", "Engineering"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "0dfa2ca69f8d1986c71be7aec2876f50ae5dd9ef", "url": "https://www.semanticscholar.org/paper/0dfa2ca69f8d1986c71be7aec2876f50ae5dd9ef", "title": "XML document clustering based on common tag names anywhere in the structure", "abstract": "One of the most effective ways to extract knowledge from large information resources is applying data mining methods. Since the amount of information on the Internet is exploding, using XML documents is common as they have many advantages. Knowledge extraction from XML documents is a way to provide more utilizable results. XCLS is one of the most efficient algorithms for XML documents clustering. In this paper we represent a new algorithm for clustering XML documents. This algorithm is an improvement over XCLS algorithm which tries to obviate its problems. We implemented both algorithms and evaluated their clustering quality and running time on the same data sets. In both cases, it is shown that the performance of the new algorithm is better.", "venue": "2009 14th International CSI Computer Conference", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "7bd00be9d1429562a6f8091ac3367f7a6f5a76ca", "url": "https://www.semanticscholar.org/paper/7bd00be9d1429562a6f8091ac3367f7a6f5a76ca", "title": "Data Linkage Graph: computation, querying and knowledge discovery of life science database networks", "abstract": "Summary To support the interpretation of measured molecular facts, like gene expression experiments or EST sequencing, the functional or the system biological context has to be considered. Doing so, the relationship to existing biological knowledge has to be discovered. In general, biological knowledge is worldwide represented in a network of databases. In this paper we present a method for knowledge extraction in life science databases, which prevents the scientists from screen scraping and web clicking approaches. We developed a method for extraction of knowledge networks from distributed, heterogeneous life science databases. To meet the requirement of the very large data volume, the method used is based on the concept of data linkage graphs (DLG).We present an efficient software which enables the joining of millions of data points over hundreds of databases. In order to motivate possible applications, we computed networks of protein knowledge, which interconnect metabolic, disease, enzyme and gene function data. The computed networks enabled a holistic relationship among measured experimental facts and the combined biological knowledge. This was successfully applied for a high throughput functional classification of barley EST and gene expression experiments with the perspective of an automated pipeline for the provisioning of controlled annotation of plant gene arrays and chips. Availability: The data linkage graphs (XML or TGF format), the schema integrated database schema (GML or GRAPH-ML) and the graph computation software may be downloaded from the following URL: http://pgrc.ipk-gatersleben.de/dlg/", "venue": "Journal of Integrative Bioinformatics", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "6af56296ae87b8f9def2f0b5eb3eb088eb6b05c6", "url": "https://www.semanticscholar.org/paper/6af56296ae87b8f9def2f0b5eb3eb088eb6b05c6", "title": "On the role of optimization algorithms in ownership-preserving data mining", "abstract": "Knowledge extraction from sensitive data often needs collaborative work. Statistical databases are generated from such data and shared among various stakeholders. In this context, the ownership protection of shared data becomes important. Watermarking is emerging to be a very effective tool for imposing ownership rights on various digital data formats. Watermarking of such datasets may bring distortions in the data. Consequently, the extracted knowledge may be inaccurate. These distortions are controlled by the usability constraints, which in turn limit the available bandwidth for watermarking. Large bandwidth ensures robustness; however, it may degrade the quality of the data. Such a situation can be resolved by optimizing the available bandwidth subject to the usability constraints. Optimization techniques, particularly bioinspired techniques, have become a preferred choice for solving such issues during the past few years. In this paper, we investigate the usability of various optimization schemes for identifying the maximum available bandwidth to achieve two objectives: (1) preserving the knowledge stored in the data; (2) maximizing the available bandwidth subject to the usability constraints to achieve maximum robustness. The first objective is achieved with a usability constraint model, which ensures that the knowledge is not compromised as a result of watermark embedding. The second objective is achieved by finding the maximum bandwidth subject to the usability constraints specified in the first objective. The performance of optimization schemes is evaluated using different metrics.", "venue": "Frontiers of Information Technology & Electronic Engineering", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "1bd27acbb0009be31742caabaad6a5589243e569", "url": "https://www.semanticscholar.org/paper/1bd27acbb0009be31742caabaad6a5589243e569", "title": "Learning from Web: Review of Approaches", "abstract": "Knowledge discovery is defined as non-trivial extraction of implicit, previously unknown and potentially useful information from given data. Knowledge extraction from web documents deals with unstructured, free-format documents whose number is enormous and rapidly growing. The artificial neural networks are well suitable to solve a problem of knowledge discovery from web documents because trained networks are able more accurately and easily to classify the learning and testing examples those represent the text mining domain. However, the neural networks that consist of large number of weighted connections and activation units often generate the incomprehensible and hard-to-understand models of text classification. This problem may be also addressed to most powerful recurrent neural networks that employ the feedback links from hidden or output units to their input units. Due to feedback links, recurrent neural networks are able take into account of a context in document. To be useful for data mining, self-organizing neural network techniques of knowledge extraction have been explored and developed. Self-organization principles were used to create an adequate neural-network structure and reduce a dimensionality of features used to describe text documents. The use of these principles seems interesting because ones are able to reduce a neural-network redundancy and considerably facilitate the knowledge representation.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "8682635db04a1772bb3799402823a991a6a3da6b", "url": "https://www.semanticscholar.org/paper/8682635db04a1772bb3799402823a991a6a3da6b", "title": "Using symbolic and connectionist algorithms to knowledge acquisition for credit evaluation", "abstract": "There are several techniques of artificial intelligence being applied on the financial market, including credit evaluation. This work investigates the performance achieved by different artificial intelligence techniques when applied to credit evaluation. The techniques used were MLP neural networks and two symbolic learning algorithms, CN2 and C4.5. In order to analyze the performance obtained by these techniques, two distinct data sets for credit evaluation were used. The knowledge used by these techniques was also compared to the knowledge extracted from trained neural networks using a knowledge extraction tool.", "venue": "1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36227)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "70907d3c15b8fa92460e4df38d4239315de8748d", "url": "https://www.semanticscholar.org/paper/70907d3c15b8fa92460e4df38d4239315de8748d", "title": "FFTM: optimized frequent tree mining with soft embedding constraints on siblings", "abstract": "Databases have become increasingly large and the data they contain is increasingly bulky. Thus the problem of knowledge extraction has become very significant and requires multiple techniques for processing the data available in order to extract the information contained from it. We particularly consider the data available on the web. Regarding the problem of the data exchange on the internet, XML is playing an increasing important role in this issue and has become a dominating standard proposed to deal with huge volumes of electronic documents. We are especially involved in extracting knowledge from complex tree structures such as XML documents.\n As they are heterogeneous and with complex structures, the resources available in such documents present the difficulty of querying these data. In order to deal with this problem, automatic tools are of compelling need. We especially consider the problem of constructing a mediator schema whose role is to give the necassary information about the resources structure and through which the data can be queried. In this paper, we present a new approach, called FFTM, dealing with the problem of schema mining through which we particularly focused on the use of soft embedding concept in order to extract more relevant knowledge. Indeed, crisp methods often discard interesting approximate patterns. For this purpose, we have adopted fuzzy constraints for discovering and validating frequent substructures in a large collection of semi-structured data, where both patterns and the data are modeled by labeled trees. The FFTM approach has been tested and validated on synthetic and XML document databases. The experimental results obtained show that our approach is very relevant and palliates the problem of the crisp approach.", "venue": "CSTST", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "692f19fcc8cde8a0472dd2873cbc18557f97cdea", "url": "https://www.semanticscholar.org/paper/692f19fcc8cde8a0472dd2873cbc18557f97cdea", "title": "Numerosity Reduction for Resource Constrained Learning", "abstract": "When coupling data mining (DM) and learning agents, one of the crucial challenges is the need for the Knowledge Extraction (KE) process to be lightweight enough so that even resource (e.g., memory, CPU etc.) constrained agents are able to extract knowledge. We propose the Stratified Ordered Selection (SOS) method for achieving lightweight KE using dynamic numerosity reduction of training examples. SOS allows for agents to retrieve differentsized training subsets based on available resources. The method employs ranking-based subset selection using a novel Level Order (LO) ranking scheme. We show representativeness of subsets selected using the proposed method, its noise tolerance nature and ability to preserve KE performance over different reduction levels. When compared to subset selection methods of the same category, the proposed method offers the best trade-off between cost, reduction and the ability to preserve performance.", "venue": "J. Inf. Process.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "9b6f6004139b1b8f40c718c2c168908bb7d92cc5", "url": "https://www.semanticscholar.org/paper/9b6f6004139b1b8f40c718c2c168908bb7d92cc5", "title": "Mining and Model Understanding on Medical Data", "abstract": "What are the basic forms of healthcare data? How are Electronic Health Records and Cohorts structured? How can we identify the key variables in such data and how important are temporal abstractions? What are the main challenges in knowledge extraction from medical data sources? What are the key machine algorithms used for this purpose? What are the main questions that clinicians and medical experts pose to machine learning researchers? In this tutorial, we provide answers to these questions by presenting state-of-the-art methods, workflows, and tools for mining and understanding medical data. Particular emphasis is given on temporal abstractions, knowledge extraction from cohorts, machine learning model interpretability, and mHealth.", "venue": "KDD", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "35936aad19f4c2183acfebf58b7fd2d1446e815b", "url": "https://www.semanticscholar.org/paper/35936aad19f4c2183acfebf58b7fd2d1446e815b", "title": "Model-Based Extraction of Knowledge about the Effect of Cloud Application Context on Application Service Cost and Quality of Service", "abstract": "With the increased usage of cloud computing in production environments, both for scientific workflows and industrial applications, the focus of application providers shifts towards service cost optimisation. One of the ways to achieve minimised service execution cost is to optimise the placement of the service in the resource pool of the cloud data centres. An increasing number of research approaches is focusing on using machine learning algorithms to deal with dynamic cloud workloads by allocating resources to services in an adaptive way. Many of such solutions are intended for cloud infrastructure providers and deal only with specific types of cloud services. In this paper, we present a model-based approach aimed at the providers of applications hosted in the cloud, which is applicable in early phases of the service lifecycle and can be used for any cloud application service. Using several machine learning methods, we create models to predict cloud service cost and response times of two cloud applications. We also explore how to extract knowledge about the effect that the cloud application context has on both service cost and quality of service so that the gained knowledge can be used in the service placement decision process. The experimental results demonstrate the ability of providing relevant information about the impact of cloud application context parameters on service cost and quality of service. The results also indicate the relevance of our approach for applications in preproduction phase since application providers can gain useful insights regarding service placement decision without acquiring extensive training datasets.", "venue": "Scientific Programming", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "2daecfb910d75ef5d0b03bf731bbd11b6cc13752", "url": "https://www.semanticscholar.org/paper/2daecfb910d75ef5d0b03bf731bbd11b6cc13752", "title": "Extraction of Knowledge from the Topographic Attentive Mapping Network and its Application in Skill Analysis of Table Tennis", "abstract": "Abstract The Topographic Attentive Mapping (TAM) network is a biologically-inspired classifier that bears similarities to the human visual system. In case of wrong classification during training, an attentional top-down signal modulates synaptic weights in intermediate layers to reduce the difference between the desired output and the classifier\u2019s output. When used in a TAM network, the proposed pruning algorithm improves classification accuracy and allows extracting knowledge as represented by the network structure. In this paper, sport technique evaluation of motion analysis modelled by the TAM network was discussed. The trajectory pattern of forehand strokes of table tennis players was analyzed with nine sensor markers attached to the right upper arm of players. With the TAM network, input attributes and technique rules were extracted in order to classify the skill level of players of table tennis from the sensor data. In addition, differences between the elite player, middle level player and beginner were clarified; furthermore, we discussed how to improve skills specific to table tennis from the view of data analysis.", "venue": "Journal of human kinetics", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "77b2b582ab2b422d1ca2a609feb62ef73e30ff5c", "url": "https://www.semanticscholar.org/paper/77b2b582ab2b422d1ca2a609feb62ef73e30ff5c", "title": "Knowledge Modeling for High Content Screening of Multimedia Biological Data", "abstract": "High-content and high-throughput screening (HCS/HTS) technologies provide powerful imaging tools for analyses of biological processes. These technologies combine sophisticated optics with automation techniques for imaging large populations of cells under different experimental perturbations and produce enormous amount of imaging data, including 2D images, 3D confocal data sets, time-lapse video sequences, and multispectral images. Manual analysis of such data is extremely time consuming, and intelligent image interpretation tools have only recently started to emerge. There is a direct need for powerful automated image understanding and spatio-temporal knowledge extraction techniques for gaining useful semantic information in biological domain consisting of multimodality multimedia data. In this tutorial we highlight key multimedia processing challenges in this domain and present a knowledge extraction and representation framework that is currently underway at Purdue University's Cytometery Laboratories and Distributed Multimedia System Laboratory. The proposed framework is being implemented using XML in order to allow extensibility and standardization.", "venue": "2007 IEEE 7th International Symposium on BioInformatics and BioEngineering", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "17080368ff9de8a7d7a5788c9540ccd0c786ffc6", "url": "https://www.semanticscholar.org/paper/17080368ff9de8a7d7a5788c9540ccd0c786ffc6", "title": "A comprehensive study of data intelligence in the context of big data analytics", "abstract": "Modern systems like the Internet of Things, cloud computing, and sensor networks generate a huge data archive. The knowledge extraction from these huge archived data requires modified approaches in algorithm design techniques. The field of study in which analysis of such huge data is carried out is called big data analytics, which helps to optimize the performance with reduced cost and retrieves the information efficiently. The enhancement of traditional data analytics needs to modify to suit big data analytics because it may not manage huge amounts of data. The real thought is how to design the data mining algorithms suitable to handle big data analysis. This paper discusses data analytics at the initial level, to begin with, the insights about the analysis process for big data. Big data analytics have a current research edge in the knowledge extraction field. This paper highlights the challenges and problems associated with big data analysis and provide inner insights into several techniques and methods used.", "venue": "Web Intell.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "47b875aa5e6f95164130546a55681d7e74ca5230", "url": "https://www.semanticscholar.org/paper/47b875aa5e6f95164130546a55681d7e74ca5230", "title": "Semi-Automatic Corpus Expansion and Extraction of Uyghur-Named Entities and Relations Based on a Hybrid Method", "abstract": "Relation extraction is an important task with many applications in natural language processing, such as structured knowledge extraction, knowledge graph construction, and automatic question answering system construction. However, relatively little past work has focused on the construction of the corpus and extraction of Uyghur-named entity relations, resulting in a very limited availability of relation extraction research and a deficiency of annotated relation data. This issue is addressed in the present article by proposing a hybrid Uyghur-named entity relation extraction method that combines a conditional random field model for making suggestions regarding annotation based on extracted relations with a set of rules applied by human annotators to rapidly increase the size of the Uyghur corpus. We integrate our relation extraction method into an existing annotation tool, and, with the help of human correction, we implement Uyghur relation extraction and expand the existing corpus. The effectiveness of our proposed approach is demonstrated based on experimental results by using an existing Uyghur corpus, and our method achieves a maximum weighted average between precision and recall of 61.34%. The method we proposed achieves state-of-the-art results on entity and relation extraction tasks in Uyghur.", "venue": "Inf.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.5}, {"paperId": "36eec8e7d80bf53451cca44b6f5ee062cce69639", "url": "https://www.semanticscholar.org/paper/36eec8e7d80bf53451cca44b6f5ee062cce69639", "title": "Privacy Preserving Data Mining using Attribute Encryption and Data Perturbation", "abstract": "Data mining is a very active research area that deals with the extraction of \u00a0knowledge from very large databases. Data mining has made knowledge extraction and decision making easy. The extracted knowledge could reveal the personal information , if the data contains various private and sensitive attributes about an individual. This poses a threat to the personal information as there is a possibility of misusing the information behind the scenes without the knowledge of the individual. So, privacy becomes a great concern for the data owners and the organizations \u00a0as none of the organizations would like to share their data. To solve this problem Privacy Preserving Data Mining technique have emerged and also solved problems of various domains as it provides the benefit of data mining without compromising the privacy of an individual. This paper proposes a privacy preserving data mining technique the uses randomized perturbation and cryptographic technique. The performance evaluation of the proposed technique shows the same result with the modified data and the original data.", "venue": "BIOINFORMATICS 2013", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "c826ff029429bb44c1aaeacd0a017b11ce180723", "url": "https://www.semanticscholar.org/paper/c826ff029429bb44c1aaeacd0a017b11ce180723", "title": "AutoMFIS: Fuzzy Inference System for multivariate time series forecasting", "abstract": "A time series is the most commonly used representation for the evolution of a given variable over time. In a time series forecasting problem, a model aims at predicting the series' future values, assuming that all information needed to do so is contained in the series' past behavior. Since the phenomena described by the time series does not always exist in isolation, it is possible to enhance the model with historical data from other related time series. The structure formed by several different time series occurring in parallel, each featuring the same interval and dimension, is called a multivariate time series. This paper presents a methodology for the generation of a Fuzzy Inference System (FIS) for multivariate time series forecasting from historical data, aiming at good performance in both forecasting accuracy and rule base interpretability - in order to extract knowledge about the relationship between the modeled time series. Several aspects related to the operation and construction of such a FIS are investigated regarding complexity and semantic clarity. The model is evaluated by applying it to multivariate time series obtained from the complete M3 competition database and by comparing it to other methods in terms of accuracy. In addition knowledge extraction possibilities from the resulting rule base are explored.", "venue": "2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "170e321c25bc66cd9804bbb0c4242123a24abf7a", "url": "https://www.semanticscholar.org/paper/170e321c25bc66cd9804bbb0c4242123a24abf7a", "title": "Interactive Explanations of Internal Representations of Neural Network Layers: An Exploratory Study on Outcome Prediction of Comatose Patients", "abstract": "Supervised machine learning models have impressive predictive capabilities, making them useful to support human decision-making. However, most advanced machine learning techniques, such as Artificial Neural Networks (ANNs), are black boxes and therefore not interpretable for humans. A way of explaining an ANN is visualizing the internal feature representations of its hidden layers (neural embeddings). However, interpreting these visualizations is still difficult. We therefore present InterVENE: an approach that visualizes neural embeddings and interactively explains this visualization, aiming for knowledge extraction and network interpretation. We project neural embeddings in a 2-dimensional scatter plot, where users can interactively select two subsets of data instances in this visualization. Subsequently, a personalized decision tree is trained to distinguish these two sets, thus explaining the difference between the two sets. We apply InterVENE to a medical case study where interpretability of decision support is critical: outcome prediction of comatose patients. Our experiments confirm that InterVENE can successfully extract knowledge from an ANN, and give both domain experts and machine learning experts insight into the behaviour of an ANN. Furthermore, InterVENE\u2019s explanations about outcome prediction of comatose patients seem plausible when compared to existing neurological domain knowledge.", "venue": "KDH@ECAI", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "a3c3ba600cf02f4af8ba53212861261dabec618c", "url": "https://www.semanticscholar.org/paper/a3c3ba600cf02f4af8ba53212861261dabec618c", "title": "The Annotation of Liber Abbaci, a Domain-Specific Latin Resource", "abstract": "The Liber Abbaci (13th century) is a milestone in the history of mathematics and accounting. Due to the late stage of Latin, its features and its very specialized content, it also represents a unique resource for scholars working on Latin corpora. In this paper we present the annotation and linking work carried out in the frame of the project Fibonacci 1202-2021. A gold-standard lemmatization and part-ofspeech tagging allow us to elaborate some first observations on the linguistic and historical features of the text, and to link the text to the Lila Knowledge Base, that has as its goal to make distributed linguistic resources for Latin interoperable by following the principles of the Linked Data paradigm. Starting from this specific case, we discuss the importance of annotating and linking scientific and technical texts, in order to (a) compare and search them together with other (non-technical) Latin texts (b) train, apply and evaluate NLP resources on a non-standard variety of Latin. The paper also describes the fruitful interaction and coordination between NLP experts and traditional Latin scholars on a project requiring a large range of expertise.", "venue": "CLiC-it", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "ld", "kg"], "mention_counts": {"ld": 1, "nlp": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"ld": 1, "kg": 1}, "relevance_score": 0.5}, {"paperId": "c998304b48791e199a5179e2d2cda55beb07d0f4", "url": "https://www.semanticscholar.org/paper/c998304b48791e199a5179e2d2cda55beb07d0f4", "title": "Keyword extraction using neural networks", "abstract": "The research presented in this paper investigates domain independent techniques for automatic knowledge extraction from text. The knowledge is to be organised into a knowledge representation (KR) scheme. The techniques presented are aimed at the first stage: the automatic identification of keywords (any word closely associated with a particular domain as defined by one or more seed word). The aim is to discover any key concepts from any section of text given a small number of seed words associated with any domain. Artificial Neural Networks (ANNs) are trained to recognise keywords on the basis of their relationships to one or more seed words which define a subject domain. The relationships are obtained from an electronic dictionary. Training data is generated using example keywords that humans have identified as being keywords associated with particular seed words. After training, the ANN can be used to extract keywords automatically from other documents. To evaluate this new approach, new measures based on the concept of generalisation have been introduced. Also, analogue versions of recall and precision measures commonly used in knowledge extraction research have been developed to accommodate the ANN analogue outputs. Natural generalisation is the percentage of nouns in new text that are correctly categorised as keywords or non-keywords. Pure generalisation is the percentage of nouns with previously unseen input patterns in the new text that are correctly classified. Experiments so far, on documents concerning education show good natural and pure generalisation for non-keywords at 84% and 82% respectively and reasonable generalisation for keywords (62% for natural and 47% for pure). Results for recall and precision are, for keywords: 59% (analogue recall), 63% (analogue precision), 62% (binary recall), 38% (binary precision) and for non-keywords: 84% (analogue recall), 88% (analogue precision), 87% (binary recall), 95% (binary precision).", "venue": "CLIN", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "43c0721c80ad7fabad08a5b2566034d3e018ad69", "url": "https://www.semanticscholar.org/paper/43c0721c80ad7fabad08a5b2566034d3e018ad69", "title": "Automatic Detection Pipeline for Accessing the Motor Severity of Parkinson\u2019s Disease in Finger Tapping and Postural Stability", "abstract": "Parkinson\u2019s disease (PD) is a nervous disorder that can cause motor impairment. PD severity assessment based on a series of motor movements illustrated in Unified Parkinson\u2019s Disease Rating Scale (UPDRS) is an important part of clinical PD diagnosis. However, the current quantifying method heavily relies on human observation, which is time-consuming and subjective. Therefore, automatic severity estimation stemming from machine learning methods is receiving increasing amount of research attention. However, these advances are still limited by data availability and interpretability. In this paper, we release a large PD motor dataset of over 300 real PD patients collected under doctors\u2019 instructions and propose a pipeline to automatically quantify the motor severity of PD in finger tapping and postural stability. These two selected movements are representative of local and global motor control, exhibiting great clinical importance. The pipeline contains three-stage: pose estimation, domain knowledge extraction, and classification stage. The pose estimation uses deep-learning-based methods to extract 21 and 17 key points for finger tapping and postural stability respectively. The domain knowledge extraction stage extracts several explicit features pre-defined by experienced neuro-physicians. Finally, a classifier is trained to infer PD severity under MDS-UPDRS. To combine deep-learning-based features from pose estimation and domain features from the expert, the pipeline achieves a better trade-off between the model efficiency and clinical interpretability. Experiments show that our method achieves an micro average f1-score of 88%, 84%, and 84%, respectively on left finger tapping, right finger tapping and postural stability, outperforming previous methods by a large margin. In addition, involving expert knowledge in the feature extraction stage greatly improves our model\u2019s interpretability, which is essential in automatic PD detection.", "venue": "IEEE Access", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "b1a172bb456100ef8e50039d40ddcb896b4206d7", "url": "https://www.semanticscholar.org/paper/b1a172bb456100ef8e50039d40ddcb896b4206d7", "title": "Improving Distantly Supervised Relation Extraction by Knowledge Base-Driven Zero Subject Resolution", "abstract": "This paper introduces a technique for automatically generating potential training data from sentences in which entity pairs are not apparently presented in a relation extraction. Most previous works on relation extraction by distant supervision ignored cases in which a relationship may be expressed via null-subjects or anaphora. However, natural language text basically has a network structure that is composed of several sentences. If they are closely related, this is not expressed explicitly in the text, which can make relation extraction difficult. This paper describes a new model that augments a paragraph with a \u201csalient entity\u201d that is determined without parsing. The entity can create additional tuple extraction environments as potential subjects in paragraphs. Including the salient entity as part of the sentential input may allow the proposed method to identify relationships that conventional methods cannot identify. This method also has promising potential applicability to languages for which advanced natural language processing tools are lacking. key words: relation extraction, zero subject, distant supervision, Wikipedia", "venue": "IEICE Trans. Inf. Syst.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "kg"], "mention_counts": {"nlp": 1, "ke": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.5}, {"paperId": "e2c04989198c9c101c277e156acf3da6e4d354e5", "url": "https://www.semanticscholar.org/paper/e2c04989198c9c101c277e156acf3da6e4d354e5", "title": "Towards Document Distribution in Private Cloud Storage Systems", "abstract": "Cloud storage technology has becoming a cost-effective solution for organizations to manage their data in an efficient manner. However, the information stored in private clouds are not usually analyzed by organizations. This avoids organizations obtaining knowledge and taking advantages for data management. This paper presents a method for the extraction of semantic knowledge from private cloud storage repositories as well as the visualization of the acquired knowledge. In this approach, the knowledge extraction is based on the topic detection from repositories of text files (documents) stored in a private cloud storage, the extracted semantic knowledge is indexed as structured data, whereas an application, based on a topic index, enables the organization to visualize the knowledge in the form of graphs of topics per cloud storage location. The implementation of the proposed method shows the feasibility of this approach to get and visualize semantic knowledge extracted from documents.", "venue": "ENC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "16fe74a0f03dd3008e39b0717048d079ff0a5a97", "url": "https://www.semanticscholar.org/paper/16fe74a0f03dd3008e39b0717048d079ff0a5a97", "title": "On Modal Logic Association Rule Mining", "abstract": "The most iconic duality of machine learning models is symbolic learning versus functional learning. While functional learning is based on a numerical approach to knowledge extraction and modelling, the purpose of symbolic machine learning is to extract knowledge from data in such a form that it can be understood, discussed, modified, and applied by humans, as well as serve as the basis of artificial intelligence applications. The typical problems associated with machine learning are classification and rule extraction; while classification can be dealt with using both functional and symbolic learning, rule extraction is essentially symbolic. One element common to nearly all definitions and tools for rule extraction is that they are applied to static datasets and based on propositional logic; unfortunately, very often real-world applications give rise to non-static sets of data (e.g., temporal, spatial, graph-based data) and may require more-than-propositional expressive power. In order to extract association rules from non-static data, in this paper we propose a definition of modal association rules based on modal logic, and we study how a standard rule extraction algorithm such as APRIORI can be generalized to the modal case while keeping the properties of the canonical, non-modal case, namely, correctness and completeness.", "venue": "Italian Conference on Theoretical Computer Science", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "adad193b3d66c4b06f5eefec66c8f9c5996110d8", "url": "https://www.semanticscholar.org/paper/adad193b3d66c4b06f5eefec66c8f9c5996110d8", "title": "Inferring disease correlation from healthcare data", "abstract": "Electronic Health Records maintained in health care settings are a potential source of substantial clinical knowledge. The massive volume of data, unstructured nature of records and obligatory requirement of domain acquaintance together pose a challenge in knowledge extraction from it. The aim of this study is to overcome this challenge with a methodical analysis, abstraction and summarization of such data. This is an attempt to explain clinical observations through bio-medical and genomic data. Discharge summaries of obesity patients were processed to extract coherent patterns. This was supported by Machine Learning and Natural Language Processing based technologies and concept mapping tool along with biomedical, clinical and genomic knowledge bases. Semantic relations between diseases were extracted and filtered through Chi square test to remove spurious relations. The remaining relations were validated against biomedical literature and gene interaction networks. A collection of binary relations of diseases was derived from the data. One set implied co-morbidity while the other set contained diseases which are risk factors of others. Validation against bio-medical literature increased the prospect of correlation between diseases. Gene interaction network revealed that the diseases are related and their corresponding genes are in close proximity. Conclusion: This study focuses on deducing meaningful relations between diseases from discharge summaries. For analytical purpose, the scope has been limited to a few common, well-researched diseases. It can be extended to incorporate relatively unknown, complex diseases and discover new traits to help in clinical assessments.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "kg"], "mention_counts": {"nlp": 1, "kg": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.5}, {"paperId": "ae0f176f3a0f582943fdd793cee7814d9c20331f", "url": "https://www.semanticscholar.org/paper/ae0f176f3a0f582943fdd793cee7814d9c20331f", "title": "Mixture ratio modeling of dynamic systems", "abstract": "Any knowledge extraction relies (possibly implicitly) on a hypothesis about the modelled\u2010data dependence. The extracted knowledge ultimately serves to a decision\u2010making (DM). DM always faces uncertainty and this makes probabilistic modelling adequate. The inspected black\u2010box modeling deals with \u201cuniversal\u201d approximators of the relevant probabilistic model. Finite mixtures with components in the exponential family are often exploited. Their attractiveness stems from their flexibility, the cluster interpretability of components and the existence of algorithms for processing high\u2010dimensional data streams. They are even used in dynamic cases with mutually dependent data records while regression and auto\u2010regression mixture components serve to the dependence modeling. These dynamic models, however, mostly assume data\u2010independent component weights, that is, memoryless transitions between dynamic mixture components. Such mixtures are not universal approximators of dynamic probabilistic models. Formally, this follows from the fact that the set of finite probabilistic mixtures is not closed with respect to the conditioning, which is the key estimation and predictive operation. The paper overcomes this drawback by using ratios of finite mixtures as universally approximating dynamic parametric models. The paper motivates them, elaborates their approximate Bayesian recursive estimation and reveals their application potential.", "venue": "International Journal of Adaptive Control and Signal Processing", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "6a85f79feec2a1b049bf10c48e2b7e71f6bf4806", "url": "https://www.semanticscholar.org/paper/6a85f79feec2a1b049bf10c48e2b7e71f6bf4806", "title": "A Near-Real-Time Answer Discovery for Open-Domain With Unanswerable Questions From the Web", "abstract": "With the proliferation of question and answering (Q&A) services, studies on building a knowledge base (KB) using various information extraction (IE) methodologies from unstructured data on the Web have received significant attention. Existing IE approaches, including machine reading comprehension (MRC), can find the correct answer to a question if the correct answer exists in the document. However, most are prone to extracting incorrect answers rather than producing no answers when the correct answer does not exist in the given documents. This problem is likely to cause serious real-world problems when we apply such technologies to practical services such as AI speakers. We propose a novel open-domain IE system to alleviate the weaknesses of previous approaches. The proposed system integrates an elaborated document selection, sentence selection, and knowledge extraction ensemble method to obtain high specificity while maintaining a realistically achievable level of precision. Based on this framework, we extract answers on Korean open-domain user queries from unstructured documents collected from multiple Web sources. For evaluating our system, we build a benchmark dataset with the SKTelecom AI Speaker log. The baseline models KYLIN infobox generator and BiDAF were used to evaluate the performance of the proposed approach. The experimental results demonstrate that the proposed method outperforms the baseline models and is practically applicable to real-world services.", "venue": "IEEE Access", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "kg"], "mention_counts": {"ke": 1, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.5}, {"paperId": "2b2ba131ceb409ca52705e15a81850dcde2fd582", "url": "https://www.semanticscholar.org/paper/2b2ba131ceb409ca52705e15a81850dcde2fd582", "title": "Image Algorithms and Systems Research", "abstract": "Nowadays, many imaging systems produce such a flood of digital data that a new style of collaboration is emerging to meet two increasingly common challenges: the difficulty of extracting knowledge from the data, and the need to make the data widely available. Examples will be discussed in which the difficulty of knowledge extraction results primarily from the quantity of data generated, while the broad access to the data is required both by the data's intrinsic \"reusability\" and by the high cost of its acquisition. These new, emergent collaborations typically have three classes of stakeholder. Specialists in an application area (which might be astronomy, or remote sensing, or biology, or one of a wealth of other possibilities), set the agenda on the issues to be explored, and define the requirements on the experimental data needed. Specialists in the design and operation of the hardware, computer systems, software and data storage build and operate the system to gather and analyze the data. Specialists from a broad community of users greatly enlarge the value of the enterprise by proposing new hypotheses and experiments. The cost of major instruments is often such that the general community must be included to make the project worthwhile We have been working for several years to develop a methodology that supports collaborations between these various specialists without forcing them either to change their habits or to adopt \"service\" roles within the collaboration. Rather, we keep the perspective that each of them brings equally important insights. Practical examples of our methods will be given.", "venue": "PICS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "fdf9be5146c551855b0b0eb4d7b33a8f73f793a1", "url": "https://www.semanticscholar.org/paper/fdf9be5146c551855b0b0eb4d7b33a8f73f793a1", "title": "Extracting concepts from triadic contexts using Binary Decision Diagram", "abstract": "Due to the high complexity of real problems, a considerable amount of research that deals with high volumes of information has emerged. The literature has considered new applications of data analysis for high dimensional environments in order to manage the difficulty in extracting knowledge from a database, especially with the increase in social and professional networks. Tri- adic Concept Analysis (TCA) is a technique used in the applied mathematical area of data analysis. Its main purpose is to enable knowledge extraction from a context that contains objects, attributes, and conditions in a hierarchical and systematized representation. There are several algorithms that can extract concepts, but they are inefficient when applied to large datasets because the compu- tational costs are exponential. The objective of this paper is to add a new data structure, binary decision diagrams (BDD), in the TRIAS algorithm and retrieve triadic concepts for high dimen- sional contexts. BDD was used to characterize formal contexts, objects, attributes, and conditions. Moreover, to reduce the computational resources needed to manipulate a high-volume of data, the usage of BDD was implemented to simplify and represent data. The results show that this method has a considerably better speedup when compared to the original algorithm. Also, our approach discovered concepts that were previously unachievable when addressing high dimensional contexts.", "venue": "J. Univers. Comput. Sci.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "5ed052c2fe36637e34b4b15e780a869392c8fc51", "url": "https://www.semanticscholar.org/paper/5ed052c2fe36637e34b4b15e780a869392c8fc51", "title": "Electronic health records - beyond meaningful use.", "abstract": "For academic medical centers (AMCs), the past decade has seen two watershed moments in the adoption of health information technology (HIT). The first of these was the Institute of Medicine\u2019s 2001 landmark call to action urging the widespread adoption of modern medical informatics, including electronic health records (EHRs), computerized physician order entry, and the leveraging of existing data resources [1]. And indeed, the past few years have seen significant uptake and use of HIT in the healthcare and clinical research enterprises. But while many institutions have been eager to work with these new tools, few were fully prepared to adopt these systems and only a handful were able to show measurable improvements in the quality and safety of the health care they were delivering. \n \nHowever, the second watershed moment \u2013 the recent emphasis on meaningful use as a key priority of HIT projects funded through American Recovery and Reinvestment Act (ARRA) grants [2] \u2013 illuminates the potential pitfalls of a simplistic approach to implementing EHRs and other aspects of HIT. It is seductively easy for healthcare institutions, including academic medical centers (AMCs), to focus on EHRs as aggregations of more or less desirable features and capabilities. However, if AMCs are to achieve the true goal of meaningful use by \u201c...[enabling] significant and measurable improvements in population health through a transformed healthcare delivery system [3],\u201d and even progress beyond this goal, a different perspective will be required. \n \nWe at Duke have argued [4] for just such a change in perspective, one that views HIT as critical tool of patient care, research, and continuous quality improvement that must be thoughtfully integrated with carefully designed workflows and existing institutional data stores. We would further argue that for AMCs, achieving meaningful use in the context of EHRs must go beyond the basic steps of selection and adoption, where EHRs are too often viewed in terms of their functionality alone, to instead focus on measures of care delivery quality, electronic adoption, and patient engagement. \n \nFor this approach to succeed, it is imperative that AMCs emphasize learning from within and applying knowledge through the thoughtful leveraging of health analytics to ensure that their EHR systems continuously evolve to become better and more integrated care delivery platforms. Success in this context will hinge upon the organization\u2019s capacity to harness the EHR to drive a cycle of continuous learning and improvement. At Duke, we have identified five foundational aspects that we believe are critical to an AMC\u2019s journey through and beyond meaningful use: \n \n\u2022 First, EHRs must enable and support translational research by helping to rapidly move scientific discovery from the laboratory to the patient bedside. \n\u2022 Second, EHRs must support patient empowerment and engagement. \n\u2022 Third, EHRs should help to streamline care delivery models/roles. The emphasis must be on the entire care process and not, for example, on IT silos focused on inpatient/outpatient and physician/nursing systems. \n\u2022 Fourth, EHRs must enable improvements in cost efficiency (clinical research and care delivery). \n\u2022 Fifth and finally, EHRs must enable knowledge extraction and application to the entire environment, working as a feedback mechanism to fuel the cycle of continuous quality improvement. \n \n \nGetting to this point will not be easy. Many U.S. AMCs still seem to view the purchase and deployment of an EHR system as sufficient in itself. Using knowledge extracted from EHRs as the foundation for building learning healthcare environments within hospitals and systems is a concept that is not well understood or even much discussed. Although some of the nation\u2019s AMCs have advanced the idea of integrating health informatics and business analytics into the healthcare enterprise, current commercial EHR systems are not designed to support the critical capabilities outlined in the five points above. \n \nWe as a nation must move past the simplistic assumptions that underlie the naive adoption of EHRs by engineering a disciplined approach to developing indicators that we can use to track and evaluate HIT in terms of its impact on patient care, research, and organizational efficiency. We stand at a critical juncture in the national effort to translate the promise of HIT into real health improvements for individuals and populations. Instead of participating in a mad dash to purchase the most popular single vendor EHR system of the moment, we should instead focus on use of clinical knowledge that could be achieved from \u201cbest of breed\u201d multi-vendor integration of various subsystems which may already be deployed. AMCs should seize this opportunity to focus on technologies that can accommodate continuous redesign and that can be adapted to meet the changing needs of clinicians, researchers, hospital staff, patients, and communities.", "venue": "Applied clinical informatics", "citationCount": 3, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "3183034d2717b87b22d0c9773696c748d258a539", "url": "https://www.semanticscholar.org/paper/3183034d2717b87b22d0c9773696c748d258a539", "title": "Why Topology for Machine Learning and Knowledge Extraction?", "abstract": "Data has shape, and shape is the domain of geometry and in particular of its \u201cfree\u201d part, called topology. The aim of this paper is twofold. First, it provides a brief overview of applications of topology to machine learning and knowledge extraction, as well as the motivations thereof. Furthermore, this paper is aimed at promoting cross-talk between the theoretical and applied domains of topology and machine learning research. Such interactions can be beneficial for both the generation of novel theoretical tools and finding cutting-edge practical applications.", "venue": "Machine Learning and Knowledge Extraction", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "6de25b7246bfb7760b2674c2e9406c7b57efec4a", "url": "https://www.semanticscholar.org/paper/6de25b7246bfb7760b2674c2e9406c7b57efec4a", "title": "Scalable Knowledge Extraction and Visualization for Web Intelligence", "abstract": "Understanding stakeholder perceptions and assessing the impact of campaigns are key questions of communication experts. Web intelligence platforms help to answer such questions, provided that they are scalable enough to analyze and visualize information flows from volatile online sources in real time. This paper presents a distributed architecture for aggregating Web content repositories from Web sites and social media streams, memory-efficient methods to extract factual and affective knowledge, and interactive visualization techniques to explore the extracted knowledge. The presented examples stem from the Media Watch on Climate Change, a public Web portal that aggregates environmental content from a range of online sources.", "venue": "2016 49th Hawaii International Conference on System Sciences (HICSS)", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "6d89c54b32beb0fa195969263f29a14793cbcfe1", "url": "https://www.semanticscholar.org/paper/6d89c54b32beb0fa195969263f29a14793cbcfe1", "title": "Introducing Distiller: A Unifying Framework for Knowledge Extraction", "abstract": "The Digital Libraries community has shown over the last years a growing interest in Semantic Search technologies. Content analysis and annotation is a vital task, but for large corpora it\u2019s not feasible to do it manually. Several automatic tools are available, but such tools usually provide little tuning possibilities and do not support integration with different systems. Search and adaptation technologies, on the other hand, are becoming increasingly multi-lingual and cross-domain to tackle the continuous growth of the available information. So, we claim that to tackle such criticalities a more systematic and flexible approach, such as the use of a framework, is needed. In this paper we present a novel framework for Knowledge Extraction, whose main goal is to support the development of new applications and to ease the integration of", "venue": "IT@LIA@AI*IA", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "f6e35a8aa7f458dcdd025d32f178d3cb70b49f81", "url": "https://www.semanticscholar.org/paper/f6e35a8aa7f458dcdd025d32f178d3cb70b49f81", "title": "Joint Inference for Knowledge Extraction from Biomedical Literature", "abstract": "Knowledge extraction from online repositories such as PubMed holds the promise of dramatically speeding up biomedical research and drug design. After initially focusing on recognizing proteins and binary interactions, the community has recently shifted their attention to the more ambitious task of recognizing complex, nested event structures. State-of-the-art systems use a pipeline architecture in which the candidate events are identified first, and subsequently the arguments. This fails to leverage joint inference among events and arguments for mutual disambiguation. Some joint approaches have been proposed, but they still lag much behind in accuracy. In this paper, we present the first joint approach for bio-event extraction that obtains state-of-the-art results. Our system is based on Markov logic and adopts a novel formulation by jointly predicting events and arguments, as well as individual dependency edges that compose the argument paths. On the BioNLP'09 Shared Task dataset, it reduced F1 errors by more than 10% compared to the previous best joint approach.", "venue": "North American Chapter of the Association for Computational Linguistics", "citationCount": 136, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "1ee0186cce5d17a6e7baf7324c3442be3164cfee", "url": "https://www.semanticscholar.org/paper/1ee0186cce5d17a6e7baf7324c3442be3164cfee", "title": "Knowledge Extraction and Reuse within \"Smart\" Service Centers", "abstract": "In this paper, we describe the initial version of a text analytics system under development and use at Cisco, where the objective is to \"optimize\" the productivity and effectiveness of the service center. More broadly, we discuss the practical needs in industry for developing powerful \"Smart\" Service Centers and the gaps in research to meet these needs. Ideally, service engineers in service centers should be utilized to handle issues which have not been solved previously and machines should be used to solve problems already solved, or at least help the service engineers obtain pertinent information from related and solved service cases when responding to a new request. Such a role for a machine would be a core element of the \"Smart Services\" offering. Hence, design of a highly efficient human-machine combination to derive insights from text and respond to a user request, is critical and fundamental, this enables service agents to capture relevant information quickly and accurately, and to develop the foundation for upper layer applications. Despite extensive earlier literature, the optimization for service process that involves very long, unstructured documents referencing a number of technology and product related terms with implicit inter-relationships has not been fully investigated. Our approach enables firms such as Cisco to achieve efficient service delivery by automating knowledge extraction to support \"Self Service\" by end users. The Cisco text analytics system termed Service Request Analyzer and Recommender (SRAR) addresses gaps in the Support Services function, by optimizing the use of human resources and software analytics in the service delivery process. The Analyzer is able to handle complex service requests (SRs) and to present categorized and pertinent information to service agents, based on which the Recommender, an upper layer application, is built to retrieve similar solved SRs, when presented with a new request. Our contributions in the context of text analysis and system design are three-fold. First, we identify the elements of the diagnostic process underlying the creation of SRs, and design a hierarchical classifier to decompose the complex SRs into those elements. Such decomposition provides specific information from the functional perspectives about \"What was the problem?\" \"Why did it occur?\" and \"How was it solved?\" which assists service agents in acquiring the knowledge they need more effectively and rapidly. Second, we build an SR Recommender on top of SR Analyzer to extend the system functionality for improved knowledge reuse, to measure SR similarity for more accurate recommendation of SRs. Third, we validate our SRAR in an initial pilot study in the service center for Cisco network diagnostics and support, and demonstrate the effectiveness and extensibility of our system. Our system appears applicable to the service centers across multiple domains, including networks, aerospace, semiconductors, automotive, health care, and financial services, and potentially adapted and expanded to all the other business functions of an enterprise. We conclude by indicating open research problems and new research directions, to expand the set of problems that need to be addressed in developing a Smart Support Services capability, and the solutions required to achieve them. These include the capture, retrieval, and reuse of more refined, structured and granulated knowledge, as well as the use of forum threads and semi-automated, dynamic categorization, together with considerations of the optimal use of humans and machine learning based software. Other aspects we discuss include recommendation systems based on temporal pattern clustering and incentives for experts to permit their expertise to be captured for machine (re-)use.", "venue": "2011 Annual SRII Global Conference", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "977e41cb0cc02b54db2ff75272a741d8145b1e29", "url": "https://www.semanticscholar.org/paper/977e41cb0cc02b54db2ff75272a741d8145b1e29", "title": "Knowledge Extraction from Evolving Spiking Neural Networks with Rank Order Population Coding", "abstract": "This paper demonstrates how knowledge can be extracted from evolving spiking neural networks with rank order population coding. Knowledge discovery is a very important feature of intelligent systems. Yet, a disproportionally small amount of research is centered on the issue of knowledge extraction from spiking neural networks which are considered to be the third generation of artificial neural networks. The lack of knowledge representation compatibility is becoming a major detriment to end users of these networks. We show that a high-level knowledge can be obtained from evolving spiking neural networks. More specifically, we propose a method for fuzzy rule extraction from an evolving spiking network with rank order population coding. The proposed method was used for knowledge discovery on two benchmark taste recognition problems where the knowledge learnt by an evolving spiking neural network was extracted in the form of zero-order Takagi-Sugeno fuzzy IF-THEN rules.", "venue": "International Journal of Neural Systems", "citationCount": 69, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "7d44fd95c7bbb264e588e0c2023e9b69e247b747", "url": "https://www.semanticscholar.org/paper/7d44fd95c7bbb264e588e0c2023e9b69e247b747", "title": "Knowledge Extraction on Multidimensional Concepts: Corpus Pattern Analysis (CPA) and Concordances", "abstract": "Multidimensionality of concepts in multidisciplinary domains is a problem terminographers have to deal with. We apply Corpus Pattern Analysis (CPA; Pustejovsky, Hanks, & Rumshisky, 2004) to extract conceptual dimensions according to context. The dynamic nature of these concepts is exemplified with the case study of SAND. On the other hand, knowledge patterns (KPs) often convey different conceptual relations and are therefore polysemic structures. The development of pattern-based constraints can help to disambiguate them and at the same time avoid conceptual noise, which would be a first step towards the systematization of automatic knowledge extraction. Two KPs are analyzed in detail: rang* from, which conveys the conceptual relation is_a, and the polysemic KP formed by.", "venue": "International Conference on Terminology and Artificial Intelligence", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "0b6b073c1cb20a21a71dfebbd667f2484797bc82", "url": "https://www.semanticscholar.org/paper/0b6b073c1cb20a21a71dfebbd667f2484797bc82", "title": "Knowledge extraction from artificial neural network models", "abstract": "The paper describes the development and application of several techniques for knowledge extraction from trained ANN models, such as the identification of redundant inputs and hidden neurons, derivation of causal relationships between inputs and outputs, and analysis of the hidden neuron behavior in classification ANNs. An example of the application of these techniques is given of the faulty LED display benchmark. References of the application of these techniques are given of diverse large scale ANN models of industrial processes.", "venue": "1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation", "citationCount": 266, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "8c452308848645b53529bb865f100af847887b7d", "url": "https://www.semanticscholar.org/paper/8c452308848645b53529bb865f100af847887b7d", "title": "LAKER: location aided knowledge extraction routing for mobile ad hoc networks", "abstract": "In this paper we present a location aided knowledge extraction routing (LAKER) protocol for MANETs, which utilizes a combination of caching strategy in dynamic source routing (DSR) and limited flooding area in location aided routing (LAR) protocol. The key novelty of LAKER is that it can gradually discover knowledge of topological characteristics such as population density distribution of the network. This knowledge can be organized in the form of a set of guiding/spl I.bar/routes, which includes a chain of important positions between a pair of source and destination locations. The guiding/spl I.bar/route information is learned during the route discovery phase, and it can be used to guide future route discovery process in a more efficient manner. LAKER is especially suitable for mobility models where nodes are not uniformly distributed. LAKER can exploit the topological characteristics in these models and limit the search space in route discovery process in a more refined granularity. Simulation results show that LAKER outperforms LAR and DSR in term of routing overhead, saving up to 30% broadcast routing message compared to the LAR approach.", "venue": "2003 IEEE Wireless Communications and Networking, 2003. WCNC 2003.", "citationCount": 59, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "0e186b286a44226a1ea3bbcc7c13eb91084e9f21", "url": "https://www.semanticscholar.org/paper/0e186b286a44226a1ea3bbcc7c13eb91084e9f21", "title": "Analog IP migration using design knowledge extraction", "abstract": "Demonstrated in this paper is a technique for automatic circuit resizing between different technologies. It relies on design knowledge extraction, which renders it very fast compared to full optimization approaches and allows handling of much larger circuits. This technique studies the original design and extracts its major features (basic devices and blocks features, device matching, parasitics, symmetry) and then reproduces a new sized design in the target technology with the same performance as the original design. The migration of a folded-cascode amplifier, low voltage delta sigma A/D and a USB transmitter is presented in this paper to validate the migration engine.", "venue": "Proceedings of the IEEE 2004 Custom Integrated Circuits Conference (IEEE Cat. No.04CH37571)", "citationCount": 4, "fieldsOfStudy": ["Engineering", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "0f5ed97a79c5194092f2148686532289c4da7e30", "url": "https://www.semanticscholar.org/paper/0f5ed97a79c5194092f2148686532289c4da7e30", "title": "Distant Supervision for Cancer Pathway Extraction from Text", "abstract": "Biological pathways are central to understanding complex diseases such as cancer. The majority of this knowledge is scattered in the vast and rapidly growing research literature. To automate knowledge extraction, machine learning approaches typically require annotated examples, which are expensive and time-consuming to acquire. Recently, there has been increasing interest in leveraging databases for distant supervision in knowledge extraction, but existing applications focus almost exclusively on newswire domains. In this paper, we present the first attempt to formulate the distant supervision problem for pathway extraction and apply a state-of-the-art method to extracting pathway interactions from PubMed abstracts. Experiments show that distant supervision can effectively compensate for the lack of annotation, attaining an accuracy approaching supervised results. From 22 million PubMed abstracts, we extracted 1.5 million pathway interactions at a precision of 25%. More than 10% of interactions are mentioned in the context of one or more cancer types, analysis of which yields interesting insights.", "venue": "Pacific Symposium on Biocomputing", "citationCount": 47, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "cceb698cbbb828537f2f195fb70b6fdc586d3327", "url": "https://www.semanticscholar.org/paper/cceb698cbbb828537f2f195fb70b6fdc586d3327", "title": "Reporting bias and knowledge acquisition", "abstract": "Much work in knowledge extraction from text tacitly assumes that the frequency with which people write about actions, outcomes, or properties is a reflection of real-world frequencies or the degree to which a property is characteristic of a class of individuals. In this paper, we question this idea, examining the phenomenon of reporting bias and the challenge it poses for knowledge extraction. We conclude with discussion of approaches to learning commonsense knowledge from text despite this distortion.", "venue": "Conference on Automated Knowledge Base Construction", "citationCount": 165, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "a49e9a8d29b5838ba392d5d33fb9694f4667c59e", "url": "https://www.semanticscholar.org/paper/a49e9a8d29b5838ba392d5d33fb9694f4667c59e", "title": "BERTese: Learning to Speak to BERT", "abstract": "Large pre-trained language models have been shown to encode large amounts of world and commonsense knowledge in their parameters, leading to substantial interest in methods for extracting that knowledge. In past work, knowledge was extracted by taking manually-authored queries and gathering paraphrases for them using a separate pipeline. In this work, we propose a method for automatically rewriting queries into \u201cBERTese\u201d, a paraphrase query that is directly optimized towards better knowledge extraction. To encourage meaningful rewrites, we add auxiliary loss functions that encourage the query to correspond to actual language tokens. We empirically show our approach outperforms competing baselines, obviating the need for complex pipelines. Moreover, BERTese provides some insight into the type of language that helps language models perform knowledge extraction.", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "citationCount": 38, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "5e15a448caaca6e9a483fe5ed71c2205553e6e59", "url": "https://www.semanticscholar.org/paper/5e15a448caaca6e9a483fe5ed71c2205553e6e59", "title": "Extracting knowledge from evaluative text", "abstract": "Capturing knowledge from free-form evaluative texts about an entity is a challenging task. New techniques of feature extraction, polarity determination and strength evaluation have been proposed. Feature extraction is particularly important to the task as it provides the underpinnings of the extracted knowledge. The work in this paper introduces an improved method for feature extraction that draws on an existing unsupervised method. By including user-specific prior knowledge of the evaluated entity, we turn the task of feature extraction into one of term similarity by mapping crude (learned) features into a user-defined taxonomy of the entity's features. Results show promise both in terms of the accuracy of the mapping as well as the reduction in the semantic redundancy of crude features.", "venue": "International Conference on Knowledge Capture", "citationCount": 216, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "9f9fd44171b218180f43232c0bd76eddf5631df7", "url": "https://www.semanticscholar.org/paper/9f9fd44171b218180f43232c0bd76eddf5631df7", "title": "Extracting causation knowledge from natural language texts", "abstract": "SEKE is a semantic expectation\u2010based knowledge extraction system for extracting causation knowledge from natural language texts. It is inspired by human behavior on analyzing texts and capturing information with semantic expectations. The framework of SEKE consists of different kinds of generic templates organized in a hierarchical fashion. There are semantic templates, sentence templates, reason templates, and consequence templates. The design of templates is based on the expected semantics of causation knowledge. They are robust and flexible. The semantic template represents the target relation. The sentence templates act as a middle layer to reconcile the semantic templates with natural language texts. With the designed templates, SEKE is able to extract causation knowledge from complex sentences. Another characteristic of SEKE is that it can discover unseen knowledge for reason and consequence by means of pattern discovery. Using simple linguistic information, SEKE can discover extraction pattern from previously extracted causation knowledge and apply the newly generated patterns for knowledge discovery. To demonstrate the adaptability of SEKE for different domains, we investigate the application of SEKE on two domain areas of news articles, namely the Hong Kong stock market movement domain and the global warming domain. Although these two domain areas are completely different, in respect to their expected semantics in reason and consequence, SEKE can effectively handle the natural language texts in these two domains for causation knowledge extraction. \u00a9 2005 Wiley Periodicals, Inc. Int J Int Syst 20: 327\u2013358, 2005.", "venue": "International Journal of Intelligent Systems", "citationCount": 36, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "e4315c0017327f00cd4fc75754f03a306e6af562", "url": "https://www.semanticscholar.org/paper/e4315c0017327f00cd4fc75754f03a306e6af562", "title": "Using a group decision support system environment for knowledge acquisition: a field study", "abstract": "A field study using a GDSS (group decision support systems) environment to acquire knowledge from multiple experts to build an expert system for an information center was conducted. This field study indicated that a GDSS environment facilitates the acquisition of knowledge from a group of experts by documenting knowledge electronically, by supporting knowledge extraction from individual experts in a parallel fashion, by offering possibilities to resolve conflicts during the knowledge extraction phase, and by providing a group interaction atmosphere to enrich the domain of expertise. In addition, it was found that structured analysis techniques were useful in planning for knowledge acquisition and that a designated primary expert was of great help when dealing with multiple experts.<<ETX>>", "venue": "Twenty-Third Annual Hawaii International Conference on System Sciences", "citationCount": 20, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "89716742f439a311f2da5cda00dfa6ec120fc0cd", "url": "https://www.semanticscholar.org/paper/89716742f439a311f2da5cda00dfa6ec120fc0cd", "title": "Intelligent question answering system based on Artificial Neural Network", "abstract": "The ability of the machine to infer knowledge from the user documents can be tested based on its ability to answer the question asked. Conventional Artificial Neural Network (ANN) models for knowledge extraction only answer to the questions which are simple and objective as they don't analyze the questions and don't try to understand what really the content of document mean. The proposed question answering system (QAS) uses deep cases along with ANN to understand the contents of the documents. We divide the sentences of natural language into knowledge units and assign deep case to each word to improve the quality of knowledge extraction.", "venue": "IEEE International Conference on Engineering and Technology", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "6cbb1f38b5eaa08a6cb055b7129ed1c943f2176e", "url": "https://www.semanticscholar.org/paper/6cbb1f38b5eaa08a6cb055b7129ed1c943f2176e", "title": "Using social network analysis to investigate students' collaboration patterns in eMUSE platform", "abstract": "Social network analysis can be used to investigate collaboration in learning networks, which can be modeled as social graphs. We have already proposed a conceptual framework for knowledge extraction and visualization from a social media-based learning environment, starting from specific educational needs identified by the instructors. In the current paper, we experimentally validate this framework on our eMUSE social learning platform. In particular, we investigate students' collaboration patterns in a project-based learning scenario. Social network analysis techniques are used to extract knowledge regarding specific differences in blog vs. microblog support, as well as intra-team vs. inter-team collaboration; several salient students and teams are also analyzed in more detail.", "venue": "International Conference on System Theory, Control and Computing", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "cbb9b4626a6c8e4ce655d6b2570672b08b26da4e", "url": "https://www.semanticscholar.org/paper/cbb9b4626a6c8e4ce655d6b2570672b08b26da4e", "title": "A personalized search engine based on Web\u2010snippet hierarchical clustering", "abstract": "We propose a (meta\u2010)search engine, called SnakeT (SNippet Aggregation for Knowledge ExtracTion), which queries more than 18 commodity search engines and offers two complementary views on their returned results. One is the classical flat\u2010ranked list, the other consists of a hierarchical organization of these results into folders created on\u2010the\u2010fly at query time and labeled with intelligible sentences that capture the themes of the results contained in them. Users can browse this hierarchy with various goals: knowledge extraction, query refinement and personalization of search results. In this novel form of personalization, the user is requested to interact with the hierarchy by selecting the folders whose labels (themes) best fit her query needs. SnakeT then personalizes on\u2010the\u2010fly the original ranked list by filtering out those results that do not belong to the selected folders. Consequently, this form of personalization is carried out by the users themselves and thus results fully adaptive, privacy preserving, scalable and non\u2010intrusive for the underlying search engines. We have extensively tested SnakeT and compared it against the best available Web\u2010snippet clustering engines. SnakeT is efficient and effective, and shows that a mutual reinforcement relationship between ranking and Web\u2010snippet clustering does exist. In fact, the better the ranking of the underlying search engines, the more relevant the results from which SnakeT distills the hierarchy of labeled folders, and hence the more useful this hierarchy is to the user. Vice versa, the more intelligible the folder hierarchy, the more effective the personalization offered by SnakeT on the ranking of the query results. Copyright \u00a9 2007 John Wiley & Sons, Ltd.", "venue": "Software, Practice & Experience", "citationCount": 88, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "d264fa0871b0a190c89442159fad2e83981dc81e", "url": "https://www.semanticscholar.org/paper/d264fa0871b0a190c89442159fad2e83981dc81e", "title": "Fuzzy inference systems implemented on neural architectures for motor fault detection and diagnosis", "abstract": "Motor fault detection and diagnosis involves processing a large amount of information of the motor system. With the combined synergy of fuzzy logic and neural networks, a better understanding of the heuristics underlying the motor fault detection/diagnosis process and successful fault detection/diagnosis schemes can be achieved. This paper presents two neural fuzzy (NN/FZ) inference systems, namely, fuzzy adaptive learning control/decision network (FALCON) and adaptive network based fuzzy inference system (ANFIS), with applications to induction motor fault detection/diagnosis problems. The general specifications of the NN/FZ systems are discussed. In addition, the fault detection/diagnosis structures are analyzed and compared with regard to their learning algorithms, initial knowledge requirements, extracted knowledge types, domain partitioning, rule structuring and modifications. Simulated experimental results are presented in terms of motor fault detection accuracy and knowledge extraction feasibility. Results suggest new and promising research areas for using NN/FZ inference systems for incipient fault detection and diagnosis in induction motors.", "venue": "IEEE transactions on industrial electronics (1982. Print)", "citationCount": 187, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "9954048379a85148f0ae4698b84c51f0ef897e2e", "url": "https://www.semanticscholar.org/paper/9954048379a85148f0ae4698b84c51f0ef897e2e", "title": "Guided HTM: Hierarchical Topic Model with Dirichlet Forest Priors", "abstract": "Despite the proliferation of topic models, the organization of topics from the probabilistic models needs improvement in two ways: the better structured presentation of topics and the incorporation of domain knowledge on the corpus. The structured presentation, i.e., the hierarchical topic model, helps in categorizing similar topics; the incorporation of domain knowledge enables the concentrated sampling of predefined keywords in the mixture parameter learning. This paper presents a hierarchical topic models with incorporated domain knowledge, called Guided Hierarchical Topic Model (GHTM). Specifically, we allocated the prior information from the knowledge to the Dirichlet Forest prior. From the prior adjustment, we obtained the topic tree guided by the domain knowledge. This paper also contributes in enumerating four different knowledge extraction methods and applying the extracted knowledge to GHTM. We evaluated the performance of GHTM in terms of the hierarchical clustering accuracy, and we found a significant improvement of hierarchical clustering measured by F-measures. This improvement is also verified by the perplexity analyses. Additionally, we measured topic quality with KL-divergence and visualization, and these confirm the ability to better separate topic distributions. Finally, we tested the hierarchical topic quality through human experiments, and this also revealed significant improvements originating from the guidance.", "venue": "IEEE Transactions on Knowledge and Data Engineering", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "7425e595d28461af97308dbc74d01f41deb13d65", "url": "https://www.semanticscholar.org/paper/7425e595d28461af97308dbc74d01f41deb13d65", "title": "Extracting Knowledge from Neural Networks", "abstract": "Neural networks (NN) as classifier systems have shown great promise in many problem domains in empirical studies over the past two decades. Using case classification accuracy as the criteria, neural networks have typically outperformed traditional parametric techniques (e.g., discriminant analysis, logistic regression) as well as other non-parametric approaches (e.g., various inductive learning systems such as ID3, C4.5, CART, etc.). In spite of this strong evidence of superior performance, the use of neural networks in organizations has been hampered by the lack of an \u201ceasy\u201d way of explaining what the neural network has learned about the domain being studied. It is well known that knowledge in a neural network is \u201cmysteriously\u201d encapsulated in its connection weights. It is well accepted that decision-makers prefer techniques that can provide good explanations about the knowledge found in a domain even if they are less effective in terms of classification accuracy. Over the past decade, neural network researchers have thus begun an active research stream that focuses on developing techniques for extracting usable knowledge from a trained neural network. The literature has become quite vast and, unfortunately, still lacks any form of consensus on the best way to help neural networks be more useful to knowledge discovery practitioners. This article will then provide a brief review of recent work in one specific area of the neural network/knowledge discovery research stream. This review considers knowledge extraction techniques that create IF-THEN rules from trained feed-forward neural networks used as classifiers. We chose this narrow view for a couple of important reasons. First, as mentioned, the research in this area is extraordinarily broad and a critical review cannot be done without focusing", "venue": "Encyclopedia of Knowledge Management", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "333d80fe1ebb81958d451d1b9395466c3cdc8d8e", "url": "https://www.semanticscholar.org/paper/333d80fe1ebb81958d451d1b9395466c3cdc8d8e", "title": "Golden years, golden shores: A study of elders in online travel communities", "abstract": "In this paper we present our exploratory findings related to extracting knowledge and experiences from a community of senior tourists. By using tools of qualitative analysis as well as review of literature, we managed to verify a set of hypotheses related to the content created by senior tourists when participating in on-line communities. We also produced a codebook, representing various themes one may encounter in such communities. This codebook, derived from our own qualitative research, as well a literature review will serve as a basis for further development of automated tools of knowledge extraction. We also managed to find that older adults more often than other poster in tourists forums, mention their age in discussion, more often share their experiences and motivation to travel, however they do not differ in relation to describing barriers encountered while traveling.", "venue": "2017 7th International Conference on Computer and Knowledge Engineering (ICCKE)", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Sociology"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "8d61f7982369006cdd834d70eb71d8302d8b2388", "url": "https://www.semanticscholar.org/paper/8d61f7982369006cdd834d70eb71d8302d8b2388", "title": "Symbol Knowledge Extraction from a Simple Graphical Language", "abstract": "In this paper, we study the problem of symbol knowledge extraction. We assume that some unknown symbols are used to compose a handwritten message, and from a dataset of handwritten samples, we would like to recover the symbol set used in the corresponding language. We applied our approach on online handwriting, and select the domain of numerical expressions, mixing digits and operators, to test the ability to retrieve the corresponding symbol classes. The proposed method is based on three steps: a quantization of the stroke space, a description of the layout of strokes with a relational graph, and the extraction of an optimal lexicon using a minimum description length algorithm. At the symbol level, a recall rate of 74% is obtained on the test dataset produced by 100 writers.", "venue": "2011 International Conference on Document Analysis and Recognition", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "358d7d6333d3edd530e37efd8004cb9da8cfd5d4", "url": "https://www.semanticscholar.org/paper/358d7d6333d3edd530e37efd8004cb9da8cfd5d4", "title": "A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos", "abstract": "Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in a structured form.", "venue": "NLPBT", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "d4b89f5e4b1bf9e4ee4dd8fecd9c8249c45d2578", "url": "https://www.semanticscholar.org/paper/d4b89f5e4b1bf9e4ee4dd8fecd9c8249c45d2578", "title": "Inscriptis - A Python-based HTML to text conversion library optimized for knowledge extraction from the Web", "abstract": "Inscriptis provides a library, command line client and Web service for converting HTML to plain text. Its development has been triggered by the need to obtain accurate text representations for knowledge extraction tasks that preserve the spatial alignment of text without drawing upon heavyweight, browser-based solutions such as Selenium [9]. In contrast to existing software packages such as HTML2text [23], jusText [2] and Lynx [5], Inscriptis", "venue": "Journal of Open Source Software", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "1298df70fb685febae4fff629707cbed3aaec78d", "url": "https://www.semanticscholar.org/paper/1298df70fb685febae4fff629707cbed3aaec78d", "title": "Knowledge extraction from a class of support vector machines using the fuzzy all-permutations rule-base", "abstract": "Support vector machines (SVMs) proved to be highly efficient in various classification tasks. However, the knowledge learned by the SVM is encoded in a long list of parameter values and it is not easy to comprehend what the SVM is actually computing. We show that certain types of SVMs are mathematically equivalent to a specific fuzzy \u2014 rule base, the fuzzy all \u2014 permutations rule base (FARB). This equivalence can be used to provide a symbolic representation of the SVM functioning. This leads to a new approach for knowledge extraction from SVMs. Two simple examples demonstrate the effectiveness of this approach.", "venue": "2011 IEEE Symposium on Computational Intelligence, Cognitive Algorithms, Mind, and Brain (CCMB)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "1438b391f272499f8d07c50091281cb618b6f37f", "url": "https://www.semanticscholar.org/paper/1438b391f272499f8d07c50091281cb618b6f37f", "title": "Introducing Distiller: a Lightweight Framework for Knowledge Extraction and Filtering", "abstract": "Semantic content analysis is an activity that can greatly support a broad range of user modelling applications. Several automatic tools are available, however such systems usually provide little tuning possibilities and do not support integration with di erent systems. Personalization applications, on the other hand, are becoming increasingly multi-lingual and cross-domain. In this paper we present a novel framework for Knowledge Extraction, whose main goal is to support the development of new strategies and technologies and to ease the integration of the existing ones.", "venue": "UMAP Workshops", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "5a516b17a2a16130242697413663577e007817e7", "url": "https://www.semanticscholar.org/paper/5a516b17a2a16130242697413663577e007817e7", "title": "Integrating CRF and Rule Method for Knowledge Extraction in Patent Mining Task at NTCIR-8", "abstract": "We participate in the subtask \"technical trend map creation\" of patent mining task at NTCIR-8. In this paper, we define this task as a knowledge extraction task for patent abstracts and the CRF method and Rule method are introduced in our approach. Compare with the evaluation results, we find out the effect of method of integrating CRF model and Rule model is better than that only using CRF model. However, extraction task of tag is more difficult than tags.", "venue": "NTCIR", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "0c3c91228a2119fd102cb1fa7de3d16891deb37b", "url": "https://www.semanticscholar.org/paper/0c3c91228a2119fd102cb1fa7de3d16891deb37b", "title": "Knowledge Extraction from a Mixed Transfer Function Artificial Neural Network", "abstract": "One of the main problems with Artificial Neural Networks (ANNs) is that their results are not intuitively clear. For example, commonly used hidden neurons with sigmoid activation function can approximate any continuous function, including linear functions, but the coefficients (weights) of this approximation are rather meaningless. To address this problem, current paper presents a novel kind of a neural network that uses transfer functions of various complexities in contrast to mono-transfer functions used in sigmoid and hyperbolic tangent networks. The presence of transfer functions of various complexities in a Mixed Transfer Functions Artificial Neural Network (MTFANN) allow easy conversion of the full model into user-friendly equation format (similar to that of linear regression) without any pruning or simplification of the model. At the same time, MTFANN maintains similar generalization ability to mono-transfer function networks in a global optimization context. The performance and knowledge extraction of MTFANN were evaluated on a realistic simulation of the Puma 560 robot arm and compared to sigmoid, hyperbolic tangent, linear and sinusoidal networks.", "venue": "Journal of Advanced Computational Intelligence and Intelligent Informatics", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "820c9d5591fee3c68bb1273be8b0ca1fb467d7b8", "url": "https://www.semanticscholar.org/paper/820c9d5591fee3c68bb1273be8b0ca1fb467d7b8", "title": "Knowledge Extraction for Discriminating Male and Female in Logical Reasoning from Student Model", "abstract": "The learning process is a process of communication and interaction between the teacher and his students on one side and between the students and each others on the other side. Interaction of the teacher with his students has a great importance in the process of learning and education. The pattern and style of this interaction is determined by the educational situation, trends and concerns, and educational characteristics. Classroom interaction has an importance and a big role in increasing the efficiency of the learning process and raising the achievement levels of students. Students need to learn skills and habits of study, especially at the university level. The effectiveness of learning is affected by several factors that include the prevailing patterns of interactive behavior in the classroom. These patterns are reflected in the activities of teacher and learners during the learning process. The effectiveness of learning is also influenced by the cognitive and non cognitive characteristics of teacher that help him to succeed, the characteristics of learners, teaching subject, and the teaching methods. This paper presents a machine learning algorithm for extracting knowledge from student model. The proposed algorithm utilizes the inherent characteristic of genetic algorithm and neural network for extracting comprehensible rules from the student database. The knowledge is used for discriminating male and female levels in logical reasoning as a part of an expert system course.", "venue": "ArXiv", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "0b43194811ff52350c671ec8f8314ac6f2f4a31f", "url": "https://www.semanticscholar.org/paper/0b43194811ff52350c671ec8f8314ac6f2f4a31f", "title": "Knowledge extraction to automate CFD analysis in abdominal aneurysm diagnosis and treatment", "abstract": "In the last few years, advanced simulation tools have been tested in academic researches to evaluate the causes and evolution of abdominal aortic aneurysm AAA. This study describes the activity of knowledge extraction aimed at automating the CFD analysis of an AAA. A simple benchmark geometry is considered. Knowledge and rules extraction are done by comparing: 2D and 3D CFD solutions, results from Newtonian and non-Newtonian formulations, influence of the prism layer and inlet boundary conditions, and numerical solver schemes. A grid refinement study using the verification and validation approach was done. As a result, finer hexahedral meshes with a prism layer near the vessel walls is necessary to capture the velocity gradient and wall shear stress correctly; a simple Newtonian formulation is enough to capture the fluid flow behaviour. A first step in the validation process is taken through the application of the extracted rules on a real patient's specific geometries.", "venue": "International Journal of Information Technology and Management", "citationCount": 2, "fieldsOfStudy": ["Engineering", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "498f3e03a41fd9209c516984dccb8364af759be4", "url": "https://www.semanticscholar.org/paper/498f3e03a41fd9209c516984dccb8364af759be4", "title": "A Differential Evolution-Based System Supporting Medical Diagnosis through Automatic Knowledge Extraction from Databases", "abstract": "In this paper, a new approach based on Differential Evolution for the automatic classification of items in medical databases is proposed. Based on it, a tool called DERExis presented, which automatically extracts explicit knowledge from the database under the form of IF -- THEN rules. DERExis thought as a useful support to decision making whenever explanations on why an item is assigned to a given class should be provided, as it is the case for diagnosis in the medical domain. The tool has been compared over seven medical databases against a set of fifteen classification tools widely used in literature. The results have proven the effectiveness of the proposed approach, since DEREx turns out to be among the very best tools in terms of highest classification accuracy, so it is preferable because it automatically extracts knowledge and provides users with it under an easily comprehensible form.", "venue": "IEEE International Conference on Bioinformatics and Biomedicine", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "a5379d9007e88e59590111e57b6f2d9fcdb26474", "url": "https://www.semanticscholar.org/paper/a5379d9007e88e59590111e57b6f2d9fcdb26474", "title": "Knowledge extraction from diagram and text for media integration", "abstract": "We introduce a new framework for knowledge extraction from written texts and diagrams and utilization of the obtained knowledge. We use a combination of multiple media for effective communication. To realize this mechanism as a hyper-medium, we propose a noble idea of media integration. First, the correlation between the two media is analyzed to automatically interpret the semantic structure of both media. The integrated hypermedia obtained as a result of the correspondence and the semantic interpretation is quite useful to derive knowledge about topics being described. To demonstrate the usefulness of the integrated media, we have constructed a prototype system for flexible explanation generation. Various kinds of explanation can be generated in our system.", "venue": "Proceedings of the Third IEEE International Conference on Multimedia Computing and Systems", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "0045cb1c4497286b4026b2a618958eca31f288fb", "url": "https://www.semanticscholar.org/paper/0045cb1c4497286b4026b2a618958eca31f288fb", "title": "Sequential RAM-based Neural Networks: Learnability, Generalisation, Knowledge Extraction, and Grammatical Inference", "abstract": "A fundamental question in the field of artificial neural networks is what set of problems a given class of networks can perform (computability). Such a problem can be made less general, but no less important, by asking what these networks could learn by using a given training procedure (learnability). The basic purpose of this paper is to address the learnability problem. Specifically, it analyses the learnability of sequential RAM-based neural networks. The analytical tools used are those of Automata Theory. In this context, this paper establishes which class of problems and under what conditions such networks, together with their existing learning rules, can learn and generalize. This analysis also yields techniques for both extracting knowledge from and inserting knowledge into the networks. The results presented here, besides helping in a better understanding of the temporal behaviour of sequential RAM-based networks, could also provide useful insights for the integration of the symbolic/connectionist paradigms.", "venue": "International Journal of Neural Systems", "citationCount": 4, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "c2569ca31313b0192a8a8fcf50fb626085aebec8", "url": "https://www.semanticscholar.org/paper/c2569ca31313b0192a8a8fcf50fb626085aebec8", "title": "SCAS-IS: Knowledge Extraction and Reuse in Multiprocessor Task Scheduling Based on Cellular Automata", "abstract": "Static Task Scheduling Problem (STSP) in multiprocessors is a NP-Complete problem. Cellular Automata (CA) have been recently proposed to solve STSP. The main feature of CA-based models to STSP is the extraction of knowledge while scheduling an application and its subsequent reuse in other instances. Previous works showed this approach is promising. However some desirable features have not been successfully exploited yet, such as: (i) the usage of an arbitrary number of processors, (ii) the massive parallelism inherent to CA and (iii) the reuse of evolved rules with competitive results. This paper presents a new model called SCAS-IS (Synchronous Cellular Automata Scheduler with Initialization Strategies). Its major innovation is the employment of fixed initialization strategies to start up CA dynamics. Parallel program graphs found in literature and others randomly generated were used to test the new model. Results show SCAS-IS overcame related models both in make span obtained as computational performance. It is also competitive with meta-heuristics.", "venue": "Brazilian Symposium on Neural Networks", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "b7c4abe3a149297b7fb0d434315692d5ce934aac", "url": "https://www.semanticscholar.org/paper/b7c4abe3a149297b7fb0d434315692d5ce934aac", "title": "A Learning Agent for Knowledge Extraction from an Active Semantic Network", "abstract": "This paper outlines the development of a learning retrieval agent. Task of this agent is to extract knowledge of the Active Semantic Network in respect to user-requests. Based on a reinforcement learning approach, the agent learns to interpret the user\u2019s intention. Especially, the learning algorithm focuses on the retrieval of complex long distant relations. Increasing its learnt knowledge with every request-result-evaluation sequence, the agent enhances his capability in finding the intended information. Keywords\u2014Reinforcement learning, learning retrieval agent, search in semantic networks.", "venue": "IEC", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "f25a7a9e1d8e5b4a96c9a2a6a9d7c884534da751", "url": "https://www.semanticscholar.org/paper/f25a7a9e1d8e5b4a96c9a2a6a9d7c884534da751", "title": "Knowledge Extraction and Designing of Child Development Detection System for a New Parent", "abstract": "Child development at an early stage is important for learning progress. New parents need to learn to observe the development of those children. However, it requires knowledge and expertise skills to discover and monitor. This study investigated child development knowledge extraction that is appropriate for a new parent. The child development detection system has been designed by adopting the knowledge engineering method with user experience and high-fidelity prototyping design. This study shows the positive satisfaction result of a new parent taking part in these experiments to confirm on how to develop a good child development detection system in further step.", "venue": "2020 Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunications Engineering (ECTI DAMT & NCON)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "c45250f9b8804701a0667aca2a0a517285b634ef", "url": "https://www.semanticscholar.org/paper/c45250f9b8804701a0667aca2a0a517285b634ef", "title": "Symbolic Knowledge Extraction from Opaque Machine Learning Predictors: GridREx & PEDRO", "abstract": "Procedures aimed at explaining outcomes and behaviour of opaque predictors are becoming more and more essential as machine learning (ML) black-box (BB) models pervade a wide variety of fields and, in particular, critical ones - e.g., medical or financial -, where it is not possible to make decisions on the basis of a blind automatic prediction. A growing number of methods designed to overcome this BB limitation is present in the literature, however some ML tasks are nearly or completely neglected-e.g., regression and clustering. Furthermore, existing techniques may be not applicable in complex real-world scenarios or they can affect the output predictions with undesired artefacts.\n\nIn this paper we present the design and the implementation of GridREx, a pedagogical algorithm to extract knowledge from black-box regressors, along with PEDRO, an optimisation procedure to automate the GridREx hyper-parameter tuning phase with better results than manual tuning. We also report the results of our experiments involving the application of GridREx and PEDRO in real case scenarios, including GridREx performance assessment by using as benchmarks other similar state-of-the-art techniques. GridREx proved to be able to give more concise explanations with higher fidelity and predictive capabilities.", "venue": "KR", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "f4992f8b62334303f074836ed3025982d96fcc18", "url": "https://www.semanticscholar.org/paper/f4992f8b62334303f074836ed3025982d96fcc18", "title": "Automatic extraction of knowledge from student essays", "abstract": "This paper presents a characterisation of argumentation in student essays and analyses patterns for extracting knowledge from them. Having analysed their complexity in light of the kinds of logic that may be used in an automatic argumentation extraction system, the main characteristic of these patterns appears to be the polymorphism of the pattern variables. Therefore, systems that learn patterns automatically ought to be able to generate many-sorted logic formulae, so that polymorphic types may be associated to the extraction slots (or - equivalently - to a logic formulae). An analysis of existing (pattern learning) systems was carried out to gauge the possibility of using them within our framework. However, we concluded that none of the existing systems can handle our requirements. Finally, we present our vision of an agent-based student portal as the front-end of a system that can locate argumentation links in a student essay and integrates with related educational services.", "venue": "Int. J. Knowl. Learn.", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "310fcb4912c62b191e5dd65cba07d5fda7d8b15e", "url": "https://www.semanticscholar.org/paper/310fcb4912c62b191e5dd65cba07d5fda7d8b15e", "title": "Personalized recommendation for web-based learning based on ant colony optimization with segmented-goal and meta-control strategies", "abstract": "Personalized web-based learning has become an important learning form in the 21st century. An earlier research result showed that a fuzzy knowledge extraction model can be established to extract personalized recommendation knowledge by discovering effective learning paths from an access database through an ant colony model. However, critical limitations arose when considering its applications in real world situations. In this paper, the aim is to improve the model by devising more efficient algorithms that requires a reasonable number of learners and training cycles to find satisfying good results. The key approaches to resolving the practical issues include revising the global update policy, an adaptive search policy and a segmented-goal training strategy. Based on simulation results, it is shown that these new ingredients added to the original knowledge extraction algorithm result in more efficient ones that can be applied in practical situations.", "venue": "IEEE International Conference on Fuzzy Systems", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "6ec556276fa4b780be409dbbb7690c8cb3e56315", "url": "https://www.semanticscholar.org/paper/6ec556276fa4b780be409dbbb7690c8cb3e56315", "title": "Web knowledge extraction for improving search", "abstract": "In this paper, we describe the extraction of Web knowledge from Web search engines and their usage for improving conventional Web search, such as supporting user's recall of vocabulary (query keywords), evaluation (quality, trustworthiness etc.) of search results, and re-ranking of search results. The extraction is based on the ideas of the \"structural term co-occurrence\" relationships and aggregating social annotation information, which can be easily computed by conventional Web search engines. Also, we describe a database interface to extract knowledge from Web search engines, which wraps accesses to Web search engines as virtual relational tables.", "venue": "ICUIMC '08", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "971d0bab472aa06258310b3b7da08a9c4d7f33ea", "url": "https://www.semanticscholar.org/paper/971d0bab472aa06258310b3b7da08a9c4d7f33ea", "title": "Virtual Reality-Based Fuzzy Spatial Relation Knowledge Extraction Method for Observer-Centered Vague Location Descriptions", "abstract": "Many documents contain vague location descriptions of observed objects. To represent location information in geographic information systems (GISs), these vague location descriptions need to be transformed into representable fuzzy spatial regions, and knowledge about the location descriptions of observer-to-object spatial relations must serve as the basis for this transformation process. However, a location description from the observer perspective is not a specific fuzzy function, but comes from a subjective viewpoint, which will be different for different individuals, making the corresponding knowledge difficult to represent or obtain. To extract spatial knowledge from such subjective descriptions, this research proposes a virtual reality (VR)-based fuzzy spatial relation knowledge extraction method for observer-centered vague location descriptions (VR-FSRKE). In VR-FSRKE, a VR scene is constructed, and users can interactively determine the fuzzy region corresponding to a location description under the simulated VR observer perspective. Then, a spatial region clustering mechanism is established to summarize the fuzzy regions identified by various individuals into fuzzy spatial relation knowledge. Experiments show that, on the basis of interactive scenes provided through VR, VR-FSRKE can efficiently extract spatial relation knowledge from many individuals and is not restricted by requirements of a certain place or time; furthermore, the knowledge obtained by VR-FSRKE is close to the knowledge obtained from a real scene.", "venue": "ISPRS Int. J. Geo Inf.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "8b2be89c53b294775c287478dd67ba8fcdb7b19d", "url": "https://www.semanticscholar.org/paper/8b2be89c53b294775c287478dd67ba8fcdb7b19d", "title": "A knowledge-extraction approach to identify and present verbatim quotes in free text", "abstract": "In news stories verbatim quotes of persons play a very important role, as they carry reliable information about the opinion of that person concerning specific aspects. As thousands of new quotes are published every hour it is very difficult to keep track of them. In this paper we describe a set of algorithms to solve the knowledge management problem of identifying, storing and accessing verbatim quotes. We handle the verbatim quote task as a relation extraction problem from unstructured text. Using a workflow of knowledge extraction algorithms we provide the required features for the relation extraction algorithm. The central relation extraction procedures is trained using manually annotated documents. It turns out that structural grammatical information is able to improve the F-vale for verbatim quote detection to 84.1%, which is sufficient for many exploratory applications. We present the results in a smartphone app connected to a web server, which employs a number of algorithms like linkage to Wikipedia, topics extraction and search engine indices to provide a flexible access to the extracted verbatim quotes.", "venue": "i-KNOW '12", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "e380e7b459fdc78006ad91cb868da5e0f2341cff", "url": "https://www.semanticscholar.org/paper/e380e7b459fdc78006ad91cb868da5e0f2341cff", "title": "Corpus for Coreference Resolution on Scientific Papers", "abstract": "The ever-growing number of published scientific papers prompts the need for automatic knowledge extraction to help scientists keep up with the state-of-the-art in their respective fields. To construct a good knowledge extraction system, annotated corpora in the scientific domain are required to train machine learning models. As described in this paper, we have constructed an annotated corpus for coreference resolution in multiple scientific domains, based on an existing corpus. We have modified the annotation scheme from Message Understanding Conference to better suit scientific texts. Then we applied that to the corpus. The annotated corpus is then compared with corpora in general domains in terms of distribution of resolution classes and performance of the Stanford Dcoref coreference resolver. Through these comparisons, we have demonstrated quantitatively that our manually annotated corpus differs from a general-domain corpus, which suggests deep differences between general-domain texts and scientific texts and which shows that different approaches can be made to tackle coreference resolution for general texts and scientific texts.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "29759f931c13b76341450bc20ad7b94aecdcc679", "url": "https://www.semanticscholar.org/paper/29759f931c13b76341450bc20ad7b94aecdcc679", "title": "Alignahead: Online Cross-Layer Knowledge Extraction on Graph Neural Networks", "abstract": "Existing knowledge distillation methods on graph neural networks (GNNs) are almost offline, where the student model extracts knowledge from a powerful teacher model to improve its performance. However, a pre-trained teacher model is not always accessible due to training cost, privacy, etc. In this paper, we propose a novel online knowledge distillation framework to resolve this problem. Specifically, each student GNN model learns the extracted local structure from another simultaneously trained counterpart in an alternating training procedure. We further develop a cross-layer distillation strategy by aligning ahead one student layer with the layer in different depth of another student model, which theoretically makes the structure information spread over all layers. Experimental results on five datasets including PPI, Coauthor-CS/Physics and Amazon-Computer/Photo demonstrate that the student performance is consistently boosted in our collaborative training framework without the supervision of a pre-trained teacher model. In addition, we also find that our alignahead technique can accelerate the model convergence speed and its effectiveness can be generally improved by increasing the student numbers in training. Code is available: https://github.com/GuoJY-eatsTG/Alignahead", "venue": "2022 International Joint Conference on Neural Networks (IJCNN)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "6dad40ad07fe37b8fe8c5123bd664680c6670351", "url": "https://www.semanticscholar.org/paper/6dad40ad07fe37b8fe8c5123bd664680c6670351", "title": "A yield management strategy for semiconductor manufacturing based on information theory", "abstract": "A model based on information theory, which allows technology managers to choose the optimal strategies for yield management in the semiconductor industry, is presented. The knowledge extraction rate per experimentation cycle and knowledge extraction rate per unit time serve as benchmarking metrics for yield learning. They enable managers to make objective comparisons of apparently unrelated technologies. Combinations of four yield analysis tools-electrical testing, automatic defect classification, spatial signature analysis and wafer position analysis-are examined in detail to determine an optimal yield management strategy for both the R&D and volume production environments.", "venue": "PICMET '99: Portland International Conference on Management of Engineering and Technology. Proceedings Vol-1: Book of Summaries (IEEE Cat. No.99CH36310)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "eba8641b7e3b635470f036339ae6b1154440d989", "url": "https://www.semanticscholar.org/paper/eba8641b7e3b635470f036339ae6b1154440d989", "title": "\u57fa\u65bc\u8a5e\u5f59\u8a9e\u7fa9\u7684\u767e\u79d1\u8fad\u5178\u77e5\u8b58\u63d0\u53d6\u5be6\u9a57 (An Experiment on Knowledge Extraction from an Encyclopedia Based on Lexicon Semantics)", "abstract": "The typical approaches to extracting text knowledge are sentential parsing and pattern matching. Theoretically, text knowledge extraction should be based on complete understanding, so the technology of sentential parsing is used in the field. However, the fragility of systems and highly ambiguous parse results are serious problems. On the other hand, by avoiding thorough parsing, pattern matching becomes highly efficient. However, different expressions of the same information will dramatically increase the number of patterns and nullify the simplicity of the approach. Parsing in Chinese encounters greater barriers than that in English does. Firstly, Chinese lacks morphology. For example, recognition of base-NP in Chinese is more difficult than that in English because its left boundary is hard to discern. * \u5317\u4eac\u8a9e\u8a00\u5927\u5b78\u8a08\u7b97\u6a5f\u7cfb Beijing Language and Culture University E-mail: songrou@blcu.edu.cn + \u5317\u4eac\u5de5\u696d\u5927\u5b78\u8a08\u7b97\u6a5f\u5b78\u9662 Beijing Polytechnic University E-mail: hopexy163@163.com", "venue": "Int. J. Comput. Linguistics Chin. Lang. Process.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "41791d10c672dcd835c6ef23c242597d16b51b82", "url": "https://www.semanticscholar.org/paper/41791d10c672dcd835c6ef23c242597d16b51b82", "title": "An artificial immune system model for knowledge extraction and representation", "abstract": "This paper presents an approach to knowledge extraction and representation based on an artificial immune system. The main idea is to extract the important concepts from a set of text documents, and find the relations between such concepts. At the end, a graph representation is obtained, which is intended to present a picture of the documentspsila contents. Some experiments were carried out in order to validate the proposed approach, and very promising results were obtained.", "venue": "2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "15a573dceb948b37a470185b641161920273c97e", "url": "https://www.semanticscholar.org/paper/15a573dceb948b37a470185b641161920273c97e", "title": "SAAN: Semantic Attention Adaptation Network for Face Super-Resolution", "abstract": "Face super-resolution (Face SR) is a sub-domain of SR that reconstructs high-resolution face images from low-resolution ones. The prior knowledge of face is widely used for recovering more realistic facial details, which will increase the complexity of the network and introduce additional knowledge extraction procession both in the training and evaluating stage. To address the above issues, we propose to combine face semantic prior extraction and face SR with the attention adaptation model and design a Semantic Attention Adaptation Network (SAAN) for face SR. Specifically, we train the face semantic parsing network and face SR network jointly, by adopting the semantic attention adaptation (SAA) model to transfer the ability of extracting face prior knowledge to the SR network. Then our SR network can work independently in the testing stage without using the prior knowledge extraction network. To generate realistic face images, we also utilize GAN loss to enrich the texture with more details (i.e. SAAN-G). Extensive experiments on the benchmark dataset illustrate that our SAAN and SAAN-G improve the state-of-the-art both on quality and efficiency.", "venue": "IEEE International Conference on Multimedia and Expo", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "bf61f95aaae5f32b1d304cbc283c30395488c621", "url": "https://www.semanticscholar.org/paper/bf61f95aaae5f32b1d304cbc283c30395488c621", "title": "Perceptual Knowledge Extraction Using Bayesian Networks of Salient Image Objects", "abstract": "A novel approach to perceptual knowledge extraction from images based on the concept of salient image objects is proposed. Salient image object - a concise description of a image fragment within a circular region - is a vector of salient image features, which describes the fragment invariantly to geometrical transformations and some intensity changes. Bayesian network of salient image objects - a kind of generative image modeling - is used as a model for the knowledge representation, which includes semantic entities (e.g., real-world objects) and provides probabilistic relations between image features and semantic entities. The proposed technique of multi-scale image relevance function permits a fast and ordered extraction of salient image objects", "venue": "18th International Conference on Pattern Recognition (ICPR'06)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "601d45b3576210f1a445ec5a69526839196061ce", "url": "https://www.semanticscholar.org/paper/601d45b3576210f1a445ec5a69526839196061ce", "title": "A collaborative approach to knowledge extraction from rough relational databases", "abstract": "This paper proposes a collaborative approach, which combines two processes: the extraction of approximate information from a rough relational database and its generalization into symbolical rules. A rough relational database model is basically a standard relational database model extended with some of the essential features of the rough set theory. The rough approach to relational databases allows the user to represent imprecision in querying, which gives a greater flexibility to the querying and also, improves the representational power of a relational database. The paper describes the prototype system ROUGH-ID3, which implements a hybrid knowledge extraction approach by integrating a set of rough database operators with the symbolic system ID3.", "venue": "Fifth International Conference on Hybrid Intelligent Systems (HIS'05)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "ae17dc73630d45f132e55abbfe6c08fcfff1e433", "url": "https://www.semanticscholar.org/paper/ae17dc73630d45f132e55abbfe6c08fcfff1e433", "title": "Design knowledge extraction in multi-objective optimization problems", "abstract": "This work concerns the post-optimal analysis of the trade-off front of a multi-objective optimization problem to extract useful design knowledge pertaining to these high-performing solutions. The expected knowledge basically consists of statistically significant relationships between the objective functions and decision variables. These relationships are represented in an intuitive and easy-to-use mathematical form. Since a number of such relationships may exist, for efficiency it is desirable that they are obtained in a single knowledge extraction step. Further, problem knowledge can be explored at two levels: lower and higher. At the lower-level, our interest is in finding a subset of the trade-off solutions to which the obtained relationships are exclusive. The higher-level knowledge addresses the effect of varying the problem parameters (that are kept constant in one run) on the trade-off front and therefore on the relationships. These concepts are explained through different examples.", "venue": "GECCO", "citationCount": 0, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "9889b3fd416bc99ed8a152a1a7937a04bf8be6b2", "url": "https://www.semanticscholar.org/paper/9889b3fd416bc99ed8a152a1a7937a04bf8be6b2", "title": "ARIADNA: a Knowledge Elicitation Support System", "abstract": "Information sources for building KBS are: domain experts, text documents, structured examples of problem solving and reality. In accordance with the type of information sources, one distinguishes the following subtypes of knowledge extraction: knowledge elicitation, text analysis, inductive learning; observation and systems analysis of reality. They are in a generalization relationship [15] with knowledge extraction.", "venue": "AI Commun.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "3b5b234bfe3fc5f8822e4262bd98c27c29bbf829", "url": "https://www.semanticscholar.org/paper/3b5b234bfe3fc5f8822e4262bd98c27c29bbf829", "title": "Syllabuses crawling and knowledge extraction of courses for global standardization education", "abstract": "To realize lively situation of the global standardization education in universities, it is very important to know the situation for the global standardization education in universities. In this paper, a new crawling technology to collect and analyze syllabuses published on the websites of universities is proposed. Using the technology proposed, syllabuses of 132 Japanese universities including all the 88 national universities have been crawled. As a result of the extraction of the global standardization courses, it is made clear that 45 courses of the global standardization education are being offered by 24 Japanese universities. This paper also shows a result of knowledge extraction from the 45 syllabuses. In addition, collaboration for global standardization education between universities is proposed based on the successful results of collaboration education between two universities by using TV conference ICT system.", "venue": "Proceedings of the 2014 ITU kaleidoscope academic conference: Living in a converged world - Impossible without standards?", "citationCount": 0, "fieldsOfStudy": ["Engineering", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "22cda9c895e710d45b63bf694ace3eb56f1e2802", "url": "https://www.semanticscholar.org/paper/22cda9c895e710d45b63bf694ace3eb56f1e2802", "title": "Knowledge Extraction on the Nexus Between Terrorism Criteria and Attacks on Private Citizens and Property", "abstract": "This study extracted knowledge on the association between terrorism inclusion criteria and one of the terrorism target/victim types known as \u201cprivate citizens and property.\u201d Three criteria determine what constitutes a terror attack: Criterion 1: the action is done with intention to attain a political, economic, religious, or social goal; Criterion 2: the action is done with the intention to coerce, intimidate or publicize to larger audience(s); Criterion 3: the action is outside international humanitarian law. Literally, all terror incidents satify Criterion 3. As for Criteria 1 and 2, the odds ratio was deployed on the global terrorism database, consisting of 170,350 records of terrorist attack incidents, to evaluate the nexus between each of these criteria and terror attacks on private citizens and property. The results showed that any terror attack on private citizens and property is 2.2 times more likely to have been perpetrated by a terror group in order to achieve Criterion 2 than to achieve Criterion 1. The implications of the outcome in counterterrorism are discussed.", "venue": "Int. J. Cyber Warf. Terror.", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Political Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "f3656aba41ee0333beab85340d3eb578f0e080a5", "url": "https://www.semanticscholar.org/paper/f3656aba41ee0333beab85340d3eb578f0e080a5", "title": "Business Process Event Log Transformation into Bayesian Belief Network", "abstract": "Business process (BP) mining has been recognized in business intelligence and reverse engineering fields because of the capabilities it has to discover knowledge about the implementation and execution of BP for analysis and improvement. Existing business knowledge extraction solutions in process mining context requires repeating analysis of event logs for each business knowledge extraction task. The probabilistic modelling could allow improved performance of BP analysis. Bayesian belief networks are a probabilistic modelling tool and the paper presents their application in BP mining. The paper shows that existing process mining algorithms are not suited for this, since they allow for loops in the extracted BP model that do not really exist in the event log, and presents a custom solution for directed acyclic graph extraction. The paper presents results of a synthetic log transformation into Bayesian belief network showing possible application in business intelligence extraction and improved decision support capabilities.", "venue": "Integrated Spatial Databases", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "b885a1c43990b7bb6bc37cf1897ea30cb4ba3fed", "url": "https://www.semanticscholar.org/paper/b885a1c43990b7bb6bc37cf1897ea30cb4ba3fed", "title": "A cognitive path-guidance-system for minimally invasive surgery", "abstract": "The presented path-guidance system is able to learn movements and to predict motion. It shall enhance safe navigation for surgeons in minimally invasive surgery by creating a virtual fixture which holds the end-effector's motion to a desired path and warning the surgeon in a dangerous situation. Surgeons can demonstrate interventions and best practices. The system collects information from surgeon demonstrated trajectories, defined as best practices, and extracts knowledge to provide guidance for other users to carry out the same intervention. Knowledge extraction is achieved through trajectory clustering, maximum likelihood classification and a Markov model to predict states. The fundamental task is to guide a surgeon along a desired trajectory (navigated path) and prevent them entering into zones of risk. The path is not sequential, furcations are permitted and modeled showing alternatives in the ongoing intervention. An evaluation with a pelvitrainer showed good results with over 89% hit rate in predicting the motion.", "venue": "IEEE 8th International Symposium on Intelligent Systems and Informatics", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "10a85eba940069dd3b877a2f268e888e940828eb", "url": "https://www.semanticscholar.org/paper/10a85eba940069dd3b877a2f268e888e940828eb", "title": "Knowledge Discovery and Case Based Reasoning in Health Promotion: Development of a Help-Desk for Prevention of Occupational Injuries", "abstract": "This paper presents the concepts, ideas and techniques behind Case Based Reasoning (CBR) in relation to knowledge extraction techniques for health promotion. The ultimate goal is to develop a help-desk service for advice about preventive measures to be taken concerning concrete occupational injury hazards. CBR has been suggested to be a complimentary method to knowledge extraction in order to take direct advantages of large databases for building decision support systems. In this work a database on work injuries is being used to develop a CBR application using a CBR shell-called Recall.", "venue": "MedInfo", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "f1b5f99f9ee9083635d6b3ea85dc2dd131d39a68", "url": "https://www.semanticscholar.org/paper/f1b5f99f9ee9083635d6b3ea85dc2dd131d39a68", "title": "Accelerating News Integration in Automatic Knowledge Extraction Ecosystems: an API-first Outlook", "abstract": "Leveraging Application Programming Interfaces (APIs) has been widely acknowledged as a valuable approach to software and system design that have promoted the acceleration of products and services development by allowing the decoupling of interface design from service implementation details. Many organizations in the news and journalism industry have adopted and promoted this API oriented approach. In the first part of this paper, we provide a survey of the most significant recent work around traditional news and journalistic open APIs and how these have been influenced by and impacted the news product landscape. In the second part of the paper, we identify two disruptive technology trends that we believe will impact the role and value of news/journalism products in the future: API-first development methodologies, and the increased role of news-supported automatic knowledge extraction and analytic services. We anticipate that these two driving forces will create a new wave of adoption, open collaboration, standardization and overall progress in news content adoption in knowledge platforms. We provide a brief overview of our experience in this area at Dow Jones.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "28f7106bb17b51a35ba03e72f0910129c0fe6224", "url": "https://www.semanticscholar.org/paper/28f7106bb17b51a35ba03e72f0910129c0fe6224", "title": "PAKES: A Reinforcement Learning-Based Personalized Adaptability Knowledge Extraction Strategy for Adaptive Learning Systems", "abstract": "Advancements in adaptive educational technologies, specifically the adaptive learning system, have made it possible to automatically optimize the sequencing of the pedagogical instructions according to the needs of individual learners. The crux of such systems lies in the instructional sequencing policy, which recommends personalized learning material based on the learning experiences of the learner to maximize their learning outcomes. However, limited available information such as cognitive, affective states, and competence levels of the learners ongoing knowledge points servers critical challenges to optimizing individual-specific pedagogical instructions in real-time. Moreover, making such decisions policy for every learner with a unique knowledge profile demands a trade-off between learner current knowledge and curiosity to learn next knowledge point. To address these challenges, this paper proposes a personalized adaptability knowledge extraction strategy (PAKES) using cognitive diagnosis and reinforcement learning (RL). We apply the general diagnostic model to track the current knowledge state of the learners. Subsequently, an RL-based Q-learning algorithm is employed to recommend optimal pedagogical instructions for individuals to meet their learning objectives while maintaining equilibrium among the learner-control and teaching trajectories. The results indicate that the learning analytics of the proposed framework can fairly deliver the optimal pedagogical paths for the learners based upon their learning profiles. A 62% learning progress score was achieved with the pedagogical paths recommended by the PAKES, showing a 20% improvement compared to the baseline model.", "venue": "IEEE Access", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "02a470ef1074d478803f0db5428022fd05ce8f0f", "url": "https://www.semanticscholar.org/paper/02a470ef1074d478803f0db5428022fd05ce8f0f", "title": "Knowledge extraction from a time-series using segmentation, fuzzy matching and predictor graphs", "abstract": "In this paper, a novel multi-stage approach to knowledge extraction from a time-series is proposed. A given time-series is modeled as a sequence of well-known primitive patterns with the purpose of identifying first-order probabilistic transition rules for prediction. The first stage of the proposed model segments a time-series into structurally distinct temporal blocks of non-uniform length such that each block possesses a relatively low variation of dynamic slope. In the second stage, the temporal segments thus obtained are normalized and matched with one of four well-known primitive patterns using a fuzzy matching algorithm. Finally, the sequence of matched segments is used to represent the time-series as a set of four directed graphs corresponding to the four primitive patterns. Each vertex in the graphs represents a horizontal partition of the time-series and each directed edge indicates the transitions between two such partitions caused by the occurrence of one or more temporal segments. In the test phase, the graphs are employed to predict possible future values of the time-series. Experiments carried out on the TAIEX close-price time-series indicate a high prediction accuracy, thereby validating the use of the model for real-life forecasting applications.", "venue": "2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)", "citationCount": 0, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "900b128ed5ba03f008fdda3761a4fc51b540ad50", "url": "https://www.semanticscholar.org/paper/900b128ed5ba03f008fdda3761a4fc51b540ad50", "title": "Deep learning Arabic printed document knowledge extraction", "abstract": "This paper presents how to utilize deep learning to extract knowledge from Arabic printed document images. The fundamental goal of deep learning is automatically extracting significant features from images, eliminating the need for a classic feature extraction method. We describe how to extract high-quality and coherent knowledge from Arabic printed document images using deep learning. This system is constructed on keywords used to classify Arabic document images according to these keywords. We used A questionnaire to choose valuable words according to historical, scientific, or religious documents. The evaluation of the proposed system is applied to Arabic printed document images to extract keywords. The accuracy of the proposed deep learning extraction approach is hugely affected by image preprocessing and image quality. The proposed system has a higher level of accuracy while extracting keywords. We achieve a 3.78% character error rate in the proposed system and a 15.46% word error rate.", "venue": "ICFNDS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "f5e7039bd2efed5c1638eb26672be3ea1c592f09", "url": "https://www.semanticscholar.org/paper/f5e7039bd2efed5c1638eb26672be3ea1c592f09", "title": "Automated Knowledge Extraction of Liver Cysts From CT Images Using Modified Whale Optimization and Fuzzy C Means Clustering Algorithm", "abstract": "In this study, the integrated modified whale optimization and modified fuzzy c-means clustering algorithm using morphological operations are developed and implemented for appropriate knowledge extraction of a cyst from computer tomography (CT) images of the liver to facilitate modern intelligent healthcare systems. The proposed approach plays an efficient role in diagnosing the liver cyst. To evaluate the efficiency, the outcomes of the proposed approach have been compared with the minimum cross entropy based modified whale optimization algorithm (MCE and MWOA), teaching-learning optimization algorithm based upon minimum cross entropy (MCE and TLBO), particle swarm intelligence algorithm (PSO), genetic algorithm (GA), differential evolution (DE) algorithm, and k-means clustering algorithm. For this, various parameters such as uniformity (U), mean structured similarity index (MSSIM), structured similarity index (SSIM), random index (RI), and peak signal-to-noise ratio (PSNR) have been considered. The experimental results show that the proposed approach is more efficient and accurate than others.", "venue": "International Journal of Information System Modeling and Design", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "8db3cf6d82a60997fbc283eb28443cbbfff43011", "url": "https://www.semanticscholar.org/paper/8db3cf6d82a60997fbc283eb28443cbbfff43011", "title": "AI Driven Knowledge Extraction from Clinical Practice Guidelines: Turning Research into Practice", "abstract": "Background and Objectives: Clinical Practice Guidelines (CPGs) represent the foremost methodology for sharing state-of-the-art research findings in the healthcare domain with medical practitioners to limit practice variations, reduce clinical cost, improve the quality of care, and provide evidence based treatment. However, extracting relevant knowledge from the plethora of CPGs is not feasible for already burdened healthcare professionals, leading to large gaps between clinical findings and real practices. It is therefore imperative that state-of-the-art Computing research, especially machine learning is used to provide artificial intelligence based solution for extracting the knowledge from CPGs and reducing the gap between healthcare research/guidelines and practice. Methods: This research presents a novel methodology for knowledge extraction from CPGs to reduce the gap and turn the latest research findings into clinical practice. First, our system classifies the CPG sentences into four classes such as condition-action, condition-consequences, action, and not-applicable based on the information presented in a sentence. We use deep learning with state-of-the-art word embedding, improved word vectors technique in classification process. Second, it identifies qualifier terms in the classified sentences, which assist in recognizing the condition and action phrases in a sentence. Finally, the condition and action phrase are processed and transformed into plain rule If Condition(s) Then Action format. Results: We evaluate the methodology on three different domains guidelines including Hypertension, Rhinosinusitis, and Asthma. The deep learning model classifies the CPG sentences with an accuracy of 95%. While rule extraction was validated by user-centric approach, which achieved a Jaccard coefficient of 0.6, 0.7, and 0.4 with three human experts extracted rules, respectively.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "dc69c68618c6edcbc7a78f7575c04f3f2ae8c1aa", "url": "https://www.semanticscholar.org/paper/dc69c68618c6edcbc7a78f7575c04f3f2ae8c1aa", "title": "A pruned higher-order network for knowledge extraction", "abstract": "Usually, the learning stage of a neural network leads to a single model. But a complex problem cannot always be solved adequately by a global system. On the other side, several systems specialized on a subspace have some difficulties to deal with situations located at the limit of two classes. This article presents a new adaptive architecture based upon higher-order computation to adjust a general model to each pattern and using a pruning algorithm to improve the generalization and extract knowledge. We use one small multi-layer perceptron to predict each weight of the model from the current pattern (we have one estimator per weight). This architecture introduces a higher-order computation, biologically inspired, similar to the modulation of a synapse between two neurons by a third neuron. The general model can then be smaller, more adaptative and more informative.", "venue": "Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "b2a7428335b622a5349c87fca408f068b5ab058e", "url": "https://www.semanticscholar.org/paper/b2a7428335b622a5349c87fca408f068b5ab058e", "title": "Acne Severity Grading on Face Images via Extraction and Guidance of Prior Knowledge", "abstract": "Acne Vulgaris seriously affects people\u2019s daily life. In this paper, we propose a face acne grading framework which is a new paradigm to solve the image classification problem where the number and type of small objects are the evidence. This framework includes two components: prior knowledge extraction and prior knowledge guided network. The prior knowledge extraction uses an excellent segmentation method to predict the lesion areas as prior knowledge. The prior knowledge guided network fuses the prior knowledge and its corresponding image to grade the severity. The experiment results demonstrate that our framework achieves the state-of-the-art and diagnosis level of dermatologists.", "venue": "IEEE International Conference on Bioinformatics and Biomedicine", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "acfed6c086fdb6588a509d7e47eb234a0927cf53", "url": "https://www.semanticscholar.org/paper/acfed6c086fdb6588a509d7e47eb234a0927cf53", "title": "Coupling Computers to Living Cells for Automated Knowledge Extraction", "abstract": "A general method for coupling individual living cells to computers is presented. This method is called visual-servoing optical microscopy\u00bf (VSOM\u00bf). Protocols, instrumentation, and software are described and experimental results are presented. During VSOM experiments, automated decisions are made as single cell responses to computer-applied stimuli are interpreted in real-time. Well-annotated live cell responses are stored in distributed VSOM databases in real-time. This closed-loop method enables automated extraction of knowledge from individual living cells.", "venue": "2008 IEEE Fourth International Conference on eScience", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "e300b5c2faed56fdd0dd7f60440f980a2032f424", "url": "https://www.semanticscholar.org/paper/e300b5c2faed56fdd0dd7f60440f980a2032f424", "title": "Leveraging hardware-dependent knowledge extraction with multiple program analysis techniques", "abstract": "Hardware-dependent systems must follow coding rules which are specific to the employed hardware. Among these rules, coding rules related to register-access are frequently required. Verifying these coding rules is challenging because of their variances. This paper proposed a practical approach to handle this problem. The approach is to extract the information related to register-access, called hardware-dependent knowledge, in the source code beforehand. The extracted knowledge is convenient to adapt for the variances of the coding rules. The process of extracting this knowledge is performed by combining multiple program analysis techniques: pattern matching, abstract interpretation-based static program analysis, bounded model checking, and counterexample-guided abstraction refinement. The proposed approach is evaluated by applying for an industrial source code. The result shows that combining multiple program analysis techniques is effective in extracting hardware-dependent knowledge.", "venue": "ACM Symposium on Applied Computing", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "b1febd21651000d6d12d2fb43330ecaf4d9bd371", "url": "https://www.semanticscholar.org/paper/b1febd21651000d6d12d2fb43330ecaf4d9bd371", "title": "Inmplode: A framework to interpret multiple related rule\u2010based models", "abstract": "There is a growing trend to split problems into separate subproblems and develop separate models for each (e.g., different churn models for separate customer segments; different failure prediction models for separate university courses, etc.). While it may lead to better predictive models, the use of multiple models makes interpretability more challenging. In this paper, we address the problem of synthesizing the knowledge contained in a set of models without a significant loss of prediction performance. We focus on decision tree models because their interpretability makes them suitable for problems involving knowledge extraction. We detail the process, identifying alternative methods to address the different phases involved. An extensive set of experiments is carried out on the problem of predicting the failure of students in courses at the University of Porto. We assess the effect of using different methods for the operations of the methodology, both in terms of the knowledge extracted as well as the accuracy of the combined models.", "venue": "Expert Syst. J. Knowl. Eng.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "aefa29e71afde4ceb1e061febee88a0204e61521", "url": "https://www.semanticscholar.org/paper/aefa29e71afde4ceb1e061febee88a0204e61521", "title": "Surrogate modeling and knowledge extraction in ga applied to a parameter's estimation case", "abstract": "The Genetic Algorithm is a well-known heuristic-based optimization technique, but by using it in expensive problems, the excessive use of the fitness function can produce heavy computational loads. The purpose of this paper is to avoid this drawback by employing surrogate algorithms, providing approximate but sufficiently accurate solutions. Particularly the surrogate is based on granular computing and fuzzy logic. The difference from our related previous works consists into treat the algorithm's search, not as a black box, but extracting certain knowledge of granules' behavior represented by a new form of fuzzy aptitude functions. The primary objective is to manage the process intelligently, not only avoiding unnecessary usage of fitness evaluations but also improving the convergence with the extracted knowledge and the parameters' update with a newly built neural network structure. The algorithm shows satisfactory results in saving unnecessary evaluations and in time; in this case, we proved the optimization process related to parameters' estimation of a permanent magnet synchronous machine (PMSM) and some common benchmark functions used in GA assessments.", "venue": "2017 IEEE Symposium Series on Computational Intelligence (SSCI)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "7cedd43e5a1b6eba695f804d472e88b5f9d01724", "url": "https://www.semanticscholar.org/paper/7cedd43e5a1b6eba695f804d472e88b5f9d01724", "title": "Reasoning with Textual Queries: A Case of Medical Text", "abstract": "Text understanding and reasoning is a very difficult but highly important problem with many practical applications like chatbots. Babylon Health is building an AI-based symptom checking service offered through a chatbot. Depending on what medical terms appear in the text, nodes in a Probabilistic Graph Model (PGM) need to be activated in order to start the symptom checking process. We developed a Semantic Technologies-based solution where OWL concepts are build from user text (in an attempt to capture its meaning) and then compared with respect to subsumption against the conditions in PGM which are encoded using concepts form a medical KB. We developed a knowledge extraction method as well as a hybrid reasoning algorithm that compares concepts using both logical axioms from the medical KB as well as potentially additional information hidden in their labels. We implemented all our algorithms and conducted an experimental evaluation comparing to a baseline text annotation and an ML-based approach obtaining encouraging results.", "venue": "SEMWEB", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "tp"], "mention_counts": {"ke": 1, "onto": 1, "tp": 1}, "nlp_mention_counts": {"ke": 1, "tp": 1}, "ld_mention_counts": {"ke": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "8d0b74ae2268a231a9936ee8e64374c7a7bfbf2b", "url": "https://www.semanticscholar.org/paper/8d0b74ae2268a231a9936ee8e64374c7a7bfbf2b", "title": "Knowledge extraction from creators of presentation slides on a slide sharing service", "abstract": "In this study, we present a method for extracting and representing knowledge of presentation slide creators based on the slide contents that are published on a slide sharing service. The proposed method regards the number of views, downloads, and likes from other users as the users rating for a presentation slide, and extract knowledge of the slide creator in terms of the usefulness and knowledge amount of the slide contents. In the experiment, we evaluate the feasibility of the proposed method by analyzing the presentation slides created by users on SlideShare, which is one of the most popular slide sharing service.", "venue": "2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "72e42f0f2e89f8d17469307657934003dafd1fa2", "url": "https://www.semanticscholar.org/paper/72e42f0f2e89f8d17469307657934003dafd1fa2", "title": "Analysis of Serious Games based Learning Requirements using Feedback and Traces of Users", "abstract": "Identify the games that best meet the needs and expectations of teachers and objectives of their courses remains a necessity about the integration of serious games among active teaching methods. Indeed, several serious games have developed in recent years, and it is often difficult for a teacher, not a computer scientist in particular, to find a game that meets these specific needs. Our aim is to develop models and tools enabling the teacher to find serious games adapted to his needs, considering user feedback and their traces of interaction with the game. To this end, we have explored the evaluation methods of serious games as well as methods of extracting knowledge from traces and texts. In this paper, we present our method of knowledge extraction of educational objectives. Thus, our proposal is assisting and supporting teachers/trainers to choose serious games and easily integrate them into their learning processes and devices.", "venue": "CSEDU", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "3429b3f87dce9b6603a9eaaf39f59498ca145c59", "url": "https://www.semanticscholar.org/paper/3429b3f87dce9b6603a9eaaf39f59498ca145c59", "title": "Utility of SNOMED CT in automated expansion of clinical terms in discharge summaries: Testing issues of coverage", "abstract": "Objective: This study tests coverage of SNOMED CT as an expansion source in the process of automated expansion of clinical terms found in discharge summaries. Term expansion is commonly used as a technique in knowledge extraction, query formulation and semantic modelling among other applications. However, characteristics of the sources might affect credibility of outputs, and coverage is one of them. Method: We developed an automated method for testing coverage of more than one source at a time. We used several methods to clean our corpus of discharge summaries before we extracted text fragments as candidates for clinical concepts. We then used Unified Medical Language System (UMLS) sources and UMLS REST API to filter concepts from the pool of text fragments. Statistical measures like true positive rate and false negative rate were used to decide on the coverage of the source. We also tested the coverage of the individual SNOMED CT hierarchies using the same methods. Results: Findings suggest that a combination of four terminologies tested (SNOMED CT, NCI, LNC and MSH) achieves over 90% of coverage for term expansion. We also found that the SNOMED CT hierarchies that hold clinically relevant concepts provided 60% of coverage. Conclusion: We believe that our findings and the method we developed will be of use to both scientists and practitioners working in the domain of knowledge extraction.", "venue": "Health information management : journal of the Health Information Management Association of Australia", "citationCount": 1, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "bc63cf203be65730a789e79e10c956fb71d810dc", "url": "https://www.semanticscholar.org/paper/bc63cf203be65730a789e79e10c956fb71d810dc", "title": "Incrementally Verifiable Computation via Rate-1 Batch Arguments", "abstract": "Non-interactive delegation schemes enable producing succinct proofs (that can be efficiently verified) that a machine M transitions from c1 to c2 in a certain number of deterministic steps. We here consider the problem of efficiently merging such proofs: given a proof \u03a01 that M transitions from c1 to c2, and a proof \u03a02 that M transitions from c2 to c3, can these proofs be efficiently merged into a single short proof (of roughly the same size as the original proofs) that M transitions from c1 to c3? To date, the only known constructions of such a mergeable delegation scheme rely on strong non-falsifiable \u201cknowledge extraction\u201d assumptions. In this work, we present a provably secure construction based on the standard LWE assumption. As an application of mergeable delegation, we obtain a construction of incrementally verifiable computation (IVC) (with polylogarithmic length proofs) for any (unbounded) polynomial number of steps based on LWE; as far as we know, this is the first such construction based on any falsifiable (as opposed to knowledge-extraction) assumption. The central building block that we rely on, and construct based on LWE, is a rate-l batch argument (BARG): this is a non-interactive argument for NP that enables proving k NP statements $x_{1},\\ldots, x_{k}$ with communication/verifier complexity m + o(m), where m is the length of one witness. rate-1 BARGs are particularly useful as they can be recursively composed a super-constant number of times.", "venue": "IEEE Annual Symposium on Foundations of Computer Science", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "14d691d8a06c8aeea9d2ad500e806d00eb8ab00e", "url": "https://www.semanticscholar.org/paper/14d691d8a06c8aeea9d2ad500e806d00eb8ab00e", "title": "Scalable Label Propagation Algorithms for Heterogeneous Networks", "abstract": "Heterogeneous networks are large graphs consisting of different types of nodes and edges. They are an important category of complex networks, but the process of knowledge extraction and relations discovery from these networks are so complicated and time-consuming. Moreover, the scale of these networks is steadily increasing. Thus, scalable and accurate methods are required for efficient knowledge extraction. In this paper, two distributed label propagation algorithms, namely DHLP-1 and DHLP-2, in the heterogeneous networks have been introduced. The Apache Giraph platform is employed which provides a vertex-centric programming model for designing and running distributed graph algorithms. Complex heterogeneous networks have many examples in the real world and are widely used today for modeling complicated processes. Biological networks are one of such networks. As a case study, we have measured the efficiency of our proposed DHLP-1 and DHLP-2 algorithms on a biological network consisting of drugs, diseases, and targets. The subject we have studied in this network is drug repositioning, aimed at saving both time and cost by suggesting new indications for the current drugs. We compared the proposed algorithms with similar non-distributed versions of them namely MINProp and Heter-LP. The experiments revealed that the runtime of the algorithms has decreased in the distributed versions rather than non-distributed ones dramatically. The effectiveness of our proposed algorithms against other algorithms is supported through statistical analysis of 10-fold cross-validation as well as experimental analysis.", "venue": "bioRxiv", "citationCount": 1, "fieldsOfStudy": ["Biology", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "1f3d39f31465b3374251768dced1c19fe0624bd1", "url": "https://www.semanticscholar.org/paper/1f3d39f31465b3374251768dced1c19fe0624bd1", "title": "Extraction of Knowledge-Rich Contexts in Russian \u2013 A Study in the Automotive Domain", "abstract": "This paper presents ongoing research aiming at the automated extraction of knowledge-rich contexts (KRCs) from a Russian language corpus. The notion of KRCs was introduced by Meyer (2001) and refers to a term\u2019s co-text (Sebeok, 1986) as a reservoir of potentially important information about a concept. From a terminological point of view, it seems that KRCs contain exactly the kind of information that should be included into a terminology database. Accordingly, the question how KRCs can be automatically acquired has been widely studied in recent years. However, many languages including Russian still lack thorough study. This paper presents preliminary experimental results obtained on a specialized corpus in the automotive domain.", "venue": "NODALIDA", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "96aec635345d9197d7cc4a89aee17bed65ac7584", "url": "https://www.semanticscholar.org/paper/96aec635345d9197d7cc4a89aee17bed65ac7584", "title": "CustNER: A Rule-Based Named-Entity Recognizer With Improved Recall", "abstract": "This article describes CustNER: a system for named-entity recognition (NER) of person, location, and organization. Realizing the incorrect annotations of existing NER, four categories of false negatives have been identified. The NEs not annotated contain nationalities, have corresponding resource in DBpedia, are acronyms of other NEs. A rule-based system, CustNER, has been proposed that utilizes existing NERs and DBpedia knowledge base. CustNER has been trained on the open knowledge extraction (OKE) challenge 2017 dataset and evaluated on OKE and CoNLL03 (Conference on Natural Language Learning) datasets. The OKE dataset has also been annotated with the three types. Evaluation results show that CustNER outperforms existing NERs with F score 12.4% better than Stanford NER and 3.1% better than Illinois NER. On another standard evaluation dataset for which the system is not trained, the CoNLL03 dataset, CustNER gives results comparable to existing systems with F score 3.9% better than Stanford NER, though Illinois NER F score is 1.3% better than CustNER.", "venue": "International Journal on Semantic Web and Information Systems (IJSWIS)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nll", "ke", "kg"], "mention_counts": {"ke": 1, "kg": 1, "nll": 1}, "nlp_mention_counts": {"ke": 1, "nll": 1}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.5}, {"paperId": "88b82615df72ed95232d32191f2265099afc6951", "url": "https://www.semanticscholar.org/paper/88b82615df72ed95232d32191f2265099afc6951", "title": "Extracting Knowledge Tokens from Text Streams", "abstract": "This problem analysis paper presents our position on how could the solution be sought to the problem of extracting semantically rich fragments from a stream of plain text posts. We first present our understanding of the problem context and explain the focus of our research. Further, in the problem setting section we elaborate the workflow for knowledge extraction from incoming information tokens. This workflow is then used as a key to structure our review of the literature on the relevant component techniques which may be exploited in a combination to achieve the desired outcome. We finally outline our plan for conducting the experiments with an aim to validate the workflow and find a proper combination of the component techniques for all steps which may solve our specific research problem.", "venue": "ICTERI", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "38fd91843734242c77dd92bd775582426ebe5dac", "url": "https://www.semanticscholar.org/paper/38fd91843734242c77dd92bd775582426ebe5dac", "title": "Acquisition and Analysis of Drivers' Action Intelligence", "abstract": "By using the behavior instruction based (BIB) knowledge extraction method, the intention rules for drivers' action intelligence are obtained for further analysis. First, the characteristic of knowledge between drivers is discussed; next, in order to analyze the limitation of drivers' obstacle avoidance, experiments on the relationship between the success rate of driving and the number of obstacles are conducted and two vital conclusions for further knowledge extraction under complex environments are initially drawn", "venue": "2004 IEEE International Conference on Robotics and Biomimetics", "citationCount": 0, "fieldsOfStudy": ["Engineering", "Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "02c2dec1345ff4710a71320a6ae2f45da4b72f88", "url": "https://www.semanticscholar.org/paper/02c2dec1345ff4710a71320a6ae2f45da4b72f88", "title": "Concept Extraction: A Modular Approach to Extraction of Source Code Concepts", "abstract": "Code examples have always been one of the most sought after pieces of information when it comes to understanding and mastering programming concepts. Research shows extracting knowledge from such examples in any online tutoring system is a challenging task. Current methods rely upon specifically formed regular expressions that must be tailor made to the input language, or generation of an AST for the given input program. In our paper, we extend upon existing implementations in code recommendation software using a novel keyword based search tree (k-BST) method. K-BST recommends relevant code fragments by extracting existing keywords, matching with relevant coding examples by k-means clustering, and recommending the relevant coding examples back to the user. K-BSTs also address several major issues which modern knowledge extraction software often run into, like ease of use, extendibility to other domains and run time. With that in mind, K-BSTs are designed to tackle ease of use with popular recognizable file formats such as CSV while keeping the run time of extracting relevant keywords to be extremely low (compared to the more popular method that uses AST).", "venue": "2018 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "9d858ed65b3b81a7f2ec1364eab9ba5e89fe6f4c", "url": "https://www.semanticscholar.org/paper/9d858ed65b3b81a7f2ec1364eab9ba5e89fe6f4c", "title": "KEP-1.0: An Automatic Pipeline to Assist a Rapid Learning of COVID-19 Publications", "abstract": "Toward efficient learning of massive publications during the COVID-19 pandemic, we propose a pipeline, Knowledge Extraction for COVID-19 Publications (KEP), that aims at automatic extraction and representation of key knowledge from user-interested publications. The first version, KEP-1.0, has been developed and published on the Python Package Index (PyPI) (URL: https://pypi.org/project/KEP/). In this first release, knowledge about key topics, disease discussions, and location mentions for each publication is provided. KEP-1.0 not only extracts relevant knowledge but, more importantly, emphasizes the top discussed entities and presents visualizable plots, including bar graphs and word clouds. This allows a rapid preliminary understanding of the main discussions in the publication from these three aspects. Moreover, an enhanced TF-IDF algorithm, the weighted TF-IDF, targeting the publication topic identification purpose, has been proposed and evaluated. The pipeline is fully open-sourced and customizable. KEP-1.0 is ready for use in its current form or to be embedded into existing literature platforms. This pipeline is designed for COVID-related publications, but it has the potential to benefit similar knowledge extraction tasks for other topics of interest with a rapidly increasing number of publications.", "venue": "IEEE International Conference on Bioinformatics and Biomedicine", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "4cd430f7ba1122bf0a4eaeb93af857c358114837", "url": "https://www.semanticscholar.org/paper/4cd430f7ba1122bf0a4eaeb93af857c358114837", "title": "Towards Architecture-Agnostic Neural Transfer: a Knowledge-Enhanced Approach", "abstract": "The ability to enhance deep representations with prior knowledge is receiving a lot of attention from the AI community as a key enabler to improve the way modern Artificial Neural Networks (ANN) learn. In this paper we introduce our approach to this task, which comprises of a knowledge extraction algorithm, a knowledge injection algorithm and a common intermediate knowledge representation as an alternative to traditional neural transfer. As a result of this research, we envisage a knowledge-enhanced ANN, which will be able to learn, characterise and reuse knowledge extracted from the learning process, thus enabling more robust architecture-agnostic neural transfer, greater explainability and further integration of neural and symbolic approaches to learning.", "venue": "IJCAI", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "ae42939364c21cce23b363fe62388acdbee2ab8e", "url": "https://www.semanticscholar.org/paper/ae42939364c21cce23b363fe62388acdbee2ab8e", "title": "Comparing Methods to Extract the Knowledge from Neural Networks", "abstract": "Neural networks (NN) have been shown to be accurate classifiers in many domains. Unfortunately, the lack of NN\u2019s explanatory capability of knowledge learned has somewhat limited their application. A stream of research has therefore developed focusing on knowledge extraction from within neural networks. The literature, unfortunately, lacks consensus on how best to extract knowledge from help neural networks. Additionally, there is a lack of empirical studies that compare existing algorithms on relevant performance measures. Therefore, this study attempts to help fill this gap by comparing two different approaches to extracting IF-THEN rules from feedforward NN. The results show a significant difference in the performance of the two algorithms depending on the structure of the dataset utilized.", "venue": "AMCIS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "76f6a97b6a34499f2ad954eb004bce9f6e097723", "url": "https://www.semanticscholar.org/paper/76f6a97b6a34499f2ad954eb004bce9f6e097723", "title": "Review and classification of trajectory summarisation algorithms: From compression to segmentation", "abstract": "With the continuous development and cost reduction of positioning and tracking technologies, a large amount of trajectories are being exploited in multiple domains for knowledge extraction. A trajectory is formed by a large number of measurements, where many of them are unnecessary to describe the actual trajectory of the vehicle, or even harmful due to sensor noise. This not only consumes large amounts of memory, but also makes the extracting knowledge process more difficult. Trajectory summarisation techniques can solve this problem, generating a smaller and more manageable representation and even semantic segments. In this comprehensive review, we explain and classify techniques for the summarisation of trajectories according to their search strategy and point evaluation criteria, describing connections with the line simplification problem. We also explain several special concepts in trajectory summarisation problem. Finally, we outline the recent trends and best practices to continue the research in next summarisation algorithms.", "venue": "Int. J. Distributed Sens. Networks", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "135e2471a51e1c640b8b70dc71a6aa46606f9a06", "url": "https://www.semanticscholar.org/paper/135e2471a51e1c640b8b70dc71a6aa46606f9a06", "title": "Coarse-To-Careful: Seeking Semantic-Related Knowledge for Open-Domain Commonsense Question Answering", "abstract": "It is prevalent to utilize external knowledge to help machine answer questions that need background commonsense, which faces a problem that unlimited knowledge will transmit noisy and misleading information. Towards the issue of introducing related knowledge, we propose a semantic-driven knowledge-aware QA framework, which controls the knowledge injection in a coarse-to-careful fashion. We devise a tailoring strategy to filter extracted knowledge under monitoring of the coarse semantic of question on the knowledge extraction stage. And we develop a semantic-aware knowledge fetching module that engages structural knowledge information and fuses proper knowledge according to the careful semantic of questions in a hierarchical way. Experiments demonstrate that the proposed approach promotes the performance on the CommonsenseQA dataset comparing with strong baselines.", "venue": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "de12daff3174459463906075ef0212bc9020865f", "url": "https://www.semanticscholar.org/paper/de12daff3174459463906075ef0212bc9020865f", "title": "Proposal of the Hierarchical File Server Groups for Implementing Mandatory Access Control", "abstract": "An accessible implementation of MAC for file access is proposed. Instead of traditional but unfamiliar MAC aware tools such as secure-OSs, the security enforcement mechanism of our proposal is FSG (file server group) which is structured so that they reflect a security policy of the organization. Using ordinal file servers, it is accessible to the most of commercial office environment. We also propose the use of FCA (formal concept analysis), a technology for knowledge extraction, to derive the structure of FSG for information flow enforcement. An advantage of use of FCA is that it directly produces configuration parameters such as access points of users as the knowledge extracted from organizational security policy. The configuration of the file server group is easy to understand, and the management cost of FSG is lower than that of the ordinary flat structured file servers.", "venue": "2012 Sixth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "e3e3913d2aaf07ac9d9e238fdcdcfcaa0e16459d", "url": "https://www.semanticscholar.org/paper/e3e3913d2aaf07ac9d9e238fdcdcfcaa0e16459d", "title": "Visualising Developing Nations Health Records: Opportunities, Challenges and Research Agenda", "abstract": "The benefits of effectively visualizing health records in huge volumes has resulted in health organizations, insurance companies, policy and decision makers, governments and drug manufactures\u2019 transformation in the way research is conducted. This has also played a key role in determining investment of resources. Health records contain highly valuable information; processing these records in large volumes is now possible due to technological advancement which allows for the extraction of highly valuable knowledge that has resulted in breakthroughs in scientific communities. To visualize health records in large volumes, the records need to be stored in electronic forms, properly documented, processed, and analyzed. A good visualization technique is used to present the analyzed information, allowing for effective knowledge extraction which is done in a secured manner protecting the privacy of the patients whose health records were used. As research and technological advancement have improved, the quality of knowledge extracted from health records have also improved; unfortunately, the numerous benefits of visualizing health records have only been felt in developed nations, unlike other sectors where technological advancement in developed nations have had similar impact in developing nations. This paper identifies the characteristics of health records and the challenges involved in processing large volumes of health records. This is to identify possible steps that could be taken for developing nations to benefit from visualizing health records in huge volumes.", "venue": "IAIT", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "81c19feaadc7bf9f12f164a513854cd190186c51", "url": "https://www.semanticscholar.org/paper/81c19feaadc7bf9f12f164a513854cd190186c51", "title": "Prioritizing Non-Rig Well Work Candidates Using Data Science", "abstract": "\n According to Wikipedia, \"Data Science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining.\"\n The oil and gas industry is increasingly expanding its activities by moving into the Data Science and analytics space to increase efficiency, reduce costs, make better decisions and improve quality of technical products and services. Through the extraction of knowledge and insights from historical data, oil and gas companies can systematically process the huge data available to them using scientific methods and algorithms to identify trends for problem identification and optimization opportunities. The data processing can also be used to perform analytics to provide Descriptive, Diagnostic, predictive or Prescriptive solutions for value creation.\n For Chevron offshore and onshore non-rig wellwork, the existing methodology of planning and scheduling Non-Rig Workovers (NRWOs) for execution is a spreadsheet or a Project typically run on Microsoft applications or software. This process does not incorporate numerous factors that affect the value realization through executing the NRWO such as historical Data Analytics, predictions and several extreme constraints. The value in building a prioritized candidate selection schedule is allowing the business to shift to a data-driven model based from a method of simple basic programs with limited options and typically biased by human input. Historical data from various sources is being collected to provide an encompassing view of the NRWO prioritization, planning and scheduling environment.\n The scope of this study involves utilizing Data Science to generate solutions comprising of prioritized scheduled workovers that are optimized by various constraints to rank these workovers such as individual well Non-Rig workover cost per barrel. The approach can be replicated using other operational and well related constraints to generate alternative optimized rigless well prioritization solutions. The resulting wells will be gauged against established business drivers to develop an optimal prioritized solution which is then applied at the start of the business plan year to provide an optimized wellwork schedule for the planning year.\n Data Science applied to this project utilizes the various systems of records within the offshore and onshore fields such as Wellwork candidate listings and categorization database, project maturation database, cost schedules, possibility of success, reserves, production profiles, etc. The systems of records are then integrated through Data Science and prioritized by ranking the various parameters through automation based on constraints specified by customers.\n The long-term project will reduce NPT by 2-3% annually, save well work maturation recycle time, and increase efficiency in executing wellwork through an optimized schedule. Equivalent cost savings of between $650,000 and $1m was estimated for the initial pilot simulation run for the business planning cycle evaluated.\n The methodology applied in this study provides a multidiscipline and integrated approach to bridge the conventional optimization void of Data Science and the big data approach to make quicker non-rig well scheduling decisions.", "venue": "Day 3 Wed, August 07, 2019", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "dfb6cbff29ccabb8114d59438efcb4e92821afbc", "url": "https://www.semanticscholar.org/paper/dfb6cbff29ccabb8114d59438efcb4e92821afbc", "title": "Efficient Mining of Data Streams Using Associative Classification Approach", "abstract": "Data stream associative classification poses many challenges to the data mining community. In this paper, we address four major challenges posed, namely, infinite length, extraction of knowledge with single scan, processing time, and accuracy. Since data streams are infinite in length, it is impractical to store and use all the historical data for training. Mining such streaming data for knowledge acquisition is a unique opportunity and even a tough task. A streaming algorithm must scan data once and extract knowledge. While mining data streams, processing time, and accuracy have become two important aspects. In this paper, we propose PSTMiner which considers the nature of data streams and provides an efficient classifier for predicting the class label of real data streams. It has greater potential when compared with many existing classification techniques. Additionally, we propose a compact novel tree structure called PSTree (Prefix Streaming Tree) for storing data. Extensive experiments conducted on 24 real datasets from UCI repository and synthetic datasets from MOA (Massive Online Analysis) show that PSTMiner is consistent. Empirical results show that performance of PSTMiner is highly competitive in terms of accuracy and performance time when compared with other approaches under windowed streaming model.", "venue": "Int. J. Softw. Eng. Knowl. Eng.", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "06dad421472d28b9a9d715f667453a21571a35eb", "url": "https://www.semanticscholar.org/paper/06dad421472d28b9a9d715f667453a21571a35eb", "title": "Sharing Knowledge between Independent Grid Communities", "abstract": "In recent years, grid-based approaches for processing scientific data became popular in various fields of research. A multitude of communities has emerged that all benefit from the processing and storage power the grid offers to them. So far there has not yet been much collaboration between these independent communities. But applying semantic technologies to create knowledge bases, sharing this knowledge, and providing access to data maintained by a community, allows to exploit a synergy effect that all communities can benefit from. In this paper, we propose a framework that applies information extraction to generate abstract knowledge from source doc- uments to be shared among participating communities. The framework also enables users to search for documents based on keywords or metadata as well as to search for extracted knowledge. This search is not restricted to the community the user is registered at but covers all registered communities and the data they are willing to share with others.", "venue": "GI-Jahrestagung", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ie"], "mention_counts": {"kg": 1, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.5}, {"paperId": "28e85b5c8d3a1d8b54d23bf652ac6f245689bc76", "url": "https://www.semanticscholar.org/paper/28e85b5c8d3a1d8b54d23bf652ac6f245689bc76", "title": "A Representation-Based Methodology for Developing High-Value Knowledge Engineering Software Products: Theory, Application, and Implementation", "abstract": "Most enterprises, technology and otherwise, are routinely collecting massive amounts of unstructured information from customer interactions and then attempting to extract knowledge from this information in order to improve core activities such as product development, customer support, and marketing. The knowledge engineering processes for extracting knowledge from this information are often largely manual and extremely inefficient in both cost and time. Therefore, software automation of these manual activities through the creation of highly user-centric Knowledge Engineering Software Products (KESPs) is critical to enabling the rapid and efficient extraction of high-quality knowledge. The primary intent of this paper is to provide a comprehensive theory, including its application and implementation, for developing high-value Knowledge Engineering Software Products. To this end, we have created a representation-based approach to the design and development of KESPs. The theoretical framework of our representational-based approach is the integrated metarepresentational model (IMRM) which provides a natural sequence of representations for guiding the development of complex artifacts such as KESPs. The application of the IMRM to the development of high-value KESPs resulted in the integrated representation-based process methodology (IRPM) which combines, in a rational and structured manner, methods and tools from the technical domains of Knowledge Engineering, Product Design, and Software Engineering. Each domain contributes a distinct set of methods to the IRPM. The knowledge engineering domain provides tools\u2014such as the CommonKADS Agent/Task model\u2014for modeling current work processes that the KESP will automate. The product design domain provides formal tools\u2014such as the House of Quality, Function Structure, Morphological Matrix, and Utility Function\u2014for explicitly defining the user needs for the KESP, and for exploring different design concepts in order to ensure the KESP is high-quality and low-cost. The software engineering domain provides tools\u2014such as Unified Modeling Language (UML) Use Case, Component, and Class diagrams\u2014in order to ensure that a reliable and easy to use KESP is delivered on time and within budget. We have demonstrated the feasibility of the IRPM by implementing it within the context of a real knowledge engineering problem involving the extraction of problem-solution pairs from customer service requests in order to create \u201csmart\u201d products and services. The developed KESP, called the \u201cService Request Portal\u201d (SRP), used search and content filters to achieve a 30% productivity improvement over the previously manual work process.", "venue": "J. Comput. Inf. Sci. Eng.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "f9c06afe72a4384f0cde496e43c3eb2b1aa2af94", "url": "https://www.semanticscholar.org/paper/f9c06afe72a4384f0cde496e43c3eb2b1aa2af94", "title": "A Rule Induction Approach to Modeling Regional Pronunciation Variation", "abstract": "This paper describes the use of rule induction techniques for the automatic extraction of phonemic knowledge and rules from pairs of pronunciation lexica. This extracted knowledge allows the adaptation of speech processing systems to regional variants of a language. As a case study, we apply the approach to Northern Dutch and Flemish (the variant of Dutch spoken in Flanders, a part of Belgium), based on Celex and Fonilex, pronunciation lexica for Northern Dutch and Flemish, respectively. In our study, we compare two rule induction techniques, Transformation-Based Error-Driven Learning (TBEDL) (Brill, 1995) and C5.0 (Quinlan, 1993), and evaluate the extracted knowledge quantitatively (accuracy) and qualitatively (linguistic relevance of the rules). We conclude that, whereas classification-based rule induction with C5.0 is more accurate, the transformation rules learned with TBEDL can be more easily interpreted.", "venue": "International Conference on Computational Linguistics", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "13a0f60b8e1808b7aa4e236e69ca80d9773b3b08", "url": "https://www.semanticscholar.org/paper/13a0f60b8e1808b7aa4e236e69ca80d9773b3b08", "title": "Bioinformatic Workflow Extraction from Scientific Texts based on Word Sense Disambiguation", "abstract": "This paper introduces a method for automatic workflow extraction from texts using Process-Oriented Case-Based Reasoning (POCBR). While the current workflow management systems implement mostly different complicated graphical tasks based on advanced distributed solutions (e.g., cloud computing and grid computation), workflow knowledge acquisition from texts using case-based reasoning represents more expressive and semantic case representations. We propose in this context, an ontology-based workflow extraction framework to acquire processual knowledge from texts. Our methodology extends the classic NLP techniques to extract and disambiguate complex tasks and relations in texts. Using a graph-based representation of workflows and a domain ontology, our extraction process uses a context-aware approach to recognize workflow components in texts: data and control flows. We applied our framework in a technical domain in bioinformatics: i.e., phylogenetic analyses. An evaluation based on workflow semantic similarities in a gold standard proves that our approach provides promising results in the process extraction domain. Both data and implementation of our framework are available in:\u00a0http://labo.bioinfo.uqam.ca/tgowler.", "venue": "IEEE/ACM Transactions on Computational Biology & Bioinformatics", "citationCount": 10, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "wsd", "onto", "nlp"], "mention_counts": {"nlp": 1, "wsd": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 1, "wsd": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "c96f93e24bd21ec64409ee066b3fb757ae0aaa51", "url": "https://www.semanticscholar.org/paper/c96f93e24bd21ec64409ee066b3fb757ae0aaa51", "title": "Analyzing the Semantic Relatedness of Paper Abstracts: An Application to the Educational Research Field", "abstract": "Each domain, along with its knowledge base, changes over time and every timeframe is centered on specific topics that emerge from different ongoing research projects. As searching for relevant resources is a time-consuming process, the automatic extraction of the most important and relevant articles from a domain becomes essential in supporting researchers in their day-to-day activities. The proposed analysis extends other previous researches focused on extracting co-citations between the papers, with the purpose of comparing their overall importance within the domain from a semantic perspective. Our method focuses on the semantic analysis of paper abstracts by using Natural Language Processing (NLP) techniques such as Latent Semantic Analysis, Latent Dirichlet Allocation or specific ontology distances, i.e., Word Net. Moreover, the defined mechanisms are enforced on two different sub domains from the corpora generated around the keywords \"e-learning\" and \"computer\". Graph visual representations are used to highlight the keywords of each sub domain, links among concepts and between articles, as well as specific document similarity views, or scores reflecting the keyword-abstract overlaps. In the end, conclusions and future improvements are presented, emphasizing nevertheless the key elements of our research support framework.", "venue": "Computer Science in Cars Symposium", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "kg": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "cc889aef7e6feb01831905698c022508fa28cfa4", "url": "https://www.semanticscholar.org/paper/cc889aef7e6feb01831905698c022508fa28cfa4", "title": "A Semantic Web Platform for Online Vaccine Sentiment Surveillance", "abstract": "The Vaccon Sentiment Ontology (VASON) provides knowledge on the factors driving vaccine refusal by analyzing content of online social media. VASON facilitates concept extraction and analysis of the extracted concepts using an Natural Language Processing (NLP) module.", "venue": "Online Journal of Public Health Informatics", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "sw"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "cee7e97af18b48c96a5d7178b8edbdb51f1da28d", "url": "https://www.semanticscholar.org/paper/cee7e97af18b48c96a5d7178b8edbdb51f1da28d", "title": "TCM-SAS: A Semantic Annotation System and Knowledgebase of Traditional Chinese Medicine", "abstract": "Objective: To construct a natural language processing (NLP) system focused on named entity recognition (NER) and semantic relation extraction (RE) of ancient Chinese medical books, it supports annotated corpora management and semantic knowledge retrieval. Methods: We integrate the 47 ontologies and terminologies as the terminology database. After that, we trained a preprocessing NER model using spaCy and used a hybrid approach combining automated annotation and manual review to annotate corpora of ancient Chinese medical books. Results: The semantic annotation system of Chinese ancient texts named traditional Chinese medicine - semantic annotation system (TCM-SAS), was constructed based on ontologies and terminologies. Annotations and knowledge retrieval of TCM's ancient texts were realized. Conclusion: TCM-SAS is a user-friendly semantic annotation system for ancient Chinese medical books that includes a large-scale manual annotation of TCM literature and semantic knowledge of TCM. TCM-SAS could provide users with two modes of automatic and manual NER and RE for ancient Chinese texts, as well as annotated entity and corpora management. Support the discovery of new knowledge from ancient Chinese medical texts in the future.", "venue": "IEEE International Conference on Bioinformatics and Biomedicine", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "08bcb04783a091e414c0f564ba0bd98f6ba660e5", "url": "https://www.semanticscholar.org/paper/08bcb04783a091e414c0f564ba0bd98f6ba660e5", "title": "A Tracing System for User Interactions towards Knowledge Extraction of Power Users in Business Intelligence Systems", "abstract": "Business intelligence has been widely integrated in enterprises to help their employees in their decision making process by delivering the needed information at the right time. Statistics from Gartner Group showed that the investment in the business intelligence domain has recently been very high. However, different studies and market researches showed that the pervasiveness and the usage percentage rate of business intelligence are still very low. The reason behind that is the complexity of the usage of business intelligence systems. Moreover, enterprise users lack analytical skills. To mitigate this problem, a new concept of self-service business intelligence has been developed. Within this system, the knowhow of power user is extracted and delivered to business users in form of recommendations. In this paper, we present the conception and development of the tracing module of this new system. This module has the goal of tracing the interactions of power users as the first step to extract their procedural knowledge in form of analysis paths. This is done by creating a user interaction catalogue in which the interactions are defined based on their relevance to the knowledge extraction process. Finally, this paper presents the internal architecture of this tracing module and its components.", "venue": "International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "8ea34995dfc1ffcc7c58d68c9f20f1a316a16cbd", "url": "https://www.semanticscholar.org/paper/8ea34995dfc1ffcc7c58d68c9f20f1a316a16cbd", "title": "Analyzing the Semantic Relatedness of Paper Abstracts", "abstract": "Each domain, along with its knowledge base, changes over time and every timeframe is centered on specific topics that emerge from different ongoing research projects. As searching for relevant resources is a time-consuming process, the automatic extraction of the most important and relevant articles from a domain becomes essential in supporting researchers in their day-today activities. The proposed analysis extends other previous researches focused on extracting co-citations between the papers, with the purpose of comparing their overall importance within the domain from a semantic perspective. Our method focuses on the semantic analysis of paper abstracts by using Natural Language Processing (NLP) techniques such as Latent Semantic Analysis, Latent Dirichlet Allocation or specific ontology distances, i.e., WordNet. Moreover, the defined mechanisms are enforced on two different subdomains from the corpora generated around the keywords \" e-learning \" and \" computer \". Graph visual representations are used to highlight the keywords of each subdomain, links among concepts and between articles, as well as specific document similarity views, or scores reflecting the keyword-abstract overlaps. In the end, conclusions and future improvements are presented, emphasizing nevertheless the key elements of our research support framework.", "venue": "CSCL 2015", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "kg"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "5f5823303a12f604aedfce00740a854180e50bb0", "url": "https://www.semanticscholar.org/paper/5f5823303a12f604aedfce00740a854180e50bb0", "title": "SOAR \u2014 Sparse Oracle-based Adaptive Rule extraction: Knowledge extraction from large-scale datasets to detect credit card fraud", "abstract": "This paper presents a novel approach to knowledge extraction from large-scale datasets using a neural network when applied to the real-world problem of payment card fraud detection. Fraud is a serious and long term threat to a peaceful and democratic society. We present SOAR (Sparse Oracle-based Adaptive Rule) extraction, a practical approach to process large datasets and extract key generalizing rules that are comprehensible using a trained neural network as an oracle to locate key decision boundaries. Experimental results indicate a high level of rule comprehensibility with an acceptable level of accuracy can be achieved. The SOAR extraction outperformed the best decision tree induction method and produced over 10 times fewer rules aiding comprehensibility. Moreover, the extracted rules discovered fraud facts of key interest to industry fraud analysts.", "venue": "The 2010 International Joint Conference on Neural Networks (IJCNN)", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "03c20dbf6d07479a0d54ca9d07251148d6c54d72", "url": "https://www.semanticscholar.org/paper/03c20dbf6d07479a0d54ca9d07251148d6c54d72", "title": "Research on Knowledge Extraction and Visualization in Knowledge Retrieve", "abstract": "In order to adapt to the development tendency of Knowledge Organization and resolve the problem about the low efficiency of Information Retrieve, Knowledge Retrieve as a new retrieval theory is proposed. Based on the knowledge organization, it realizes the intelligent retrieve of Knowledge Correlation and Concept Semantic Retrieve. It synthetically applies many new theories and technologies, such as the information science, artificial intelligence, cognitive science and linguistics. This paper mainly discusses the realizing mechanism of Knowledge Retrieve based on Knowledge Extraction and Visualization Technology.", "venue": "International Conference on Intelligent Human-Machine Systems and Cybernetics", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "ccce4587cd74990c9b2046e4c6e54a457d2ad477", "url": "https://www.semanticscholar.org/paper/ccce4587cd74990c9b2046e4c6e54a457d2ad477", "title": "The Subworld Concept Lexicon and the Lexicon Management System", "abstract": "Natural language processing systems require three different types of lexicons: the concept lexicon that describes the (sub)world ontology and the analysis and generation lexicons for natural languages. We argue that the acquisition of the concept lexicon must precede any lexical work on natural language and that a comprehensive lexicon management system (LMS) is necessary for lexicon acquisition in large-scale applications. We describe the interactive concept lexicon acquisition module of the LMS for TRANSLATOR, a knowledge-based, sublanguage-oriented machine translation project.", "venue": "Computational Linguistics", "citationCount": 72, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "nlp", "onto", "kg"], "mention_counts": {"nlp": 1, "onto": 1, "kg": 1, "mt": 1}, "nlp_mention_counts": {"nlp": 1, "mt": 1}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "75b2060b0346a07f3a6c4f5a499a8f2620db2070", "url": "https://www.semanticscholar.org/paper/75b2060b0346a07f3a6c4f5a499a8f2620db2070", "title": "Transferring research into the real world: How to improve RE with AI in the automotive industry", "abstract": "For specifications, people use natural language. We show that processing natural language and combining this with intelligent deduction and reasoning with ontologies can possibly replace some manual processes associated with requirements engineering (RE). Our prior research shows that the software tools we developed can indeed solve problems in the RE process. This paper shows this does not only work in the software engineering domain, but also for embedded software in the automotive industry. We use artificial intelligence in the sense of combining semantic knowledge from ontologies and natural language processing. This enables computer systems to \u201cunderstand\u201d requirement texts and process these with \u201ccommon sense\u201d. Our specification improver RESI detects flaws in texts such as ambiguous words, incomplete process words, and erroneous quantifiers and determiners.", "venue": "International Workshop on Artificial Intelligence for Requirements Engineering", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "7179043686dec2245140c2440de64ae3d37523f1", "url": "https://www.semanticscholar.org/paper/7179043686dec2245140c2440de64ae3d37523f1", "title": "Extracting Human Goals from Weblogs", "abstract": "Knowledge about human goals has been found to be an important kind of knowledge for a range of challenging problems, such as goal recognition from peoples\u2019 actions or reasoning about human goals. Necessary steps towards conducting such complex tasks involve (i) acquiring a broad range of human goals and (ii) making them accessible by structuring and storing them in a knowledge base. In this work, we focus on extracting goal knowledge from weblogs, a largely untapped resource that can be expected to contain a broad variety of human goals. We annotate a small sample of weblogs and devise a set of simple lexico-syntactic patterns that indicate the presence of human goals. We then evaluate the quality of our patterns by conducting a human subject study. Resulting precision values favor patterns that are not merely based on part-of-speech tags. In future steps, we intend to improve these preliminary patterns based on our observations. 1 Knowledge about Human Goals Knowledge about human goals has been found to be an important kind of knowledge for a range of challenging research problems, such as goal recognition from people\u2019s actions, reasoning about people\u2019s goals or the generation of action sequences that implement goals (planning) [Schank and Abelson, 1977]. In contrast to other kinds of knowledge, e.g. commonsense, knowledge about human goals provides a different perspective on textual resources putting more emphasis on future aspects and activities. We regard the acquisition of this knowledge as a first step towards conducting complex tasks such as planning. Regardless whether the knowledge to extract is about human goals, commonsense [Liu and Singh, 2004] or the world in general [Schubert and Tong, 2003; Clarke, 2009], the acquisition process often includes the application of indication and extraction patterns. Moreover, knowledge acquisition approaches differ in how much manual intervention is necessary (or desired) in the knowledge acquisition process. Existing approaches include utilizing human knowledge engineering [Lenat, 1995], volunteer-based [Liu and Singh, 2004], game-based [Lieberman et al., 2007; von Ahn, 2006] or semiautomatic approaches [Eslick, 2006]. Yet, in this paper we are interested in approaching the question how knowledge about human goals can be automatically derived from social media text, in our case weblogs. To give an example, here is a snippet of a blog post where human goals are underlined: Last September, we moved into our new home. I had plans for this home, the first house--not apartment-my husband and I would live in. I was going to refinish some hand-me-down furniture we have, and I was going to plant a wonderful garden, starting with bulbs that would bloom in the spring. Crocuses, hyacinths, tulips--all of my favorites. And I would know, all winter long, that they were sleeping in the dark, cold soil, waiting to awake with the first light and warmth of spring Though weblogs exhibit some disadvantages when it comes to quality issues, e.g., textual content is prone to noise, we can expect that weblogs contain a broad variety of human goals. In the remaining part, we describe our approach to address goal extraction from open text by deriving and evaluating a first set of lexico-syntactic patterns. We then discuss strengths and weaknesses of our patterns based on a small human subject study in order to improve them in future steps. 2 Patterns To Extract Human Goals We employ and adapt the definition from [Tatu, 2005] who defines human goals as: \u201cExpressions of a particular action that shall take place in the future, in which the speaker is some sort of agent.\u201d The following, exemplary sentence taken from the blog post snippet presented above: \u201cI was going to refinish some hand-me-down furniture\u201d indicates the person\u2019s intention to prettify some furniture. In contrast to Tatu\u2019s definition, we do not include information about the speaker into our patterns to keep them simple. However, the idea to include this kind of information is discussed in Section 3.3. When comparing our setup to [Tatu, 2005]\u2019s, we can observe three differences. Firstly, the author developed part-of-speech patterns by annotating and examining samples from the Brown corpus. Working on the Brown corpus is advantageous because this corpus has already been tagged \u2013 the chance of getting incorrect part-of-speech tags is thereby reduced. Secondly, the language used in the Brown corpus is different than language used in weblogs. Thirdly, [Tatu, 2005]\u2019s motivation to address challenges in question answering (QA). She expected that sentences containing expressions of human goals are better suited to answer a certain kind of questions. Textual resources in the QA domain exhibit other characteristics than weblogs, for instance, people use weblogs to tell stories or write diarylike entries. We hypothesize that extraction patterns yield different results depending on weblog characteristics, e.g., does the weblog contain a story-like structure or not? We followed a common path to acquire knowledge from textual resources by manually examining the textual environment to identify appropriate patterns [Hearst, 1992]. As a first step, we drew a small, random sample (~ 100 blog posts) from the ICWSM 2009 Spinn3r Dataset [Burton et al., 2009] and annotated the textual contents according to the above definition. The annotation task was conducted by one of the authors and an undergraduate student. Table 1 illustrates ten resulting, lexico-syntactic patterns based on these annotations which are partly inspired by patterns by [Tatu, 2005]. She employs these patterns to identify sentences containing intentional expressions in order to build up a training set for further experiments. Part-of-speech tags throughout this paper are consistent with the Penn Treebank Tag Set. Table 1: Lexico-syntactic patterns to identify and extract human goals and matching instances. (*) denotes no, one or several occurrences, (+) denotes at least one occurrence, (?) denotes one optional occurrence and (|) denotes a logical OR. Nr. Lexico-Syntactic Patterns Matching Instances 1 <VB|VBZ> <TO> <VB> needs/VBZ to/TO organize/VB 2 <NN.*> <TO> <VB> alcohol/NN to/TO get/VB 3 <JJ> <TO> <VB.*> available/JJ to/TO read/VB 4 <VB> <DT> <NN.*> find/VB a/DT keyboard/NN 5 <WANT> <TO> <VB> wanted/VBD to/TO kill/VB 6 <INTEND> <TO> <VB> intend/VBP to/TO quit/VB 7 <INTENT|PURPOSE|GOAL| OBJECTIVE><VBZ><TO><VB| NN.*>* goal/NN is/VBZ to/TO eat/VB 8 <LIKE> <TO> <VB.*> like/VB to/TO share/VB 9 <WANT> < PRP> <TO> <VB> wants/VBZ them/PRP to/TO go/VB 10 <GET> <PRP> <DT>? <NN.*> | <VB.*> get/VB you/PRP to/TO purchase/VB In the next section, we apply our extraction patterns to a larger sample of weblogs. We then evaluate the quality of every pattern by calculating precision values. 3 Quality & Characteristics In this section, we briefly describe our data preparation steps and pattern matching process. We report precision results of preliminary study on a set of ~205.000 blog posts and discuss observed weaknesses of our patterns. We conclude this section with suggesting several possibilities to improve and extend the patterns to extract knowledge about human goals. 3.1 Data Sets For our experiments, we used the ICWSM 2009 Spinn3r Dataset which comprises 44 million blog posts made between August 1 and October 1, 2008. We randomly drew ~205.000 blog posts and further separated them into two datasets \u2013 one with posts containing stories \u2013 one with posts containing non-stories. We hypothesize that blog posts telling a story contain more human goals than other blog posts. We use work from [Gordon and Reid, 2009] that defines a story as a series of causally related events in the past. They developed an automatic algorithm to identify blog posts most likely containing a story (reported precision values up to 75%). Moreover, they provide an index of all blog posts in the ICWSM 2009 Spinn3r Dataset that were classified as containing storylike structures. Using this information, we obtained two datasets \u2013 one containing posts with stories (~3000) and one containing posts without stories (~202.000). 3.2 Data Preparation We first extracted the content of the <description> field in the corresponding xml files of the random sample. Since the textual content of the weblogs was often messy, we had to clean it as preparation for the subsequent part-ofspeech tagging. The cleaning procedure included removing html snippets and special characters. For the process of part-of-speech tagging and pattern matching, we used functionality of the Natural Language Processing Toolkit (NLTK) in combination with Python as programming language. 3.3 Strengths and Weaknesses of our Goal Extraction Patterns We applied our patterns from Table 1 to two datasets (see Section 3.1) which were randomly drawn from the ICWSM 2009 Spinn3r Dataset (tiergroups 1-3). Table 2 shows the number of matches per extraction pattern. The frequency numbers corroborate our hypothesis that there is a higher potential for the presence of human goals weblogs containing a story. Since there are ~67 times more blog posts containing non-stories than stories, the numbers are not directly comparable. In order to compare them, we calculate the ratio of (number of found goal instances) vs. (number of blog posts). We notice that the ratio is always highly in favor of the blog posts containing stories. Consider for example ratios for the first pattern <VB|VBZ> <TO> <VB>: 486/3,000 = 0.16 for stories vs. 6,220/202,000 = 0.03 for non-stories. Table 2 illustrates the number of matched goal instances per extraction pattern as well as precision values (sample size of 20) for both story and non-story content. Lexico-Syntactic Patterns Story Set (#3.000) Non-Story Set (#202.000) Freq. Prec. Freq. Prec. <VB|VBZ> <TO> <VB> 486 0.1 6220 0 <NN.*> <TO> <VB> 2018 0 6661 0 <JJ> <TO> <VB.*> 677 0.05 5424 0.06 <VB> <DT> <NN.*> 1405 0.06 15129 0 <WANT> <TO> <VB> 398 0,53 3", "venue": "LWA", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "ke"], "mention_counts": {"nlp": 1, "kg": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.5}, {"paperId": "8496a6c93bba7dfcb46ae389ac5c1b9f43ad8490", "url": "https://www.semanticscholar.org/paper/8496a6c93bba7dfcb46ae389ac5c1b9f43ad8490", "title": "The TextCap Semantic Interpreter", "abstract": "The lack of large amounts of readily available, explicitly represented knowledge has long been recognized as a barrier to applications requiring semantic knowledge such as machine translation and question answering. This problem is analogous to that facing machine translation decades ago, where one proposed solution was to use human translators to post-edit automatically produced, low quality translations rather than expect a computer to independently create high-quality translations. This paper describes an attempt at implementing a semantic parser that takes unrestricted English text, uses publically available computational linguistics tools and lexical resources and as output produces semantic triples which can be used in a variety of tasks such as generating knowledge bases, providing raw material for question answering systems, or creating RDF structures. We describe the TEXTCAP system, detail the semantic triple representation it produces, illustrate step by step how TEXTCAP processes a short text, and use its results on unseen texts to discuss the amount of post-editing that might be realistically required.", "venue": "STEP", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "mt", "rdf", "kg"], "mention_counts": {"kg": 1, "rdf": 1, "mt": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"kg": 1, "rdf": 1}, "relevance_score": 0.5}, {"paperId": "aba75ed0c59748d2874c5beff5a8d7b634f76f52", "url": "https://www.semanticscholar.org/paper/aba75ed0c59748d2874c5beff5a8d7b634f76f52", "title": "Acquisition of Triples of Knowledge from Lecture Notes: A Natural Langauge Processing Approach", "abstract": "Automated acquisition of knowledge from text has been utilised across several research areas including domain modeling of knowledge-based systems and semantic web. Primarily, knowledge is decomposed as fragments in the form of entities and relations called triples (or triplets). Although empirical studies have already been developed to extract entities (or concepts), relation extraction is still considered as a challenging task and hence, performed semi-automatically or manually in educational applications such as Intelligent Tutoring Systems. This paper presents Natural Language Processing (NLP) techniques to identify subject-verb-object (SVO) in lecture notes, supporting the creation of concept-relation-concept triple for visualisation in concept map activities. Domain experts have already been invested in producing legible slides. However, automated knowledge acquisition is challenging due to potential issues such as the use of sentence fragments, ambiguity and confusing use of idioms. Our work integrates the naturally-structured layout of presentation environments to solve semantically, syntactically missing or ambiguous elements. We evaluate our approach using a corpus of Computer Science lecture notes and discuss further uses of our technique in the educational context.", "venue": "EDM", "citationCount": 16, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "sw", "nlp"], "mention_counts": {"nlp": 2, "sw": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "sw": 1}, "relevance_score": 0.5}, {"paperId": "2f8f048c4bae8a47137303c33651b38bc1bda9fc", "url": "https://www.semanticscholar.org/paper/2f8f048c4bae8a47137303c33651b38bc1bda9fc", "title": "The Semantic Web: Apotheosis of Annotation, but What Are Its Semantics?", "abstract": "This article discusses what kind of entity the proposed semantic Web is, principally by reference to the relationship of natural language structure to knowledge representation. The concept of the SW has two distinct origins, and this, bifurcation persists in two differing lines of SW research: one closely allied to notions of documents and natural language processing (NLP) and one not. These differences of emphasis or content carry with them different commitments about what it is to interpret a KR and what the interpretation method has to do with meaning in natural language.", "venue": "IEEE Intelligent Systems", "citationCount": 55, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "nlp", "nlp"], "mention_counts": {"nlp": 2, "sw": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 2}, "relevance_score": 0.5}, {"paperId": "348277b7241bd1cf957b8b01e7bfd665108c3db6", "url": "https://www.semanticscholar.org/paper/348277b7241bd1cf957b8b01e7bfd665108c3db6", "title": "Automatically Harvesting and Ontologizing Semantic Relations", "abstract": "With the advent of the Web and the explosion of available textual data, it is key for modern natural language processing systems to access, represent and reason over large amounts of knowledge in semantic repositories. Separately, the knowledge representation and natural language processing communities have been developing representations/engines for reasoning over knowledge and algorithms for automatically harvesting knowledge from textual data, respectively. There is a pressing need for collaboration between the two communities to provide large-scale robust reasoning capabilities for knowledge rich applications like question answering. In this chapter, we propose one small step by presenting algorithms for harvesting semantic relations from text and then automatically linking the knowledge into existing semantic repositories. Experimental results show better than state of the art performance on both relation harvesting and ontologizing tasks.", "venue": "Ontology Learning and Population", "citationCount": 72, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "14a7198849ef4fae81c8255c493945eb1fc81f2a", "url": "https://www.semanticscholar.org/paper/14a7198849ef4fae81c8255c493945eb1fc81f2a", "title": "Cross-lingual RDF Thesauri Interlinking", "abstract": "Various lexical resources are being published in RDF. To enhance the usability of these resources, identical resources in different data sets should be linked. If lexical resources are described in different natural languages, then techniques to deal with multilinguality are required for interlinking. In this paper, we evaluate machine translation for interlinking concepts, i.e., generic entities named with a common noun or term. In our previous work, the evaluated method has been applied on named entities. We conduct two experiments involving different thesauri in different languages. The first experiment involves concepts from the TheSoz multilingual thesaurus in three languages: English, French and German. The second experiment involves concepts from the EuroVoc and AGROVOC thesauri in English and Chinese respectively. Our results demonstrate that machine translation can be beneficial for cross-lingual thesauri interlinking independently of a dataset structure.", "venue": "LREC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "mt", "rdf", "rdf"], "mention_counts": {"rdf": 2, "mt": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"rdf": 2}, "relevance_score": 0.5}, {"paperId": "0cf581cdcd7e6042e38081219e7da19a761d1832", "url": "https://www.semanticscholar.org/paper/0cf581cdcd7e6042e38081219e7da19a761d1832", "title": "Linked Data & the Semantic Web Standards", "abstract": "On the traditional World Wide Web we all know and love, machines are used as brokers of content: they store, organize, request, route, transmit, receive, and display content encapsulated as documents. In order for machines to process the content of documents automatically\u2014for whatever purpose\u2014 they primarily require two things: machine-readable structure and semantics. Unfortunately, despite various advancements in the area of Natural Language Processing (NLP) down through the decades, modern computers still struggle to meaningfully process the idiosyncratic structure and semantics of natural language due to ambiguities present in grammar, coreference and word-sense. Hence, machines require a more \u201cformal\u201d notion of structure and semantics using unambiguous grammar, referencing, and vocabulary.", "venue": "Linked Data Management", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "sw", "nlp", "nlp"], "mention_counts": {"ld": 1, "sw": 1, "nlp": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"ld": 1, "sw": 1}, "relevance_score": 0.5}, {"paperId": "a2dc0198690ee14796b8a3711483c0040f1a3881", "url": "https://www.semanticscholar.org/paper/a2dc0198690ee14796b8a3711483c0040f1a3881", "title": "Interlinking English and Chinese RDF Data Sets Using Machine Translation", "abstract": "Data interlinking is a difficult task particularly in a multilingual environment like the Web. In this paper, we evaluate the suitability of a Machine Translation approach to interlink RDF resources described in English and Chinese languages. We represent resources as text documents, and a similarity between documents is taken for similarity between resources. Documents are represented as vectors using two weighting schemes, then cosine similarity is computed. The experiment demonstrates that TF*IDF with a minimum amount of preprocessing steps can bring high results.", "venue": "KNOW@LOD", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "mt", "rdf", "mt"], "mention_counts": {"mt": 2, "rdf": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"rdf": 2}, "relevance_score": 0.5}, {"paperId": "094ed6c6c78be4349f6cdc82fb80f44e57ec44d1", "url": "https://www.semanticscholar.org/paper/094ed6c6c78be4349f6cdc82fb80f44e57ec44d1", "title": "Reducing the impact of out of vocabulary words in the translation of natural language questions into SPARQL queries", "abstract": "Accessing the large volumes of information available in public knowledge bases might be complicated for those users unfamiliar with the SPARQL query language. Automatic translation of questions posed in natural language in SPARQL has the potential of overcoming this problem. Existing systems based on neural-machine translation are very effective but easily fail in recognizing words that are Out Of the Vocabulary (OOV) of the training set. This is a serious issue while querying large ontologies. In this paper, we combine Named Entity Linking, Named Entity Recognition, and Neural Machine Translation to perform automatic translation of natural language questions into SPARQL queries. We demonstrate empirically that our approach is more effective and resilient to OOV words than existing approaches by running the experiments on Monument, QALD-9, and LC-QuAD v1, which are well-known datasets for Question Answering over DBpedia.", "venue": "ArXiv", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "mt", "onto", "mt"], "mention_counts": {"kg": 1, "onto": 1, "mt": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "49eeaf79f03bc9b346a4f4f2d5d1897d21e2f1b8", "url": "https://www.semanticscholar.org/paper/49eeaf79f03bc9b346a4f4f2d5d1897d21e2f1b8", "title": "The IMAGACT4ALL Ontology of Animated Images: Implications for Theoretical and Machine Translation of Action Verbs from English-Indian Languages", "abstract": "Action verbs are one of the frequently occurring linguistic elements in any given natural language as the speakers use them during every linguistic intercourse. However, each language expresses action verbs in its own inherently unique manner by categorization. One verb can refer to several interpretations of actions and one action can be expressed by more than one verb. The inter-language and intra-language variations create ambiguity for the translation of languages from the source language to target language with respect to action verbs. IMAGACT is a corpus-based ontological platform of action verbs translated from prototypic animated images explained in English and Italian as meta-languages. In this paper, we are presenting the issues and challenges in translating action verbs of Indian languages as target and English as source language by observing the animated images. Among the ten Indian languages which have been annotated so far on the platform are Sanskrit, Hindi, Urdu, Odia (Oriya), Bengali, Manipuri, Tamil, Assamese, Magahi and Marathi. Out of them, Manipuri belongs to the Sino-Tibetan, Tamil comes off the Dravidian and the rest owe their genesis to the Indo-Aryan language family. One of the issues is that the one-word morphological English verbs are translated into most of the Indian languages as verbs having more than one-word form; for instance as in the case of conjunct, compound, serial verbs and so on. We are further presenting a cross-lingual comparison of action verbs among Indian languages. In addition, we are also dealing with the issues in disambiguating animated images by the L1 native speakers using competence-based judgements and the theoretical and machine translation implications they bear.", "venue": "WSSANLP@COLING", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "mt", "mt"], "mention_counts": {"onto": 2, "mt": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "091bd068cd61ff9edb7fe971fc1eb5ed5fd11654", "url": "https://www.semanticscholar.org/paper/091bd068cd61ff9edb7fe971fc1eb5ed5fd11654", "title": "Predicting chemical reaction outcomes: A grammar ontology\u2010based transformer framework", "abstract": "Discovering and designing novel materials is a challenging problem as it often requires searching a combinatorially large space of potential candidates. Evaluation of all candidates experimentally is typically infeasible as it requires great amounts of effort, time, expertise, and money. The ability to predict reaction outcomes without performing extensive experiments is, therefore, important. Towards that goal, we report an approach that uses context-free grammar (CFG) based representations of molecules in a neural machine translation framework. We formulate the reaction-prediction task as a machine translation problem that involves discovering the transformations from the source sequence (comprising the reactants and agents) to the target sequence (comprising the major product) in the reaction. The grammar ontology-based representation of molecules hierarchically incorporates rich molecular structure information that, in principle, should be valuable for modeling chemical reactions. We achieve an accuracy of 80.1% on a standard reaction dataset using a model characterized by only a fraction of the number of training parameters in other sequence-to-sequence models based works in this area. Moreover, 99% of the predictions made on the same reaction dataset were found to be syntactically valid. We conclude that CFGs-based ontological representations could be an efficient way of incorporating structural information, ensuring chemically valid predictions, and overcoming overfitting in complex machine learning architectures employed in reaction prediction tasks.", "venue": "AIChE Journal", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "mt", "mt"], "mention_counts": {"onto": 2, "mt": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "cddb9cd9d3afcf9aa331e89e39362b43c42f288e", "url": "https://www.semanticscholar.org/paper/cddb9cd9d3afcf9aa331e89e39362b43c42f288e", "title": "Using Recurrent Neural Network for Learning Expressive Ontologies", "abstract": "Recently, Neural Networks have been proven extremely effective in many natural language processing tasks such as sentiment analysis, question answering, or machine translation. Aiming to exploit such advantages in the Ontology Learning process, in this technical report we present a detailed description of a Recurrent Neural Network based system to be used to pursue such goal.", "venue": "ArXiv", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "mt"], "mention_counts": {"nlp": 1, "onto": 2, "mt": 1}, "nlp_mention_counts": {"nlp": 1, "mt": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "bde45cb86edd8c4a873b784d70cb3a4765edfa16", "url": "https://www.semanticscholar.org/paper/bde45cb86edd8c4a873b784d70cb3a4765edfa16", "title": "Combining statistical and semantic approaches to the translation of ontologies and taxonomies", "abstract": "Ontologies and taxonomies are widely used to organize concepts providing the basis for activities such as indexing, and as background knowledge for NLP tasks. As such, translation of these resources would prove useful to adapt these systems to new languages. However, we show that the nature of these resources is significantly different from the \"free-text\" paradigm used to train most statistical machine translation systems. In particular, we see significant differences in the linguistic nature of these resources and such resources have rich additional semantics. We demonstrate that as a result of these linguistic differences, standard SMT methods, in particular evaluation metrics, can produce poor performance. We then look to the task of leveraging these semantics for translation, which we approach in three ways: by adapting the translation system to the domain of the resource; by examining if semantics can help to predict the syntactic structure used in translation; and by evaluating if we can use existing translated taxonomies to disambiguate translations. We present some early results from these experiments, which shed light on the degree of success we may have with each approach.", "venue": "SSST@ACL", "citationCount": 36, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "onto", "onto", "nlp"], "mention_counts": {"nlp": 1, "onto": 2, "mt": 1}, "nlp_mention_counts": {"nlp": 1, "mt": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "54f9084086de1789269c810a4044caeeebf180eb", "url": "https://www.semanticscholar.org/paper/54f9084086de1789269c810a4044caeeebf180eb", "title": "Natural Language Question Generation from Connected Open Data: A Study of Possibilities", "abstract": "Accelerated growth of Linked Open Data has been observed. There are several motives for this. Some situations are the generation of these bases from texts, another is the amount of open data generated from information systems. Their growth results in a large volume of information available. This fact brings the possibility of large-scale use in question-and-answer systems. Which have a general function that is based on a structure of information to generate phrases and to conference of produced answers. From literature studies, we foresee an opportunity of bigger use, in several applications. It is possible to generate natural language phrases from linked open databases, in Portuguese. In addition, we identified challenges to effective applicate linked open data resources for this purpose effectively. In this way, this work aims to see which aspects of the structure of open linked databases could be used as support in generating natural language questions.", "venue": "2018 XLIV Latin American Computer Conference (CLEI)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["lod", "nlg", "lod", "nlg"], "mention_counts": {"nlg": 2, "lod": 2}, "nlp_mention_counts": {"nlg": 2}, "ld_mention_counts": {"lod": 2}, "relevance_score": 0.5}, {"paperId": "490aa6de242e9a6a928041fe59f7020d51ad13f5", "url": "https://www.semanticscholar.org/paper/490aa6de242e9a6a928041fe59f7020d51ad13f5", "title": "On the Robustness of Standalone Referring Expression Generation Algorithms Using RDF Data", "abstract": "A sub-task of Natural Language Generation (NLG) is the generation of referring expressions (REG). REG algorithms are expected to select attributes that unambiguously identify an entity with respect to a set of distractors. In previous work we have defined a methodology to evaluate REG algorithms using real life examples. In the present work, we evaluate REG algorithms using a dataset that contains alterations in the properties of referring entities. We found that naturally occurring ontological re-engineering can have a devastating impact in the performance of REG algorithms, with some more robust in the presence of these changes than others. The ultimate goal of this work is observing the behavior and estimating the performance of a series of REG algorithms as the entities in the data set evolve over time.", "venue": "WebNLG", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "nlg", "onto", "rdf"], "mention_counts": {"nlg": 2, "onto": 1, "rdf": 1}, "nlp_mention_counts": {"nlg": 2}, "ld_mention_counts": {"onto": 1, "rdf": 1}, "relevance_score": 0.5}, {"paperId": "de30f0b535a261849157afe0603d6c87c3d0a3c2", "url": "https://www.semanticscholar.org/paper/de30f0b535a261849157afe0603d6c87c3d0a3c2", "title": "Evaluating PageRank methods for structural sense ranking in labeled tree data", "abstract": "Link analysis methods like the popular PageRank are increasingly being applied to lexical knowledge bases to deal with a number of natural language processing problems, including unsupervised word sense ranking and disambiguation. Compared to plain-text, the topic of sense ranking in semistructured data has been however studied marginally.\n This paper aims to bridge PageRank-based word sense ranking and tree-structured data. We propose PageRank-style methods for the structural sense ranking problem, which take into account tree structural relations as well as semantic relatedness in the constituents of tree data. The proposed methods are comparatively evaluated with existing Page-Rank methods for word sense disambiguation. Effectiveness and efficiency of PageRank methods have been assessed on various data with different domain vocabularies.", "venue": "Web Intelligence, Mining and Semantics", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "kg", "nlp", "ld"], "mention_counts": {"nlp": 1, "kg": 1, "wsd": 1, "ld": 1}, "nlp_mention_counts": {"nlp": 1, "wsd": 1}, "ld_mention_counts": {"ld": 1, "kg": 1}, "relevance_score": 0.5}, {"paperId": "db6ebf472ff6479212f1960896ced3a08b01cc73", "url": "https://www.semanticscholar.org/paper/db6ebf472ff6479212f1960896ced3a08b01cc73", "title": "An ontology-based text processing approach for simplifying ambiguity of requirement specifications", "abstract": "In the last few years, several works in the literature of software engineering have addressed the problem of requirement management. A majority problem of software errors is introduced during the requirements phase because much of requirements specification is written in natural language format. As this, it is hard to identify consistencies because of too ambiguous for specification purpose. Therefore, this paper aims to propose a method for simplifying ambiguity of requirement specification documents through two concepts of ontology-based probabilistic text processing: Text classification and Text Filtering. Text classification is used to analyze and classify requirement specification having similar detail into the same class. This contributes to a better understanding of the impact of the requirements and to elaborate them. Meanwhile, text filters are used to leverage synopsis requirements in documents through probabilistic text classification technique. After testing by F-measure, the experimental results return a satisfactory accuracy. These demonstrate that our method may provide more effectiveness for simplifying ambiguity of requirement specifications.", "venue": "IEEE Asia-Pacific Services Computing Conference", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "tp", "tp"], "mention_counts": {"onto": 2, "tp": 2}, "nlp_mention_counts": {"tp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "e0967a122aac6c02ab82f626afa08e1963d0950e", "url": "https://www.semanticscholar.org/paper/e0967a122aac6c02ab82f626afa08e1963d0950e", "title": "Natural Language Directed Inference in the Presentation of Ontologies", "abstract": "It is hard to come up with a general formalisation of the problem of content determination in natural language generation because of the degree of domaindependence that is involved. This paper presents a novel way of looking at a class of content determination problems in terms of a non-standard kind of inference, which we call natural language directed inference. This is illustrated through examples from a system under development to present parts of ontologies in natural language. Natural language directed inference represents an interesting challenge to research in automated reasoning and natural language processing.", "venue": "ENLG", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlg"], "mention_counts": {"nlp": 1, "nlg": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 1, "nlg": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "e471d6429f21f0ac53e2d75728f73763fd768531", "url": "https://www.semanticscholar.org/paper/e471d6429f21f0ac53e2d75728f73763fd768531", "title": "The horizontal and vertical nature of patient phenotype retrieval: new directions for clinical text processing", "abstract": "The author reviews the historical problem of identifying appropriate patients for retrieval from a clinical repository of patient records, compares the competing features of document classification and natural language processing, and proposes an alternative approach. The alternative approach 1) codes inquiries in an ontology to lend a vertical axis to retrieval knowledge instead of coding the target body of notes, 2) invokes natural language indexing and lexical normalizations on the corpus of notes that is scalable and tractable, and 3) leverages thesauri of word-level synonyms and near-synonyms to expand term searches \"horizontally\" around the concept spaces drawn from the ontology in which the queries were \"coded.\"", "venue": "AMIA", "citationCount": 15, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "onto", "onto", "tp"], "mention_counts": {"nlp": 1, "onto": 2, "tp": 1}, "nlp_mention_counts": {"nlp": 1, "tp": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "0d38ca7f3b631e731d6275497cb0230b27610508", "url": "https://www.semanticscholar.org/paper/0d38ca7f3b631e731d6275497cb0230b27610508", "title": "SDN2GO: An Integrated Deep Learning Model for Protein Function Prediction", "abstract": "The assignment of function to proteins at a large scale is essential for understanding the molecular mechanism of life. However, only a very small percentage of the more than 179 million proteins in UniProtKB have Gene Ontology (GO) annotations supported by experimental evidence. In this paper, we proposed an integrated deep-learning-based classification model, named SDN2GO, to predict protein functions. SDN2GO applies convolutional neural networks to learn and extract features from sequences, protein domains, and known PPI networks, and then utilizes a weight classifier to integrate these features and achieve accurate predictions of GO terms. We constructed the training set and the independent test set according to the time-delayed principle of the Critical Assessment of Function Annotation (CAFA) and compared it with two highly competitive methods and the classic BLAST method on the independent test set. The results show that our method outperforms others on each sub-ontology of GO. We also investigated the performance of using protein domain information. We learned from the Natural Language Processing (NLP) to process domain information and pre-trained a deep learning sub-model to extract the comprehensive features of domains. The experimental results demonstrate that the domain features we obtained are much improved the performance of our model. Our deep learning models together with the data pre-processing scripts are publicly available as an open source software at https://github.com/Charrick/SDN2GO.", "venue": "Frontiers in Bioengineering and Biotechnology", "citationCount": 24, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "74116fc6078413bb6791b5d5073d3846b6704fd4", "url": "https://www.semanticscholar.org/paper/74116fc6078413bb6791b5d5073d3846b6704fd4", "title": "Elasticity on Ontology Matching of Folksodriven Structure Network", "abstract": "Nowadays folksonomy tags are used not just for personal organization, but for communication and sharing between people sharing their own local interests. In this paper is considered the new concept structure called \"Folksodriven\" to represent folksonomies. The Folksodriven Structure Network (FSN) was thought as folksonomy tags suggestions for the user on a dataset built on chosen websites - based on Natural Language Processing (NLP). Morphological changes, such as changes in folksonomy tags chose have direct impact on network connectivity (structural plasticity) of the folksonomy tags considered. The goal of this paper is on defining a base for a FSN plasticity theory to analyze. To perform such goal it is necessary a systematic mathematical analysis on deformation and fracture for the ontology matching on the FSN. The advantages of that approach could be used on a new interesting method to be employed by a knowledge management system.", "venue": "ArXiv", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "7a235e816704ed72aaac594bb4ffd91623cae69d", "url": "https://www.semanticscholar.org/paper/7a235e816704ed72aaac594bb4ffd91623cae69d", "title": "Vaccine attitude surveillance using semantic analysis: constructing a semantically annotated corpus", "abstract": "This paper reports work in progress to semantically annotate blog posts about vaccines to use in the Vaccine Attitude Surveillance using Semantic Analysis (VASSA) framework. The VASSA framework combines semantic web and natural language processing (NLP) tools and techniques to provide a coherent semantic layer across online social media for assessment and analysis of vaccination attitudes and beliefs. We describe how the blog posts were sampled and selected, our schema to semantically annotate concepts defined in our ontology, details of the annotation process, and inter-annotator agreement on a sample of blog posts.", "venue": "The Web Conference", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "nlp", "onto"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "a38ded8b5b29f3e3d72e55b4cc97b681f0768fd0", "url": "https://www.semanticscholar.org/paper/a38ded8b5b29f3e3d72e55b4cc97b681f0768fd0", "title": "Medical documents processing for summary generation and keywords highlighting based on natural language processing and ontology graph descriptor approach", "abstract": "In this paper a new method of data retrieval from free text documents in medical domain is proposed. Presented approach gives the document summary and highlights important keywords in the text to support further analysis of multiple medical documents. The document is processed with natural language processing techniques to find medical keywords and assign them to concepts in the medical ontology. These concepts contribute to higher levels in the hierarchy and build the document descriptor as a graph with concepts in the nodes and corresponding relevance points. The descriptor is used to generate the summary in a form of tree. Finally, we highlight the most important keywords in the original text. Presented experiments demonstrate the proposed approach, which successfully summarizes and highlights meaningful medical information.", "venue": "iiWAS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "f40c5a28d716ac2c50797291ffbbfb601f43fa96", "url": "https://www.semanticscholar.org/paper/f40c5a28d716ac2c50797291ffbbfb601f43fa96", "title": "Are Ontologies Involved in Natural Language Processing?", "abstract": "For certain disable persons unable to communicate, we present a palliative aid which consists of a virtual pictographic keyboard associated to a text processing from a pictographic scripture. Words and the grammar are given as pictograms. The pictographic lexicon must be organized following the mental lexicon of the user to propose the pictograms of grammar in order to facilitate his (her) task of writing. We discuss the utility of ontologies in the organization of lexicons and in the building of texts.", "venue": "FLAIRS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "tp", "nlp"], "mention_counts": {"nlp": 1, "onto": 2, "tp": 1}, "nlp_mention_counts": {"nlp": 1, "tp": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "d1c2893821c3bf87d329600f8776d8de6c56f730", "url": "https://www.semanticscholar.org/paper/d1c2893821c3bf87d329600f8776d8de6c56f730", "title": "Building a Timeline Network for Evacuation in Earthquake Disaster", "abstract": "In this paper, we propose an approach that automatically extract users\u2019 activities in sentences retrieved from Twitter. We then design a timeline action network based on Web Ontology Language (OWL). By using the proposed activity extraction approach, we can automatically collect data for the action network. Finally, we propose a novel action-based collaborative filtering, which predicts missing activity data, in order to complement this timeline network. Moreover, with a combination of collaborative filtering and natural language processing (NLP), our method can deal with minority actions such as successful actions. Based on evaluation of tweets which related to the massive Tohoku earthquake, we indicated that our timeline action network can provide useful action patterns in real-time. Not only earthquake disaster, our research can also be applied to other disasters and business models, such as typhoon, travel, marketing, etc.", "venue": "Semantic Cities @ AAAI", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "6a02831b30f22e4340718c825fca7a86c6c164d7", "url": "https://www.semanticscholar.org/paper/6a02831b30f22e4340718c825fca7a86c6c164d7", "title": "Towards Transforming Natural Language Queries into SPARQL Queries", "abstract": "Ontology Based Data Access (OBDA) improves knowledge sharing and reuse and grants a set of options to enhance the power of knowledge management. In OBDA systems, user queries can be expressed in SPARQL. This prevents ordinary users from formulating their own queries. For such lay users, it should be possible to express queries in their own languages and terms instead of being forced to learn SPARQL. To enable this, a transformation from a natural language to SPARQL is needed. To cope with such translation, several challenges, such as natural language processing, ontology handling, string matching, and SPARQL query building should be addressed. In this PhD study, we will use natural language processing techniques and investigate the role of user involvement in the query translation process. Also, string-matching algorithms will be used. We will start with the processing of the natural query. Then the mapping between the query entities and corresponding entities in the datasets. The user interaction validating and confirming this mapping should take place. Finally, building the SPARQL query. The proposed approach will be evaluated based on a bench mark by means of query efficiency and accuracy.", "venue": "Doctoral Consortium/Forum@DB&IS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "aba036cf6475b235d77f129e7a7a8d7e182fb3ad", "url": "https://www.semanticscholar.org/paper/aba036cf6475b235d77f129e7a7a8d7e182fb3ad", "title": "NLP and Ontology Matching - A Successful Combination for Trialogical Learning", "abstract": "Trialogical Learning refers to those forms of learning where learners are collaboratively developing, transforming, or creating shared objects of activity in a systematic fashion. In order to be really productive, systems supporting Trialogical Learning must rely on intelligent services to let knowledge co-evolve with social practices, in an automatic or semi-automatic way, according to the users\u2019 emerging needs and practical innovations. These requirements raise problems related to knowledge evolution, content retrieval and classification, dynamic suggestion of relationships among knowledge objects. In this paper, we propose to exploit Natural Language Processing and Ontology Matching techniques for facing the problems above. The Knowledge Practice Environment of the KP-Lab project has been used as a test bed for demonstrating the feasibility of", "venue": "International Conference on Agents and Artificial Intelligence", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "f148cf44414ec8d780d400dca311629ffb90ea94", "url": "https://www.semanticscholar.org/paper/f148cf44414ec8d780d400dca311629ffb90ea94", "title": "Ontology-Based Natural Language Processing of Social Media Data in the Assessment of Health Information Sought During Pregnancy", "abstract": "This study analyzed collected social media data from South Korea containing keywords related to \"pregnancy\" using ontology-based natural language processing. Of the 504,725 documents, those containing concepts related to \"maternal emotion\" were the most frequent, followed by \"family support\". Social media were used as a means of exchanging information and expressing emotions.", "venue": "Nursing Informatics", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "1c28b10e799e8ee6636d8f7833b503b24a8a5699", "url": "https://www.semanticscholar.org/paper/1c28b10e799e8ee6636d8f7833b503b24a8a5699", "title": "Conversation-Based Information Delivery Method for Facility Management", "abstract": "Facility management platforms are widely used in the facility maintenance phase of the building life cycle. However, a large amount of complex building information affects facility managers\u2019 efficiency and user experience in retrieving specific information on the facility management platform. Therefore, this research aims to develop a conversation-based method to improve the efficiency and user experience of facility management information delivery. The proposed method contains four major modules: decision mechanism, equipment dataset, intent analysis, and knowledge base. A chatbot prototype was developed based on the proposed method. The prototype was then validated through a feasibility test and field test at the Shulin Arts Comprehensive Administration Building in Taiwan. The results showed that the proposed method changes the traditional information delivery between users and the facility management platform. By integrating natural language processing (NLP), building information modelling (BIM), and ontological techniques, the proposed method can increase the efficiency of FM information retrieval.", "venue": "Italian National Conference on Sensors", "citationCount": 3, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["kg", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "kg": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "56d7138fb6178332fb789382aa0386382934f932", "url": "https://www.semanticscholar.org/paper/56d7138fb6178332fb789382aa0386382934f932", "title": "A Word Embedding Analysis towards Ontology Enrichment", "abstract": "Word Embedding is a set of language modeling and feature learning techniques in Natural Language Processing where words or phrases are mapped to vectors of real numbers. This approach could be used in many tasks of Natural Language Processing, such as Text Classification, Part-Of-Speech Tagging, Named Entity Recognition, Sentiment Analysis, and others. In this paper we created different Word Embedding models, using TripAdvisor\u2019s hotel reviews. The corpus was pre-processed, in order to reduce noise, and then submitted to four Word Embedding algorithms: Word2Vec, FastText, Wang2Vec, and GloVe. Finally, HOntology concepts and relations are compared with the outputs of models created aiming to improve it, enriching this domain ontology.", "venue": "Research on computing science", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "f41f34a20818766b392afc3602cf69eb00db5c3b", "url": "https://www.semanticscholar.org/paper/f41f34a20818766b392afc3602cf69eb00db5c3b", "title": "ALIN results for OAEI 2021", "abstract": "4 Alin is a system for interactive ontology matching. In last year\u2019s version, Alin applied natural language processing techniques (NLP) to standardize the concept names of the ontologies that participate in the matching process. In the current version, we modified the grammars used during the process in order to improve the quality of the alignments. This article describes the participation of Alin at OAEI 2021 and discusses its results.", "venue": "OM@ISWC", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "1797a1eba1aece6e6b7113876473e58ec860e311", "url": "https://www.semanticscholar.org/paper/1797a1eba1aece6e6b7113876473e58ec860e311", "title": "Assisted curricula design based on generation of domain ontologies and the use of NLP techniques", "abstract": "The following work proposes an approach that allows educators to manage curricula of different academic disciplines, with the support of recommendations and suggestions of contents and educational materials. The recommendations will be process through by domain ontologies, constructed from digital texts and the use of natural language process techniques. These recommendations will also support students during the learning process on specific academic areas.", "venue": "2017 IEEE Second Ecuador Technical Chapters Meeting (ETCM)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "1418d3f5adccd79d89881bbf33f6de9fcdc05f27", "url": "https://www.semanticscholar.org/paper/1418d3f5adccd79d89881bbf33f6de9fcdc05f27", "title": "Semantic representation of text captions to aid sport image retrieval", "abstract": "Image captions represent manual semantic annotation of images. These act as essential cues to represent the semantics of an image. This paper describes the process of representing, discovering, storing the semantics in a knowledge base, and then applying the semantics to aid the retrieval of visual information. We exploit a Natural Language Processing (NLP) framework in order to extract the knowledge from image captions and to transform those unstructured data into a semantic model. The novelty of the proposed framework is to use a semantic model to find implicit relationships among the concepts of photographs which are not mentioned directly in text captions. Latent Semantic Indexing (LSI) is deployed to handle ontology imperfections. Experiments tested and validated the major hypotheses of this approach.", "venue": "2008 International Symposium on Intelligent Signal Processing and Communications Systems", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "kg"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "2bb2992cc93bdf026ded64e58a152f1d77bb2620", "url": "https://www.semanticscholar.org/paper/2bb2992cc93bdf026ded64e58a152f1d77bb2620", "title": "Ontology-based Semantic Relatedness Measures: Applications and Calculation", "abstract": "| @ | gelbukh.com Abstract. We propose a procedure for measuring semantic relatedness of two words using an ontology, or semantic network dictionary. We discuss applications of this procedure in detail for lexical, syntactical, and co- reference disambiguation in natural language processing as well as in ma- chine translation. In addition, we use a simplified version of this proce- dure for automatic translation of the semantic network itself into other languages. This simplifies creation and maintenance of semantic network dictionaries for different languages, thus enabling the described methods for processing of texts in languages other than English.", "venue": "Res. Comput. Sci.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["tp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 2, "tp": 1}, "nlp_mention_counts": {"nlp": 1, "tp": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "594dd6b3849092cf2bad510e50cf869a43364f75", "url": "https://www.semanticscholar.org/paper/594dd6b3849092cf2bad510e50cf869a43364f75", "title": "How to Tune Parameters in Geographical Ontologies Embedding", "abstract": "Many Natural Language Processing (NLP) tasks, like question answering or analyzing verbatim comments, have started to use word embeddings due to their ability to capture semantic relations between words. Recently, embeddings have been also applied in the geospatial context to represent geospatial ontologies, thanks to their ability to capture semantic similarity. In this paper, we present an analysis of a promising embedding technique particularly suitable for representing hierarchical structures. We conduct a deep technical evaluation of many parameters and their impact on the quality of the representation.", "venue": "LocalRec@SIGSPATIAL", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "876ba4f35f18d612defeba170acaaa5bdd35cb77", "url": "https://www.semanticscholar.org/paper/876ba4f35f18d612defeba170acaaa5bdd35cb77", "title": "Multilingual document clustering : state of the art (Construction de corpus multilingues : \u00e9tat de l\u2019art) [in French]", "abstract": "Multilingual document clustering : state of the art Multilingual corpora are extensively exploited in several branches of natural language processing. This paper presents an overview of works in the automatic construction of such corpora. We address this topic by first providing an overview of different perceptions of comparability. We then examine the main approaches to similarity computation, construction and evaluation developed in the field. We notice that the measurement of the textual similarity is usually based on corpus statistics or the structure of ontological resources or on a combination of these two approaches. In a multilingual framework, with the use of a multilingual dictionary or a machine translator, many problems arise. The exploitation of a multilingual ontological ressource seems to be a worthy option. In clustering, the problem of adding documents to the initial base without affecting the quality of clusters remains open. MOTS-CL\u00c9S : corpus multilingues, comparabilit\u00e9, similarit\u00e9 textuelle translingue, classification.", "venue": "JEP/TALN/RECITAL", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Art"], "mentions": ["onto", "nlp", "mt", "onto"], "mention_counts": {"nlp": 1, "onto": 2, "mt": 1}, "nlp_mention_counts": {"nlp": 1, "mt": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "33275e51127d21b21da5d75f2c7ac1b88bb940c1", "url": "https://www.semanticscholar.org/paper/33275e51127d21b21da5d75f2c7ac1b88bb940c1", "title": "DomainBuilder: the knowledge authoring system for SlideTutor Intelligent Tutoring system", "abstract": "One of the major challenges in the development of medical Intelligent Tutoring Systems (ITS) is the development of authored content, a time-consuming process that requires participation of discipline experts. In this publication, we describe the development of software systems called DomainBuilder and TutorBuilder, designed to streamline and simplify the authoring process for general medical ITSs. The aim of these systems is to allow physicians without programming or ITSs background to create a domain knowledge base and author tutor cases in a time efficient manner.\u00a0 DomainBuilder combined knowledge authoring, case authoring, and validation tasks into a single work environment, enabling multiple authoring strategies. Natural Language Processing (NLP) methods were integrated for parsing existing clinical reports to speed case authoring. Similarly, TutorBuilder was designed to allow users to customize all aspects of ITSs, including user interface, pedagogic module, feedback module, etc. Both systems underwent formal usability studies with physicians specializing in dermatology. Open-ended questions assessed usability of the system and satisfaction with its features. Incorporating feedback from usability studies, DomainBuilder and TutorBuilder systems were deployed and used across multiple universities to create customized medical tutoring curriculum. Overall, both systems were well received by medical professionals participating in usability studies with participants highlighting ease of utilization and clarity of presentation. Usability study participants were able to successfully use the system for the authoring tasks. DomainBuilder and TutorBuilder are novel tools that combine comprehensive aspects of content creation, including creation of domain ontologies, case authoring, and validation.", "venue": "F1000Research", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "76d9ed1dd14c84fb6a36361513cb854a64e08e6f", "url": "https://www.semanticscholar.org/paper/76d9ed1dd14c84fb6a36361513cb854a64e08e6f", "title": "Team UVA_ART at TREC 2018 Precision Medicine Track: Graph-based Concept Expansion and NLP for Document Relevance Boosting", "abstract": "This paper describes the UVA_ART* team entries in the TREC 2018 workshop series Precision Medicine Track. We submitted 5 runs for the Scientific Abstracts task. Our approach used an exclusivity-based relatedness measure defined on the UMLS Metathesaurus ontologies to add context to our queries. We combined this with natural language processing using cTAKES for concept annotation to effect a graph-based query expansion on an enriched document corpus. We used Elasticsearch as our ranking and query engine with different query templates for each run. Our efforts demonstrate that the existing medical ontologies can be leveraged to achieve moderate results with little to no other clinical input.", "venue": "TREC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "443aa1a2093a8719307c20beafd293b8635cd335", "url": "https://www.semanticscholar.org/paper/443aa1a2093a8719307c20beafd293b8635cd335", "title": "Managing CSCL Activity through networking models", "abstract": "This study aims at managing activity carried out in Computer-Supported Collaborative Learning (CSCL) environments. We apply an approach that gathers and manages the knowledge underlying huge data structures, resulting from collaborative interaction among participants and stored as activity logs. Our method comprises a variety of important issues and aspects, such as: deep understanding of collaboration among participants in workgroups, definition of an ontology for providing meaning to isolated data manifestations, discovering of knowledge structures built in huge amounts of data stored in log files, and development of high-semantic indicators to describe diverse primitive collaborative acts, and binding these indicators to formal descriptions defined in the collaboration ontology; besides our method includes gathering collaboration indicators from web forums using natural language processing (NLP) techniques.", "venue": "Int. J. Emerg. Technol. Learn.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "a6f29e1088f85762d94fce4a1f2dee46971e5bde", "url": "https://www.semanticscholar.org/paper/a6f29e1088f85762d94fce4a1f2dee46971e5bde", "title": "Integrating image and text information for biomedical information retrieval", "abstract": "The search for relevant and actionable information is key to achieving clinical and research goals in biomedicine. Biomedical information exists in different forms: as text and illustrations in journal articles and other documents, in \"images\" stored in databases, and as patients' cases in electronic health records. In the context of this work an \"image\" includes not only biomedical images, but also illustrations, charts, graphs, and other visual material appearing in biomedical journals, electronic health records, and other relevant databases. The tutorial will cover methods and techniques to retrieve information from these entities, by moving beyond conventional text-based searching to combining both text and visual features in search queries. The approaches to meeting these objectives use a combination of techniques and tools from the fields of Information Retrieval (IK), Content-Based Image Retrieval (CBIR), and Natural Language Processing (NLP). The tutorial will discuss steps to improve the retrieval of biomedical literature by targeting the text describing the visual content in articles (figures, including illustrations and images), a rich source of information not typically exploited by conventional bibliographic or full-text databases. Taking this a step further we will explore challenges in finding information relevant to a patient's case from the literature and then link it to the patient's health record. The case is first represented in structured form using both text and image features, and then literature and EHR databases can be searched for similar cases. Further, we will discuss steps to automatically find semantically similar images in image databases, which is an important step in differential diagnosis. Automatic image annotation and retrieval steps will be described that use image features and a combination of image and text features. We explore steps toward generating a \"visual ontology\", i.e., concepts assigned to image patches. Elements from the visual ontology are called \"visual keywords\" and are used to find images with similar concepts. The tutorial will demonstrate some of these techniques by demonstrating our Image and Text Search Engine (ITSE), a hybrid system combining NLM's Essie text search engine with CEB's image similarity engine.", "venue": "2010 IEEE 23rd International Symposium on Computer-Based Medical Systems (CBMS)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "6b3724d70c2fbd3d3a7af7c319a22e9eda1d16f2", "url": "https://www.semanticscholar.org/paper/6b3724d70c2fbd3d3a7af7c319a22e9eda1d16f2", "title": "Practical approach for recommender systems", "abstract": "Every day, number of pages gets added on web which makes tracking of their links cumbersome. Due to this, problem of overloaded data has come up. This issue led researchers to thoroughly go through different aspects of Web Usage Mining (WUM). Another issue of traditional system is of recommendations which are also a part of WUM and Web logs. This paper proposed a system of recommendations which uses tokenization to separate the users and information is conveyed to Resource Description Framework (RDF) for Semantic data generation and display. Session based clusters are formed and frequency of items is noted. Algorithms are also proposed for smooth working of the system. Web log information is needed for understanding the general mentality or behaviour of the user. We have also proposed Natural Language Processing (NLP) techniques for conditions where users preferences regarding products are not generated.", "venue": "2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "rdf", "rdf", "nlp"], "mention_counts": {"nlp": 2, "rdf": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"rdf": 2}, "relevance_score": 0.5}, {"paperId": "51dae14f9191f61c850917bcde273cb3eeafb786", "url": "https://www.semanticscholar.org/paper/51dae14f9191f61c850917bcde273cb3eeafb786", "title": "A ternary Relation Algebra of directed lines", "abstract": "We define a ternary Relation Algebra (RA) of relative position relations on two-dimensional directed lines (d-lines for short). A d-line has two degrees of freedom (DFs): a rotational DF (RDF), and a translational DF (TDF). The representation of the RDF of a d-line will be handled by an RA of 2D orientations, CYC_t, known in the literature. A second algebra, TA_t, which will handle the TDF of a d-line, will be defined. The two algebras, CYC_t and TA_t, will constitute, respectively, the translational and the rotational components of the RA, PA_t, of relative position relations on d-lines: the PA_t atoms will consist of those pairs of a TA_t atom and a CYC_t atom that are compatible. We present in detail the RA PA_t, with its converse table, its rotation table and its composition tables. We show that a (polynomial) constraint propagation algorithm, known in the literature, is complete for a subset of PA_t relations including almost all of the atomic relations. We will discuss the application scope of the RA, which includes incidence geometry, GIS (Geographic Information Systems), shape representation, localisation in (multi-)robot navigation, and the representation of motion prepositions in NLP (Natural Language Processing). We then compare the RA to existing ones, such as an algebra for reasoning about rectangles parallel to the axes of an (orthogonal) coordinate system, a ``spatial Odyssey'' of Allen's interval algebra, and an algebra for reasoning about 2D segments.", "venue": "ArXiv", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["rdf", "nlp", "nlp", "rdf"], "mention_counts": {"nlp": 2, "rdf": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"rdf": 2}, "relevance_score": 0.5}, {"paperId": "ab57d295665e48ed1e1b8d97916b6660dfe5b4c5", "url": "https://www.semanticscholar.org/paper/ab57d295665e48ed1e1b8d97916b6660dfe5b4c5", "title": "Scholarly Data Mining: Making Sense of Scientific Literature", "abstract": "During the last decade the amount of scientific information available on-line increased at an unprecedented rate and this situation is unlikely to change. As a consequence, nowadays researchers are overwhelmed by an enormous and continuously growing number of publications to consider when they perform research activities like the exploration of advances in specific topics, peer reviewing, writing and evaluation of proposals. Natural Language Processing technology plays a key role in enabling intelligent access to the content of scientific publications. By mining the contents of scientific papers, for example, rich scientific knowledge bases can be built, thus supporting more effective information discovery and question answering approaches. Moreover, text summarization technology can help condense long papers to their essential contents so as to speed up the selection of scientific articles of interest or to assist in the manual or automatic generation of state of the art reports. Paraphrase and textual entailment techniques can contribute to the identification of relations across different scientific textual sources, thus, for instance, identifying implicit links between publications. This tutorial provides an overview of approaches to the extraction of knowledge from scientific literature, including the in-depth analysis of the structure of the scientific articles, their semantic interpretation, content extraction, summarization, and visualization.", "venue": "ACM/IEEE Joint Conference on Digital Libraries", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "ke"], "mention_counts": {"nlp": 1, "kg": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.5}, {"paperId": "2bf257acf56fb214af71b123f1b734263242cc36", "url": "https://www.semanticscholar.org/paper/2bf257acf56fb214af71b123f1b734263242cc36", "title": "One-shot Scene Graph Generation", "abstract": "As a structured representation of the image content, the visual scene graph (visual relationship) acts as a bridge between computer vision and natural language processing. Existing models on the scene graph generation task notoriously require tens or hundreds of labeled samples. By contrast, human beings can learn visual relationships from a few or even one example. Inspired by this, we design a task named One-Shot Scene Graph Generation, where each relationship triplet (e.g., \"dog-has-head'') comes from only one labeled example. The key insight is that rather than learning from scratch, one can utilize rich prior knowledge. In this paper, we propose Multiple Structured Knowledge (Relational Knowledge and Commonsense Knowledge) for the one-shot scene graph generation task. Specifically, the Relational Knowledge represents the prior knowledge of relationships between entities extracted from the visual content, e.g., the visual relationships \"standing in'', \"sitting in'', and \"lying in'' may exist between \"dog'' and \"yard'', while the Commonsense Knowledge encodes \"sense-making'' knowledge like \"dog can guard yard''. By organizing these two kinds of knowledge in a graph structure, Graph Convolution Networks (GCNs) are used to extract knowledge-embedded semantic features of the entities. Besides, instead of extracting isolated visual features from each entity generated by Faster R-CNN, we utilize an Instance Relation Transformer encoder to fully explore their context information. Based on a constructed one-shot dataset, the experimental results show that our method significantly outperforms existing state-of-the-art methods by a large margin. Ablation studies also verify the effectiveness of the Instance Relation Transformer encoder and the Multiple Structured Knowledge.", "venue": "ACM Multimedia", "citationCount": 16, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "nlp"], "mention_counts": {"nlp": 1, "kg": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.5}, {"paperId": "33bb3a5bee7545bd55e75df4c00e28ebdef91f2f", "url": "https://www.semanticscholar.org/paper/33bb3a5bee7545bd55e75df4c00e28ebdef91f2f", "title": "Automatic problem extraction and analysis from unstructured text in IT tickets", "abstract": "IT services are extremely human labor intensive, and a key focus is to provide efficient services at low cost. Automation of repeatable IT tasks using software service agents that reduce human effort is therefore an important component of service management. A large fraction of the work done by IT service personnel involves troubleshooting of problems. However, the complexity of IT systems makes automated problem determination and resolution a challenging research problem. Using a database of prior customer problems and solutions, we build a system that extracts knowledge about different classes of problems arising in the IT infrastructure, mine problem linkages to recent system changes, and identify the resolution activities to mitigate problems. The system, at its core, uses data mining, machine learning, and natural language parsing techniques. By using extracted knowledge, one can (i) understand the kind of problems and the root causes affecting the IT infrastructure, (ii) proactively remediate the causes so that they no longer result in problems, and (iii) estimate the scope for automation for service management. In the future, a large cost differentiator for any IT company will often involve being able to build automated service agents from these technologies, which will result in a reduction in human effort.", "venue": "IBM Journal of Research and Development", "citationCount": 25, "fieldsOfStudy": ["Computer Science", "Biology"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "b5153bde87b9939be95ec26d5d0f4b1c9071ffa4", "url": "https://www.semanticscholar.org/paper/b5153bde87b9939be95ec26d5d0f4b1c9071ffa4", "title": "ALICE: Active Learning with Contrastive Natural Language Explanations", "abstract": "Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides several bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. ALICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then it extracts knowledge from these explanations using a semantic parser. Finally, it incorporates the extracted knowledge through dynamically changing the learning model's structure. We applied ALICE in two visual recognition tasks, bird species classification and social relationship classification. We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data. We found that adding 1 explanation leads to similar performance gain as adding 13-30 labeled training data points.", "venue": "Conference on Empirical Methods in Natural Language Processing", "citationCount": 37, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "88690601b9e0ff619217781a6c3022fd92e4137a", "url": "https://www.semanticscholar.org/paper/88690601b9e0ff619217781a6c3022fd92e4137a", "title": "The CLARIN-DK Text Tonsorium", "abstract": "The Text Tonsorium (TT) is a workflow management system (WMS) for Natural Language Processing (NLP). The software implements a design goal that sets it apart from other WMSes: it operates without manually composed workflow designs. The TT invites tool providers to register and integrate their tools, without having to think about the workflow designs that new tools can become part of. Both input and output of new tools are specified by expressing language, file format, type of content, etc. in terms of an ontology. Likewise, users of the TT define their goal in terms of this ontology and let the TT compute the workflow designs that fulfill that goal. When the user has chosen one of the proposed workflow designs, the TT enacts it with the user\u2019s input. This untraditional approach to workflows requires some familiarization. In principle, the TT cannot predict which of the proposed workflow designs is most appropriate, because the text may have\u00a0 peculiarities that are as yet uncharted. The user has to make the choice. In this paper, we reflect on the experiences with providing, testing and using workflows aimed at annotating transcripts of parliamentary debates. We propose possible improvements of the TT that can facilitate its use by the wider clarin community.", "venue": "CLARIN Annual Conference", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "47ba9f3c8fba4cf82e48cdc05b8dfd5fd6636f87", "url": "https://www.semanticscholar.org/paper/47ba9f3c8fba4cf82e48cdc05b8dfd5fd6636f87", "title": "The role of informatics in promoting patient safety", "abstract": "This issue of JAMIA is focused on informatics applications to enhance patient safety. This is one of the most important, yet underemphasized, aspects of the informatics curriculum across the country. Although media attention occasionally concentrates on what can go wrong when information systems are employed in practice, there is also much to say on what might go wrong if information systems were not employed. Additionally, a proper amount of standardization of clinical practices can elevate sub-optimal care to an acceptable level, often reducing cost and patient suffering as a result. Decision support for medication prescribing and dispensing has always been one of the most direct ways for information systems to promote patient safety. Whalen (p. 849) reports on lessons learned in a pediatrics hospital from a transition to a new electronic health record (EHR) system, and Walsh (p. 911) studies the accuracy of the medication list in the EHR and its implications for care, research, and improvement. Cheng (p. 873) shows how using drug knowledgebase information to distinguish between commonly confused drugs can prevent errors, and Vajravelu (p. 780) proposes a new algorithm to analyze multiple pharmacologic exposures using EHR data. Additionally, Samwald (p. 895) shares the experience of implementing pharmacogenomics decision support across seven European countries. The domains in which information systems can improve patient safety are numerous. Waters (p. 901) studies current use, interest, and perceived usability of clinical pathways for primary care, while Sittig (p. 915) describes the levels of adherence to recommended EHR safety practices across eight healthcare organizations. EHRbased intervention and reports on several safety topics are also presented in this issue of JAMIA: Ray (p. 863) uses statistical anomaly detection models for decision support system malfunctions, Chen (p. 790) analyzes interaction patterns of trauma providers that are associated with increased patients\u2019 lengths of stay in the hospital, while Vahdat (p. 827) reports on a simulation study of the effects of EHR implementation on timeliness of care in a dermatology clinic. Berger (p. 833) integrates physical abuse measures into a pediatric clinical decision support system, while Meyer (p. 841) evaluates a mobile application to improve clinical laboratory test ordering. The applications and algorithms described in this issue of JAMIA would be hard to implement without standardization of terminologies, ontologies, and foundational research in natural language processing and information retrieval. Examples of advances in these areas are also featured: Cuzzola (p. 819) links UMLS to DBpedia to promote knowledge discovery, Wang (p. 809) describes efforts involving RxNorm that are leading to a normalized clinical drug knowledge base in China, Vreeman (p. 886) presents a unified terminology for radiology procedures (the \u201cLOINC RSNA Radiology Playbook\u201d), and Blosnich (p. 907) shows how it is possible to use EHR-based clinician text notes to validate transgender-related ICD codes. Additional articles describe approaches that enable a variety of information systems: Mei (p. 800) describes an interactive medical word sense disambiguation method, Kilicoglu (p. 856) reports on the results of automatic recognition of self-acknowledged limitations in the clinical research literature, and Baladron (p. 774) proposes a tool for filtering PubMed search results by sample size. JAMIA continues to publish a combination of application and foundational articles that allows our readers to stay abreast with the best developments in the field. Patient safety is an important topic that has been in the informatics portfolio since its start and for which novel solutions continue to emerge. New topics are also continuously enlarging the informatics portfolio. Stay tuned for the August issue that highlights articles on informatics applications focused on patients, their family and friends, and the expanding scope of our field.", "venue": "J. Am. Medical Informatics Assoc.", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["kg", "nlp", "onto", "wsd"], "mention_counts": {"nlp": 1, "kg": 1, "wsd": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 1, "wsd": 1}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.5}, {"paperId": "bf6065efd911f3f5186e03737792c95a8dcf3122", "url": "https://www.semanticscholar.org/paper/bf6065efd911f3f5186e03737792c95a8dcf3122", "title": "An Ontology-Based NLP Approach to Semantic Annotation of Annual Report", "abstract": "Annual Reports of Chinese Securities Companies have become the most significant and reliable source of information for domestic and foreign investors. Semantic annotation of them enhanced information retrieval and improved interoperability. In this paper we first review the major features of annual reports which are Tagged PDF format, then propose a novel ontology-based NLP approach to semantic annotate them. The experimental results show in the paper state a good accuracy of our approach.", "venue": "2009 International Conference on Computational Intelligence and Security", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "5bde7cdff5e43b238d28bf8923e886496e60037d", "url": "https://www.semanticscholar.org/paper/5bde7cdff5e43b238d28bf8923e886496e60037d", "title": "Domain modelling and NLP: Formal ontologies? Lexica? Or a bit of both?", "abstract": "There are a number of genuinely open questions concerning the use of domain models in nlp. It would be great if contributors to Applied Ontology could help addressing them rather than adding to an already long polemical literature ...", "venue": "Appl. Ontology", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "5838b3d8c07e7944d46d66c9e0ed953c0aa69e80", "url": "https://www.semanticscholar.org/paper/5838b3d8c07e7944d46d66c9e0ed953c0aa69e80", "title": "MEDCIS: Multi-Modality Epilepsy Data Capture and Integration System", "abstract": "Sudden Unexpected Death in Epilepsy (SUDEP) is the leading mode of epilepsy-related death and is most common in patients with intractable, frequent, and continuing seizures. A statistically significant cohort of patients for SUDEP study requires meticulous, prospective follow up of a large population that is at an elevated risk, best represented by the Epilepsy Monitoring Unit (EMU) patient population. Multiple EMUs need to collaborate, share data for building a larger cohort of potential SUDEP patient using a state-of-the-art informatics infrastructure. To address the challenges of data integration and data access from multiple EMUs, we developed the Multi-Modality Epilepsy Data Capture and Integration System (MEDCIS) that combines retrospective clinical free text processing using NLP, prospective structured data capture using an ontology-driven interface, interfaces for cohort search and signal visualization, all in a single integrated environment. A dedicated Epilepsy and Seizure Ontology (EpSO) has been used to streamline the user interfaces, enhance its usability, and enable mappings across distributed databases so that federated queries can be executed. MEDCIS contained 936 patient data sets from the EMUs of University Hospitals Case Medical Center (UH CMC) in Cleveland and Northwestern Memorial Hospital (NMH) in Chicago. Patients from UH CMC and NMH were stored in different databases and then federated through MEDCIS using EpSO and our mapping module. More than 77GB of multi-modal signal data were processed using the Cloudwave pipeline and made available for rendering through the web-interface. About 74% of the 40 open clinical questions of interest were answerable accurately using the EpSO-driven VISual AGregagator and Explorer (VISAGE) interface. Questions not directly answerable were either due to their inherent computational complexity, the unavailability of primary information, or the scope of concept that has been formulated in the existing EpSO terminology system.", "venue": "American Medical Informatics Association Annual Symposium", "citationCount": 37, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "tp", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 2, "tp": 1}, "nlp_mention_counts": {"nlp": 1, "tp": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "de9e27f7f9ee747c941c6513d892c87e0a7ef2d0", "url": "https://www.semanticscholar.org/paper/de9e27f7f9ee747c941c6513d892c87e0a7ef2d0", "title": "MEXIN: Multidialectal Ontology supporting NLP approach to improve government electronic communication with the Mexican Ethnic Groups", "abstract": "The government services usually target all citizens, but sometimes physical services nor technology-based services do not cover all people. This research aims to tackle services given to underrepresented citizens in Mexico (Indigenous people) and apply NLP techniques supported by ontologies to achieve accurate translation to most dialects spoken in Mexico. The scope of this paper only tests with Mayan dialect spoken primarily in the Mexican peninsula.", "venue": "DG.O", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "a21fa2d18c721b6d61444d8f0680790f94aa0f45", "url": "https://www.semanticscholar.org/paper/a21fa2d18c721b6d61444d8f0680790f94aa0f45", "title": "Adverbs in Semantic Lexica for NLP - The extension of the Danish SIMPLE lexicon with Time Adverbs", "abstract": "In this paper we will discuss the treatment of adverbs in semantic lexica for NLP. On the basis of a semantic classification of Danish lexical time adverbs as well as a test carried out wrt. their ability to combine with different tenses and types of Aktionsart, an ontology on time adverbs is established. We will discuss which semantic characteristics exposed by the test that should be included in a computational lexicon, and propose how the ideas can be incorporated in the SIMPLE lexicon model, partly by reusing already implemented features from the model, partly by an extension of the set of features. Furthermore we will show how some adverbs will inherit information from several nodes in a SIMPLE ontology for adverbs, and how semantic relations, e.g. synonymi and antonomy, is relevant also in the case of adverbs. Finally we will give som examples on lexical entries of adverbs. The result can easily be applied on other adverbials with a time sense, and will therefore in fact also cover a large group of lexicalised multiword entities.", "venue": "LREC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "c26506fe5b2f69a649bf1e15e43aae713dd62d67", "url": "https://www.semanticscholar.org/paper/c26506fe5b2f69a649bf1e15e43aae713dd62d67", "title": "Hypotheses Generation Pertaining to Ayurveda Using Automated Vocabulary Generation and Transitive Text Mining", "abstract": "Automated extraction of knowledge from voluminous documents is a vast research area. Text mining is a promising approach for extracting knowledge from unstructured textual documents. The objective of this paper is to mine documents pertaining to Ayurveda, which are retrieved from PubMed into a databank, and find novel transitive associations among biological objects. This paper discusses the extraction of biological objects from the databank using an Automated Vocabulary Discovery (AVD) algorithm. A text-mining process is described for finding transitive (novel) associations among the extracted biological objects. The text mining algorithm, in addition to identifying novel associations (termed hypotheses), also assigns a numerical significance score to them. The expectation is that those with higher score have greater likelihood of being true than those with lower scores. Experimental results as well as their validation results are presented, demonstrating that the method has the potential to predict novel and interesting true associations.", "venue": "2009 International Conference on Network-Based Information Systems", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "62723f39ff24330270d5753f49605d72416d7245", "url": "https://www.semanticscholar.org/paper/62723f39ff24330270d5753f49605d72416d7245", "title": "Kernel Methods for Word Sense Disambiguation and Acronym Expansion", "abstract": "The scarcity of manually labeled data for supervised machine learning methods presents a significant limitation on their ability to acquire knowledge. The use of kernels in Support Vector Machines (SVMs) provides an excellent mechanism to introduce prior knowledge into the SVM learners, such as by using unlabeled text or existing ontologies as additional knowledge sources. Our aim is to develop three kernels - one that makes use of knowledge derived from unlabeled text, the second using semantic knowledge from ontologies, and finally a third, additive kernel consisting of the first two kernels - and study their effect on the tasks of word sense disambiguation and automatic expansion of ambiguous acronyms.", "venue": "AAAI", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "wsd", "wsd", "onto"], "mention_counts": {"wsd": 2, "onto": 2}, "nlp_mention_counts": {"wsd": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "952e551fa2173f932da002e7f6268b0d858cf559", "url": "https://www.semanticscholar.org/paper/952e551fa2173f932da002e7f6268b0d858cf559", "title": "Using Patterns Co-occurrence Matrix for Cleaning Closed Sequential Patterns for Text Mining", "abstract": "With the overwhelming increase in the amount of texts on the web, it is almost impossible for people to keep abreast of up-to-date information. Text mining is a process by which interesting information is derived from text through the discovery of patterns and trends. Text mining algorithms are used to guarantee the quality of extracted knowledge. However, the extracted patterns using text or data mining algorithms or methods leads to noisy patterns and inconsistency. Thus, different challenges arise, such as the question of how to understand these patterns, whether the model that has been used is suitable, and if all the patterns that have been extracted are relevant. Furthermore, the research raises the question of how to give a correct weight to the extracted knowledge. To address these issues, this paper presents a text post-processing method, which uses a pattern co-occurrence matrix to find the relation between extracted patterns in order to reduce noisy patterns. The main objective of this paper is not only reducing the number of closed sequential patterns, but also improving the performance of pattern mining as well. The experimental results on Reuters Corpus Volume 1 data collection and TREC filtering topics show that the proposed method is promising.", "venue": "2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke"], "mention_counts": {"ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.5}, {"paperId": "3e1d7192aa69d4a5a363e092a05648f49725e3a9", "url": "https://www.semanticscholar.org/paper/3e1d7192aa69d4a5a363e092a05648f49725e3a9", "title": "Chinese Word Sense Disambiguation Based on Lexical Semantic Ontology", "abstract": "This paper describes preliminary works of word sense disambiguation on Chinese verbs using the information derived from lexical semantic ontology (LSO). In spite of sophisticated methods, simple algorithm is employed to underline the characters of the features chosen from LSO data. Several groups of tests are designed to find different effects of the features and other aspects. Some promising results are gotten from the prime tests on nine Chinese ambiguous verbs. The results show what informative features the LSO provides and the potential improving ways.", "venue": "J. Chin. Lang. Comput.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "wsd", "onto", "wsd"], "mention_counts": {"wsd": 2, "onto": 2}, "nlp_mention_counts": {"wsd": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "521a161a139efe6a8cbe5a15699f2593370074b2", "url": "https://www.semanticscholar.org/paper/521a161a139efe6a8cbe5a15699f2593370074b2", "title": "What\u2019s the Matter? Knowledge Acquisition by Unsupervised Multi-Topic Labeling for Spoken Utterances", "abstract": "Systems such as Alexa, Cortana, and Siri appear rather smart.\nHowever, they only react to predefined wordings and do not actually\ngrasp the user\u2019s intent. To overcome this limitation, a system must understand the topics the user is talking about. Therefore, we apply unsupervised multi-topic labeling to spoken utterances. Although topic labeling is a well-studied task on textual documents, its potential for spoken\ninput is almost unexplored. Our approach for topic labeling is tailored\nto spoken utterances; it copes with short and ungrammatical input.\nThe approach is two-tiered. First, we disambiguate word senses. We utilize Wikipedia as pre-labeled corpus to train a na\u00efve-bayes classifier.\nSecond, we build topic graphs based on DBpedia relations. We use two\nstrategies to determine central terms in the graphs, i.e. the shared topics. One focuses on the dominant senses in the utterance and the other\ncovers as many distinct senses as possible. Our approach creates multiple\ndistinct topics per utterance and ranks results.\nThe evaluation shows that the approach is feasible; the word sense disambiguation achieves a recall of 0.799. Concerning topic labeling, in a user\nstudy subjects assessed that in 90.9% of the cases at least one proposed\ntopic label among the first four is a good fit. With regard to precision,\nthe subjects judged that 77.2% of the top ranked labels are a good fit or\ngood but somewhat too broad (Fleiss\u2019 kappa \u03ba = 0.27).\nWe illustrate areas of application of topic labeling in the field of programming in spoken language. With topic labeling applied to the spoken\ninput as well as ontologies that model the situational context we are able\nto select the most appropriate ontologies with an F1-score of 0.907.", "venue": "International Journal of Humanized Computing and Communication", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "wsd", "onto", "wsd"], "mention_counts": {"wsd": 2, "onto": 2}, "nlp_mention_counts": {"wsd": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.5}, {"paperId": "c54a0336ce2be750b413ed2520a1042a926af956", "url": "https://www.semanticscholar.org/paper/c54a0336ce2be750b413ed2520a1042a926af956", "title": "Integration of heterogeneous language resources: A monolingual dictionary and a thesaurus", "abstract": "Linguistic knowledge plays a crucial role in natural language processing. Constructing large linguistic knowledge bases requires a lot of human effort and much cost. There have been many attempts to construct linguistic knowledge automatically, based on two primary strategies: knowledge extraction from annotated corpora and the augmentation of existing knowledge bases using annotated corpora. This paper describes an algorithm to enlarge existing linguistic knowledge through integration with heterogeneous linguistic resources. Specifically, this algorithm links a word sense defined in a monolingual dictionary to semantic classes in a thesaurus. Experiments show that we achieve a linking precision of 85.5% and coverage of 61.4%.", "venue": "NLPRS", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "nlp", "kg"], "mention_counts": {"nlp": 1, "kg": 2, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "3600f53e6a7bd5b72b00b6d173cfc30c8520dc91", "url": "https://www.semanticscholar.org/paper/3600f53e6a7bd5b72b00b6d173cfc30c8520dc91", "title": "Chemical Compounds Knowledge Visualization with Natural Language Processing and Linked Data", "abstract": "This paper proposes a visualization system for chem ical compounds. New chemical compounds are being pr oduced by every moment and registration of chemical compounds to databases strongly depends on human labor. Our system uses Natural Language Processing technologies for extracting information of chemical ompounds from text and for storing the extracted results as Linked Data (LD). By combining the extracted results with LD-based exist ing chemical compound knowledge, our system provide s visualization of chemical compound information such as integrated view of sev eral databases and chemical compounds that have sim ilar structures.", "venue": "LREC", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "ld", "ie", "nlp", "nlp"], "mention_counts": {"ld": 2, "nlp": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"ld": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "b8250f6987dee4c471be3ca15a48ff035d09ae00", "url": "https://www.semanticscholar.org/paper/b8250f6987dee4c471be3ca15a48ff035d09ae00", "title": "Demo of the Linguistic Field Data Management and Analysis System - LiFE", "abstract": "In the proposed demo, we will present a new software - Linguistic Field Data Management and Analysis System - LiFE - an open-source, web-based linguistic data management and analysis application that allows for systematic storage, management, sharing and usage of linguistic data collected from the field. The application allows users to store lexical items, sentences, paragraphs, audio-visual content including photographs, video clips, speech recordings, etc, along with rich glossing / annotation; generate interactive and print dictionaries; and also train and use natural language processing tools and models for various purposes using this data. Since its a web-based application, it also allows for seamless collaboration among multiple persons and sharing the data, models, etc with each other. The system uses the Python-based Flask framework and MongoDB (as database) in the backend and HTML, CSS and Javascript at the frontend. The interface allows creation of multiple projects that could be shared with the other users. At the backend, the application stores the data in RDF format so as to allow its release as Linked Data over the web using semantic web technologies - as of now it makes use of the OntoLex-Lemon for storing the lexical data and Ligt for storing the interlinear glossed text and then internally linking it to the other linked lexicons and databases such as DBpedia and WordNet. Furthermore it provides support for training the NLP systems using scikit-learn and HuggingFace Transformers libraries as well as make use of any model trained using these libraries - while the user interface itself provides limited options for tuning the system, an externally-trained model could be easily incorporated within the application; similarly the dataset itself could be easily exported into a standard machine-readable format like JSON or CSV that could be consumed by other programs and pipelines. The system is built as an online platform; however since we are making the source code available, it could be installed by users on their internal / personal servers as well.", "venue": "ICON", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlp", "rdf", "ld", "nlp"], "mention_counts": {"nlp": 2, "sw": 1, "ld": 1, "rdf": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"ld": 1, "sw": 1, "rdf": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "6d9a152c6df119b630d44d6638ba610d884359ae", "url": "https://www.semanticscholar.org/paper/6d9a152c6df119b630d44d6638ba610d884359ae", "title": "Getting Started in Text Mining: Part Two", "abstract": "We are, in a sense, drowning in information. Today, it is unusual for scientists even to read a journal cover to cover\u2014much less to personally parse all information pertinent to even a narrow research area. Increasingly complex content, large digital supplements, and a staggering volume of publications are now threatening old-fashioned scientific reading with extinction. But by using computers to sift through and scour published articles, the nascent technology of text mining promises to automate the rote information-gathering stage\u2014hopefully leaving to human minds the more challenging (and rewarding) activity of higher thinking. \n \nThis article is intended to continue where Cohen and Hunter [1] left off in \u201cGetting Started in Text Mining,\u201d an introduction in the January 2008 issue of PLoS Computational Biology which covered the actual mining of text and its digestion into small quanta of computer-manageable information (http://www.ploscompbiol.org/doi/pcbi.0040020). In this overview of the field, we begin by summarizing the major stages of current text-processing pipelines. We now focus on the downstream questions scientists can ask using text-mining and literature-mining engines. At times, we (deliberately) blur the boundary between today's approaches and tomorrow's possibilities. \n \nFigure 1 shows a high-level overview of the stages in text mining, with a focus on its applications.\u00a0We begin at the top left of the figure, which shows the process of information retrieval\u2014how we select relevant documents [2]. Unfortunately, free full-text access remains impossible for a large portion of scientific journals. In some fields, such as chemistry, even article abstracts are inaccessible for a large-scale analysis. The obvious outcome is that articles published in open-access journals have a better chance of being identified as relevant hits than others appearing in traditional \u201cclosed-access\u201d journals. Electronic access to text obviously impacts all stages of text mining. \n \n \n \nFigure 1 \n \nMajor techniques and applications of text mining. \n \n \n \nOnce the documents have been chosen by an information retrieval engine, a computer scans the text and picks out the various entities (objects, concepts, and symbols) in each sentence. This process, called named-entity recognition [3], draws upon dictionaries of synonyms and homonyms, in addition to machine-learning tools [4], so that an individual entity (say, a protein) is recognized consistently\u2014even though it may be referred to by several different names and acronyms [5]. Named-entity recognition is closely related to the design of controlled terminologies [6] and ontologies for the annotation of texts and experimental data [7]\u2014a process often requiring a monumental community effort [8]. \n \nThe next step is information extraction (IE) (see pp. 545\u2013559 in [9]). Here, entities are assembled into simple phrases and clauses that capture the meaning of the mined text. To accomplish this, two or more entities are juxtaposed, and meaningful action words\u2014called predicates\u2014are chosen to link the entities. For instance, we might say gene X genetically interacts with gene Y, or protein A binds to protein B. Each completed clause describes a basic relationship between entities. The question then becomes, what can we do with all these simple or complex clauses? \n \nThe answer is, quite a lot\u2014which helps explain why text mining is poised to become a powerful central pillar in scientific research and recordkeeping. The lower two-thirds of Figure 1 illustrates how the results of information extraction (IE) can be synthesized and used. \n \nBecause IE yields a collection of phrases linking entities through predicates, one of its simplest but valuable uses is to answer simple questions posed to an automated system [10]. In this approach, human questions are digested by a linguistic engine (likely using the same process as employed on original mined text) and mapped to simple phrases. These question phrases are then queried against the database of phrases already stored in the computer, which were generated through the application of IE to analyzed text. (Another mode of question answering, bypassing generation and querying of a database entirely, involves direct search and analysis of relevant texts. These texts can be stored at a local computer disk or distributed on numerous computers around the world.) Figure 1 outlines the basic process by which the machine interprets the question, queries its database of stored relationships, and returns an answer. \n \nIE-generated knowledge often tracks closely the needs of experimental biologists. Typical IE systems are developed in direct response to acute practical problems, such as large-scale annotation of regulatory regions in genomes [11], collecting published claims about experimental evidence supporting a collection of assertions [12], and condensing sparse information about phenotypic effects of mutations in proteins [13]. \n \nOf course, IE-generated databases can be supplemented with additional data gleaned from experiment, or contributed through other non\u2013text-mining means. A simple user interface could facilitate contributing raw experimental data or other information into the database of relationships expressed as simple phrases\u2014again, entities linked by actions (see, for example, the REFLECT system, http://reflect.ws/). Adding more such data should correspondingly increase the effectiveness of the computer's answers to user questions. \n \nAnother major use for the database of IE-generated phrases is to employ the collection itself for the discovery of new information [14],[15]. One approach to this is to seek out \u201cidea isomorphisms\u201d, by which we mean identifying similar types of logical constructs across different contexts. Finding that similar small ideas (or phrases) occur in different fields might allow researchers to bridge different areas of inquiry. Such bridging of fields, in turn, might uncover new connections, thereby suggesting new and unexpected hypotheses that can then be tested experimentally. \n \nThe collection of phrases can also be used to vet and prune itself by examining the consistency among many entries. For instance, conflicting or erroneous data can be flagged. By examining each record situated within a large number of records, the preponderance of evidence could assist in identifying and resolving errors. Say, for example, that 20 distinct phrases all indicate that protein A interacts with protein B, and one phrase suggests otherwise; we might probabilistically argue, then, that the lone conflicting statement is false and should be disregarded\u2014unless it is supported some other way. \n \nAn additional approach to using these phrases\u2014in a mega-scale fashion\u2014is to construct a \u201cmap of science\u201d, a global description of the interrelationships between different fields of inquiry. This is similar conceptually to PubNet [16], which highlights connections between authors. However, the map of science would be generated not through coauthor relationships but through clustering the underlying scientific fact claims themselves, as represented in the IE phrase collection. To do this, researchers would cluster papers according to their IE-derived phrase content; any two papers can be compared in this way to derive a measure of their similarity and overlap in terms of information content. By repeating this process, researchers could create a distance map of all papers in science, and, along the way, of all the factoids that the information content of the papers themselves comprise. \n \nIn addition, researchers might track the changing nature of the IE phrases over time to examine the dynamics of scientific belief. This could involve observing as simple phrases themselves change in occurrence or content over time, or we might watch these simple ideas and truth claims crop up in the scientific literature and track their development that way. \n \nFinally, the middle right-hand section of Figure 1 depicts a very simple type of analysis involving the IE-generated simple phrase collection. This approach involves simply looking at the phrases' occurrence in the databases, and recording which statements tend to occur more than others. This type of analysis normally generates a kind of power law\u2013type structure, where it becomes apparent that a few phrases occur many times, but most others only occur a few times. \n \nText/literature mining is a powerful approach, one we expect to substantially bolster the scientific reporting and discovery process in coming years. Applying the organizational, storage, and pattern-matching capabilities of modern computers to the vast corpus of scientific information contained in the literature (present, past, and future) will not only transform the vast archives of science into rapid-access searchable computerized data, but no doubt also catalyze the discovery of much new knowledge. We hope that this brief \u201cgetting started\u201d report highlights some of the major and promising avenues opening as a result of advances in text mining. \n \nNote to the reader: The field of text mining is young and growing rapidly, and our own interests and experiences have in large part shaped our perspective on it. We are constrained by length limits here to (reluctantly) omit several topics, such as text mining in conjunction with image analysis, important community text-annotation efforts, and ontology engineering\u2014each important in its own right. Furthermore, every issue touched upon in this essay comes with a rich diversity of views and approaches in the text-mining community. While we cannot possibly do justice to this complexity, the reader should reject the impression that there is but a single correct way to perform text analysis.", "venue": "PLoS Comput. Biol.", "citationCount": 43, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "ie", "tp", "ie", "onto"], "mention_counts": {"onto": 2, "tp": 1, "ie": 2}, "nlp_mention_counts": {"tp": 1, "ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "7ddbc44cf6835c734cb15cac7b94441917250e99", "url": "https://www.semanticscholar.org/paper/7ddbc44cf6835c734cb15cac7b94441917250e99", "title": "Towards an Integrated Corpus for the Evaluation of Named Entity Recognition and Object Consolidation", "abstract": "The internet as we know it today is based mainly on unstructured data, i.e. natural language text. While the information contained in these texts is accessible to human readers, it remains unaccessible and opaque to machine processing. In contrast, the Semantic Web (SW) relies heavily on semi-structured data, such as ontologies and related instance sets, or knowledge bases. This kind of data is hard to create and maintain manually. Human Language Technology (HLT) in general, and Information Extraction (IE) and Named Entity Recognition (NER) in specific, could provide a means to close the gap between unstructured and semi-structured data, by extracting formal data from natural language. Object consolidation approaches could help to integrate the newly created data with existing data. Currently, a number of projects are looking into solving both of these tasks. However, in order to evaluate such projects, a novel kind of corpus is needed. In this paper, we propose an Integrated Corpus for the Evaluation of Named Entity Recognition and Object Consolidation. The original incentive for proposing this corpus was the Geco project (GATE/Seco Integration). As this project is relatively small-scale, the corpus is similarly small-scale in its current version. However, our corpus is designed to be easily scalable as the need arises. Moreover, we will make the corpus freely available to enable researchers working on projects aiming in similar directions to benefit from our work.", "venue": "SemAnnot@ISWC", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["hlt", "sw", "ie", "onto", "kg"], "mention_counts": {"onto": 1, "kg": 1, "sw": 1, "hlt": 1, "ie": 1}, "nlp_mention_counts": {"hlt": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 1, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "4d4235417186725bbb6840deb5a442b4ca362c09", "url": "https://www.semanticscholar.org/paper/4d4235417186725bbb6840deb5a442b4ca362c09", "title": "Linguistically analyzed labels of knowledge objects: How can they support OBIE? Lessons learned from the Monnet and TrendMiner projects", "abstract": "We are investigating the use of natural language expressions included in Knowledge Organization Systems (KOS) for supporting Ontology-Based Information Extraction (OBIE), in a multiand cross-lingual context. Very often, Knowledge Organization Systems include so-called annotation properties, in the form of labels, comments, definitions, etc, which have the purpose of introducing human readable information in the formal description of the domain modelled in the KOS. An approach developed in the Monnet project, and continued in the TrendMiner project, consists in transforming the content of annotation properties into linguistically analysed data. Natural language processing of such language expressions, also called sometimes lexicalisation of Knowledge Organisation Systems, are thus transforming the unstructured content of annotation properties into linguistically structured data, which can be used in comparing language data included in a KOS with linguistically annotated texts. If some match of linguistic features between those two types of documents can be established, corresponding segments of the textual documents can be semantically annotated with the elements of the KOS the content of the annotation property is associated with. Evidently, this semantic annotation procedure can be of great help for OBIE, relating text segment to relevant parts of thesauri, taxonomy or ontologies. But looking in more details at the language data contained in annotation properties, we can see that this data very often has to be modified in order to be better used in the context of OBIE. Also there is a need for a formal representation of such linguistically annotated language data in order to ensure interoperability with semantic data available in the Linked Data Framework. The talk will expand on those issues.", "venue": "SWAIE@RANLP", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ld", "nlp", "onto", "ie"], "mention_counts": {"ld": 1, "nlp": 1, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"ld": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "c90cca3eea110251fd7d494a561a79ce5bcc095a", "url": "https://www.semanticscholar.org/paper/c90cca3eea110251fd7d494a561a79ce5bcc095a", "title": "WarMemoirSampo: A Semantic Portal for War Veteran Interview Videos", "abstract": "This paper presents WarMemoirSampo, a portal that provides semantic search and navigation of video interviews with Finnish World War II veterans. The portal associates video fragments with contextual data extracted from the video transcriptions, enabling users to find suitable video segments via faceted search and highlighting relevant content in the video being watched. This is carried out by processing natural language texts in order to extract named entities, keywords and lemmas. The result is a Linked Data Knowledge Graph that underpins the portal. We describe the collaboration between Natural Language Processing and Semantic Web technologies used in order to produce these results.", "venue": "DHNB", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "sw", "kg", "nlp", "nlp"], "mention_counts": {"ld": 1, "sw": 1, "nlp": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"ld": 1, "sw": 1, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "9329a6950d10175bd9e7a9c13d574ad9b028f7cb", "url": "https://www.semanticscholar.org/paper/9329a6950d10175bd9e7a9c13d574ad9b028f7cb", "title": "Standardized Multilingual Language Resourcesfor the Web of Data", "abstract": "Statistical knowledge on natural languages is inevitable for various kinds of services requiring Natural Language Processing (NLP) functionality, such as information retrieval. The NLP Group at the University of Leipzig started providing such statistical information for more than 50 languages in the Leipzig Corpora Collection (LCC) [1] more than a decade ago. Some of their corpora contain more than 5 million words and more than 300 million links between them, resulting in an accumulated size of about 60 million words and 814 million links in all corpora. So far, these valuable information could be accessed in a human-readable Web site and through a SOAP Web service, and excerpts of the data could be downloaded as SQL data dumps. A linked data interface for the LCC has now become desirable in order to allow a wider range of applications to make use of the corpora. In this report, the LCC linked data interface is presented. This new service provides information about almost 60 million resources in approximately 900 million triples. Additionally, links to other vocabulary such as WordNet [2] and to DBpedia [3] are offered. The service is realized using a customized version of D2R Server [4].", "venue": "I-SEMANTICS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ld", "nlp", "ld", "nlp"], "mention_counts": {"ld": 2, "nlp": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"ld": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "7d2d4c577d64d3dc7548e4a971c5ca8081ed9ed9", "url": "https://www.semanticscholar.org/paper/7d2d4c577d64d3dc7548e4a971c5ca8081ed9ed9", "title": "Connecting Science Data Using Semantics and Information Extraction", "abstract": "We are developing prototypes that explicate our vision of connecting personal medical data to scientific literature as well as to emerging grey literature (e.g., community forums) to help people find and understand information relevant to complex medical journeys. We focus on robust combinations of natural language processing along with linked data and knowledge representation to build knowledge graphs that help people make sense of current conditions and enable new manners of scientific hypothesis generation. We present our work in the context of a breast cancer use case. We discuss the benefits of biomedical linked data resources and describe some potential assistive technology for navigating rich, diverse medical content.", "venue": "LISC@ISWC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ld", "nlp", "kg", "ld"], "mention_counts": {"ld": 2, "nlp": 1, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"ld": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "6ed60bb26df2b946368ed0c73b55be1105fba0a1", "url": "https://www.semanticscholar.org/paper/6ed60bb26df2b946368ed0c73b55be1105fba0a1", "title": "Implementing Augmented Intelligence In Systems Engineering", "abstract": "This paper will explore the opportunities for artificial intelligence (AI) in the system engineering domain, particularly in ways that unite the unique capabilities of the systems engineer with the AI. This collaboration of human and machine intelligence is known as Augmented Intelligence (AuI). There is little doubt that systems engineering productivity could be improved with effective utilization of well-established AI techniques, such as machine learning, natural language processing, and statistical models. However, human engineers excel at many tasks that remain difficult for AIs, such as visual interpretation, abstract pattern matching, and drawing broad inferences based on experience. Combining the best of AI and human capabilities, along with effective human/machine interactions and data visualization, offers the potential for orders-of-magnitude improvements in the speed and quality of delivered. INTRODUCTION Augmented Intelligence (AuI), an approach that promotes \u201cteam play\u201d of human and machine intelligence, is a modern refinement of established AI approaches. By effectively joining the human skills in pattern matching, unstructured data, and intuition with computational approaches that excel in domain search, systematic trade space exploration, and statistical evaluation, the combined \u201cteam\u201d has been proven to be more effective than either in isolation. For instance, machine learning algorithms can process past system designs, learn significant design characteristics, and visually present outcomes and the various tradeoffs. The human team can evaluate the domain space quickly, and watch for exceptional cases that might not be accurately handled by the machine. This paper will explore the potential, challenges, and requirements of implementing AuI in the engineering of systems. AuI has been enabled by the adoption of Model Based Systems Engineering (MBSE), and particularly the use of formal modeling languages such as the Systems Modeling Language (SysML), UML, Architecture Analysis and Design Language (AADL), etc. Previous document-centric approaches to systems engineering resulted in less well-defined systems that, while generally intelligible to human readers, were too unstructured for algorithmic approaches. The model-centric approach, using SysML, is ideal for AuI, since SysML was designed to be both human and machine readable. For the human, there is a concrete visual Proceedings of the 2018 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) IMPLEMENTING AUGMENTED INTELLIGENCE IN SYSTEMS ENGINEERING Page 2 of 15 representation easily interpreted by a human (e.g. the familiar SysML block diagram, containing \u201cboxes and arrows\u201d). For the machine, SysML has a precise semantic representation that allows for simulation and model execution. Together, these two representations make SysML well-suited as a language for enabling AuI. The innovation of systems is a prime opportunity for the application AuI, given the rapid increase in product complexity and time constraints. As an example, the relationship between the requirements of a system and the functional performance (e.g. SWAP-C: space weight, power, and cooling) is fundamentally mapping, traceability, and parametric relationships between requirements and design parameters. In other words, if a requirement is changed, what is the impact on functionality and performance? Even with MBSE, this is still a manual process of evaluating the models, collecting the inputs of domain experts, and determining the impact. In contrast, AuI offers the ability to use every past system as inputs to machine learning algorithms. The design team can quickly visualize the opportunities and the challenges of those design decisions. Not just one course of action can be evaluated, but every course of action could be visualized, evaluated, and communicated to the design team, subject more to computational constraints rather than time constraints. MODEL-BASED SYSTEMS ENGINEERING (MBSE) The growing complexity of systems necessitates a systems engineering approach. It requires a systems paradigm which is interdisciplinary, leverages principals common to all complex systems, and applies the requisite physics-based and mathematical models to represent them. INCOSE defines Model-Based Systems Engineering (MBSE) as \u201cthe formalized application of modeling to support system requirements, design, analysis, verification and validation activities beginning in the conceptual design phase and continuing throughout development and later life cycle phases...\u201d [1] The Object Management Group\u2019s MBSE wiki notes that \u201cModeling has always been an important part of systems engineering to support functional, performance, and other types of engineering analysis.\u201d[2] The application of MBSE has increased dramatically in recent years and is becoming a standard practice. This has been enabled by the continued maturity of modeling languages such as SysML and significant advancements made by tool vendors. These advancements are improving communications and providing a foundation to integrate diverse models. MBSE is often discussed as being composed of three fundamental elements \u2013 tool, language and method. The third element, method, has not always been given proper consideration. Because the language and tool are relatively method independent, it is methodology which further differentiates the effectiveness of any MBSE approach and its ability to help manage the complex and interrelated functionality of today\u2019s systems. For the approach discussed in this paper, the \u201cmethodology\u201d includes the application of Artificial Intelligence to augments the application of Systems Engineering activities. ARTIFICIAL INTELLIGENCE Artificial Intelligence (AI) is the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages. Since the term AI is widely used for a variety of different algorithms and approaches, most definitions are functional rather than technical. For instance, according to Stanford AI researcher Jon McCarthy, \u201cAny Proceedings of the 2018 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) IMPLEMENTING AUGMENTED INTELLIGENCE IN SYSTEMS ENGINEERING Page 3 of 15 program can be considered AI if it does something that we would normally think of as intelligent in humans\u201d [3]. There are two primary categories of AI: \uf0b7 Rules Based: Rules based AI uses a set of defined rules to derive and manipulate data. This represents explicit knowledge that can be provided to the AI system. For instance, in lexical parsing of the English language for natural language processing, grammatical rules can be defined (e.g. proper nouns are capitalized, as is the first word in a sentence). Rules based is also used for solvers, such as navigation algorithms that minimize travel time while following road rules (e.g. turns are permitted at certain intersections). \uf0b7 Patterns Based: Usually known as Machine Learning (ML), patterns based approaches seek to capture tacit knowledge knowledge which is difficult or impractical to explicitly define through statistical approaches. For instance, it is difficult to completely list rules for email spam filtering. However, with a set of emails categorized as either spam and not spam, an algorithm can infer certain words or phrases that are effective predictors of spam for classification of new emails. Similarly, autonomous systems can learn appropriate driving techniques by observing a human driver. Many modern systems are hybrids of the rules and pattern approaches. For instance, chess AI systems use rules (e.g. 6 piece types, each with a small set of moves) for traversing the game tree, along with learned patterns to preemptively eliminate certain branches as poor moves. Proceedings of the 2018 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) IMPLEMENTING AUGMENTED INTELLIGENCE IN SYSTEMS ENGINEERING Page 4 of 15 HUMAN / AI INTERACTIONS Competition Chess was one of the first significant applications of AI. It was an ideal game to test the capabilities of AI, since it is a zero-sum, perfect information two-person game with a small set of rules. Zero-sum refers to the fact that an advantageous move by player A is disadvantageous to player B. Perfect information means that all positions for each player are perfectly visible (in contrast to other games like poker, where some information is hidden, and there is a role for \u201cbluffing\u201d). Chess is also a Markov process, where each arrangement of chess pieces can be evaluated independently of the moves that created that arrangement. These characteristics allowed for effective implementations of state space search algorithms, and comparisons of performance of human player versus machine [4]. Figure 1 shows the evolution of chess AIs as ranked on the ELO scale, a method for calculating the relative skill levels of players in zero-sum games such as chess (named after its creator, physicist Arpad Elo) from 1984 to 2016. On the second Y axis is MIPS (millions of instructions per second) per $100, the affordability of computational power shown in logarithmic units. As computational power grows exponentially, the chess Figure 1 Growth of chess AI capabilities as measured on the ELO ranking scale of zero-sum games. Human grandmaster performance is shown in dashed-red. The line is approximately horizontal, as human performance changes very little on the scale required to show AI growth. Proceedings of the 2018 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) IMPLEMENTING AUGMENTED INTELLIGENCE IN SYSTEMS ENGINEERING Page 5 of 15 AI performance grew linearly, characteristic of tree search algorithms. Shown in red is the best human grandmaster performance, which changes very little during this timeframe. In 1996, IBM\u2019s supercomputer Deep Blue pla", "venue": "INCOSE International Symposium", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "kg", "ke"], "mention_counts": {"nlp": 2, "kg": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "120661fc48cad07cfb64f0ed01f12131f6ae8e55", "url": "https://www.semanticscholar.org/paper/120661fc48cad07cfb64f0ed01f12131f6ae8e55", "title": "SEMCL: A Cross-Language Semantic Model for Knowledge Sharing", "abstract": "To promote global knowledge sharing, one should solve the problem that knowledge representation in diverse natural languages restricts knowledge sharing effectively. Traditional knowledge sharing models are based on natural language processing NLP technologies. The ambiguity of natural language is a problem for NLP; however, semantic web technologies can circumvent the problem by enabling human authors to specify meaning in a computer-interpretable form. In this paper, the authors propose a cross-language semantic model SEMCL for knowledge sharing, which uses semantic web technologies to provide a potential solution to the problem of ambiguity. Also, this model can match knowledge descriptions in diverse languages. First, the methods used to support searches at the semantic predicate level are given, and the authors present a cross-language approach. Finally, an implementation of the model for the general engineering domain is discussed, and a scenario describing how the model implementation handles semantic cross-language knowledge sharing is given.", "venue": "Int. J. Knowl. Syst. Sci.", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "nlp", "sw", "nlp"], "mention_counts": {"nlp": 3, "sw": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "9d2ae7bf25b1da93014a1b6e6322e73efd7a5e46", "url": "https://www.semanticscholar.org/paper/9d2ae7bf25b1da93014a1b6e6322e73efd7a5e46", "title": "An ontological model for representing computational lexicons a componential based approach", "abstract": "In the last decades the computational linguistics community has developed important and widely used lexical resources. Although they are very popular among the Natural Language Processing (NLP) community, they do not address two important characteristics of language. The first is that the meaning of a word in a language is a collective effort defined by the people who use the language. The second is that language is a dynamic entity (some words change their meaning, others become obsolete, new words are born). A computational model which aims to represent this real world entity should be structured in a way that allows for expansion, facilitates collaboration, and provides transparent meaning representation. This paper addresses these two issues and provides a solution based on Semantic Web technologies. The solution is based on an ontological model for representing computational lexicons using the field theory of semantics and componential analysis. The model has been implemented on the \u201cTime\u201d semantic field vocabulary of the Arabic language and the results of a preliminary evaluation are presented.", "venue": "Proceedings of the 6th International Conference on Natural Language Processing and Knowledge Engineering(NLPKE-2010)", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "sw", "onto", "nlp"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "58675c9e8160361a386950c57d5a53cc2f223a56", "url": "https://www.semanticscholar.org/paper/58675c9e8160361a386950c57d5a53cc2f223a56", "title": "Information Retrieval in Kannada using Ontology", "abstract": "As Internet technology has become a part of the lifestyle of the common man, research efforts are extensively made in the fields of Natural Language Processing (NLP) and Information Retrieval. Studying regional languages for developing the system to store, retrieve, extract the information from the database has gained lots of prominence nowadays. Case studies show that Ontological Information Retrieval has many advantages over keyword-based approach. In this paper we have focused on the general architecture of ontology-based Information Retrieval used for Kannada.", "venue": "2019 4th International Conference on Computational Systems and Information Technology for Sustainable Solution (CSITSS)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "b1d09f1917479d1fd66fbe10207a4f1345d43072", "url": "https://www.semanticscholar.org/paper/b1d09f1917479d1fd66fbe10207a4f1345d43072", "title": "Ontology-guided Job Market Demand Analysis: A Cross-Sectional Study for the Data Science field", "abstract": "The rapid changes in the job market, including a continuous year-on-year increase in new skills in sectors like information technology, has resulted in new challenges for job seekers and educators alike. The former feel less informed about which skills they should acquire to raise their competitiveness, whereas the latter are inadequately prepared to offer courses that meet the expectations by fast-evolving sectors like data science. In this paper, we describe efforts to obtain job demand data and employ a information extraction method guided by a purposely-designed vocabulary to identify skills requested by the job vacancies. The Ontology-based Information Extraction (OBIE) method employed relies on the Skills and Recruitment Ontology (SARO), which we developed to represent job postings in the context of skills and competencies needed to fill a job role. Skill demand by employers is then abstracted using co-word analysis based on a set of skill keywords and their co-occurrences in the job posts. This method reveals the technical skills in demand together with their structure for revealing significant linkages. In an evaluation, the performance of the OBIE method for automatic skill annotation is estimated (strict F-measure) at 79%, which is satisfactory given that human inter-annotator agreement was found to be automatic keyword indexing with an overall strict F-measure at 94%. In a secondary study, sample skill maps generated from the matrix of co-occurrences and correlation are presented and discussed as proof-of-concept, highlighting the potential of using the extracted OBIE data for more advanced analysis that we plan as future work, including time series analysis.", "venue": "International Conference on Semantic Systems", "citationCount": 26, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "ie"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "8dbc33374d4e54ff96696193063f17269a211c6a", "url": "https://www.semanticscholar.org/paper/8dbc33374d4e54ff96696193063f17269a211c6a", "title": "Word and sentence embedding tools to measure semantic similarity of Gene Ontology terms by their definitions", "abstract": "The Gene Ontology (GO) database contains GO terms that describe biological functions of genes. Previous methods for comparing GO terms have relied on the fact that GO terms are organized into a tree structure. In this paradigm, the locations of two GO terms in the tree dictate their similarity score. In this paper, we introduce two new solutions for this problem, by focusing instead on the definitions of the GO terms. We apply neural network based techniques from the natural language processing (NLP) domain. The first method does not rely on the GO tree, whereas the second indirectly depends on the GO tree. In our first approach, we compare two GO definitions by treating them as two unordered sets of words. The word similarity is estimated by a word embedding model that maps words into an N-dimensional space. In our second approach, we account for the word-ordering within a sentence. We use a sentence encoder to embed GO definitions into vectors and estimate how likely one definition entails another. We validate our methods in two ways. In the first experiment, we test the model\u2019s ability to differentiate a true protein-protein network from a randomly generated network. In the second experiment, we test the model in identifying orthologs from randomly-matched genes in human, mouse, and fly. In both experiments, a hybrid of NLP and GO-tree based method achieves the best classification accuracy. Availability github.com/datduong/NLPMethods2CompareGOterms", "venue": "bioRxiv", "citationCount": 18, "fieldsOfStudy": ["Computer Science", "Medicine", "Biology"], "mentions": ["nlp", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 3, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "3702a09e2f8240f5f21b005900ed4d1bfc371ff9", "url": "https://www.semanticscholar.org/paper/3702a09e2f8240f5f21b005900ed4d1bfc371ff9", "title": "Integration of Agile Ontology Mapping towards NLP Search in I-SOAS", "abstract": "In this research paper we address the importance of Product Data Management (PDM) with respect to its contributions in industry. Moreover we also present some currently available major challenges to PDM communities and targeting some of these challenges we present an approach i.e. I-SOAS, and briefly discuss how this approach can be helpful in solving the PDM community's faced problems. Furthermore, limiting the scope of this research to one challenge, we focus on the implementation of a semantic based search mechanism in PDM Systems. Going into the details, at first we describe the respective field i.e. Language Technology (LT), contributing towards natural language processing, to take advantage in implementing a search engine capable of understanding the semantic out of natural language based search queries. Then we discuss how can we practically take advantage of LT by implementing its concepts in the form of software application with the use of semantic web technology i.e. Ontology. Later, in the end of this research paper, we briefly present a prototype application developed with the use of concepts of LT towards semantic based search.", "venue": "ArXiv", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "a8d022e338b673f4cb45220fce83124872f984b1", "url": "https://www.semanticscholar.org/paper/a8d022e338b673f4cb45220fce83124872f984b1", "title": "Ontology-Based Ambiguity Resolution of Manufacturing Text for Formal Rule Extraction", "abstract": "Manufacturing companies maintain manufacturing knowledge primarily as unstructured text. To facilitate formal use of such knowledge, previous efforts have utilized natural language processing (NLP) to classify manufacturing documents or extract manufacturing concepts/relations. However, extracting more complex knowledge, such as manufacturing rules, has been evasive due to the lack of methods to resolve ambiguities. Specifically, standard NLP techniques do not address domain-specific ambiguities that are due to manufacturing-specific meanings implicit in the text. To address this important gap, we propose an ambiguity resolution method that utilizes domain ontology as the mechanism to incorporate the domain context. We demonstrate its feasibility by extending our previously implemented manufacturing rule extraction framework. The effectiveness of the method is demonstrated by resolving all the domain-specific ambiguities in the dataset and an improvement in correct detection of rules to 70% (increased by about 13%). We expect that this work will contribute to the adoption of semantics-based technology in manufacturing field, by enabling the extraction of precise formal knowledge from text.", "venue": "Journal of Computing and Information Science in Engineering", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 3, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "f7abac50aaebe42ccf8b64e6b7355fdb72211064", "url": "https://www.semanticscholar.org/paper/f7abac50aaebe42ccf8b64e6b7355fdb72211064", "title": "The FinSim-2 2021 Shared Task: Learning Semantic Similarities for the Financial Domain", "abstract": "The FinSim-2 is a second edition of FinSim Shared Task on Learning Semantic Similarities for the Financial Domain, colocated with the FinWeb workshop. FinSim-2 proposed the challenge to automatically learn effective and precise semantic models for the financial domain. The second edition of the FinSim offered an enriched dataset in terms of volume and quality, and interested in systems which make creative use of relevant resources such as ontologies and lexica, as well as systems which make use of contextual word embeddings such as BERT[4]. Going beyond the mere representation of words is a key step to industrial applications that make use of Natural Language Processing (NLP). This is typically addressed using either unsupervised corpus-derived representations like word embeddings, which are typically opaque to human understanding but very useful in NLP applications or manually created resources such as taxonomies and ontologies, which typically have low coverage and contain inconsistencies, but provide a deeper understanding of the target domain. Finsim is inspired from previous endeavours in the Semeval community, which organized several competitions on semantic/lexical relation extraction between concepts/words. This year, 18 system runs were submitted by 7 teams and systems were ranked according to 2 metrics, Accuracy and Mean rank. All the systems beat our baseline 1 model by over 15 points and the best systems beat the baseline 2 by over 1 \u223c 3 points in accuracy.", "venue": "The Web Conference", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "fb359b010879af7683b2ba0b165bd25217d3282a", "url": "https://www.semanticscholar.org/paper/fb359b010879af7683b2ba0b165bd25217d3282a", "title": "NLP Tools for Knowledge Extraction from Italian Archaeological Free Text", "abstract": "This paper deals with the development of advanced tools and technologies for creating relevant information and suitable metadata out of textual documentation produced by Italian archaeological research. A set of Natural Language Processing tools were developed to recognize and annotate various archaeological entities in Italian language textual reports. The CIDOC CRM is the ontology chosen for encoding resulting output, allowing for a maximum degree of standardisation of the produced metadata to guarantee interoperability with archaeological information already existing in other semantically enabled digital archives. The work took place as part of the development for the TEXTCROWD platform for the European Open Science Cloud for Research Pilot Project.", "venue": "2018 3rd Digital Heritage International Congress (DigitalHERITAGE) held jointly with 2018 24th International Conference on Virtual Systems & Multimedia (VSMM 2018)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "ke": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "abc039c97e325e0787c14c28107e979316468cc2", "url": "https://www.semanticscholar.org/paper/abc039c97e325e0787c14c28107e979316468cc2", "title": "AI Accelerated Human-in-the-loop Structuring of Radiology Reports", "abstract": "Rule-based Natural Language Processing (NLP) pipelines depend on robust domain knowledge. Given the long tail of important terminology in radiology reports, it is not uncommon for standard approaches to miss items critical for understanding the image. AI techniques can accelerate the concept expansion and phrasal grouping tasks to efficiently create a domain specific lexicon ontology for structuring reports. Using Chest X-ray (CXR) reports as an example, we demonstrate that with robust vocabulary, even a simple NLP pipeline can extract 83 directly mentioned abnormalities (Ave. recall=93.83%, precision=94.87%) and 47 abnormality/normality descriptions of key anatomies. The richer vocabulary enables identification of additional label mentions in 10 out of 13 labels (compared to baseline methods). Furthermore, it captures expert insight into critical differences between observed and inferred descriptions, and image quality issues in reports. Finally, we show how the CXR ontology can be used to anatomically structure labeled output.", "venue": "AMIA", "citationCount": 8, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "2004bdfab9b497b7265791ab35922374a92be4e6", "url": "https://www.semanticscholar.org/paper/2004bdfab9b497b7265791ab35922374a92be4e6", "title": "FML-based robotic summarization agent and its application", "abstract": "In this paper, we present a summarization agent based on Fuzzy Markup Language (FML) and its application for meeting schedule data analysis. The knowledge base and rule base of FML are constructed by referring the meeting schedule ontology of Research & Development (R&D) office in National University of Tainan (NUTN) from Jan. 2011 to Jul. 2015. We propose an intelligent agent to retrieve the meeting activities of R&D Office from open meeting schedule database of NUTN. There are three categories belonging to the R&D meeting schedule ontology, including an International Affairs division, an Academic Development division, and an Industry-Academia Collaboration division. In addition, we apply the Natural Language Processing (NLP) open API for Chinese text mining and document preprocessing. Finally, the proposed FML-based summarization agent is combined with the human-friendly robot partner PALRO, produced by Fujisoft incorporated, to construct a meeting summarization robot agent. Experimental results show that the proposed agent can work effectively.", "venue": "2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "kg": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "5cff66d37da5fa7d1c456f028869a79f4e4f56bf", "url": "https://www.semanticscholar.org/paper/5cff66d37da5fa7d1c456f028869a79f4e4f56bf", "title": "Natural Language Processing to Extract Contextual Structure from Requirements", "abstract": "The automatic extraction of structure from text can be difficult for machines. Yet, the elicitation of this information can provide many benefits and opportunities for various applications. Such benefits have been identified amongst others for the area of Requirements Engineering. By assessing the Natural Language Processing for Requirement Engineering status quo and literature, a necessity for an automatic and universal approach to elicit structure from requirement and specification documents was identified. This paper outlines the first steps and results towards a modularized approach that splits the core algorithm from the text corpus as an input and underlying rule/knowledge base. This separation of functions allows for individual modification of the included parts and eases or potentially removes restrictions as well as limitations, such as input rules or the necessity for human supervision. Furthermore, contextual information and links via ontology inference can be considered that are not explicit on a textual level. The initial results of the approach show the successful extraction of structural information from requirement text, which was validated by comparing the results to human interpretations for small and public sample sets. In addition, the contextual consideration and inference via ontologies is described conceptually. At the current stage, limitations still exist regarding scalability and handling of text ambiguities, but solutions for these caveats have been developed and are being tested. Overall, the approach and results presented will be integrated and are part of a novel requirement complexity assessment framework.", "venue": "IEEE Systems Conference", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto", "kg"], "mention_counts": {"nlp": 2, "onto": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "00fd0bebd6ed789a064126123d016f49fba3b6e9", "url": "https://www.semanticscholar.org/paper/00fd0bebd6ed789a064126123d016f49fba3b6e9", "title": "The JUMP project: Domain Ontologies and Linguistic Knowledge @ Work", "abstract": "The JUMP project aims at bringing together the knowledge stored in different information systems in order to satisfy information and training needs in knowledge-intensive organisations. Electronic Performance Support Systems provide help, advices, demonstrations, or any other informative support that a user needs to the accomplishment of job tasks in her day-to-day working environment. The paper describes the JUMP framework, which is designed to offer multiple ways for the user to query the knowledge base resulting from integration of autonomous legacy systems. Semantic Web languages and technologies are used throughout the framework to represent, exchange and query the knowledge, while Natural Language Processing Techniques are implemented to understand natural language queries formulated by the user and provide consistent and satisfying results.", "venue": "SWAP", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlu", "kg", "onto", "sw"], "mention_counts": {"onto": 1, "nlu": 1, "nlp": 1, "sw": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "nlu": 1}, "ld_mention_counts": {"kg": 1, "sw": 1, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "510691428d4ad19a1278b89121564d110722b2cd", "url": "https://www.semanticscholar.org/paper/510691428d4ad19a1278b89121564d110722b2cd", "title": "Knowledge Graph Anchored Information-Extraction for Domain-Specific Insights", "abstract": "The growing quantity and complexity of data pose challenges for humans to consume information and respond in a timely manner. For businesses in domains with rapidly changing rules and regulations, failure to identify changes can be costly. In contrast to expert analysis or the development of domain-specific ontology and taxonomies, we use a task-based approach for fulfilling specific information needs within a new domain. Specifically, we propose to extract task-based information from incoming instance data. A pipeline constructed of state of the art NLP technologies, including a bi-LSTM-CRF model for entity extraction, attention-based deep Semantic Role Labeling, and an automated verb-based relationship extractor, is used to automatically extract an instance level semantic structure. Each instance is then combined with a larger, domain-specific knowledge graph to produce new and timely insights. Preliminary results, validated manually, show the methodology to be effective for extracting specific information to complete end use-cases. Introduction: The sheer growth in unstructured content is overwhelming the ability of businesses to respond effectively. For example, there are over 180,000 pages of regulation in the federal register and they are updated frequently. In the banking industry alone, the costs of staying compliant with (local to global) regulatory requirements are expected to exceed $100 billion annually by 2020 [1]. Staying on top of this depends on a combination of human and machine approaches. The goal of this study is to be able to extract and infer just enough to be able to focus human attention on the right content. For example, given news of a regulatory change, can we understand just enough to infer what businesses might be impacted and who needs to be notified. This work at a basic level is an effort to ease up this information consumption need for specific tasks in a domain. 1 ar X iv :2 10 4. 08 93 6v 1 [ cs .A I] 1 8 A pr 2 02 1", "venue": "ArXiv", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "kg", "ie", "nlp", "kg"], "mention_counts": {"nlp": 1, "kg": 2, "onto": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "d4158e2ffa0bbd3e2c890074b875b4cabefb8985", "url": "https://www.semanticscholar.org/paper/d4158e2ffa0bbd3e2c890074b875b4cabefb8985", "title": "Building an integrated requirements engineering process based on Intelligent Systems and Semantic Reasoning on the basis of a systematic analysis of existing proposals", "abstract": ": Requirements Engineering is one of the fundamental activities in the software development process and is oriented toward what should be produced. One of the development team\u2019s most common problems is a lack of communication regarding an understanding of the discourse domain and how to integrate and process excessive information originating from different sources. This may lead to errors of omission and the consequent production of incomplete and inconsistent artifacts, which will have a direct effect on the quality of the software. The use of machine learning techniques helps the development team produce successful software on the basis of the acquisition of knowledge and human experience with which to understand the domain of the application. This paper, therefore, presents a proposal for a new methodological process oriented toward the construction of a vocabulary concerning the application domain. The authors propose to do this by employing Natural Language Processing (NLP), ontologies and heuristics that will lead to the production of a Lexicon that is common to analysts and customers, both of whom will understand the universe of discourse, thus mitigating problems of completeness. This objective has been achieved by carrying out a Systematic Literature Review of the artificial intelligence techniques employed in the requirements engineering process, which led to the discovery that 41.37% use NLP, while 55.71% apply ontologies such as semantic reasoners which help solve the problem of language ambiguity, the structures in specifications or the identification of key concepts with which to establish traceability links. However, the review also showed that the problems regarding the comprehension and completeness of requirements problems have yet to be resolved.", "venue": "Journal of universal computer science (Online)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 3, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "7cb9c62172e805accd7206925582176a6e337baf", "url": "https://www.semanticscholar.org/paper/7cb9c62172e805accd7206925582176a6e337baf", "title": "MOLTO - Multilingual On-Line Translation", "abstract": "The MOLTO project aims to provide technology which can simultaneously tackle issues arising from real-time machine translation of web documents: localization to several languages, maintenance of their consistency in spite of asynchronous collaborative authoring with frequent edits, and grammatically and stylistically flawless text. Fifteen languages will be covered in the translations, including 12 of the 23 official languages of the European Union: Bulgarian, Danish, Dutch, English, Finnish, French, German, Italian, Polish, Romanian, Spanish, and Swedish. The 3 non-EU languages are Catalan, Norwegian, and Russian. Two problems have slowed down the adoption of high-quality restricted language translations: development cost for a new domain or language, and learning curve for authoring texts in a restricted language. MOLTO tools will decrease the effort of developing restricted language translators radically by using the Grammatical Framework (GF) libraries. MOLTO editing tools are now available in initial prototypes for grammarians: the web-based editor and the GF plugin for the Eclipse integrated development environment. New members of the MOLTO Enlarged EU Consortium are extending the tools to a semantic wiki platform and testing the user-friendliness of these solutions for non-expert grammar writers. MOLTO is exploring the two-way interoperability of grammars with Semantic Web conceptual models and hybrid models of combining rule-based translation systems with statistical machine translation. Data sets made available in a machine readable form, like RDF or OWL, can be used to construct a knowledge infrastructure suited to meaningful query and retrieval using natural languages. This approach is already being demonstrated with an application to the domain of cultural heritage. Combination approaches studied in MOLTO aim to integrate grammar-based and SMT models in a hybrid, robust MT system. Variants under consideration include e.g. soft integration in which phrase pairs or tree fragment pairs, generated by GF, are integrated as a discriminative probability models in a phrase-based SMT system. The testbed application for this research activity is information retrieval from patents in the pharmaceutical domain. Proceedings of the 16th EAMT Conference, 28-30 May 2012, Trento, Italy", "venue": "EAMT", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Political Science"], "mentions": ["rdf", "onto", "mt", "mt", "sw"], "mention_counts": {"mt": 2, "sw": 1, "onto": 1, "rdf": 1}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"sw": 1, "onto": 1, "rdf": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "ae541cdd6e05b69a084b911fec5403aa1ac2dbfe", "url": "https://www.semanticscholar.org/paper/ae541cdd6e05b69a084b911fec5403aa1ac2dbfe", "title": "A Semantic Approach for Automating Knowledge in Policies of Cyber Insurance Services", "abstract": "With the rapid adoption of web services, the need to protect against various threats has become imperative for organizations operating in cyberspace. Organizations are increasingly opting to get financial cover in the event of losses due to a security incident. This helps them safeguard against the threat posed to third-party services that the organization uses. It is in the organization's interest to understand the insurance requirements and procure all necessary direct and liability coverages. This helps transfer some risks to the insurance providers. However, cyber insurance policies often list details about coverages and exclusions using legalese that can be difficult to comprehend. Currently, it takes a significant manual effort to parse and extract knowledgeable rules from these lengthy and complicated policy documents. We have developed a semantically rich machine processable framework to automatically analyze cyber insurance policy and populate a knowledge graph that efficiently captures various inclusion and exclusion terms and rules embedded in the policy. In this paper, we describe this framework that has been built using technologies from AI, including Semantic Web, Modal/ Deontic Logic, and Natural Language Processing. We have validated our approach using industry standards proposed by the United States Federal Trade Commission (FTC) and applying it against publicly available policies of 7 cyber insurance vendors. Our system will enable cyber insurance seekers to automatically analyze various policy documents and make a well-informed decision by identifying its inclusions and exclusions.", "venue": "2019 IEEE International Conference on Web Services (ICWS)", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "sw", "kg", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "sw": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "sw": 1, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "c799fedd24a632688958c6f91a4f948e4334564a", "url": "https://www.semanticscholar.org/paper/c799fedd24a632688958c6f91a4f948e4334564a", "title": "Information Retrieval: Searching in the 21st Century", "abstract": "Foreword. Preface. About the Editors. List of Contributors. Introduction. 1 Information Retrieval Models ( Djoerd Hiemstra). 1.1 Introduction. 1.2 Exact Match Models. 1.3 Vector Space Approaches. 1.4 Probabilistic Approaches. 1.5 Summary and Further Reading. Exercises. References. 2 User-centred Evaluation of Information Retrieval Systems ( Pia Borlund). 2.1 Introduction. 2.2 The MEDLARS Test. 2.3 The Okapi Project. 2.4 The Interactive IR Evaluation Model. 2.5 Summary. Exercises. References. 3 Multimedia Resource Discovery ( Stefan R u ger). 3.1 Introduction. 3.2 Basic Multimedia Search Technologies. 3.3 Challenges of Automated Visual Indexing. 3.4 Added Services. 3.5 Browsing: Lateral and Geotemporal. 3.6 Summary. Exercises. References. 4 Image Users' Needs and Searching Behaviour ( Stina Westman). 4.1 Introduction. 4.2 Image Attributes and Users' Needs. 4.3 Image Searching Behaviour. 4.4 New Directions for Image Access. 4.5 Summary. Exercises. References. 5 Web Information Retrieval ( Nick Craswell and David Hawking). 5.1 Introduction. 5.2 Distinctive Characteristics of the Web. 5.3 Three Ranking Problems. 5.4 Other Web IR Issues. 5.5 Evaluation of Web Search Effectiveness. 5.6 Summary. Exercises. References. 6 Mobile Search ( David Mountain, Hans Myrhaug and Ay s e G o ker). 6.1 Introduction: Mobile Search - Why Now? 6.2 Information for Mobile Search. 6.3 Designing for Mobile Search. 6.4 Case Studies. 6.5 Summary. Exercises. References. 7 Context and Information Retrieval ( Ay s e G o ker, Hans Myrhaug and Ralf Bier). 7.1 Introduction. 7.2 What is Context? 7.3 Context in Information Retrieval. 7.4 Context Modelling and Representation. 7.5 Context and Content. 7.6 Related Topics. 7.7 Evaluating Context-aware IR Systems. 7.8 Summary. Exercises. References. 8 Text Categorisation and Genre in Information Retrieval ( Stuart Watt). 8.1 Introduction: What is Text Categorisation? 8.2 How to Build a Text Categorisation System. 8.3 Evaluating Text Categorisation Systems. 8.4 Genre: Text Structure and Purpose. 8.5 Related Techniques: Information Filtering. 8.6 Applications of Text Categorisation. 8.7 Summary and the Future of Text Categorisation. Exercises. References. 9 Semantic Search ( John Davies, Alistair Duke and Atanas Kiryakov). 9.1 Introduction. 9.2 Semantic Web. 9.3 Metadata and Annotations. 9.4 Semantic Annotations: the Fibres of the Semantic Web. 9.5 Semantic Annotation of Named Entities. 9.6 Semantic Indexing and Retrieval. 9.7 Semantic Search Tools. 9.8 Summary. Exercises. References. 10 The Role of Natural Language Processing in Information Retrieval: Searching for Meaning and Structure ( Tony Russell-Rose and Mark Stevenson). 10.1 Introduction. 10.2 Natural Language Processing Techniques. 10.3 Applications of Natural Language Processing in Information Retrieval. 10.4 Discussion. 10.5 Summary. Exercises. References. 11 Cross-Language Information Retrieval ( Daqing He and Jianqiang Wang). 11.1 Introduction. 11.2 Major Approaches and Challenges in CLIR. 11.3 Identifying Translation Units. 11.4 Obtaining Translation Knowledge. 11.5 Using Translation Knowledge. 11.6 Interactivity in CLIR. 11.7 Evaluation of CLIR Systems. 11.8 Summary and Future Directions. Exercises. References. 12 Performance Issues in Parallel Computing for Information Retrieval (Andrew MacFarlane). 12.1 Introduction. 12.2 Why Parallel IR? 12.3 Review of Previous Work. 12.4 Distribution Methods for Inverted File Data. 12.5 Tasks in Information Retrieval. 12.6 A Synthetic Model of Performance for Parallel Information Retrieval. 12.7 Empirical Examination of Synthetic Model. 12.8 Summary and Further Research. Exercises. References. Solutions to Exercises. Index.", "venue": "Information retrieval (Boston)", "citationCount": 129, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 3, "sw": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "55beb9aecfb0d9c183f36ffcfbfd4a544fa95c5c", "url": "https://www.semanticscholar.org/paper/55beb9aecfb0d9c183f36ffcfbfd4a544fa95c5c", "title": "A Framework for Fuzzy Based Semantic Web Service Publish/Subscribe", "abstract": "Semantic web service technology has a high potential for integrating heterogeneous applications. Web service capabilities are impossible to be used without having a recise service discovery mechanism. The increase in the precision of this mechanism depends upon taking QoS specification of services in service discovery process into account. Since using linguistic quantities in non-functional attribute description is popular, fuzzy logic is appropriate for being used in representation of non-functional attribute escription. In this paper, a fuzzy logic-based framework is proposed which allows publication and subscription of semantic web services alongside with maintaining its consistency with standard UDDI and application of fuzzy synthetic evaluation and multi-objective decision-making algorithms. This framework allows increasing the consistency of interpreting request and service by natural language process (NLP) and thesaurus approaches on one hand and deferring interpretation time of non-functional aspects of published service on the other hand. Moreover, the proposed framework does not try to decrease subjectivity of discovery/publication request and attempts to find the most matched service to consumer\u2019s subjective request by separately taking individual consumer\u2019s preferences into account.", "venue": "2010 IEEE Asia-Pacific Services Computing Conference", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlp", "sw", "nlp", "sw"], "mention_counts": {"nlp": 2, "sw": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "ded9577c8225e34c6674a3ec0d7758435a964917", "url": "https://www.semanticscholar.org/paper/ded9577c8225e34c6674a3ec0d7758435a964917", "title": "A domain independent semantic measure for keyword sense disambiguation", "abstract": "Understanding the user's intention is crucial in human-machine interaction. When dealing with text input, Word Sense Disambiguation (WSD) techniques play an important role. WSD techniques typically require well-formed sentences as context to operate, and predefined catalogues of word senses. However, such conditions do not always apply, such as when there is a need to disambiguate keywords from a query, or sets of tags describing any Web resource. In this paper, we propose a keyword disambiguation method based on the semantic relatedness between words and ontological terms. Taking advantage of the semantic information captured by word embeddings, our approach maps a set of input keywords to their meanings within a given target ontology. We focus on situations where the available linguistic information is very scarce, hampering natural language based approaches. Experimental results show the feasibility of our approach without previous training for target domains.", "venue": "SAC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "wsd", "wsd", "onto", "onto"], "mention_counts": {"wsd": 3, "onto": 2}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "24b09c4ec25f5ee72301cce909e9b13eed88fa37", "url": "https://www.semanticscholar.org/paper/24b09c4ec25f5ee72301cce909e9b13eed88fa37", "title": "Extracting Causal Claims from Information Systems Papers with Natural Language Processing for Theory Ontology Learning", "abstract": "The number of scientific papers published each year is growing exponentially. How can computational tools support scientists to better understand and process this data? This paper presents a softwareprototype that automatically extracts causes, effects, signs, moderators, mediators, conditions, and interaction signs from propositions and hypotheses of full-text scientific papers. This prototype uses natural language processing methods and a set of linguistic rules for causal information extraction. The prototype is evaluated on a manually annotated corpus of 270 Information Systems papers containing 723 hypotheses and propositions from the AIS basket of eight. F1results for the detection and extraction of different causal variables range between 0.71 and 0.90. The presented automatic causal theory extraction allows for the analysis of scientific papers based on a theory ontology and therefore contributes to the creation and comparison of inter-nomological networks.", "venue": "HICSS", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "ie"], "mention_counts": {"nlp": 2, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "07c5675efd584c6ff92f01ea75d1874980e0a611", "url": "https://www.semanticscholar.org/paper/07c5675efd584c6ff92f01ea75d1874980e0a611", "title": "What's in this Collection Dataset? Semantic\u00a0Annotation with GATE", "abstract": "Semantic annotations of datasets are very useful to support quality assurance, discovery, interpretability, linking and integration of datasets. However, providing such annotations manually is often a time-consuming task . If the process is to be at least partially automated and still provide good semantic annotations, precise information extraction is needed. The recognition of entity names (e.g., person, organization, location) from textual resources is the first step before linking the identified term or phrase to other semantic resources such as concepts in ontologies. A multitude of tools and techniques have been developed for information extraction. One of the big players is the text mining framework GATE (Cunningham et al. 2013) that supports annotation rules, semantic techniques and machine learning approaches. We will run GATE's default ANNIE pipeline on collection datasets to automatically detect persons, locations and time. We will also present extensions to extract organisms (Naderi et al. 2011), environmental terms, data parameters and biological processes and how to link them to ontologies and LOD resources, e.g., DBPedia (Sateli and Witte 2015). We would like to discuss the results with the conference participants and welcome comments and feedbacks on the current solution. The audience is also welcome to provide their own datasets in preparation for this session.", "venue": "Biodiversity Information Science and Standards", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "lod", "onto"], "mention_counts": {"lod": 1, "onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"lod": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "3646eab7051b932b9b41ac235317573832ec3e33", "url": "https://www.semanticscholar.org/paper/3646eab7051b932b9b41ac235317573832ec3e33", "title": "Robust neurofuzzy rule base knowledge extraction and estimation using subspace decomposition combined with regularization and D-optimality", "abstract": "A new robust neurofuzzy model construction algorithm has been introduced for the modeling of a priori unknown dynamical systems from observed finite data sets in the form of a set of fuzzy rules. Based on a Takagi-Sugeno (T-S) inference mechanism a one to one mapping between a fuzzy rule base and a model matrix feature subspace is established. This link enables rule based knowledge to be extracted from matrix subspace to enhance model transparency. In order to achieve maximized model robustness and sparsity, a new robust extended Gram-Schmidt (G-S) method has been introduced via two effective and complementary approaches of regularization and D-optimality experimental design. Model rule bases are decomposed into orthogonal subspaces, so as to enhance model transparency with the capability of interpreting the derived rule base energy level. A locally regularized orthogonal least squares algorithm, combined with a D-optimality used for subspace based rule selection, has been extended for fuzzy rule regularization and subspace based information extraction. By using a weighting for the D-optimality cost function, the entire model construction procedure becomes automatic. Numerical examples are included to demonstrate the effectiveness of the proposed new algorithm.", "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)", "citationCount": 28, "fieldsOfStudy": ["Mathematics", "Computer Science", "Medicine"], "mentions": ["ie", "kg", "kg", "ke"], "mention_counts": {"kg": 2, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "4060a19b4cf312f434a66586d33b8c7694a6c441", "url": "https://www.semanticscholar.org/paper/4060a19b4cf312f434a66586d33b8c7694a6c441", "title": "Chinese-Uyghur Bilingual Lexicon Extraction Based on Weak Supervision", "abstract": "Bilingual lexicon extraction is useful, especially for low-resource languages that can leverage from high-resource languages. The Uyghur language is a derivative language, and its language resources are scarce and noisy. Moreover, it is difficult to find a bilingual resource to utilize the linguistic knowledge of other large resource languages, such as Chinese or English. There is little related research on unsupervised extraction for the Chinese-Uyghur languages, and the existing methods mainly focus on term extraction methods based on translated parallel corpora. Accordingly, unsupervised knowledge extraction methods are effective, especially for the low-resource languages. This paper proposes a method to extract a Chinese-Uyghur bilingual dictionary by combining the inter-word relationship matrix mapped by the neural network cross-language word embedding vector. A seed dictionary is used as a weak supervision signal. A small Chinese-Uyghur parallel data resource is used to map the multilingual word vectors into a unified vector space. As the word-particles of these two languages are not well-coordinated, stems are used as the main linguistic particles. The strong inter-word semantic relationship of word vectors is used to associate Chinese-Uyghur semantic information. Two retrieval indicators, such as nearest neighbor retrieval and cross-domain similarity local scaling, are used to calculate similarity to extract bilingual dictionaries. The experimental results show that the accuracy of the Chinese-Uyghur bilingual dictionary extraction method proposed in this paper is improved to 65.06%. This method helps to improve Chinese-Uyghur machine translation, automatic knowledge extraction, and multilingual translations.", "venue": "Inf.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "mt", "ke"], "mention_counts": {"ke": 2, "mt": 1}, "nlp_mention_counts": {"ke": 2, "mt": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "e9a50a57153d259412646e7015b5f99428ec297b", "url": "https://www.semanticscholar.org/paper/e9a50a57153d259412646e7015b5f99428ec297b", "title": "User driven Information Extraction with LODIE", "abstract": "Information Extraction (IE) is the technique for transforming unstructured or semi-structured data into structured representation that can be understood by machines. In this paper we use a user-driven Information Extraction technique to wrap entity-centric Web pages. The user can select concepts and properties of interest from available Linked Data. Given a number of websites containing pages about the concepts of interest, the method will exploit (i) recurrent structures in the Web pages and (ii) available knowledge in Linked data to extract the information of interest from the Web pages.", "venue": "SEMWEB", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ld", "ie", "ie", "ld"], "mention_counts": {"ld": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"ld": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "02d0105f41b19bffbb84343ef3bebbf74cfaa6b9", "url": "https://www.semanticscholar.org/paper/02d0105f41b19bffbb84343ef3bebbf74cfaa6b9", "title": "Temporal Knowledge Extraction for Dataset Discovery", "abstract": "Linked data datasets are usually created with different data and metadata quality. This makes the exploration of these datasets a quite difficult task for the users. In this paper, we focus on improving discoverability of datasets based on their temporal characteristics. For this purpose, we identify the typology of temporal knowledge that can be observed inside data. We reuse existing temporal information extraction techniques available, and employ them to create temporal search indices. We present a particular use-case of dataset discovery based on more detailed and completed temporal descriptions for each dataset in the Czech LOD cloud based on the analyzing of the unstructured content in the literals as well as the structured properties, taking into consideration varying data and metadata quality.", "venue": "PROFILES@ISWC", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "lod", "ke", "ie"], "mention_counts": {"ld": 1, "ke": 1, "lod": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ld": 1, "ke": 1, "lod": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "7763127768c1c808f7d01d6f4c388fc64437e2ca", "url": "https://www.semanticscholar.org/paper/7763127768c1c808f7d01d6f4c388fc64437e2ca", "title": "Hypatia: An expert system proposal for documentation departments", "abstract": "Nowadays the vast amount of text-based information stored in organizations requires different approaches and new tools in order to manage it adequately. This paper presents Hypatia, a support expert system for documentation departments and regular users that exploits not only local information, but also external resources from the Web (e.g., Linked Data). The expert system uses different modules: Natural Language Processing (NLP) analysis, categorization, semantic disambiguation, Automatic Query Expansion (AQE), semantic search, summarization, knowledge extraction, and aggregation. Users can interact with the expert system in different ways, varying from giving very specific orders to writing a simple list of keywords. The latter method requires a previous interpretation before deciding the response of the system. The obtained results will benefit from semantic links referencing complementary data to improve both the information presentation and the data navigation.", "venue": "2014 IEEE 12th International Symposium on Intelligent Systems and Informatics (SISY)", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ld", "nlp", "nlp"], "mention_counts": {"ld": 1, "ke": 1, "nlp": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"ld": 1, "ke": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "a793851b509adb9482f5ede24fd1544f83305582", "url": "https://www.semanticscholar.org/paper/a793851b509adb9482f5ede24fd1544f83305582", "title": "Within-Document Event Coreference with BERT-Based Contextualized Representations", "abstract": "Event coreference continues to be a challenging problem in information extraction. With the absence of any external knowledge bases for events, coreference becomes a clustering task that relies on effective representations of the context in which event mentions appear. Recent advances in contextualized language representations have proven successful in many tasks, however, their use in event linking been limited. Here we present a three part approach that (1) uses representations derived from a pretrained BERT model to (2) train a neural classifier to (3) drive a simple clustering algorithm to create coreference chains. We achieve state of the art results with this model on two standard datasets for within-document event coreference task and establish a new standard on a third newer dataset. Introduction Event linking, or event coreference resolution, is the task of recognizing mentions of the same event either within a document or across different documents. Event linking is a critical component in information extraction pipelines since the facts about an event tend to be spread over many mentions, with each mention contributing partial information. Thus, a complete picture of an event can only be produced by accumulating information across many mentions. Event linking is a challenging task due to the lexical diversity of event triggers and that, unlike entities, events typically lack explicit proper names. Moreover, unlike entity linking where named entities can often be linked to external resources such as Wikipedia or domain specific ontologies, event coreference resolution is typically based entirely on information gleaned from the documents themselves (Shen, Wang, and Han 2015; Raiman and Raiman 2018). Further complicating the study of event coreference are the widely varying criteria that have been used to create annotated datasets. Over the years, relevant information about at events has included the event type (with respect to some ontology), predicate, argument fillers, and realis status (actual event, hypothetical, future, etc.). Early annotation efforts required matching types, predicates, arguments and realis status. This precise approach improves annotator agreement and classifier performance but results in a highly conCopyright \u00a9 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. strained set of coreference judgments that conflicts with the purpose of gathering information about events that may be dispersed among mentions. More recent efforts using the Event Hopper approach for both within and across document coreference (Song et al. 2018) rely on annotators intuitions that two events are the same given looser guidelines for matching arguments and realis status. Existing methods rely on feature engineering to capture relevant aspects of the context of event mentions, which are then used to train systems to make pair-wise coreference decisions. Recent neural methods have combined static word embeddings with explicit features in scoring event mentions. With the advent of deep contextualized language representations (Vaswani et al. 2017), there is an opportunity to leverage their strengths in the task of event coreference resolution. Specifically, these approaches (1) leverage large amounts of training data to address the issue of lexical diversity among event triggers, and (2) provide a natural semantic composition model to capture local context. Here, we use BERT (Devlin et al. 2019) to generate joint contextualized representations of event pairs using the sentences in which they appear. Then, we train classifiers to score mention pairs. The resulting scores are then used to drive a clustering algorithm that produces the required set of coreferring mentions. We show that this approach paves the way for a fully neural approach that surpasses previous SoTA results on two standard event datasets and establishes a new standard for a newer dataset producing using the Event Hopper approach.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "kg", "onto", "onto"], "mention_counts": {"kg": 1, "onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "933d516666849195940b60c4017f909cc6f5fe11", "url": "https://www.semanticscholar.org/paper/933d516666849195940b60c4017f909cc6f5fe11", "title": "SEBI: An Architecture for Biomedical Image Discovery, Interoperability and Reusability Based on Semantic Enrichment", "abstract": "Images depicting key findings of research papers contain rich information derived from a wide range of biomedical experiments, e.g. charts, gels, anatomical features, and protein or DNA sequence alignments. Efficient practices for accessing biomedical images are key to allowing the timely transfer of information from the research community to peer investigators and other healthcare practitioners. Searching for images of a certain type is error prone as images are still opaque to information retrieval and knowledge extraction engines due to the absence of explicit descriptions or annotation of the image contents. Moreover, traditional biomedical search engines which search image captions for relevant keywords only offer syntactic search mechanisms without regard for the exact meaning of the query. In order to resolve these challenges and to support interoperability and reusability of biomedical images, we propose a general framework for semantic enrichment of biomedical images called SEBI. SEBI utilizes the information extracted from images as seed data to harvest new annotations from heterogeneous online biomedical resources. The framework incorporates a variety of knowledge infrastructure components and services including image feature extraction, Semantic Web data services, linked open data and the crowd-sourced annotation. Together, these resources make it possible to automatically and/or semi-automatically discover and semantically interlink new information in a manner that supports semantic search for images. Project Page: https://code.google.com/p/sebi/", "venue": "Workshop on Semantic Web Applications and Tools for Life Sciences", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["lod", "ie", "sw", "ke"], "mention_counts": {"sw": 1, "lod": 1, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "lod": 1, "ke": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "ea4d5d0f3678ff879e6b0a06d19bd3cd6cc9178a", "url": "https://www.semanticscholar.org/paper/ea4d5d0f3678ff879e6b0a06d19bd3cd6cc9178a", "title": "Ontology based data integration of NoSQL datastores", "abstract": "NoSQL databases are suitable for network traffic loads experienced by today's websites. The important attributes that make them a favorable choice are inherent distributed nature, horizontal scalability and flexibility in data models. Hence, more data is now getting handled through NoSQL databases and sensible information extraction from these data stores is becoming a requirement. Information extraction may require sourcing data from multiple data sources, establishing relationship among them and querying across these data sources together. Ontology based semantic integration systems for Relational Database Management Systems (RDBMS) already exist that satisfies the above requirement. Many commercial systems are operational based on the above technique and tools and solutions for the above approach are very much mature. Hence, mapping the processing done in each stage of RDBMS based semantic integration systems to that of NoSQL systems can facilitate usage of existing tools and frameworks. The purpose of this work is to develop ontology based semantic integration system for a column-oriented NoSQL data store like HBase which is similar in architectural design to RDBMS.", "venue": "International Conference on Industrial and Information Systems", "citationCount": 18, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "onto"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "c882379c934807dddd93c65646aad26a654cbd3e", "url": "https://www.semanticscholar.org/paper/c882379c934807dddd93c65646aad26a654cbd3e", "title": "Extraction of knowledge from high strength steel data using soft computing techniques\u2014an overview", "abstract": "Soft computing techniques like artificial neural network, fuzzy logic and genetic algorithm are used to extract knowledge from experimentally developed data on the mechanical properties of thermomechanically processed high\u2010strength steel. Though these techniques, in most case, are used for developing models or optimizing a system, here the additional factor of gathering better understanding of the steel system, under investigation, has also been targeted. The extracted information has been validated by the existing concepts of physical metallurgy of steel. It is seen that these tools have the capability to confirm some of the hypotheses generated through experimentation and could easily be utilized for designing the steel with superior and/or tailor\u2010made properties. Copyright \u00a9 2009 Wiley Periodicals, Inc., A Wiley Company", "venue": "Stat. Anal. Data Min.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ie"], "mention_counts": {"ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "2524f490886d196be2f8052d73a1e43571449015", "url": "https://www.semanticscholar.org/paper/2524f490886d196be2f8052d73a1e43571449015", "title": "Probabilistic models for topic learning from images and captions in online biomedical literatures", "abstract": "Biomedical images and captions are one of the major sources of information in online biomedical publications. They often contain the most important results to be reported, and provide rich information about the main themes in published papers. In the data mining and information retrieval community, there has been much effort on using text mining and language modeling algorithms to extract knowledge from the text content of online biomedical publications; however, the problem of knowledge extraction from biomedical images and captions has not been fully studied yet. In this paper, a hierarchical probabilistic topic model with background distribution (HPB) is introduced to uncover the latent semantic topics from the co-occurrence patterns of caption words, visual words and biomedical concepts. With downloaded biomedical figures, restricted captions are extracted with regard to each individual image panel. During the indexing stage, the 'bag-of-words' representation of captions is supplemented by an ontology-based concept indexing to alleviate the synonym and polysemy problems. As the visual counterpart of text words, the visual words are extracted and indexed from corresponding image panels. The model is estimated via collapsed Gibbs sampling algorithm. We compare the performance of our model with the extension of the Correspondence LDA (Corr-LDA) model under the same biomedical image annotation scenario using cross-validation. Experimental results demonstrate that our model is able to accurately extract latent patterns from complicated biomedical image-caption pairs and facilitate knowledge organization and understanding in online biomedical literatures.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 20, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "onto"], "mention_counts": {"ke": 2, "onto": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "bc53a774db868d194dd82b19fdc6512467b48295", "url": "https://www.semanticscholar.org/paper/bc53a774db868d194dd82b19fdc6512467b48295", "title": "SIGIR 2017 Workshop on Open Knowledge Base and Question Answering (OKBQA2017)", "abstract": "Over the past years, several challenges and calls for research projects have pointed out the dire need for pushing natural language interfaces. In this context, the importance of Semantic Web data as a premier knowledge source is rapidly increasing. But we are still far from having accurate natural language interfaces that allow handling complex information needs in a user-centric and highly performant manner. The development of such interfaces requires collaboration of a range of different fields, including natural language processing, information extraction, knowledge base construction and population, reasoning, and question answering. With the goal to join forces in the collaborative development of natural language QA systems, the second OKBQA workshop is organized within the 40th SIGIR conference.", "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlp", "kg", "kg", "ie"], "mention_counts": {"nlp": 1, "sw": 1, "kg": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "kg": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "188d5f7a24896eeb5a6b43f0a3531c6af5ebafb4", "url": "https://www.semanticscholar.org/paper/188d5f7a24896eeb5a6b43f0a3531c6af5ebafb4", "title": "Joint Extraction of Triple Knowledge Based on Relation Priority", "abstract": "Triple knowledge extraction from texts is an important type of information extraction and plays a crucial role in such domains as knowledge base construction. To do so, most of existing methods usually first extract entities or entity pairs, which may lead to the redundant combination problem of entities and cannot tackle with the overlapping problem. To solve these problems, this paper proposes a new model, called RFTE, based on relation priority. The model first classifies texts according to the classification of relations to be extracted, then combines the predicted relations with texts to perform entity recognition based on the sequence labelling technique, and finally combines the extracted head and tail entities and the corresponding relations to obtain triple knowledge. Our model decouples entity recognition under different types of relations and significantly reduces the impact of the large search space. In addition, this paper also employs data augmentation and rules to further promote the performance of the model. Experiments on three benchmark datasets show that the proposed model successfully overcomes the overlapping problem and significantly outperforms the traditional methods.", "venue": "2020 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "kg", "kg"], "mention_counts": {"ke": 1, "kg": 2, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "kg": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "3ec1f41d7521d6bfa4f72d367b03c3a3fc6bed1a", "url": "https://www.semanticscholar.org/paper/3ec1f41d7521d6bfa4f72d367b03c3a3fc6bed1a", "title": "Unleashing the Power of Knowledge Extraction from Scientific Literature in Catalysis", "abstract": "Valuable knowledge of catalysis is often hidden in a large amount of scientific literature. There is an urgent need to extract useful knowledge to facilitate scientific discovery. This work takes the first step toward the goal in the field of catalysis. Specifically, we construct the first information extraction benchmark data set that covers the field of catalysis and also develop a general extraction framework that can accurately extract catalysis-related entities from scientific literature with 90% extraction accuracy. We further demonstrate the feasibility of leveraging the extracted knowledge to help users better access relevant information in catalysis through an entity-aware search engine and a correlation analysis system.", "venue": "Journal of Chemical Information and Modeling", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "ie"], "mention_counts": {"ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "f7b902e5eb13a5680a624141a56598160dfe4169", "url": "https://www.semanticscholar.org/paper/f7b902e5eb13a5680a624141a56598160dfe4169", "title": "Natural Language Processing Techniques for Document Classification in IT Benchmarking - Automated Identification of Domain Specific Terms", "abstract": "In the domain of IT benchmarking collected data are often stored in natural language text and therefore intrinsically \n \nunstructured. To ease data analysis and data evaluations across different types of IT benchmarking \n \napproaches a semantic representation of this information is crucial. Thus, the identification of conceptual (semantical) \n \nsimilarities is the first step in the development of an integrative data management in this domain. As \n \nan ontology is a specification of such a conceptualization an association of terms, relations between terms and \n \nrelated instances must be developed. Building on previous research we present an approach for an automated \n \nterm extraction by the use of natural language processing (NLP) techniques. Terms are automatically extracted \n \nout of existing IT benchmarking documents leading to a domain specific dictionary. These extracted terms are \n \nrepresentative for each document and describe the purpose and content of each file and server as a basis for \n \nthe ontology development process in the domain of IT benchmarking.", "venue": "International Conference on Enterprise Information Systems", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "43db87373b5101f04905e05bca93ff619c63aa7f", "url": "https://www.semanticscholar.org/paper/43db87373b5101f04905e05bca93ff619c63aa7f", "title": "Research review on key techniques of topic-based news elements extraction", "abstract": "With the development of computer and network techniques, and the digital Chinese news texts explosion, facing a massive unstructured news data, a better way for knowledge extraction and storage, on the one hand, can help readers understand the core content of news, on the other hand, completed news knowledge accumulation will support the reportage. In recent years, information extraction technology of Chinese text has developed rapidly, and has big progress on Named Entity recognition, Entity Relation Extraction and Event Extraction. In this paper, we propose a topic-based Elements Extraction and storage of news method that based on thematic event frame, and the relationship between the event elements is stored in the form of element expressions to organize the knowledge of news. Expressions can be used to discover and extract event elements, relational instances in the same thematic news text, realize topic-based knowledge of news extraction and storage. This paper uses a variety of Natural Language Processing technologies, including document filtering, classify, cluster, dependency parsing, etc. Based on these theories we designed and realized the topic-based Chinese news texts Event Elements Automatic Extraction and Expressions Automatic Generation System.", "venue": "2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "ke", "ie"], "mention_counts": {"nlp": 1, "kg": 1, "ke": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "2c8e9b622984905def84d2382e9ffaa54a0e093b", "url": "https://www.semanticscholar.org/paper/2c8e9b622984905def84d2382e9ffaa54a0e093b", "title": "Disease Identification in Electronic Health Records - An Ontology based Approach", "abstract": "Exploiting efficiently medical data from Electronic Health Records (EHRs) is a current joint research focus of the knowledge extraction and the medical communities. EHR structuring is essential for the efficient exploitation of the information they capture. To that end, concept identification and categorization represent key tasks. This paper presents a disease identification approach which applies several NLP document pre-processing steps, queries the SNOMED-CT ontology and then applies a filtering rule on the retrieved information. The hierarchical approach provides a better filtering of the concepts, reducing the amount of falsely identified disease concepts. We have performed a series of evaluations on the Medline abstracts dataset. The results obtained so far are promising \u00e2\u0080\u0093 our method achieves a precision of 87.79% and a recall of 87.12%, better than the results obtained by Apache\u00e2\u0080\u0099s cTAKES system on the same task and dataset.", "venue": "International Conference on Knowledge Discovery and Information Retrieval", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "nlp", "onto"], "mention_counts": {"nlp": 1, "ke": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "61831713ee0c1ff6ab33997f8dc0ccb906ea0961", "url": "https://www.semanticscholar.org/paper/61831713ee0c1ff6ab33997f8dc0ccb906ea0961", "title": "Experiments with geographic knowledge for information extraction", "abstract": "Here we present work on using spatial knowledge in conjunction with information extraction (IE). Considerable volume of location data was imported in a knowledge base (KB) with entities of general importance used for semantic annotation, indexing, and retrieval of text. The Semantic Web knowledge representation standards are used, namely RDF(S). An extensive upper-level ontology with more than two hundred classes is designed. With respect to the locations, the goal was to include the most important categories considering public and tasks not specially related to geography or related areas. The locations data is derived from number of publicly available resources and combined to assure best performance for domain-independent named-entity recognition in text. An evaluation and comparison to high performance IE application is given.", "venue": "HLT-NAACL 2003", "citationCount": 59, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "kg", "ie", "ie", "sw"], "mention_counts": {"kg": 1, "sw": 1, "onto": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"kg": 1, "sw": 1, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "655084d0ac99c58d558e994ee929d99f424e540a", "url": "https://www.semanticscholar.org/paper/655084d0ac99c58d558e994ee929d99f424e540a", "title": "Combining agents and Wrapper Induction for information gathering on restricted web domains", "abstract": "Web is growing constantly and exponentially every day. Thus, gathering relevant information becomes unfeasible. Existent indexing-based search engines ignore information context, which is essential to deciding on its relevance. Restraining to a single web domain, domain ontology can be used to take into consideration the related context, the fact that might enable treating web pages that belong to the considered domain more intelligently. Nevertheless, symbolic rules that exploit domain's ontology to realize this treatment are delicate and fastidious to develop, especially for information extraction task. This paper presents Boosted Wrapper Induction (BWI), a machine learning method for adaptive information extraction, and its exploitation as a replacement of the symbolic approach for information extraction task in AGATHE, a generic multi-agent architecture for information gathering on restrained web domains.", "venue": "Research Challenges in Information Science", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie", "ie"], "mention_counts": {"onto": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "38daea2f58b6d96a630f77bdfd38645817d6093d", "url": "https://www.semanticscholar.org/paper/38daea2f58b6d96a630f77bdfd38645817d6093d", "title": "Adapting Open Information Extraction to Domain-Specific Relations", "abstract": "Information extraction (IE) can identify a set of relations from free text to support question answering (QA). Until recently, IE systems were domain-specific and needed a combination of manual engineering and supervised learning to adapt to each target domain. A new paradigm, Open IE operates on large text corpora without any manual tagging of relations, and indeed without any pre-specified relations. Due to its open-domain and open-relation nature, Open IE is purely textual and is unable to relate the surface forms to an ontology, if known in advance. We explore the steps needed to adapt Open IE to a domain-specific ontology and demonstrate our approach of mapping domain-independent tuples to an ontology using domains from DARPA\u2019s Machine Reading Project. Our system achieves precision over 0.90 from as few as 8 training examples for an NFL-scoring domain.", "venue": "The AI Magazine", "citationCount": 76, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "458ca3f606697040c16090136bdfda8d68d8fff9", "url": "https://www.semanticscholar.org/paper/458ca3f606697040c16090136bdfda8d68d8fff9", "title": "Information extraction from HTML product catalogues: from source code and images to RDF", "abstract": "We describe an application of information extraction from company Web sites focusing on product offers. A statistical approach to text analysis is used in conjunction with different ways of image classification. Ontological knowledge is used to group the extracted items into structured objects. The results are stored in an RDF repository and made available for structured search.", "venue": "International Conference on Wirtschaftsinformatik", "citationCount": 29, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "rdf", "onto", "rdf", "ie"], "mention_counts": {"onto": 1, "rdf": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 1, "rdf": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "c45d3bf700525b8a40de42e2c8fa962560b9799c", "url": "https://www.semanticscholar.org/paper/c45d3bf700525b8a40de42e2c8fa962560b9799c", "title": "An Improved Ontology-Based Web Information Extraction", "abstract": "Since most of the Web information is extracted according to content, this research introduce Webpage segmentation into the stage of Webpage pretreatment, by analyzing the ontology-based Web information extraction technology. First, get the extraction region out of the Webpage by Webpage segmentation, second process it according to ontology extraction rules, finally get the information needed. The extraction algorithm promoted by this research have higher precision and recall rate, which will lead to good prospects in practical applications.", "venue": "2015 International Conference of Educational Innovation through Technology (EITT)", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto", "onto"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "3dcba175248d0e8d2da44e3731e4adbfb9f00e97", "url": "https://www.semanticscholar.org/paper/3dcba175248d0e8d2da44e3731e4adbfb9f00e97", "title": "Integrating Local Context and Global Cohesiveness for Open Information Extraction", "abstract": "Extracting entities and their relations from text is an important task for understanding massive text corpora. Open information extraction (IE) systems mine relation tuples (i.e., entity arguments and a predicate string to describe their relation) from sentences. These relation tuples are not confined to a predefined schema for the relations of interests. However, current Open IE systems focus on modeling local context information in a sentence to extract relation tuples, while ignoring the fact that global statistics in a large corpus can be collectively leveraged to identify high-quality sentence-level extractions. In this paper, we propose a novel Open IE system, called ReMine, which integrates local context signals and global structural signals in a unified, distant-supervision framework. Leveraging facts from external knowledge bases as supervision, the new system can be applied to many different domains to facilitate sentence-level tuple extractions using corpus-level statistics. Our system operates by solving a joint optimization problem to unify (1) segmenting entity/relation phrases in individual sentences based on local context; and (2) measuring the quality of tuples extracted from individual sentences with a translating-based objective. Learning the two subtasks jointly helps correct errors produced in each subtask so that they can mutually enhance each other. Experiments on two real-world corpora from different domains demonstrate the effectiveness, generality, and robustness of ReMine when compared to state-of-the-art open IE systems.", "venue": "Web Search and Data Mining", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ie", "ie", "ke"], "mention_counts": {"kg": 1, "ke": 1, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "d17b6e623fc7f5ca305b2111d695ad494fbb7e2f", "url": "https://www.semanticscholar.org/paper/d17b6e623fc7f5ca305b2111d695ad494fbb7e2f", "title": "OpenKI: Integrating Open Information Extraction and Knowledge Bases with Relation Inference", "abstract": "In this paper, we consider advancing web-scale knowledge extraction and alignment by integrating OpenIE extractions in the form of (subject, predicate, object) triples with Knowledge Bases (KB). Traditional techniques from universal schema and from schema mapping fall in two extremes: either they perform instance-level inference relying on embedding for (subject, object) pairs, thus cannot handle pairs absent in any existing triples; or they perform predicate-level mapping and completely ignore background evidence from individual entities, thus cannot achieve satisfying quality. We propose OpenKI to handle sparsity of OpenIE extractions by performing instance-level inference: for each entity, we encode the rich information in its neighborhood in both KB and OpenIE extractions, and leverage this information in relation inference by exploring different methods of aggregation and attention. In order to handle unseen entities, our model is designed without creating entity-specific parameters. Extensive experiments show that this method not only significantly improves state-of-the-art for conventional OpenIE extractions like ReVerb, but also boosts the performance on OpenIE from semi-structured data, where new entity pairs are abundant and data are fairly sparse.", "venue": "North American Chapter of the Association for Computational Linguistics", "citationCount": 16, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["kg", "ke", "kg", "ie"], "mention_counts": {"kg": 2, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "aa9b97f058aaa8798fa1c68e1f500fcc22484a7e", "url": "https://www.semanticscholar.org/paper/aa9b97f058aaa8798fa1c68e1f500fcc22484a7e", "title": "Adaptive Web Sites - A Knowledge Extraction from Web Data Approach", "abstract": "This book can be presented in two different ways; introducing a particular methodology to build adaptive Web sites and; presenting the main concepts behind Web mining and then applying them to adaptive Web sites. In this case, adaptive Web sites is the case study to exemplify the tools introduced in the text. The authors start by introducing the Web and motivating the need for adaptive Web sites. The second chapter introduces the main concepts behind a Web site: its operation, its associated data and structure, user sessions, etc. Chapter three explains the Web mining process and the tools to analyze Web data, mainly focused in machine learning. The fourth chapter looks at how to store and manage data. Chapter five looks at the three main and different mining tasks: content, links and usage. The following chapter covers Web personalization, a crucial topic if we want to adapt our site to specific groups of people. Chapter seven shows how to use information extraction techniques to find user behavior patterns. The subsequent chapter explains how to acquire and maintain knowledge extracted from the previous phase. Finally, chapter nine contains the case study where all the previous concepts are applied to present a framework to build adaptive Web sites. In other words, the authors have taken care of writing a self-contained book for people that want to learn and apply personalization and adaptation in Web sites. This is commendable considering the large and increasing bibliography in these and related topics. The writing is easy to follow and although the coverage is not exhaustive, the main concepts and topics are all covered.IOS Press is an international science, technical and medical publisher of high-quality books for academics, scientists, and professionals in all fields. Some of the areas we publish in: -Biomedicine -Oncology -Artificial intelligence -Databases and information systems -Maritime engineering -Nanotechnology -Geoengineering -All aspects of physics -E-governance -E-commerce -The knowledge economy -Urban studies -Arms control -Understanding and responding to terrorism -Medical informatics -Computer Sciences", "venue": "Frontiers in Artificial Intelligence and Applications", "citationCount": 59, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke"], "mention_counts": {"ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "b5930cecac0a64e2b1f2bc25f41b09bd949f7cea", "url": "https://www.semanticscholar.org/paper/b5930cecac0a64e2b1f2bc25f41b09bd949f7cea", "title": "IKE - An Interactive Tool for Knowledge Extraction", "abstract": "Recent work on information extraction has suggested that fast, interactive tools can be highly effective; however, creating a usable system is challenging, and few publically available tools exist. In this paper we present IKE, a new extraction tool that performs fast, interactive bootstrapping to develop high-quality extraction patterns for targeted relations. Central to IKE is the notion that an extraction pattern can be treated as a search query over a corpus. To operationalize this, IKE uses a novel query language that is expressive, easy to understand, and fast to execute essential requirements for a practical system. It is also the first interactive extraction tool to seamlessly integrate symbolic (boolean) and distributional (similarity-based) methods for search. An initial evaluation suggests that relation tables can be populated substantially faster than by manual pattern authoring while retaining accuracy, and more reliably than fully automated tools, an important step towards practical KB construction. We are making IKE publically available (http://allenai.org/ software/interactive-knowledge-extraction).", "venue": "AKBC@NAACL-HLT", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "ke"], "mention_counts": {"ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "b083b24a6b5acec5f0bd60bb3db4c0e5ed416d9a", "url": "https://www.semanticscholar.org/paper/b083b24a6b5acec5f0bd60bb3db4c0e5ed416d9a", "title": "Grounded Semantic Parsing for Complex Knowledge Extraction", "abstract": "Recently, there has been increasing interest in learning semantic parsers with indirect supervision, but existing work focuses almost exclusively on question answering. Separately, there have been active pursuits in leveraging databases for distant supervision in information extraction, yet such methods are often limited to binary relations and none can handle nested events. In this paper, we generalize distant supervision to complex knowledge extraction, by proposing the first approach to learn a semantic parser for extracting nested event structures without annotated examples, using only a database of such complex events and unannotated text. The key idea is to model the annotations as latent variables, and incorporate a prior that favors semantic parses containing known events. Experiments on the GENIA event extraction dataset show that our approach can learn from and extract complex biological pathway events. Moreover, when supplied with just five example words per event type, it becomes competitive even among supervised systems, outperforming 19 out of 24 teams that participated in the original shared task.", "venue": "North American Chapter of the Association for Computational Linguistics", "citationCount": 27, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke"], "mention_counts": {"ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "a51e134d54e6e69ad01330cc03b00e6c0138be48", "url": "https://www.semanticscholar.org/paper/a51e134d54e6e69ad01330cc03b00e6c0138be48", "title": "Multimedia Information Extraction in Ontology-based Semantic Annotation of Product Catalogues", "abstract": "\u2014The demand for efficient methods for extracting knowledge from multimedia content has led to a growing research community investigating the convergence of multimedia and knowledge technologies. In this paper we describe a methodology for extracting multimedia information from product catalogues empowered by the synergetic use and extension of a domain ontology. The methodology was implemented in the Trade Fair Advanced Semantic Annotation Pipeline of the VIKE-framework.", "venue": "Semantic Web Applications and Perspectives", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "onto", "onto"], "mention_counts": {"ke": 1, "onto": 2, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "3b4267ff56fc4d85a04c0a88c5790ec623b66ebd", "url": "https://www.semanticscholar.org/paper/3b4267ff56fc4d85a04c0a88c5790ec623b66ebd", "title": "Lights, Camera, Action: Knowledge Extraction from Movie Scripts", "abstract": "With the success of large knowledge graphs, research on automatically acquiring commonsense knowledge is revived. One kind of knowledge that has not received attention is that of human activities. This paper presents an information extraction pipeline for systematically distilling activity knowledge from a corpus of movie scripts. Our semantic frames capture activities together with their participating agents and their typical spatial, temporal and sequential contexts. The resulting knowledge base comprises about 250,000 activities with links to specific movie scenes where they occur.", "venue": "The Web Conference", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "kg", "kg"], "mention_counts": {"ke": 1, "kg": 2, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "kg": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "773c3c9606f1d11f2d09ac535545c560024824e3", "url": "https://www.semanticscholar.org/paper/773c3c9606f1d11f2d09ac535545c560024824e3", "title": "A TCM Diagnosis System Based on Textbook Information Extraction", "abstract": "The digitization of books can not only provide a electronic way to read but also make it possible to establish information systems via information extraction and correlation. This paper proposed a new approach that build up a traditional Chinese medicine (TCM) diagnosis system by extracting information from textbooks. First of all, we design a semantic ontology which can achieve targeted and precise extraction by combining it with SVM classification and regular expression match. After the popularization of the ontology, data and information in the database can be correlated automatically. Moreover, aiming at the domain of TCM textbooks, a new symptom identification algorithm is adopted. Eventually, a structural knowledge database is constructed and the experiment system shows that our method can be useful to provide new services for digital library.", "venue": "2011 International Conference on Internet of Things and 4th International Conference on Cyber, Physical and Social Computing", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "12b963970c280e064218194f6d5a75b7fe5b0126", "url": "https://www.semanticscholar.org/paper/12b963970c280e064218194f6d5a75b7fe5b0126", "title": "Using Ontology-based Information Extraction for Subject-based Auto-grading", "abstract": "The procedure for the grading of students\u2019 essays in subject-based examinations is quite challenging \nparticularly when dealing with large number of students. Hence, several automatic essay-grading systems \nhave been designed to alleviate the demands of manual subject grading. However, relatively few of the \nexisting systems are able to give informative feedbacks that are based on elaborate domain knowledge to \nstudents, particularly in subject-based automatic grading where domain knowledge is a major factor. In this \nwork, we discuss the vision of subject-based automatic essay scoring system that leverages on semiautomatic \ncreation of subject ontology, uses ontology-based information extraction approach to enable \nautomatic essay scoring, and gives informative feedback to students.", "venue": "KEOD", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "cb7aa9c4b4f187a35317193532c979383549e6b5", "url": "https://www.semanticscholar.org/paper/cb7aa9c4b4f187a35317193532c979383549e6b5", "title": "Automatic knowledge extraction for filling in biography forms from Turkish texts", "abstract": "This study presents a method for building an automatic knowledge extraction system for filling in biography forms from Turkish texts. Several biographies are analyzed in order to choose the set of biography categories to be studied. The fields of the biography form to be created are also defined based on this analysis. Information extraction techniques are used for implementation. A separate testing platform is designed to evaluate the accuracy of the extracted data. Results of the testing platform have shown this study to be a promising process to be further developed especially for creating forms in the Turkish language.", "venue": "Turkish Journal of Electrical Engineering and Computer Sciences", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ie"], "mention_counts": {"ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "866199a95f06997655bb74d1eccc478c43b66e55", "url": "https://www.semanticscholar.org/paper/866199a95f06997655bb74d1eccc478c43b66e55", "title": "Semantic Clinical Process Management", "abstract": "This work describes a clinical process management system aimed to support a process-centred vision of health care practices. The system is founded on knowledge representation and semantic information extraction approaches allowing medical knowledge modelling and acquisition. At the heart of the system there are formalisms and languages well suited for representing, clinical processes as workflows, medical and domain knowledge as ontologies and rules enabling the recognition of semantic patterns representing ontology concepts. The system acquires and stores clinical process instances into a medical knowledge base which parameters are obtained exploiting a semantic information extraction approach enabling automatic medical knowledge acquisition from unstructured clinical documents. The main goal of the system is to assists health care professional in executing and monitoring clinical processes by providing functionalities for automatic knowledge acquisition. Acquired information can be analyzed for identifying main causes of medical errors, high costs and, potentially, to suggest clinical processes restructuring or improvement able to enhance cost control and patient safety.", "venue": "Twentieth IEEE International Symposium on Computer-Based Medical Systems (CBMS'07)", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ie", "onto", "onto", "ie"], "mention_counts": {"kg": 1, "onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "83b2f3f334036ff792cdf1f8d1dff183813fae1e", "url": "https://www.semanticscholar.org/paper/83b2f3f334036ff792cdf1f8d1dff183813fae1e", "title": "An Ontology-Based Approach for Key Phrase Extraction", "abstract": "Automatic key phrase extraction is fundamental to the success of many recent digital library applications and semantic information retrieval techniques and a difficult and essential problem in Vietnamese natural language processing (NLP). In this work, we propose a novel method for key phrase extracting of Vietnamese text that exploits the Vietnamese Wikipedia as an ontology and exploits specific characteristics of the Vietnamese language for the key phrase selection stage. We also explore NLP techniques that we propose for the analysis of Vietnamese texts, focusing on the advanced candidate phrases recognition phase as well as part-of-speech (POS) tagging. Finally, we review the results of several experiments that have examined the impacts of strategies chosen for Vietnamese key phrase extracting.", "venue": "ACL/IJCNLP", "citationCount": 20, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "92a8739cc608b82fb6019b0555eec58f7008a719", "url": "https://www.semanticscholar.org/paper/92a8739cc608b82fb6019b0555eec58f7008a719", "title": "Ontology-Driven Information-Providing Dialogue Systems", "abstract": "In this paper we propose an architecture for dialogue systems that incorporate information extraction, thereby providing natural and efficient access to unstructured information sources. A key feature of the architecture is the use of ontologies as shared domain knowledge sources. We discuss how ontologies can be used for various tasks. with focus on question analysis, dialogue interaction and information extraction.", "venue": "AMCIS", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "9f51cd825e405757475f3619fac919584cde089e", "url": "https://www.semanticscholar.org/paper/9f51cd825e405757475f3619fac919584cde089e", "title": "Extracting ontology concept hierarchies from text using Markov logic", "abstract": "Ontologies have proven to be a powerful tool for many tasks such as natural language processing and information filtering and retrieval. However their development is an error prone and expensive task. One approach for this problem is to provide automatic or semi-automatic support for ontology construction. This work presents the Probabilistic Relational Hierarchy Extraction (PREHE) technique, an approach for extracting concept hierarchies from text that uses statistical relational learning and natural language processing for combining cues from many state-of-the-art techniques. A Markov Logic Network has been developed for this task and is described here. A preliminary evaluation of the proposed approach is also outlined.", "venue": "ACM Symposium on Applied Computing", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "18c110ddf74437f3873127fc41b9cabb096c93aa", "url": "https://www.semanticscholar.org/paper/18c110ddf74437f3873127fc41b9cabb096c93aa", "title": "Ontology-Driven Discourse Analysis in GenIE", "abstract": "This paper presents a novel approach to discourse analysis within information extraction systems. It makes use of DRT as formal representation of the linguistic context as well as of a domain-specific ontology as a basis to compute conceptual relations between extracted events thus establishing discourse coherence. The approach has been implemented within GenIE, an information extraction system with the aim of extracting information about biochemical pathways, about sequences, structures and functions of genomes and proteins. The approach is evaluated against a semantically hand-annotated set of Swiss-Prot protein function descriptions and shows very promising results.", "venue": "NLDB", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "ie", "onto"], "mention_counts": {"onto": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "0a253c3cac5dd8dbd492e1df1cf79829a796346b", "url": "https://www.semanticscholar.org/paper/0a253c3cac5dd8dbd492e1df1cf79829a796346b", "title": "Data-Driven and Ontological Analysis of FrameNet for Natural Language Reasoning", "abstract": "This paper focuses on the improvement of the conceptual structure of FrameNet (FN) for the sake of applying this resource to knowledge-intensive NLP tasks requiring reasoning, such as question answering, information extraction etc. In this paper we show that in addition to coverage incompleteness, the current version of FN suffers from conceptual inconsistency and lacks axiomatization which can prevent appropriate inferences. For the sake of discovering and classifying conceptual problems in FN we investigate the FrameNet-Annotated corpus for Textual Entailment. Then we propose a methodology for improving the conceptual organization of FN. The main issue we focus on in our study is enriching, axiomatizing and cleaning up frame relations. Our methodology includes a data-driven analysis of frames resulting in discovering new frame relations and an ontological analysis of frames and frame relations resulting in axiomatizing relations and formulating constraints on them. In this paper, frames and frame relations are analyzed in terms of the DOLCE formal ontology. Additionally, we have described a case study aiming at demonstrating how the proposed methodology works in practice as well as investigating the impact of the restructured and axiomatized frame relations on recognizing textual entailment.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 45, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "ie"], "mention_counts": {"nlp": 1, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "9d2e83e86442e69c6612b300b7cd5cd99a73abe6", "url": "https://www.semanticscholar.org/paper/9d2e83e86442e69c6612b300b7cd5cd99a73abe6", "title": "Ontology-Based Extraction and Summarization of Protein Mutation Impact Information", "abstract": "NLP methods for extracting mutation information from the bibliome have become an important new research area within bio-NLP, as manually curated databases, like the Protein Mutant Database (PMD) (Kawabata et al., 1999), cannot keep up with the rapid pace of mutation research. However, while significant progress has been made with respect to mutation detection, the automated extraction of the impacts of these mutations has so far not been targeted. In this paper, we describe the first work to automatically summarize impact information from protein mutations. Our approach is based on populating an OWL-DL ontology with impact information, which can then be queried to provide structured information, including a summary.", "venue": "BioNLP@ACL", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "01f7fd5d20f23c680b4a19e1dbe2cc2d763f7dc1", "url": "https://www.semanticscholar.org/paper/01f7fd5d20f23c680b4a19e1dbe2cc2d763f7dc1", "title": "A hybrid statistical and semantic model for identification of mental health and behavioral disorders using social network analysis", "abstract": "The advent of social networking and open health web forums such as PatientsLikeMe, WebMD, ehealth forum etc. have provided avenues for social user data that can prove instrumental in suggesting futuristic trends in healthcare. Homophily in social networks is a vital contributor for analyzing patterns for medical conditions, diagnosis and treatment options. Since, members with similar medical issues contribute to a common discussion pool; this offers a rich source of information that can be utilized. This paper intends to explore growing trends in Mental Health and Behavioral Studies (MHB) which lays emphasis on co-existing conditions resulting in comorbidity. We present a novel approach where personality traits inferred from unstructured text of patients and general social users are compared via statistical analysis. This is achieved by our Psychiatric Disorder Determination (PDD) algorithm. Further, Social media data of users showing personality traits of patients is subjected to semantic based text classification using Natural Language Processing (NLP) and Ontology Based Information Extraction (OBIE) in our Addiction Category Determination (ACD) algorithm. This provides categorization of user journals to common topics of discussion by referring to ontologies DBpedia, Freebase and YAGO2s. The final category hence obtained can be predicted to be a trending subject of concern for users with Psychiatric disorders developing Addictive behavioral personalities.", "venue": "International Conference on Advances in Social Networks Analysis and Mining", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "4e675a3ad80d8e79b0b2c826287b6d66ef56775e", "url": "https://www.semanticscholar.org/paper/4e675a3ad80d8e79b0b2c826287b6d66ef56775e", "title": "InPhO: a system for collaboratively populating and extending a dynamic ontology", "abstract": "InPhO is a system that combines statistical text processing, information extraction, human expert feedback, and logic programming to populate and extend a dynamic ontology for the field of philosophy. Integrated in the editorial workflow of the Stanford Encyclopedia of Philosophy (SEP), it will provide important metadata features such as automated generation of cross-references, semantic search, and ontology driven conceptual navigation.", "venue": "ACM/IEEE Joint Conference on Digital Libraries", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["tp", "onto", "onto", "onto", "ie"], "mention_counts": {"onto": 3, "tp": 1, "ie": 1}, "nlp_mention_counts": {"tp": 1, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "0368990ceb541619384770788d454c0683adfc3c", "url": "https://www.semanticscholar.org/paper/0368990ceb541619384770788d454c0683adfc3c", "title": "TELESUP - Textual Self-Learning Support Systems", "abstract": "The regular improvement and adaptation of an ontology is a key factor for the success of an ontology-based system. In this pa- per, we report on an ongoing project that aims for a methodology and tool for ontology development in a self-improving manner. The approach makes heavy use of methods known in natural language processing and information extraction.", "venue": "LWA", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "nlp", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "53b4bd88914ff732b0903f4ab485102c0f218607", "url": "https://www.semanticscholar.org/paper/53b4bd88914ff732b0903f4ab485102c0f218607", "title": "Natural language dependencies for ontological relation extraction", "abstract": "Natural Language Processing techniques play an essential role in extraction of necessary information for ontology construction from unstructured text. Identifying syntactic constituents and their dependencies in a sentence, boost the information extraction from natural language text. Main ingredients required for ontology construction are required to be extracted from text in the form of entities and relations between them. Language dependency constructs that express the binary dependencies of the lexical terms in a sentence can be considered as good indicators in identifying binary relationships existing between entities. In this paper we describe that how the typed dependencies produced by a natural language parser are used by a multi agent system to generate rules for relation extraction between two identified entities. The typed dependencies produced by parsing are processed to eliminate unnecessary dependencies and make them more appropriate to be used in rule learning for relation extraction. All the relations derived are expressed as predicate expressions of two entities. We evaluate our agent system by applying it on number of wikipedia web pages from the domain of birds.", "venue": "International Conference on Advances in ICT for Emerging Regions", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "ie"], "mention_counts": {"nlp": 1, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "0b4cbe7104eafcdb29865ac3045fd92715c92aaa", "url": "https://www.semanticscholar.org/paper/0b4cbe7104eafcdb29865ac3045fd92715c92aaa", "title": "An ontology-based framework for extracting spatio-temporal influenza data using Twitter", "abstract": "ABSTRACT Early detection of influenza outbreaks is one of the key priorities on a national level for preparedness and planning. This study presents the design and implementation of Fluwitter, which is a spatio-temporal web-based prototype framework for pseudo real-time detection of influenza outbreaks from Twitter. Specifically, the framework integrates PostgreSQL database server with PostGIS spatial extension, Twitter streaming client, pre-processor, tagger and similarity calculator for semantic information extraction (IE). The IE of tagged terms is supported by Natural Language Processing (NLP) techniques, DBpediaSpotlight and WordNet Similarity for Java (WS4J), while data analytics, visualization, and mapping are supported by GeoServer and other GIS Free Open Source Software (FOSS). The prototype was calibrated to maximize detection of influenza using rules developed from ontology-based semantic similarity scores. The Twitter-generated influenza cases were validated by weekly hospitalization records issued by Ohio Department of Health (ODH). The optimized rule produced a final F-measure value of 0.72 and accuracy (ACC) value of 94.4%. The validation suggested the existence of moderate correlations for the beginning of the time period Southeast region (r\u2009=\u20090.52), the Northwestern region (r\u2009=\u20090.38), and the Central region (r\u2009=\u20090.33) and weak correlations for the entire time period. The potential strengths and benefits of the prototype are shown through spatio-temporal assessment and visualization of influenza potential in Ohio.", "venue": "Int. J. Digit. Earth", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "ae1dcfb71db343852512ebe649c921fec2f5a72e", "url": "https://www.semanticscholar.org/paper/ae1dcfb71db343852512ebe649c921fec2f5a72e", "title": "Um M\u00e9todo N\u00e3o Supervisionado para o Povoamento de Ontologias a partir de Fontes Textuais na Web [An Unsupervised Method for Ontology Population from Textual Sources on the Web]", "abstract": "The increasing in the production and availability of unstructured information on the Web grows daily. This abundance of unstructured information is a great challenge for acquisition of structured knowledge. Many approaches have been proposed for extracting information from texts written in natural language. However, only a few studies have investigated the extraction of information from texts written in Portuguese. Thus, this work aims to propose and evaluate an unsupervised method for ontology population using the Web as a big source of information in the context of the Portuguese language. The results of the experiments are encouraging and demonstrated that the proposed approach reached a precision rate of 67% in the instances of ontological classes extraction.", "venue": "SBSI", "citationCount": 4, "fieldsOfStudy": ["Computer Science", "Engineering"], "mentions": ["onto", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "2348bfe4de72b909ed86655039ee022285533eec", "url": "https://www.semanticscholar.org/paper/2348bfe4de72b909ed86655039ee022285533eec", "title": "Concept extraction from medical documents a contextual approach", "abstract": "Efficient and precise medical information identification from Electronic Health Records (EHRs) is an important subject for both the knowledge extraction and medical communities. This paper presents an approach for medical concept identification and categorization which applies a series of Natural Language Processing methods on unstructured EHRs, queries the SNOMED-CT medical ontology and applies three filtering rules on the query result set. The strength of our approach is that it considers contextual information from the input documents together with the hierarchical information from the medical ontology to filter out irrelevant concepts while maintaining a high accuracy for the medical concept identification. We have performed a series of evaluations on the Medline abstracts dataset. Our method reaches an average recall of 88.77% and a precision of 89.69% on this data.", "venue": "International Conference on Computational Photography", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "ke", "onto"], "mention_counts": {"nlp": 1, "onto": 2, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "17cce6e0eb878526e558fea3e0ffd8eff6145db3", "url": "https://www.semanticscholar.org/paper/17cce6e0eb878526e558fea3e0ffd8eff6145db3", "title": "Further use of Controlled Natural Language for Semantic Annotation of Wikis", "abstract": "Knowledge Acquisition through Semantic Annotation is vital to the evolution, growth and success of the Semantic Web. Both Semiautomatic and Manual Annotation are constricted by a knowledge acquisition bottleneck. Manual Semantic Annotation is a complex and arduous task both time-consuming and costly, often requiring specialist annotators. Therefore, automation of this process is essential to ease the constriction inherent to knowledge acquisition. Semi-automatic annotation tools detect instances of classes within text and relationships between classes; but their usage often requires knowledge of Natural Language Processing and/or formal ontological descriptions. However, one must o er an incentive for a user to annotate his/her respective documents in an user-friendly manner. We describe work in progress concerning the application of Controlled Language Information Extraction CLIE to a Personal Semantic Wiki SemperWiki, the goal being to permit users who have no specialist knowledge in ontology tools or languages to semi-automatically annotate their respective personal Wiki pages.", "venue": "SAAW@ISWC", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "ie", "sw", "onto"], "mention_counts": {"nlp": 1, "sw": 1, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "35bc62bd22ee5b978f2192c564c86f03a6e958ed", "url": "https://www.semanticscholar.org/paper/35bc62bd22ee5b978f2192c564c86f03a6e958ed", "title": "Ontology Driven K-Portal Construction and K-Service Provision", "abstract": "Knowledge has been crucial for the country\u2019s development and business intelligence, where valuable knowledge is distributed over several websites with heterogeneous formats. Moreover, finding the needed information is a complex task since there has been lack of semantic relation and organization. Even if it has been found, an overload may occur because there is no content digestion. This paper focuses on ontology-driven knowledge extraction with natural language processing techniques and a framework of usercentric design for accessing the required information based on their demands. These demands can be expressed in the form of Knowwhat, Know-why, Know-where, Know-when, Know-how, and Know-who for a question answering system", "venue": "LREC", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 2, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "214d6e9f7e3ebdaebb1724daf87c19017489402a", "url": "https://www.semanticscholar.org/paper/214d6e9f7e3ebdaebb1724daf87c19017489402a", "title": "Towards knowledge level privacy and security using RDF/RDFS and RBAC", "abstract": "Information privacy and security plays a major role in domains where sensitive information is handled, such as case studies of rare diseases. Currently, security for accessing any sensitive information is provided by various mechanisms at the user/system level by employing access control models such as Role Based Access Control. However, these approaches leave security at the knowledge level unattended, which can be inadequate. For example, in healthcare, ontology-based information extraction is employed for extracting medical knowledge from sensitive structured/unstructured data sources. These information extraction systems act on sensitive data sources which are protected against unauthorized access at the system level based on the user, context and permissions, but the knowledge that can be extracted from these sources is not. In this paper we tackle the security or access control at the knowledge level by presenting a model, to enforce knowledge security/access by leveraging knowledge sources (currently focused on RDF) with the RBAC model. The developed model filters out knowledge by means of binary permissions on the knowledge source, providing each user with a different view of the knowledge source.", "venue": "Proceedings of the 2015 IEEE 9th International Conference on Semantic Computing (IEEE ICSC 2015)", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "ie", "ie", "rdf", "onto"], "mention_counts": {"onto": 1, "rdf": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 1, "rdf": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "73b5302824218778bcf144bc47a1f0c01fa2c3b2", "url": "https://www.semanticscholar.org/paper/73b5302824218778bcf144bc47a1f0c01fa2c3b2", "title": "Spatial Information Extraction from Text Using Spatio-Ontological Reasoning (Short Paper)", "abstract": "This paper is involved with extracting spatial information from text. We seek to geo-reference all spatial entities mentioned in a piece of text. The focus of this paper is to investigate the contribution of spatial and ontological reasoning to spatial interpretation of text. A preliminary study considering descriptions of cities and geographical regions from English Wikipedia suggests that spatial and ontological reasoning can be more effective to resolve ambiguities in text than a classical text understanding pipeline relying on parsing. 2012 ACM Subject Classification Computing methodologies \u2192 Information extraction", "venue": "International Conference Geographic Information Science", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "ie"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "e1b7c9e7a60409efa4df055644f1c4e62504999a", "url": "https://www.semanticscholar.org/paper/e1b7c9e7a60409efa4df055644f1c4e62504999a", "title": "Ontological Knowledge for Rhetorical Move Analysis", "abstract": "Scholarly writing in the experimental biomedical sciences follows the IMRaD (Introduction, Methods, Results, and Discussion) structure. Many Biomedical Natural Language Processing tasks take advantage of this structure. Recently, a new challenging information extraction task has been introduced as a means of obtaining these types of detailed information: identifying the argumentation structure in biomedical articles. Argumentation mining can be used to validate scientic claimsand experimental methodology, and to plot deeper chains of scienti creasoning. One subtask in identifying the argumentation structure is the identication of rhetorical moves, text segments that are rhetorical and perform specic communicative goals, in the Methods section. Based on a descriptive taxonomy of rhetorical moves structured around IMRaD, the foundational linguistic knowledge needed for a computationally feasible model of the rhetorical moves is described: semantic roles. One goal is to provide FrameNet and VerbNet-like ontologies for the specializeddo main of biochemistry. Using the observation that the structure of scholarly writing in the laboratory-based experimental sciences closely follows the laboratory procedures, we focus on the procedural verbs inthe Methods section. Occasionally, the text does not contain llers forall of the semantic role slots that are needed to perform an adequate analysis of a verb. To overcome this problem, an ontology of experimental procedures can be interrogated to provide a most likely candidate for the missing semantic role(s).", "venue": "Computaci\u00f3n y Sistemas", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "nlp"], "mention_counts": {"nlp": 1, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "6c2ca6ecde7d1eb66b465a23ea5a708045556077", "url": "https://www.semanticscholar.org/paper/6c2ca6ecde7d1eb66b465a23ea5a708045556077", "title": "Automatic Generation of Interactive Cooking Video with Semantic Annotation", "abstract": "Videos are one of the most frequently used forms of multimedia resources. People want to interact with videos to find a specific part or to obtain relevant information. To support user interactions, current videos should be transformed to interactive videos. This paper proposes an interactive cooking video system to generate automatically interactive cooking videos. To do this, the proposed system performs semantic video annotation on cooking videos. Semantic video annotation process includes three parts: synchronization between recipes and corresponding cooking videos based on a caption-recipe alignment algorithm, information extraction on food recipes using lexico-syntactic patterns, and semantic entity interconnection between recognized entities and semantic web entities. Cooking video annotation ontology is modeled to handle annotation data. To evaluate the proposed system, comparative experiments are performed on the caption-recipe alignment algorithm. The accuracy of information extraction and semantic entity interconnection is also measured. Experimental results show that the proposed system is superior to compared algorithms in alignment perspectives. Information extraction and semantic interconnection method also achieve high accuracy over 95%, respectively. Consequently, the proposed system generates interactive cooking videos in high accuracy and support user interactions by providing a user interface which allows users to easily find specific scenes and obtain detailed information on objects users have interested in.", "venue": "J. Univers. Comput. Sci.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "sw", "ie"], "mention_counts": {"sw": 1, "onto": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "a518882b8e94cc755048f85e4218eacaf8fbde37", "url": "https://www.semanticscholar.org/paper/a518882b8e94cc755048f85e4218eacaf8fbde37", "title": "PEEP- Privacy Enforcement in Email Project", "abstract": "Breaching information privacy is a critical problem where legal remedies intervene only after the fact rather than prevent it. This paper presents an organizational privacy compliance engine that monitors outgoing emails to detect breaches of a privacy policy in an organization. The PEEP system employs email content analysis techniques to extract information and their ownership. Access to the extracted information is verified by privacy rules assisted by an ontology-based model to represent information disclosure privileges. This paper addresses the issues of, first, the information extraction techniques from email, and second, the implementation of an ontology-based model of multilevel disclosure privileges to represent privacy rules. We experiment with the PEEP system on real life emails in an academic environment to detect breaches of privacy in emails. Our results report an F-score of 71.7% of privacy violations detection.", "venue": "PST", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "ie"], "mention_counts": {"onto": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "bb781dafb4ed1a64c27eb97af15ce8aa43516442", "url": "https://www.semanticscholar.org/paper/bb781dafb4ed1a64c27eb97af15ce8aa43516442", "title": "Ontology-driven Vaccination Information Extraction", "abstract": "Increasingly, medical institutions have access to clinical information through computers. The need to process and manage the large amount of data is motivating the recent interest in semantic approaches. Data regarding vaccination records is a common in such systems. Also, being vaccination is a major area of concern in health policies, numerous information is available in the form of clinical guidelines. However, the information in these guidelines may be difficult to access and apply to a specific patient during consultation. The creation of computer interpretable representations allows the development of clinical decision support systems, improving patient care with the reduction of medical errors, increased safety and satisfaction. This paper describes the method used to model and populate a vaccination ontology and the system which recognizes vaccination information on medical texts.The system identifies relevant entities on medical texts and populates an ontology with new instances of classes. An approach to automatically extract information regarding inter-class relationships using association rule mining is suggested.", "venue": "NLPCS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "ie"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "3a8627be18518b554990238df2b3b7af49df5100", "url": "https://www.semanticscholar.org/paper/3a8627be18518b554990238df2b3b7af49df5100", "title": "Using Part-of-Speech Patterns and Domain Ontology to Mine Imprecise Concepts from Text Documents", "abstract": "In the last few years, several works in the literature have addressed the problem of information extraction from text documents. The importance of this problem derives from the fact that, once extracted, the information can be handled in a way similar to instances of a traditional database. But, most of the information extraction systems assume that the texts are having only precise concepts and these restrict the information provider to use only precise concepts to represent their information. In this paper we have presented a system that uses part-of-speech patterns and domain ontology to extract imprecise concepts present in the text documents and hence it allows information provider to describe concepts by using linguistic variables \u2013 very, more, light, strong, slightly, quite etc. that are very common with natural languages. We have considered wine documents as case study however it can be applied to any domain for which there is an existing ontology. In this paper, we have shown how a structured knowledgebase can be designed to hold imprecise concept descriptions extracted from text documents. The structured knowledgebase can then be searched efficiently for required information.", "venue": "iiWAS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto", "onto"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "3b809ef58ae6521235403aedd209461d15f4ef26", "url": "https://www.semanticscholar.org/paper/3b809ef58ae6521235403aedd209461d15f4ef26", "title": "Extracting Semantics from Maintenance Records", "abstract": "Rapid progress in natural language processing has led to its utilization in a variety of industrial and enterprise settings, including in its use for information extraction, specifically named entity recognition and relation extraction, from documents such as engineering manuals and field maintenance reports. While named entity recognition is a well-studied problem, existing state-of-the-art approaches require large labelled datasets which are hard to acquire for sensitive data such as maintenance records. Further, industrial domain experts tend to distrust results from black box machine learning models, especially when the extracted information is used in downstream predictive maintenance analytics. We overcome these challenges by developing three approaches built on the foundation of domain expert knowledge captured in dictionaries and ontologies. We develop a syntactic and semantic rules-based approach and an approach leveraging a pre-trained language model, fine-tuned for a question-answering task on top of our base dictionary lookup to extract entities of interest from maintenance records. We also develop a preliminary ontology to represent and capture the semantics of maintenance records. Our evaluations on a real-world aviation maintenance records dataset show promising results and help identify challenges specific to named entity recognition in the context of noisy industrial data.", "venue": "ArXiv", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "ie", "nlp"], "mention_counts": {"nlp": 1, "onto": 2, "ie": 2}, "nlp_mention_counts": {"nlp": 1, "ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "7db02526d228ac31bff4ace1f55b4ba18580a5f7", "url": "https://www.semanticscholar.org/paper/7db02526d228ac31bff4ace1f55b4ba18580a5f7", "title": "Quality of Care Metric Reporting from Clinical Narratives: Assessing Ontology Components", "abstract": "The Institute of Medicine reports a growing demand in recent years for quality improvement within the healthcare industry. In response, numerous organizations have been involved in the development and reporting of quality measurement metrics. However, disparate data models from such organizations shift the burden of accurate and reliable metrics extraction and reporting to healthcare providers. Furthermore, manual abstraction of quality metrics and diverse implementation of Electronic Health Record (EHR) systems deepens the complexity of consistent, valid, explicit, and comparable quality measurement reporting within healthcare provider organizations. The main objective of this research is to evaluate an ontology-based information extraction framework to utilize unstructured clinical text for extraction and reporting quality of care metrics that are interpretable and comparable across healthcare institutions. Keywords\u2014ontology; information extraction; quality of care metric; clinical narratives", "venue": "ICBO", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Psychology"], "mentions": ["onto", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "487dc97932f09542c890a0c68686498b886557aa", "url": "https://www.semanticscholar.org/paper/487dc97932f09542c890a0c68686498b886557aa", "title": "A Method for Materials Knowledge Extraction from HTML Tables Based on Sibling Comparison", "abstract": "There are rich data resources residing in available materials websites, and most of these data resources are shown in the form of HTML tables. However, it is difficult to distinguish the attributes and values because of the semi-structured feature of HTML tables. Therefore, identifying attributes in HTML tables is the key issue for the information acquisition. In this paper, based on sibling comparison, a method for materials knowledge extraction from HTML tables is proposed, which consists of three steps: acquiring sibling tables, identifying table pattern and extracting table data. We show how to use F-measure to find the appropriate thresholds for matching of tables from materials websites when acquiring sibling tables. Further, we propose a strategy named FRFC (i.e. the First Row matching and First Column matching) to distinguish attributes and values, so that table pattern is identified. Moreover, the data from HTML tables is extracted based on their corresponding table patterns and mapped to a predefined schema, which will facilitate the population to materials ontology. The proposed approach is applicable to circumstances, where an attribute in the table may span multiple cells and matched attributes in sibling tables are more. We acquire desired accuracy (>90%) through using FRFC for identifying table pattern. The time about extraction may not increase significantly with increasing number of documents and cells in tables, so our approach is effective to process a large number of documents. A prototype named MTES is developed and demonstrates the effectiveness of our proposed approach.", "venue": "Int. J. Softw. Eng. Knowl. Eng.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ke", "ke"], "mention_counts": {"ke": 2, "onto": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "f1e83502cafa56bdb2396fd44069f9dd73280981", "url": "https://www.semanticscholar.org/paper/f1e83502cafa56bdb2396fd44069f9dd73280981", "title": "Relevance Feedback Search Based on Automatic Annotation and Classification of Texts", "abstract": "The idea behind Relevance Feedback Search (RFBS) is to build search queries as an iterative and interactive process in which they are gradually refined based on the results of the previous search round. This can be helpful in situations where the end user cannot easily formulate their information needs at the outset as a well-focused query, or more generally as a way to filter and focus search results. This paper concerns (1) a framework that integrates keyword extraction and unsupervised classification into the RFBS paradigm and (2) the application of this framework to the legal domain as a use case. We focus on the Natural Language Processing (NLP) methods underlying the framework and application, where an automatic annotation tool is used for extracting document keywords as ontology concepts, which are then transformed into word embeddings to form vectorial representations of the texts. An unsupervised classification system that employs similar techniques is also used in order to classify the documents into broad thematic classes. This classification functionality is evaluated using two different datasets. As the use case, we describe an application perspective in the semantic portal LawSampo \u2013 Finnish Legislation and Case Law on the Semantic Web. This online demonstrator uses a dataset of 82 145 sections in 3725 statutes of Finnish legislation and another dataset that comprises 13 470 court decisions. 2012 ACM Subject Classification Computing methodologies \u2192 Information extraction; Applied computing \u2192 Document searching; Information systems \u2192 Clustering and classification", "venue": "International Conference on Language, Data, and Knowledge", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "sw", "ie", "onto"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "2ad292551bce76cd36dd54316a861794b215e1d7", "url": "https://www.semanticscholar.org/paper/2ad292551bce76cd36dd54316a861794b215e1d7", "title": "OCMiner: Text Processing, Annotation and Relation Extraction for the Life Sciences", "abstract": "We present OCMiner, a high-performance text processing system for large document collections of scientific publications. Several linguistic options allow adjusting the quality of annotation results which can be specialized and fine-tuned for the recognition of Life Science terms. Recognized terms are mapped to semantic concepts which are ontologically located within their respective domain taxonomies. Relying on a correct identification and semantic interpretation of mentions of domain concepts, relations between entities are extracted. The annotated text, as well as extracted knowledge triples, can be visualized on a web-based front-end at http://www.ocminer.com/, permitting an explorative information retrieval.", "venue": "SWAT4LS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["tp", "tp", "ke", "onto"], "mention_counts": {"ke": 1, "onto": 1, "tp": 2}, "nlp_mention_counts": {"ke": 1, "tp": 2}, "ld_mention_counts": {"ke": 1, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "033435f81259ce2842daca2622332c4aee200450", "url": "https://www.semanticscholar.org/paper/033435f81259ce2842daca2622332c4aee200450", "title": "Knowledge Extraction and Management for Insider Threat Mitigation", "abstract": "This paper presents a model for insider threat mitigation. While many of the existing insider threat models concentrate on watching insiders\u2019 activities for any misbehavior, we believe that considering the insider himself/herself as a basic entity before looking into his/her activities will be more effective. In this paper, we presented an approach that relies on ontology to extract knowledge from an object. This represents expected knowledge that an insider might gain by accessing that object. We then utilized this information to build a model for insider threat mitigation which ensures that only knowledge units that are related to the insider\u2019s domain of access or his/her assigned tasks will be allowed to be accessed by such insiders.", "venue": "WOSIS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "ke"], "mention_counts": {"ke": 2, "onto": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "30b97d982ea2a850f320200d484871c4659988cd", "url": "https://www.semanticscholar.org/paper/30b97d982ea2a850f320200d484871c4659988cd", "title": "Event extraction based on open information extraction and ontology", "abstract": "The work presented in this master thesis consists of extracting a set of events from texts written in natural language. For this purpose, we have based ourselves on the basic notions of the information extraction as well as the open information extraction. First, we applied an open information extraction(OIE) system for the relationship extraction, to highlight the importance of OIEs in event extraction, and we used the ontology to the event modeling. We tested the results of our approach with test metrics. As a result, the two-level event extraction approach has shown good performance results but requires a lot of expert intervention in the construction of classifiers and this will take time. In this context we have proposed an approach that reduces the expert intervention in the relation extraction, the recognition of entities and the reasoning which are automatic and based on techniques of adaptation and correspondence. Finally, to prove the relevance of the extracted results, we conducted a set of experiments using different test metrics as well as a comparative study.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "56095f1949fc5e408b3fe7ce9ca6b77c44e9d917", "url": "https://www.semanticscholar.org/paper/56095f1949fc5e408b3fe7ce9ca6b77c44e9d917", "title": "Performing Information Extraction and Ontology Development for Mission Engineering Applications", "abstract": "The development and evaluation of Mission Engineering Threads (METs) require an understanding of the operational context for which a system-of-systems (SoS) will be employed as well as an assessment that the performance of a complex SoS is effective and safe. The information describing the design and performance parameters of the systems within the SoS is distributed in many different physical locations, and is represented in a variety of formats, both structured and unstructured. Because of this dynamic on the structuring of the data to include differing ontology frameworks, it is necessary to develop a framework and toolset to handle the automated extraction of information from disparate information sources. In addition, this extracted information needs to be categorized properly into defined data types as represented in the specific MET to correctly capture the appropriate context of the mission scenario. Semantic technique solutions will be researched, analysed, and applied as a means to infer new facts from existing facts and data. These techniques are particularly powerful when the amount of data and/or the relationships and constraints among data are too cumbersome and complex for human understanding and reasoning. Using the characteristics of wine as an example, we present our framework to show how it enables rapid and contextually relevant extraction, and represents complex information in a user-friendly format.", "venue": "J. Integr. Des. Process. Sci.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "ae6dd59baf078f71ffec9d09184dad722cf40e19", "url": "https://www.semanticscholar.org/paper/ae6dd59baf078f71ffec9d09184dad722cf40e19", "title": "Ontology-Guided Policy Information Extraction for Healthcare Fraud Detection", "abstract": "Financial losses in Medicaid, from Fraud, Waste and Abuse (FWA), in the United States are estimated to be in the tens of billions of dollars each year. This results in escalating costs as well as limiting the funding available to worthy recipients of healthcare. The Centers for Medicare & Medicaid Services mandate thorough auditing, in which policy investigators manually research and interpret the policy to validate the integrity of claims submitted by providers for reimbursement, a very time-consuming process. We propose a system that aims to interpret unstructured policy text to semi-automatically audit provider claims. Guided by a domain ontology, our system extracts entities and relations to build benefit rules that can be executed on top of claims to identify improper payments, and often in turn payment policy or claims adjudication system vulnerabilities. We validate the automatic knowledge extraction from policies based on ground truth created by domain experts. Lastly, we discuss how the system can co-reason with human investigators in order to increase thoroughness and consistency in the review of claims and policy, to identify providers that systematically violate policies and to help in prioritising investigations.", "venue": "MIE", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine", "Business"], "mentions": ["ie", "onto", "ke", "onto"], "mention_counts": {"ke": 1, "onto": 2, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "09ded55e490d67fe6bb74a98f86cad6123dd6eea", "url": "https://www.semanticscholar.org/paper/09ded55e490d67fe6bb74a98f86cad6123dd6eea", "title": "Learning the ontological theory of an information extraction system in the multi-predicate ILP setting", "abstract": "In recent years, numerous works have been carried out to design Information Extraction (IE) systems able to extract genic interaction networks from text. Usually, the extraction procedure is completed by so-called extraction patterns, which are often limited to map textual fragments to a single semantic relation. Such poor representations do not take into account the complexity of the data processed by biologists. IE systems need sophisticated representations, encoded with ontologies, allowing the definition of multiple relations, and of the (possibly recursive) dependencies between them. Up to now, Machine Learning techniques used to acquire extraction patterns, i.e. binary or multi-class learners, reflect those representation restrictions. They assume independence between target predicates, and do not handle recursion. In this paper, we use Inductive Logic Programming in a multi-predicate setting to learn extraction patterns fitted to an ontological context. Multi-predicate ILP is an important paradigm which allows to learn recursive theories. We experimented our framework on a Bacillus subtilis bacterium text corpus, in which we reach a global recall of 67.7% and a precision of 75.5% in ten-fold cross-validation.", "venue": "SAC '09", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto", "onto"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "3e80fb3a849ad90f8a1e2faf0535616d49c35483", "url": "https://www.semanticscholar.org/paper/3e80fb3a849ad90f8a1e2faf0535616d49c35483", "title": "Model Driven Architecture for Industrial Applications", "abstract": "The most important element of industrial software development is the creation of a common vocabulary of terms for exchanging information between software and industrial engineers. Based on this cooperation, technical domain knowledge is converted into data structures, algorithms and rules. Currently, when people are used to receiving short and quick messages, the most efficient way of knowledge extraction is work on examples or mockups to facilitate better understanding of the problem. Shorter rounds in the presentation of mockups allows continuous work on live object models rather than specifications which make experts more open for sharing their knowledge and provides quicker and more reliable feedback on the data structure and the completeness of the model. Latest research and progress in the area of Model Driven Architecture (MDA) resulted in advanced tools for the creation of models, automatic source code generation as well as whole frameworks for creating application skeletons based on these models. In this paper a collaborative process which uses MDA approach (model, tools and frameworks) for extracting knowledge from domain experts is presented. During presented process, a cooperation of a software engineer and a domain expert via phone calls and one live workshop resulted in a complete model of machine and drive including specific machine features and diagnostic processes. Finally, a working diagnostics application was verified by the domain expert proving that MDA resulted in the expected results. The diagnostics application was verified on real data collected on the winding machine for more than one month, collected diagnostics data included more than 150 signals and 20Gb of raw analog data to dig into before getting condensed diagnostics results. Additionally to the process itself, the article presents identified risks, benefits from applying the MDA approach and lessons learned from applying this new innovative process. For further work, the possibilities of extending and dynamically extending existing models should be studied. In previous works we have focused on an ontology based approach, which does not meet all expectations when it comes to application in real world environment. As simpler and more mature technology, MDA was shown to be more productive and easier to adapt for building industrial applications.", "venue": "2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "onto"], "mention_counts": {"ke": 2, "onto": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "77d57c2a63f44f8956be3f493a42376969ea9533", "url": "https://www.semanticscholar.org/paper/77d57c2a63f44f8956be3f493a42376969ea9533", "title": "Enriching a lexical semantic net with selectional preferences by means of statistical corpus analysis", "abstract": "Broad-coverage ontologies which represent lexical semantic knowledge are being built for more and more natural languages. Such resources provide very useful information for word sense disambiguation, which is crucial for a variety of NLP tasks (e.g. semantic annotation of corpora, information retrieval, or semantic inferencing). Since the manual encoding of such ontologies is very labour-intensive, the development of (semi-)automatic methods for acquiring lexical semantic information is an important task. This paper addresses the automatic acquisition of selectional preferences of verbs by means of statistical corpus analysis. Knowledge about such preferences is essential for inducing thematic relations, which link verbal concepts to nominal concepts that are selectionally preferred as their complements. Several approaches for learning selectional preferences from corpora have been proposed in the last years. However, their usefulness for ontology building is limited. This paper introduces a modification of one of these methods (i.e. the approach of Li & Abe [1]) and evaluates it by employing a gold standard. The results show that the modified approach is much more appropriate for the given task.", "venue": "ECAI Workshop on Ontology Learning", "citationCount": 54, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "wsd", "nlp", "onto"], "mention_counts": {"nlp": 1, "wsd": 1, "onto": 3}, "nlp_mention_counts": {"nlp": 1, "wsd": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "b65dc924325fb4892a2ea95355a20909fa00f2ab", "url": "https://www.semanticscholar.org/paper/b65dc924325fb4892a2ea95355a20909fa00f2ab", "title": "Automatic extraction of requirements expressed in industrial standards : a way towards machine readable standards ?", "abstract": "The project, under industrial funding, presented in this publication aims at the semantic analysis of a normative document describing requirements applicable to electrical appliances. The objective of the project is to build a semantic approach to extract and automatically process information related to the requirements contained in the standard. To this end, the project has been divided into three parts, covering the analysis of the requirements document, the extraction of relevant information and creation of the ontology and the comparison with other approaches. The first part of our work deals with the analysis of the requirements document under study. The study focuses on the specificity of the sentence structure, the use of particular words and vocabulary related to the representation of the requirements. The aim is to propose a representation facilitating the extraction of information, used in the second part of the study. In the second part, the extraction of relevant information is conducted in two ways: manual (the ontology being built by hand), semi-automatic (using semantic annotation software and natural language processing techniques). Whatever the method used, the aim of this extraction is to create the concept dictionary, then the ontology, enriched as the document is scanned and understood by the system. Once the relevant terms have been identified, the work focuses on identifying and representing the requirements, separating the textual writing from the information given in the tables. The automatic processing of requirements involves the extraction of sentences containing terms identified as relevant to a requirement. The identified requirement is then indexed and stored in a representation that can be used for query processing.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 1, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "9aaa8b38009e333ddbd023d34ffad76539234a32", "url": "https://www.semanticscholar.org/paper/9aaa8b38009e333ddbd023d34ffad76539234a32", "title": "A knowledge extaction system (KEYS) based on UNL knowledge infrastructure", "abstract": "With the exponential growth of information available on the internet pages, humans need to extract specific information has also witnessed an ever growing increase. This paper presents KEYS (Knowledge Extraction sYStem). It searches for information inside documents represented in Universal Networking Language (UNL), i.e., in semantic hyper-graphs. This allows for retrieval and extraction practices that are language-independent and semantically-oriented. It is expected to provide high-quality knowledge extraction through a shallow analysis of the source text into the UNL using a specific ontological relations then generate the resulting UNL document into several different target languages in a fully-automatic manner. This is expected to present a novel approach to the topic of identifying named entities; extracting names with all its types from a natural language texts. The Precision measurement of the system is 0.86 while recall measurement is 0.82.", "venue": "TENCON 2015 - 2015 IEEE Region 10 Conference", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "onto"], "mention_counts": {"ke": 2, "onto": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "6e9fa744b17077aa5516d8f839163a3b12535758", "url": "https://www.semanticscholar.org/paper/6e9fa744b17077aa5516d8f839163a3b12535758", "title": "Intelligent System for Skin Disease Detection of Dogs with Ontology Based Clinical Information Extraction", "abstract": "The largest organ in dogs, the epidermis, is crucial in supplying immunological responses. Skin will preserve all the nutrients and safeguard the cells while warding off harmful or pathogenic substances. Most dog owners today are not aware that their pet dog has a skin condition. Although they were aware of these ailments, they had no notion of how to cure them. In such a situation, the dog may experience pain and an aggravation of the condition. Owners should therefore take their dogs to the vet, even if the skin condition is minor. It can, however, be a costly procedure. There aren\u2019t many forums where dog owners may get advice from professionals and ask inquiries regarding their pets. The solution suggests a fully functional mobile application which is a combination of disease identification feature, disease severity level detection feature, domain specific knowledge base with semantic web development and a domain specific AI based chat-bot to the dog owners to overcome this problem using Convolutional Neural Network (CNN) and natural language processing (NLP).System will extract the necessary features from the images of the lesion to classify the skin condition and Severity level of the disease. The results obtained show disease type classification is within the accuracy range of 77.78% to 100% which tested again 4 CNN base models. As for the severity level identification accuracy situated around 99.62%.", "venue": "Ubiquitous Computing, Electronics & Mobile Communication Conference", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ie", "nlp", "onto", "kg"], "mention_counts": {"onto": 1, "nlp": 1, "sw": 1, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 1, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "50609ffb74fb20e3480e5098f01f975545c32b41", "url": "https://www.semanticscholar.org/paper/50609ffb74fb20e3480e5098f01f975545c32b41", "title": "Natural Language Processing for Productivity Metrics for Software Development Profiling in Enterprise Applications", "abstract": "In this paper, we utilize ontology-based information extraction for semantic analysis and terminology linking from a corpus of software requirement specification documents from 400 enterprise-level software development projects. The purpose for this ontology is to perform semi-supervised learning on enterprise-level specification documents towards an automated method of defining productivity metrics for software development profiling. Profiling an enterprise-level software development project in the context of productivity is necessary in order to objectively measure productivity of a software development project and to identify areas of improvement in software development when compared to similar software development profiles or benchmark of these profiles. We developed a semi-novel methodology of applying NLP OBIE techniques towards determining software development productivity metrics, and evaluated this methodology on multiple practical enterprise-level software projects.", "venue": "AICCC '18", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "ie", "nlp"], "mention_counts": {"nlp": 2, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "bfbe8a85662f7505fc408dd14e59eb37e8ca722b", "url": "https://www.semanticscholar.org/paper/bfbe8a85662f7505fc408dd14e59eb37e8ca722b", "title": "TERMINTEGRAL: Una plataforma para la construcci\u00f3n de bases terminol\u00f3gicas y ontolog\u00edas", "abstract": "Linguistics engineering is frequently based on a descriptive analysis of terms and their combinations in order to design tools to detect units and structures that can be relevant for knowledge extraction. The integration of all the tools needed by terminologists in a platform enables the transformation of spe-cialised texts and the construction and enrichment of ontologies. In this paper we present the design of TERMINTEGRAL, an open multimodular platform for terminology work. TERMINTEGRAL has been constructed by reusing tools previously developed by IULATERM, a research group of IULA at the Pompeu Fabra University (Barcelona). TERMINTEGRAL enables the construction of terminological glossaries and ontologies on the basis of automatic text processing.", "venue": "Linguistica Antverpiensia, New Series \u2013 Themes in Translation Studies", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["tp", "ke", "onto", "onto"], "mention_counts": {"ke": 1, "onto": 2, "tp": 1}, "nlp_mention_counts": {"ke": 1, "tp": 1}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "4523f3a04d921574edfe256e00bbdff20470747c", "url": "https://www.semanticscholar.org/paper/4523f3a04d921574edfe256e00bbdff20470747c", "title": "Extracting Knowledge from Text with PIKES", "abstract": "In this demonstration we showcase PIKES, a Semantic Role Labeling (SRL)-powered approach for Knowledge Extraction. PIKES implements a rule-based strategy that reinterprets SRL output in light of other linguistic analyses, such as dependency parsing and co-reference resolution, thus properly capturing and formalizing in RDF important linguistic aspects such as argument nominalization, frame-frame relations, and group entities.", "venue": "SEMWEB", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "rdf", "ke"], "mention_counts": {"ke": 2, "rdf": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "rdf": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "769e4e0e37584e4e02f56db98408345dd329fc14", "url": "https://www.semanticscholar.org/paper/769e4e0e37584e4e02f56db98408345dd329fc14", "title": "Using NLP and Ontologies for Notary Document Management Systems", "abstract": "In this paper we describe the use of NLP techniques and ontologies as the core for building novel e-gov based information systems, and in particular we define the main characteristics of a document management system in the legal domain, that manages a variety of paper documents, automatically transforming them into RDF statements, for suitable indexing, retrieval and long term preservation. Although we describe a general architecture that can be used for several application domains, our system is particularly suitable for the Italian notary realm.", "venue": "2008 19th International Workshop on Database and Expert Systems Applications", "citationCount": 25, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "rdf", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2, "rdf": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 2, "rdf": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "70e2267006b11d2676a5af9ae317c52c5b61bef5", "url": "https://www.semanticscholar.org/paper/70e2267006b11d2676a5af9ae317c52c5b61bef5", "title": "Learning to Extract Information for the Semantic Web", "abstract": "The goal of information extraction (IE) is to find desired pieces of information in natural language texts and store them in a form that is suitable for automatic querying and processing; the Semantic Web provides formats and standards for storing and processing such data. Hence IE-based automatic or semi-automatic approaches are a promising approach for building the Semantic Web. My Ph.D. thesis focuses on developing and refining trainable algorithms for this purpose.", "venue": "Berliner XML Tage", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "ie", "ie", "sw"], "mention_counts": {"sw": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"sw": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "a840bb3f3552b07e067b34f4457f25850f9cece3", "url": "https://www.semanticscholar.org/paper/a840bb3f3552b07e067b34f4457f25850f9cece3", "title": "Web Information Extraction Systems for Web Semantization", "abstract": "In this paper we present a survey of web information extraction systems and semantic annotation platforms. The survey is concentrated on the problem of employment of these tools in the process of web semantization. We compare the approaches with our own solutions and propose some future directions in the development of the web semantization idea.", "venue": "ITAT", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ie", "sw", "sw", "ie"], "mention_counts": {"sw": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"sw": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "6e2784afa68b881366bb90033c682467f0571bff", "url": "https://www.semanticscholar.org/paper/6e2784afa68b881366bb90033c682467f0571bff", "title": "KORE: keyphrase overlap relatedness for entity disambiguation", "abstract": "Measuring the semantic relatedness between two entities is the basis for numerous tasks in IR, NLP, and Web-based knowledge extraction. This paper focuses on disambiguating names in a Web or text document by jointly mapping all names onto semantically related entities registered in a knowledge base. To this end, we have developed a novel notion of semantic relatedness between two entities represented as sets of weighted (multi-word) keyphrases, with consideration of partially overlapping phrases. This measure improves the quality of prior link-based models, and also eliminates the need for (usually Wikipedia-centric) explicit interlinkage between entities. Thus, our method is more versatile and can cope with long-tail and newly emerging entities that have few or no links associated with them. For efficiency, we have developed approximation techniques based on min-hash sketches and locality-sensitive hashing. Our experiments on semantic relatedness and on named entity disambiguation demonstrate the superiority of our method compared to state-of-the-art baselines.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 211, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "kg", "kg"], "mention_counts": {"nlp": 1, "ke": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "kg": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "f7d82643f263994debf076826cd375d355ff86b3", "url": "https://www.semanticscholar.org/paper/f7d82643f263994debf076826cd375d355ff86b3", "title": "Extracting Knowledge from Web Text with Monte Carlo Tree Search", "abstract": "To extract knowledge from general web text, it requires to build a domain-independent extractor that scales to the entire web corpus. This task is known as Open Information Extraction (OIE). This paper proposes to apply Monte-Carlo Tree Search (MCTS) to accomplish OIE. To achieve this goal, we define a Markov Decision Process for OIE and build a simulator to learn the reward signals, which provides a complete reinforcement learning framework for MCTS. Using this framework, MCTS explores candidate words (and symbols) under the guidance of a pre-trained Sequence-to-Sequence (Seq2Seq) predictor and generates abundant exploration samples during training. We apply the exploration samples to update the reward simulator and the predictor, based on which we implement another MCTS to search the optimal predictions during inference. Empirical evaluation demonstrates that the MCTS inference substantially improves the accuracy of prediction (more than 10%) and achieves a leading performance over other state-of-the-art comparison models.", "venue": "The Web Conference", "citationCount": 16, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ie"], "mention_counts": {"ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "2f23ca5f8e86007810140595c647f5c10e3afe19", "url": "https://www.semanticscholar.org/paper/2f23ca5f8e86007810140595c647f5c10e3afe19", "title": "NERD: A Framework for Unifying Named Entity Recognition and Disambiguation Extraction Tools", "abstract": "Named Entity Extraction is a mature task in the NLP field that has yielded numerous services gaining popularity in the Semantic Web community for extracting knowledge from web documents. These services are generally organized as pipelines, using dedicated APIs and different taxonomy for extracting, classifying and disambiguating named entities. Integrating one of these services in a particular application requires to implement an appropriate driver. Furthermore, the results of these services are not comparable due to different formats. This prevents the comparison of the performance of these services as well as their possible combination. We address this problem by proposing NERD, a framework which unifies 10 popular named entity extractors available on the web, and the NERD ontology which provides a rich set of axioms aligning the taxonomies of these tools.", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "citationCount": 133, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "sw", "nlp", "ke"], "mention_counts": {"nlp": 1, "sw": 1, "onto": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"sw": 1, "onto": 1, "ke": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "20e9e2a55d6eb6259b3d9d03457724b553477b15", "url": "https://www.semanticscholar.org/paper/20e9e2a55d6eb6259b3d9d03457724b553477b15", "title": "A Natural Language Processing for Semantic Web Services", "abstract": "The problem of natural language understanding is one of the first problems researchers in AI were trying to solve, and our brain is the best proof that the problem can be solved. In this paper we propose a model that could be used to describe roughly the process of understanding as it happens in our brain. Using the proposed model we have developed a hierarchical semantic form for the representation of semantics and the corresponding SOUL (space of universal links) learning algorithm. To proof the feasibility of the concept, we have implemented a prototype semantic Web Service, that uses natural language processing t o interpret the conventional, natural language content of traditional Web page and to retrieve the information asked for in natural language as well", "venue": "EUROCON 2005 - The International Conference on \"Computer as a Tool\"", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "sw", "nlp", "sw", "nlp"], "mention_counts": {"nlp": 2, "sw": 2, "nlu": 1}, "nlp_mention_counts": {"nlp": 2, "nlu": 1}, "ld_mention_counts": {"sw": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "f9593e30d8e5f8c47ccaed38233faf306dbbf0bb", "url": "https://www.semanticscholar.org/paper/f9593e30d8e5f8c47ccaed38233faf306dbbf0bb", "title": "SEISD: An environment for extraction of Semantic Information from on-line dictionaries", "abstract": "Knowledge Acquisition constitutes a main problem as regards the development of real Knowledge-based systems. This problem has been dealt with in a variety of ways. One of the most promising paradigms is based on the use of already existing sources in order to extract knowledge from them semiautomatically which will then be used in Knowledge-based applications. The Acquilex Project, within which we are working, follows this paradigm. The basic aim of Acquilex is the development of techniques and methods in order to use Machine Readable Dictionaries (MRD) * for building lexical components for Natural Language Processing Systems. SEISD (Sistema de Extracci6n de Informaci6n Semfintica de Diccionarios) is an environment for extracting semantic information from MRDs [Agent et al. 91b]. The system takes as its input a Lexical Database (LDB) where all the information contained in the MRD has been stored in an structured format. The extraction process is not fully automatic. To some extent, the choices made by the system must be both validated and confirmed by a human expert. Thus, an interactive environment must be used for performing such a task. One of the main contribution of our system lies in the way it guides the interactive process, focusing on the choice points and providing access to the information relevant to decision taking. System performance is controlled by a set of weighted heuristics that supplies the lack of algorithmic criteria or their vagueness in several crucial decision points. We will now summarize the most important characteristics of our system: \u2022 An underlying methodology for semantic extraction from lexical sources has been developped taking into account the characteristics of LDB and the intented semantic features to be extracted. \u2022 The Environment has been conceived as a support for the Methodology. \u2022 The Environment allows both interactive and batch modes of performance. \u2022 Great attention has been paid to reusability. The design and implementation of the system has involved an intensive", "venue": "ANLP", "citationCount": 18, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "kg", "ke"], "mention_counts": {"nlp": 1, "kg": 2, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "79286f5c96fdca9d4aad95643556efb830d83035", "url": "https://www.semanticscholar.org/paper/79286f5c96fdca9d4aad95643556efb830d83035", "title": "Empirical and Sensor Knowledge-extraction for Fuzzy Logic Motor Control Design", "abstract": "This paper presents a methodology for human and sensor data knowledge-extraction to assist in the design of a Fuzzy Logic Controller (FLC) when no parameterized model of the motor is available, thus it relays mainly on linguistic motor throughput description as its main data source. Proposed design methodology achieves acceptable control objective with two design stages; first, human empirical knowledge is used to specify FLC architecture and its initial parameters, employing experts' linguistic descriptions to construct controller rule base and knowledge base in accordance with cognitive map theory; Mamdani Fuzzy Inference Engine model (FIE) enables the designer to directly use empirical knowledge to create appropriate FLC by using linguistic terms to specify FLC structures. On second design stage, sensor data is use to fine-tune FLC parameters, as FLC parameters to motor control throughput relations is known by observation. The main objective of this paper is to develop a strategy of a FLC implementation capable of self-tuning, based on cognitive map theory and linguistic descriptions.", "venue": "NAFIPS 2007 - 2007 Annual Meeting of the North American Fuzzy Information Processing Society", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "f82cc4e1ae54af46776312d69083c8ba415dc141", "url": "https://www.semanticscholar.org/paper/f82cc4e1ae54af46776312d69083c8ba415dc141", "title": "CaSIE: Canonicalize and Informative Selection of the OpenIE system", "abstract": "Knowledge extraction has become a hot topic recently with the increasing number of applications needed for large-scale knowledge bases (KBs), such as semantic search and QA systems. The goal of knowledge extraction is to extract relations and their arguments from natural language text. Recent research proposes two kinds of solutions. The first one, called Closed IE, tries to construct KB through predefined features or rules with respect to a specific domain. It requires specifying the interested predicates in advance, which restricts its application to the domains where prior knowledge about the interested predicates must be given. The second one, called Open IE, tries to extract facts by using the parsing structure from the unstructured text. However, they cannot avoid extracting redundant facts. Such extractions can hardly be directly used to populate the existing KB. Moreover, many correct extractions are not relevant to the document, which limits the applications to understand the essential information that the document conveys. In this paper, we propose an end-to-end system which takes a target incomplete KB and documents as input. It first performs joint entity and relation linking to the existing KB based on both contexts of document and background KB information. Then it summarizes the extracted facts by considering the relevance to the document and the diversity between them. Extensive experiments over real datasets demonstrate the effectiveness and efficiency of the proposed methods.", "venue": "IEEE International Conference on Data Engineering", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "42850441db26ad8f5ddba2bb7f57346b214149ae", "url": "https://www.semanticscholar.org/paper/42850441db26ad8f5ddba2bb7f57346b214149ae", "title": "An approach to rule-based knowledge extraction", "abstract": "The extraction of easily interpretable knowledge from the large amount of data measured in experiments is very desirable. This paper proposes a method to achieve this. A fuzzy rule system is first generated and optimized using evolution strategies. This fuzzy system is then converted to an RBF neural network to refine the obtained knowledge. In order to extract understandable fuzzy rules from the trained RBF network, a neural network regularization technique called adaptive weight sharing is developed. Simulation results on the Mackey-Glass system show that the proposed approach to knowledge extraction is effective and practical.", "venue": "1998 IEEE International Conference on Fuzzy Systems Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36228)", "citationCount": 39, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "f612ef8ff39093c2d1640ff34bd2e85eb1f03b9a", "url": "https://www.semanticscholar.org/paper/f612ef8ff39093c2d1640ff34bd2e85eb1f03b9a", "title": "Lynx: a database and knowledge extraction engine for integrative medicine", "abstract": "We have developed Lynx (http://lynx.ci.uchicago.edu)\u2014a web-based database and a knowledge extraction engine, supporting annotation and analysis of experimental data and generation of weighted hypotheses on molecular mechanisms contributing to human phenotypes and disorders of interest. Its underlying knowledge base (LynxKB) integrates various classes of information from >35 public databases and private collections, as well as manually curated data from our group and collaborators. Lynx provides advanced search capabilities and a variety of algorithms for enrichment analysis and network-based gene prioritization to assist the user in extracting meaningful knowledge from LynxKB and experimental data, whereas its service-oriented architecture provides public access to LynxKB and its analytical tools via user-friendly web services and interfaces.", "venue": "Nucleic Acids Res.", "citationCount": 34, "fieldsOfStudy": ["Biology", "Computer Science", "Medicine"], "mentions": ["kg", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 1, "ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "c776f3871ba1206942252f78e3f597f1d26dfa54", "url": "https://www.semanticscholar.org/paper/c776f3871ba1206942252f78e3f597f1d26dfa54", "title": "Knowledge Extraction and Semantic Annotation of Text from the Encyclopedia of Life", "abstract": "Numerous digitization and ontological initiatives have focused on translating biological knowledge from narrative text to machine-readable formats. In this paper, we describe two workflows for knowledge extraction and semantic annotation of text data objects featured in an online biodiversity aggregator, the Encyclopedia of Life. One workflow tags text with DBpedia URIs based on keywords. Another workflow finds taxon names in text using GNRD for the purpose of building a species association network. Both workflows work well: the annotation workflow has an F1 Score of 0.941 and the association algorithm has an F1 Score of 0.885. Existing text annotators such as Terminizer and DBpedia Spotlight performed well, but require some optimization to be useful in the ecology and evolution domain. Important future work includes scaling up and improving accuracy through the use of distributional semantics.", "venue": "PLoS ONE", "citationCount": 20, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "onto"], "mention_counts": {"ke": 2, "onto": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "983c128fcd48f287945cd9d9896b5c141e65f8b4", "url": "https://www.semanticscholar.org/paper/983c128fcd48f287945cd9d9896b5c141e65f8b4", "title": "Agent-based knowledge extraction services inside enterprise data warehousing systems environments", "abstract": "During their lifetimes, enterprises have been storing and maintaining huge amounts of business and operational information. As time passed their information repositories have grown almost exponentially and they have typically got \"drown\" in data. So, they felt the urge for more suitable systems, capable of gathering, treating and storing this data avalanche in appropriated manners. Therefore, it is necessary to conceive and implant adequate analytical systems, capable of dealing with this huge potential of information in the best way, and providing new knowledge about the activities of organizations. Taking these real scenarios into account, we designed and developed a multi-agent system specially conceived to support knowledge extraction processes over data warehousing systems, covering the definition and preparation of the data repositories to be mined, till the visualization and manipulation of the results.", "venue": "12th International Workshop on Database and Expert Systems Applications", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "58c843a3f08ee2322afe0cb2711ae68ee816766e", "url": "https://www.semanticscholar.org/paper/58c843a3f08ee2322afe0cb2711ae68ee816766e", "title": "Iterative Knowledge Extraction from Social Networks", "abstract": "Knowledge in the world continuously evolves, and ontologies are largely incomplete, especially regarding data belonging to the so-called long tail. We propose a method for discovering emerging knowledge by extracting it from social content. Once initialized by domain experts, the method is capable of finding relevant entities by means of a mixed syntactic-semantic method. The method uses seeds, i.e. prototypes of emerging entities provided by experts, for generating candidates; then, it associates candidates to feature vectors built by using terms occurring in their social content and ranks the candidates by using their distance from the centroid of seeds, returning the top candidates. Our method can run iteratively, using the results as new seeds. as new seeds. In this paper we address the following research questions: (1) How does the reconstructed domain knowledge evolve if the candidates of one extraction are recursively used as seeds (2) How does the reconstructed domain knowledge spread geographically (3) Can the method be used to inspect the past, present, and future of knowledge (4) Can the method be used to find emerging knowledge", "venue": "The Web Conference", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ke", "ke"], "mention_counts": {"ke": 2, "onto": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "e83c3899404e649e130b13bde91697500c78f6ce", "url": "https://www.semanticscholar.org/paper/e83c3899404e649e130b13bde91697500c78f6ce", "title": "Automatic Non-Taxonomic Relation Extraction from Big Data in Smart City", "abstract": "The explosive data growth in smart city is making domain big data a hot topic for knowledge extraction. Non-taxonomic relations refer to any relations between concept pairs except the is-a relation, which is an important part of Knowledge Graph. In this paper, toward big data in smart city, we present a multi-phase correlation search framework to automatically extract non-taxonomic relations from domain documents. Different kinds of semantic information are used to improve the performance of the system. First, inspired by the works of network representation; we propose a Semantic Graph-Based method to combine structure information of semantic graph and context information of terms together for non-taxonomic relationships identification. Second, different semantic types of verb sets are extracted based on the dependency syntactic information, which are ranked to act as non-taxonomic relationship labels. Extensive experiments demonstrate the efficiency of the proposed framework. The F1 value reaches 81.4% for identification of non-taxonomic relationships. The total precision of the non-taxonomic relationship labels extraction is 73.4%, and 87.8% non-taxonomic relations can be provided with \u201cgood\u201d labels. We hope this article can provide a useful way for domain big data knowledge extraction in smart city.", "venue": "IEEE Access", "citationCount": 40, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "b0a63b8856bd125529d14efe8806627e46ddb17d", "url": "https://www.semanticscholar.org/paper/b0a63b8856bd125529d14efe8806627e46ddb17d", "title": "On \"deep\" knowledge extraction from documents", "abstract": "SynDiKATe comprises a family of natural language understanding systems for automatically acquiring knowledge from real-world texts (e.g., information technology test reports, medical finding reports), and for transferring their content to formal representation structures which constitute a corresponding text knowledge base. We present a general system architecture which integrates requirements from the analysis of single sentences, as well as those of referentially linked sentences forming cohesive texts. Properly accounting for text cohesion phenomena is a prerequisite for the soundness and validity of the generated text representation structures. It is also crucial for any information system application making use of automatically generated text knowledge bases in a reliable way.", "venue": "RIAO", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "kg", "ke", "kg"], "mention_counts": {"kg": 2, "ke": 1, "nlu": 1}, "nlp_mention_counts": {"ke": 1, "nlu": 1}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "97e54a101c29c95fbbc427fb9f23cf6c4d18d938", "url": "https://www.semanticscholar.org/paper/97e54a101c29c95fbbc427fb9f23cf6c4d18d938", "title": "Study of Automatic Knowledge Extraction in Specific Chinese Language Domain", "abstract": "The paper presents hierarchy bootstrapping as an alternative approach to learning from a large quantity of unlabeled data in the Chinese language domain. It advocates using a small amount of seed information and a large collection of easily-obtained unlabeled data. Hierarchy bootstrapping initializes a learner with seed information; then it iterates applying the learner to calculate for the unlabeled data. Two case studies of this approach are presented in order to solve the problem of automatic knowledge extraction in information extraction (IE) systems. The first algorithm makes use of seed words and seed patterns to build a learner, which extracts more characteristic words using scalar clusters method. These characteristic words have semantic similarity with seed words. Then more extraction patterns could be learned automatically and added to the knowledge database by using the second algorithm, they are a foundation for analysis of IE. Experimental results are promising", "venue": "2006 IEEE International Conference on Networking, Sensing and Control", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke"], "mention_counts": {"ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "c653596b0523e380e64d2d45b550aea6af02532b", "url": "https://www.semanticscholar.org/paper/c653596b0523e380e64d2d45b550aea6af02532b", "title": "Knowledge Extraction for the Web of Things (KE4WoT): WWW 2018 Challenge Summary", "abstract": "The Web of Things (WoT) is an extension of the Internet of Things (IoT) to ease the access to data generated by things/devices using the benefits of Web technologies. Data is exploited by WoT applications to monitor healthcare or even control home automation devices. The purpose of the Knowledge Extraction for the Web of Things (KE4WoT) challenge is to automatically extract the relevant knowledge from already designed smart WoT applications in various applicative domains. Those applications design and release Knowledge Bases (e.g., datasets and/or models) on the web.", "venue": "WWW", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 1, "ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "a6be9756e4eda4156018f6a160d36c02ea79fd32", "url": "https://www.semanticscholar.org/paper/a6be9756e4eda4156018f6a160d36c02ea79fd32", "title": "Wikipedia Mining for Triple Extraction Enhanced by Co-reference Resolution", "abstract": "Since Wikipedia has become a huge scale database storing wide-range of human knowledge, it is a promising corpus for knowledge extraction. A considerable number of researches on Wikipedia mining have been conducted and the fact that Wikipedia is an invaluable corpus has been confirmed. Wikipedia\u2019s impressive characteristics are not limited to the scale, but also include the dense link structure, URI for word sense disambiguation, well structured Infoboxes, and the category tree. In previous researches on this area, the category tree has been widely used to extract semantic relations among concepts on Wikipedia. In this paper, we try to extract triples (Subject, Predicate, Object) from Wikipedia articles, another promising resource for knowledge extraction. We propose a practical method which integrates link structure mining and parsing to enhance the extraction accuracy. The proposed method consists of two technical novelties; two parsing strategies and a co-reference resolution", "venue": "SDoW@ISWC", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "wsd"], "mention_counts": {"ke": 2, "wsd": 1}, "nlp_mention_counts": {"ke": 2, "wsd": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "6472971a427ecdaf86cb887ed2f28fb0308c5235", "url": "https://www.semanticscholar.org/paper/6472971a427ecdaf86cb887ed2f28fb0308c5235", "title": "A Survey on Knowledge Extraction Techniques for Web Tables", "abstract": "Web tables are worthy sources of relational information. The number of high-quality tables with useful relational information is rapidly increasing to hundreds of millions. Some search engines usually ignore meanings of entities and relationships in indexing thus they have poor performance in tabular data to a suitable field of research is the transformation of web tables into machine-readable knowledge. We first study overview of the use of web tables in different domains then focus on understanding knowledge of web tables. The results indicate that by combining old Information Extraction techniques, and table features and general inference models can extract Knowledge from web tables.", "venue": "2019 5th International Conference on Web Research (ICWR)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "ke"], "mention_counts": {"ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "886155239764cf547039616e05902295e79532a7", "url": "https://www.semanticscholar.org/paper/886155239764cf547039616e05902295e79532a7", "title": "Named Entity Recognition Method for Fault Knowledge based on Deep Learning", "abstract": "Aiming at the problem that fault text is difficult to be directly parsed and utilized, a fault knowledge extraction method is proposed based on deep learning method. Firstly, the characteristics of fault knowledge are analyzed, and a fault knowledge extraction model is established based on multi-layer neural network. Finally, the presented model is discussed comprehensively from the extraction accuracy, recall rate and F1 value, which proves the feasibility of the method. The unstructured text data is used to provide reference for fault diagnosis and prediction.", "venue": "International Conference on Machine Learning and Soft Computing", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 1, "ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "a46dd28c5c848b6805a5d66dfa6b02b287170303", "url": "https://www.semanticscholar.org/paper/a46dd28c5c848b6805a5d66dfa6b02b287170303", "title": "Knowledge Curation and Knowledge Fusion: Challenges, Models and Applications", "abstract": "Large-scale knowledge repositories are becoming increasingly important as a foundation for enabling a wide variety of complex applications. In turn, building high-quality knowledge repositories critically depends on the technologies of knowledge curation and knowledge fusion, which share many similar goals with data integration, while facing even more challenges in extracting knowledge from both structured and unstructured data, across a large variety of domains, and in multiple languages. Our tutorial highlights the similarities and differences between knowledge management and data integration, and has two goals. First, we introduce the Database community to the techniques proposed for the problems of entity linkage and relation extraction by the Knowledge Management, Natural Language Processing, and Machine Learning communities. Second, we give a detailed survey of the work done by these communities in knowledge fusion, which is critical to discover and clean errors present in sources and the many mistakes made in the process of knowledge extraction from sources. Our tutorial is example driven and hopes to build bridges between the Database community and other disciplines to advance research in this important area.", "venue": "SIGMOD Conference", "citationCount": 19, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "ke"], "mention_counts": {"nlp": 1, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "05d9cf634fd067203890d181d51746261032fede", "url": "https://www.semanticscholar.org/paper/05d9cf634fd067203890d181d51746261032fede", "title": "Interpretable Credit Risk Assessment Based on Heuristic Knowledge Extraction Method", "abstract": "Building explainable model has become an important issue for credit risk assessment. Results can be presented as rule-based knowledge and are therefore considered interpretable because they indicate causation of classification. However, traditional knowledge extraction methods are not suited to finance because credit data involves discrete and continuous data, with missing values and imbalanced labels. In this study, a novel multi-label classification method is proposed, which summarizes high-dimensional structured data observations into a rule-based classifier in the form of a rule list. The solution is a novel hybrid evolutionary algorithm (hEA) which avoids preprocessing original data from complex credit datasets. The proposed method has significant advantages over established interpretable classification methods in terms of classification performance on complex credit data. The complete code of our proposed method are available at https://github.com/wenge963/The-RATP-method.", "venue": "IEEE International Conference on Tools with Artificial Intelligence", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 1, "ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "8c1c85d99fff5bc31b013644fa661e1f425488c2", "url": "https://www.semanticscholar.org/paper/8c1c85d99fff5bc31b013644fa661e1f425488c2", "title": "Explainable Pulmonary Disease Diagnosis with Prompt-Based Knowledge Extraction", "abstract": "Recent studies show that deep learning models perform well in many medical tasks such as medical imaging and automated diagnosis. With qualified training datasets, some models can achieve or even surpass expert-level performance on some tasks. However, as a typical black-box-style approach, deep learning lacks theoretical interpretability, which is especially important for medical tasks. On the other hand, there are many sources of domain knowledge for medical diagnosis from human experts, such as clinical guidelines. How to sufficiently integrate human knowledge in the model is crucial for explainable diagnosis. In this paper, we propose a novel framework for explainable automated diagnosis that leverages explicit medical knowledge. We automate the knowledge extraction from textual clinical guidelines with prompt-based learning, train a set of weighted first-order logical rules with constructed evidence database, and finally infer the diagnosis result with integrated knowledge and multi-sourced data. We instantiate the framework for pulmonary disease diagnosis, and our experiments on a real dataset show that our method outperforms the state-of-the-art baselines in accuracy and interpretability.", "venue": "IEEE International Conference on Bioinformatics and Biomedicine", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 1, "ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "c27c7d8e744255c3dcd3b28c2b8a8db6551e0792", "url": "https://www.semanticscholar.org/paper/c27c7d8e744255c3dcd3b28c2b8a8db6551e0792", "title": "Improved medical decision support with multimethod approach", "abstract": "An enormous proliferation of databases in almost all areas of human life has created great need to develop tools for automatic knowledge extraction. Extracted knowledge can be used for categorizing, organizing or predictive purposes. One of the problems encountered is how to make a good induction with good generalization and knowledge representation what is especial important in medical domain. Although the research filed is very active, it is mainly focused on a specific method or on a specific combination of those methods. In this paper a multimethod approach is presented. This approach unlike other conventional hybrid approaches applies different methods on the same knowledge base where each method may contain inherent limitations with the expectation that the combined multiple methods may produce better results. It also addresses unbalanced nature of medical data.", "venue": "16th IEEE Symposium Computer-Based Medical Systems, 2003. Proceedings.", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "011e6977fa2d4cd75a2b191c93d5981ace09906a", "url": "https://www.semanticscholar.org/paper/011e6977fa2d4cd75a2b191c93d5981ace09906a", "title": "Performance evaluation for semantic-based risk factors extraction from clinical narratives", "abstract": "Precision medicine is the new perspective in healthcare that requires a personalized diagnosis and treatment plan for a patient. This new approach mandates that clinical decision support system (CDSS), an essential element of preventive and precision medicine processes, uses state-of-the-art technologies in terms of clinical knowledge extraction. To assist a physician's precise prognosis using CDSS, it is important that a patient's health data is properly and automatically analyzed. The unstructured part of the data in electronic health records (EHR) is critical, as it may contain hidden risk factors. We propose a new approach for CDSS to extract risk factors concepts from the clinical narratives using natural language processing techniques (NLP) and semantic web technologies (SWT). We evaluate our model using a case study dataset of patients\u2019 records with venous thromboembolism (VTE). Our model extracts risk factors of VTE to make a prognosis. Results of proposed technique yielded precision of 85% and recall of 84% to identify and extract risk factors concepts.", "venue": "2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC)", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "sw", "ke"], "mention_counts": {"nlp": 2, "sw": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"sw": 1, "ke": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "11d272305d547bfd0abd3cc9bdddcb7a3428df14", "url": "https://www.semanticscholar.org/paper/11d272305d547bfd0abd3cc9bdddcb7a3428df14", "title": "Hybrid Knowledge Extraction Framework Using Modified Adaptive Genetic Algorithm and BPNN", "abstract": "Fault diagnosis based on the expert system (ES) is still a research topic of manufacturing in Industry 4.0 because of the stronger interpretability. As the core component of the ES, fault diagnosis accuracy is positively correlated to the precision of the knowledge base. But it is difficult for users to understand the knowledge obtained from the original dataset utilizing the existing knowledge extraction method. Therefore, it is of great significance to extract easy-to-understand and exact rules from the NN framework. This paper proposes a hybrid extraction framework to perform the rule extraction for overcoming this drawback. First, an improved adaptive genetic algorithm (GA) using a logistic function, namely LAGA, is proposed to solve the traditional GA\u2019s insufficient prediction performance issue. Compared with the other three mainstream adaptive GAs, the experiment results of optimizing six selected test functions by these GA variants show that the LAGA algorithm\u2019s convergence accuracy and speed have been greatly improved, especially for high latitude functions. On this basis, a rule extraction method based on the symbol rule and NN, namely the LAGA-BP framework, is discussed in this manuscript to classify the real-valued attributes. This framework obtains hidden knowledge (knowledge refinement process) by NN and further transforms the acquired hidden knowledge into more easy-to-understand rule knowledge (rule extraction process). The execution of the LAGA-BP framework could be separated into two phases. The first phase is to optimize a back propagation NN (BPNN) using the LAGA and refine prediction classification knowledge over the optimized BPNN. In the second phase, an attribute reduction algorithm using multi-layered NN (SD algorithm) based on two different superposed networks is used in this framework to reduce data-set attributes and then uses the K-means clustering algorithm to extract the if-then rule from the simplified attributes. The Wisconsin breast cancer dataset is used as a case study to reveal the correctness and robustness of the proposed LAGA-BP method. Consulting relevant medical personnel and referencing relevant data shows that the rules extracted using this method help verify the diagnosis results, thus verifying the proposed framework\u2019s feasibility and practicality.", "venue": "IEEE Access", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 1, "ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "042da2f5b3c4e91015e01a7bbb884d192cd03df8", "url": "https://www.semanticscholar.org/paper/042da2f5b3c4e91015e01a7bbb884d192cd03df8", "title": "Generating Actionable Knowledge from Big Data", "abstract": "The last few years have seen a rapid increase of sheer amount of data produced and communicated over the Internet and the Web. While it is widely believed that the availability of such ``Big Data'' holds the potential to revolutionize many aspects of our modern society (e.g., intelligent transportation, environmental monitoring, and energy saving), many challenges need to be addressed before this potential can be realized. This PhD project focuses on one critical challenge, namely extracting actionable knowledge from Big Data. Tremendous efforts have been contributed on mining large-scale data on the Web and constructing comprehensive knowledge bases (KBs). However, existing knowledge extraction systems retrieve data from limited types of Web sources. In addition, data fusion approaches consider very little of the noises produced by those knowledge extraction systems. Consequently, the constructed KBs are far from being comprehensive and accurate. In this paper, we present our initial design of a framework for extracting machine-readable data with high precision and recall from four types of data sources, namely Web texts, Document Object Model (DOM) trees, existing KBs, and query stream. Confidence scores are attached to the resulting knowledge, which can be used to further improve the knowledge fusion results.", "venue": "SIGMOD PhD Symposium", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "7c7204092249a53b3bdc6ec1e63f85470f0ce4f5", "url": "https://www.semanticscholar.org/paper/7c7204092249a53b3bdc6ec1e63f85470f0ce4f5", "title": "Applying Knowledge Inference on Event-Conjunction for Automatic Control in Smart Building", "abstract": "Smart building, one of IoT-based emerging applications is where energy-efficiency, human comfort, automation, security could be managed even better. However, at the current stage, a unified and practical framework for knowledge inference inside the smart building is still lacking. In this paper, we present a practical proposal of knowledge extraction on event-conjunction for automatic control in smart buildings. The proposal consists of a unified API design, ontology model, inference engine for knowledge extraction. Two types of models: finite state machine(FSMs) and bayesian network (BN) have been used for capturing the state transition and sensor data fusion. In particular, to solve the problem that the size of time interval observations between two correlated events was too small to be approximated for estimation, we utilized the Markov Chain Monte Carlo (MCMC) sampling method to optimize the sampling on time intervals. The proposal has been put into use in a real smart building environment. 78-days data collection of the light states and elevator states has been conducted for evaluation. Several events have been inferred in the evaluation, such as room occupancy, elevator moving, as well as the event conjunction of both. The inference on the users\u2019 waiting time of elevator-using revealed the potentials and effectiveness of the automatic control on the elevator.", "venue": "Applied Sciences", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "ke"], "mention_counts": {"ke": 2, "onto": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "65017016b35928cd0e2ab0037200d0f92619b120", "url": "https://www.semanticscholar.org/paper/65017016b35928cd0e2ab0037200d0f92619b120", "title": "Low-resource Learning with Knowledge Graphs: A Comprehensive Survey", "abstract": "\u2014Machine learning methods especially deep neural networks have achieved great success but many of them often rely on a number of labeled samples for training. In real-world applications, we often need to address sample shortage due to e.g., dynamic contexts with emerging prediction targets and costly sample annotation. Therefore, low-resource learning, which aims to learn robust prediction models with limited resources (especially training samples), is now being widely investigated. Among all the low-resource learning studies, many prefer to utilize some auxiliary information in the form of Knowledge Graph (KG), which is becoming more and more popular for knowledge representation, to reduce the reliance on labeled samples. In this survey, we very comprehensively reviewed over 90 papers about KG-aware research for two major low-resource learning settings \u2014 zero-shot learning (ZSL) where new classes for prediction have never appeared in training, and few-shot learning (FSL) where new classes for prediction have only a small number of labeled samples that are available. We \ufb01rst introduced the KGs used in ZSL and FSL studies as well as the existing and potential KG construction solutions, and then systematically categorized and summarized KG-aware ZSL and FSL methods, dividing them into different paradigms such as the mapping-based, the data augmentation, the propagation- based and the optimization-based. We next presented different applications, including not only KG augmented prediction tasks in Computer Vision and Natural Language Processing (e.g., image classi\ufb01cation, visual question answering, text classi\ufb01cation and knowledge extraction), but also tasks for KG curation (e.g., inductive KG completion), and some typical evaluation resources for each task. We eventually discussed some challenges and future directions on aspects such as new learning and reasoning paradigms, and the construction of high quality KGs.", "venue": "ArXiv", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "kg", "kg"], "mention_counts": {"nlp": 1, "kg": 2, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "kg": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "402f9536dc44f930d98f43a9a6c16273030e3170", "url": "https://www.semanticscholar.org/paper/402f9536dc44f930d98f43a9a6c16273030e3170", "title": "Topical Expressivity in Short Texts", "abstract": "With each passing minute, online data is growing exponentially. A bulk of such data is generated from short text social media platforms such as Twitter. Such platforms are fundamental in social media knowledge-based applications like recommender systems. Twitter, for example, provides rich real-time streaming information. Extracting knowledge from such short texts without automated support is not feasible due to Twitter's platform streaming nature. Therefore, an automated method for comprehending patterns in such text is a need for many knowledge systems. This paper provides solutions to generate topics from Twitter data. We present several techniques related to topical modelling to identify topics of interest in short texts. Topic modelling is inherently problematic in shorter texts with very sparse vocabulary in addition to the informal language used in their dissemination. Such findings are informative in knowledge extraction for social media-based recommender systems as well as in understanding tweeters over time.", "venue": "CONF-IRM", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "d885089ff3723c70010230607de1dd0c644982b6", "url": "https://www.semanticscholar.org/paper/d885089ff3723c70010230607de1dd0c644982b6", "title": "Gextext: Unsupervised Knowledge Modelling in Biomedical Literature", "abstract": "PURPOSE: Literature review is a complex task, requiring the expert analysis of unstructured data. Computational automation of this process presents a valuable opportunity for high throughput knowledge extraction and meta analysis. Currently available methods are limited to the detection of explicit and short-context relationships. We address this challenge with Gextext, which extracts a knowledge graph of latent relationships directly from unstructured text. METHODS: Let C be a corpus of n text chunks. Let V_target be a set of query terms and V_random a random selection of terms in C. Let X indicate the occurrence of V_target and V_random in C. Gextext learns a graph G(V,E) by correlation thresholding on the covariance matrix of X, where thresholds are estimated by the correlations with randomly selected terms. Gextext was benchmarked against GloVE in tasks where embedding distance matrices were correlated against real world similarity matrices. A general corpus was generated from 5,000 randomly selected Wikipedia articles and a biomedical corpus from 961 research papers on stroke. RESULTS: Embeddings generated by Gextext preserved relative geographical distances between countries (Gextext: rho = 0.255, p < 2.22e-16; GloVE: rho = 0.086, p = 1.859e-09) and capital cities (Gextext: rho = 0.282, p < 2.22e-16 ; Glove: rho = 0.093, p = 8.0805e-11). Gextext embeddings organised drug names by shared target (Gextext: rho = 0.456, p < 2.22e-16; GloVE: rho = 0.091, p = 0.00087) and stroke phenotypes by body system (Gextext: rho = 0.446, p < 2.22e-16; GloVE: rho = 0.129, p = 1.7464e-11). CONCLUSIONS: Gextext extracts latent relationships from unstructured text, enabling fully unsupervised automation of the literature review process.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "3292839b94e0a32c4f40a913898b6d59ddcaa745", "url": "https://www.semanticscholar.org/paper/3292839b94e0a32c4f40a913898b6d59ddcaa745", "title": "Enabling Folksonomies for Knowledge Extraction: A Semantic Grounding Approach", "abstract": "Folksonomies emerge as the result of the free tagging activity of a large number of users over a variety of resources. They can be considered as valuable sources from which it is possible to obtain emerging vocabularies that can be leveraged in knowledge extraction tasks. However, when it comes to understanding the meaning of tags in folksonomies, several problems mainly related to the appearance of synonymous and ambiguous tags arise, specifically in the context of multilinguality. The authors aim to turn folksonomies into knowledge structures where tag meanings are identified, and relations between them are asserted. For such purpose, they use DBpedia as a general knowledge base from which they leverage its multilingual capabilities.", "venue": "Int. J. Semantic Web Inf. Syst.", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "38e6767ce92490042f0b2a331227ba7e8aafb1fd", "url": "https://www.semanticscholar.org/paper/38e6767ce92490042f0b2a331227ba7e8aafb1fd", "title": "Text Mining approaches for Automated Literature Knowledge Extraction and Representation", "abstract": "Due to the overwhelming volume of published scientific papers, information tools for automated literature analysis are essential to support current biomedical research. We have developed a knowledge extraction tool to help researcher in discovering useful information which can support their reasoning process. The tool is composed of a search engine based on Text Mining and Natural Language Processing techniques, and an analysis module which process the search results in order to build annotation similarity networks. We tested our approach on the available knowledge about the genetic mechanism of cardiac diseases, where the target is to find both known and possible hypothetical relations between specific candidate genes and the trait of interest. We show that the system i) is able to effectively retrieve medical concepts and genes and ii) plays a relevant role assisting researchers in the formulation and evaluation of novel literature-based hypotheses.", "venue": "Medinfo", "citationCount": 20, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "nlp", "ke"], "mention_counts": {"nlp": 1, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "4298d12b9ff5dd2c49a50abc3b83dacf34c839e3", "url": "https://www.semanticscholar.org/paper/4298d12b9ff5dd2c49a50abc3b83dacf34c839e3", "title": "Knowledge Extraction From Texts by Sintesi", "abstract": "In this paper we present SINTESI, a system for the knowledge extraction from Italian inputs, currently under development in our research centre. It is used on short descriptive diagnostic texts, in order to summarise their technical content and to build a knowledge base on faults. Often in these texts complex linguistic constructions like conjunctions, negations, ellipsis and anaphorae are involved. The presence of extragrammaticalities and of implicit knowledge is also frequent, especially because of the use of a sublanguage. SINTESI extracts the diagnostic information by performing a full text analysis; it is based on a semantics driven approach integrated by a general syntactic module and it is able to cope with the complexity of the (sub)language, maintaining both accuracy and robustness. Currently the system has been tested on about 1.000 texts and by a few users; in the near future it will be used by dozens of users every day.", "venue": "International Conference on Computational Linguistics", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "url": "https://www.semanticscholar.org/paper/98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning", "abstract": "Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "citationCount": 201, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "nlu"], "mention_counts": {"ke": 2, "nlu": 1}, "nlp_mention_counts": {"ke": 2, "nlu": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "9fbdeda9408b69b086aa362e3f59d5ed6866a640", "url": "https://www.semanticscholar.org/paper/9fbdeda9408b69b086aa362e3f59d5ed6866a640", "title": "A Re-examination of Dependency Path Kernels for Relation Extraction", "abstract": "Extracting semantic relations between entities from natural language text is an important step towards automatic knowledge extraction from large text collections and the Web. The state-of-the-art approach to relation extraction employs Support Vector Machines (SVM) and kernel methods for classification. Despite the diversity of kernels and the near exhaustive trial-and-error on kernel combination, there lacks a clear understanding of how these kernels relate to each other and why some are superior than others. In this paper, we provide an analysis of the relative strength and weakness of several kernels through systematic experimentation. We show that relation extraction can benefit from increasing the feature space through convolution kernel and introducing bias towards more syntactically meaningful feature space. Based on our analysis, we propose a new convolution dependency path kernel that combines the above two benefits. Our experimental results on the standard ACE 2003 datasets demonstrate that our new kernel gives consistent and significantly better performance than baseline methods, obtaining very competitive results to the state-ofthe-art performance.", "venue": "International Joint Conference on Natural Language Processing", "citationCount": 48, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke"], "mention_counts": {"ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "d82a9682917b8825571dfd9d513c134297e1c6f8", "url": "https://www.semanticscholar.org/paper/d82a9682917b8825571dfd9d513c134297e1c6f8", "title": "Semantic Analysis of Tag Similarity Measures in Collaborative Tagging Systems", "abstract": "Social bookmarking systems allow users to organise collections of resources on the Web in a collaborative fashion. The increasing popularity of these systems as well as first insights into their emergent semantics have made them relevant to disciplines like knowledge extraction and ontology learning. The problem of devising methods to measure the semantic relatedness between tags and characterizing it semantically is still largely open. Here we analyze three measures of tag relatedness: tag co-occurrence, cosine similarity of co-occurrence distributions, and FolkRank, an adaptation of the PageRank algorithm to folksonomies. Each measure is computed on tags from a large-scale dataset crawled from the social bookmarking system del.icio.us. To provide a semantic grounding of our findings, a connection to WordNet (a semantic lexicon for the English language) is established by mapping tags into synonym sets of WordNet, and applying there well-known metrics of semantic similarity. Our results clearly expose different characteristics of the selected measures of relatedness, making them applicable to different subtasks of knowledge extraction such as synonym detection or discovery of concept hierarchies.", "venue": "LWA", "citationCount": 83, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ke", "ke"], "mention_counts": {"ke": 2, "onto": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "b9a62fe32020cebf47837c6b1b062cc7fe55e829", "url": "https://www.semanticscholar.org/paper/b9a62fe32020cebf47837c6b1b062cc7fe55e829", "title": "Object-oriented knowledge extraction using universal exploiters", "abstract": "This paper contains analysis and extension of exploiters-based knowledge extraction methods, which allow generation of new knowledge, based on the basic ones. The main achievement of the paper is useful features of some universal exploiters proof, which allow extending set of basic classes and set of basic relations by finite set of new classes of objects and relations among them, which allow creating of complete lattice. Proposed approach gives an opportunity to compute quantity of new classes, which can be generated using it, and quantity of different types, which each of obtained classes describes; constructing of defined hierarchy of classes with determined subsumption relation; avoidance of some problems of inheritance and more efficient restoring of basic knowledge within the database.", "venue": "2017 12th International Scientific and Technical Conference on Computer Sciences and Information Technologies (CSIT)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "108b705542233e7142054953e120325a3bec323b", "url": "https://www.semanticscholar.org/paper/108b705542233e7142054953e120325a3bec323b", "title": "Event-based knowledge extraction from free-text descriptions for art images by using semantic role labeling approaches", "abstract": "Purpose \u2013 The purpose of this paper is to show how previous studies have demonstrated that non\u2010professional users prefer using event\u2010based conceptual descriptions, such as \u201ca woman wearing a hat\u201d, to describe and search images. In many art image archives, these conceptual descriptions are manually annotated in free\u2010text fields. This study aims to explore technologies to automate event\u2010based knowledge extractions from these free\u2010text image descriptions.Design/methodology/approach \u2013 This study presents an approach based on semantic role labeling technologies for automatically extracting event\u2010based knowledge, including subject, verb, object, location and temporal information from free\u2010text image descriptions. A query expansion module is applied to further improve the retrieval recall. The effectiveness of the proposed approach is evaluated by measuring the retrieval precision and recall capabilities for experiments with real life art image collections in museums.Findings \u2013 Evaluations results indicate that ...", "venue": "Electronic library", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "960f78a08aa305492970438a81ee50ff48e21532", "url": "https://www.semanticscholar.org/paper/960f78a08aa305492970438a81ee50ff48e21532", "title": "Loop - Enabling 3D stochastic geological modelling", "abstract": "Summary Loop is a new open source 3D geological and geophysical modelling platform in full development. The new platform consists of 4 main work packages: Knowledge Management: use of AI techniques for knowledge extraction from literature, maps and reports using geological ontology. Geological rules will be encoded to ensure proper knowledge extraction. Geological Event Management: Loop is a time-aware geological modelling platform and the event manager is capturing topological and time relationship between geological objects and structural events Forward and inverse structural modelling: we will encode structural geological rules in a time-aware context to account for folds (including overprinting), faults, shear zones, unconformities and intrusions. The modelling is based on probabilistic modelling and allows for the definition of an objective function for geology and quantification of uncertainty via posterior probabilities. Uncertainty characterisation and modelling: using stochastic simulations or the result of Bayesian modelling, Loop allows for characterisation and quantification of 3D uncertainty.", "venue": "ASEG Extended Abstracts", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ke", "ke"], "mention_counts": {"ke": 2, "onto": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "0f45c7be305c567293ab1fa6bcdb249fe89efded", "url": "https://www.semanticscholar.org/paper/0f45c7be305c567293ab1fa6bcdb249fe89efded", "title": "Optimized Automotive Fault-Diagnosis based on Knowledge Extraction from Web Resources", "abstract": "The maintenance and repair of modern vehicles is a challenge for garages, as different causes of faults lead to similar symptoms in the highly complex vehicles these days. Existing processes for fault-diagnosis based on manufacturer service manuals and human experiences are often inadequate and result in high effort and wrong decisions. In addition to these service manuals which provide basic models for e.g., diagnostic terms, primary physical quantities, causal relationships, and plausibilities, nowadays, internet forums offer a comprehensive source of experiences for solutions to these challenges. This paper, therefore, presents methods for the extraction of knowledge from unstructured and informal contributions in internet forums with the goal to synthesize diagnostic graphs from the established knowledge base, which are part of a maintenance software to supports garages in the maintenance of vehicles by suggesting more efficient and target-oriented diagnostic and maintenance actions in real-time.", "venue": "2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "a44c1b0e21f37de260068f81210ee1ad2dd20f16", "url": "https://www.semanticscholar.org/paper/a44c1b0e21f37de260068f81210ee1ad2dd20f16", "title": "SOA-Based Integration of Text Mining Services", "abstract": "Text Mining has established itself as a valuable tool for knowledge extraction in many commercial and scientific areas. Accordingly, a large number of different methods have been developed focusing on a broad range of different tasks. We report on a novel system architecture that is fundamentally service-based, i.e., it models and implements text mining and knowledge extraction routines as independent, yet federated services. The system has several layers: (1) Base services perform various fundamental extraction tasks. They all implement a fixed interface but keep their particular algorithms and functionality. (2) A metaservice acting as a central access point to those base services, thus providing a homogeneous interface to different algorithms. (3) An aggregation service on top of the metaservice which implements functionality to graphically show, compare, and aggregate the results of different base services. Each layer is accessible as a Web Service and thus ready to be integrated in applications that are higher up in the value chain, such as authoring tools or systems for the automatic construction of knowledge bases. We developed our system with a focus on the mining of Life Science text collections. It is available from http://www.bc-viscon.net.", "venue": "2009 Congress on Services - I", "citationCount": 8, "fieldsOfStudy": ["Computer Science", "Materials Science"], "mentions": ["kg", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 1, "ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "d36f5e96606531ff99a83d95bb650ad39ce13a97", "url": "https://www.semanticscholar.org/paper/d36f5e96606531ff99a83d95bb650ad39ce13a97", "title": "Knowledge Extraction from Bangla Documents using NLP: A Case Study", "abstract": "In this paper, we have proposed a system that determines and extracts the user query from the vast store of official Bangla digital documents, and performs detection, and analysis of the documents. A set of different rules and knowledge based methods is used to extract the required decisions from the resolutions of Bangla documents of a specific domain. Then, with diverse parameters, the effects of the process are discoursed and classified and compared with exact match and sematics features of the words with relation with other reative sentences where it achieved a higher performance for a sample dataset with a knowledge base of the documents.", "venue": "2019 International Conference on Electrical, Computer and Communication Engineering (ECCE)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "kg", "kg"], "mention_counts": {"nlp": 1, "ke": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "kg": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "25e335d98e91203d5ce80a820c230d2d1cff9145", "url": "https://www.semanticscholar.org/paper/25e335d98e91203d5ce80a820c230d2d1cff9145", "title": "Silverfish: A Contextual Knowledge Extraction and Aggregation System for Academics", "abstract": "Repositories like arXiv 1 and knowledge bases like CiteSeer 2 are increasingly becoming central to academicians and researchers. However, current systems provide too little semantic support like determining the topic of a paper, modelling the interests of a user, etc. Silversh is a contextual knowledge extraction and aggregation system that adds a signicant amount of semantics to academic repositories. Silversh automatically extracts pertinent key phrases and models semantic content in the form of a back end cooccurrence graph. This is used for \\intelligent\" operations like recommendations, nding related content and semantics based routing of announcements.", "venue": "International Conference on Management of Data", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "e7b39b6d8f1d8c0eba43590703723f895fff6786", "url": "https://www.semanticscholar.org/paper/e7b39b6d8f1d8c0eba43590703723f895fff6786", "title": "A hybrid approach for standardized Dictionary-based knowledge extraction for Arabic morpho-semantic retrieval", "abstract": "We propose in this paper to exploit Arabic dictionaries to enhance Arabic Information Retrieval (IR). We use standardized LMF dictionaries. We first put forward to mine such dictionaries and to represent them into graph-based representation. This graph will also be mined with a hybrid approach that combines both linguistic and statistical techniques to extract useful knowledge for IR. We study how extracted knowledge from such resource and added to the initial queries can attentively affect the retrieval process and results. Several query expansion strategies are carried based on morphological, semantic and morpho-semantic queries terms relations.", "venue": "2018 IEEE 2nd International Workshop on Arabic and Derived Script Analysis and Recognition (ASAR)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "a1779ab6793bd1d808c65350a75d2810979882df", "url": "https://www.semanticscholar.org/paper/a1779ab6793bd1d808c65350a75d2810979882df", "title": "Antecedent Redundancy Exploitation in Fuzzy Rule Interpolation-based Reinforcement Learning", "abstract": "This paper introduces novel methods which could improve the efficiency of the automated knowledge extraction methods used in the FRIQ-learning (Fuzzy Rule Interpolationbased Q-learning) machine learning method. For solving a given problem, the FRIQ-learning reinforcement learning method is capable of constructing a sparse fuzzy rule-base, which does not need to contain all the possible rules as traditional fuzzy control requires. Hence it is sufficient to keep only the most important rules due to Fuzzy Rule Interpolation (FRI). Finding those specific rules which are important to solve the given problem is not a trivial task. Some possible strategies for removing these kinds of unimportant rules from the rule-base have already been introduced, but no strategies addressing the antecedents of the rules have been developed yet. The solutions proposed in this paper allow the further reduction of these rule-bases, thus facilitating the creation of a sparse fuzzy rule-base from which the knowledge can be directly extracted. Since the form of fuzzy rules are inherently self-describing, the size of the rule-base is the key for keeping this kind of knowledge base human-readable. Possible mechatronics applications of these methods include optimizing behaviour-based control models for robotics, and also knowledge extraction in a fuzzy rule-base format from models where the real operating knowledge of the model is not known, which rule-bases then can be easily adopted in robot control applications.", "venue": "2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "3d46e008543b353b51407fb805edbd1da23dfe6c", "url": "https://www.semanticscholar.org/paper/3d46e008543b353b51407fb805edbd1da23dfe6c", "title": "Language Models as a Knowledge Source for Cognitive Agents", "abstract": "Language models (LMs) are sentence-completion engines trained on massive corpora. LMs have emerged as a significant breakthrough in natural-language processing, providing capabilities that go far beyond sentence completion including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, exploiting language models as a source of task knowledge, especially for task learning, offers significant, near-term benefits. We introduce language models and the various tasks to which they have been applied and then review methods of knowledge extraction from language models. The resulting analysis outlines both the challenges and opportunities for using language models as a new knowledge source for cognitive systems. It also identifies possible ways to improve knowledge extraction from language models using the capabilities provided by cognitive systems. Central to success will be the ability of a cognitive agent to itself learn an abstract model of the knowledge implicit in the LM as well as methods to extract high-quality knowledge effectively and efficiently. To illustrate, we introduce a hypothetical robot agent and describe how language models could extend its task knowledge and improve its performance and the kinds of knowledge and methods the agent can use to exploit the knowledge within a language model.", "venue": "ArXiv", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "ke"], "mention_counts": {"nlp": 1, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "49d64ea3ee90932903ca19efb8f4140e46d2b825", "url": "https://www.semanticscholar.org/paper/49d64ea3ee90932903ca19efb8f4140e46d2b825", "title": "Knowledge Extraction and Knowledge Integration governed by \u0141ukasiewicz Logics", "abstract": "The development of machine learning in particular and artificial intelligent in general has been strongly conditioned by the lack of an appropriate interface layer between deduction, abduction and induction. In this work we extend traditional algebraic specification methods in this direction. Here we assume that such interface for AI emerges from an adequate Neural-Symbolic integration. This integration is made for universe of discourse described on a Topos governed by a many-valued {\\L}ukasiewicz logic. Sentences are integrated in a symbolic knowledge base describing the problem domain, codified using a graphic-based language, wherein every logic connective is defined by a neuron in an artificial network. This allows the integration of first-order formulas into a network architecture as background knowledge, and simplifies symbolic rule extraction from trained networks. For the train of such neural networks we changed the Levenderg-Marquardt algorithm, restricting the knowledge dissemination in the network structure using soft crystallization. This procedure reduces neural network plasticity without drastically damaging the learning performance, allowing the emergence of symbolic patterns. This makes the descriptive power of produced neural networks similar to the descriptive power of {\\L}ukasiewicz logic language, reducing the information lost on translation between symbolic and connectionist structures. We tested this method on the extraction of knowledge from specified structures. For it, we present the notion of fuzzy state automata, and we use automata behaviour to infer its structure. We use this type of automata on the generation of models for relations specified as symbolic background knowledge.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 1, "ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "466c4e340fa244cddd63213e314d46152a1ffb74", "url": "https://www.semanticscholar.org/paper/466c4e340fa244cddd63213e314d46152a1ffb74", "title": "Knowledge-Rich Context Extraction and Ranking with KnowPipe", "abstract": "This paper presents ongoing Phd thesis work dealing with the extraction of knowledge-rich contexts from text corpora for terminographic purposes. Although notable progress in the field has been made over recent years, there is yet no methodology or integrated workflow that is able to deal with multiple, typologically different languages and different domains, and that can be handled by non-expert users. Moreover, while a lot of work has been carried out to research the KRC extraction step, the selection and further analysis of results still involves considerable manual work. In this view, the aim of this paper is two-fold. Firstly, the paper presents a ranking algorithm geared at supporting the selection of high-quality contexts once the extraction has been finished and describes ranking experiments with Russian context candidates. Secondly, it presents the KnowPipe framework for context extraction: KnowPipe aims at providing a processing environment that allows users to extract knowledge-rich contexts from text corpora in different languages using shallow and deep processing techniques. In its current state of development, KnowPipe provides facilities for preprocessing Russian and German text corpora, for pattern-based knowledge-rich context extraction from these corpora using shallow analysis as well as tools for ranking Russian context candidates.", "venue": "LREC", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 1, "ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "6d3f32977247afacc5f91f8fc548dcb3614de950", "url": "https://www.semanticscholar.org/paper/6d3f32977247afacc5f91f8fc548dcb3614de950", "title": "A Knowledge Discovery Methodology for Semantic Categorization of Unstructured Textual Sources", "abstract": "We describe a methodology for identifying characterizing terms from a source text or paper and automatically building an ontology around them, with the purpose of semantically categorizing a paper corpus where documents sharing similar subjects may be subsequently clustered together by means of ontology alignment. We first employ a Natural Language Processing pipeline to extract relevant terms from the source text, and then use a combination of a pattern-based and machine-learning approach to establish semantic relationships among those terms, with some user's feedback required in-between. This methodology for discovering characterizing knowledge from textual sources finds its inception as an extension of PRAISED, our abbreviation discovery framework, in order to enhance its resolution capabilities. By moving from a paper-by-paper, mainly syntactical process to a corpus-based, semantic approach, it was in fact possible to overcome earlier limits of the system related to abbreviations whose explanation could not be found within the same paper they were cited in. At the same time, though, the methodology we present is not tied to this specific task, but is instead of relevance for a variety of contexts, and might therefore be used to build a stand-alone system for advanced knowledge extraction and semantic categorization.", "venue": "2012 Eighth International Conference on Signal Image Technology and Internet Based Systems", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "ke"], "mention_counts": {"nlp": 1, "onto": 2, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "055b27558b78861d716b89632a345069a0dffb89", "url": "https://www.semanticscholar.org/paper/055b27558b78861d716b89632a345069a0dffb89", "title": "Towards Comprehensive Noise Detection in Automatically Created Knowledge Graphs", "abstract": "Knowledge Graphs (KGs) play a key role in many artificial intelligence applications. Large KGs are often constructed through a noisy automatic knowledge extraction process. Noise detection is, therefore, an important task for having high-quality KGs. We argue that the current noise detection approaches only focus on a specific type of noise (i.e., fact checking) whereas knowledge extraction methods result in more than one type of noise. To this end, we propose a classification of noise found in automatically-constructed KGs, and an approach for noise detection focused on specific types of noise.", "venue": "SEMWEB", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "0322b19f26f96ebc0a47f4862f24d912dd8abc5d", "url": "https://www.semanticscholar.org/paper/0322b19f26f96ebc0a47f4862f24d912dd8abc5d", "title": "Extraction of knowledge on spatial distribution and spatial relationship from scanned topographic map using Convolutional Neural Networks", "abstract": "Abstract. Topographic maps (TM) contain plenty of geographic information, such as topographic fluctuations, hydrological networks, vegetation, administrative regions, residential areas, transportation routes and facilities and other man-made features. Based on geographic information, the map knowledge extracted from topographic maps has been widely used in many research fields, such as landscape ecology, land and resources management and urbanization.Traditional topographic maps are generally in paper-format. It is difficult to use them for the spatial or multi-temporal analysis. Thus many research work focus on the extraction of geographic information based on scanned topographic maps (STM).Most of the existing studies developed many methods and algorithms to extract the geographical information from scanned topographic maps. However, these proposed methods usually only can extract a certain kind of feature, and parameters used in these methods are needed to set manually. However, for map knowledge, e.g. spatial distribution and spatial relationship among different map features, it is difficult to effectively combine different methods to extract map knowledge. Therefore, this paper proposes a method of extracting geographic knowledge based on deep-learning, which can be object-oriented and efficiently extract geographic knowledge. This method contains three steps: 1) establishing samples for different map features; 2) using the Convolutional Neural Networks (CNN), which is suited to the image recognition (Karpathy A et al. 2014), to classify the scanned topographic map; 3) estimating the proportion of different map features on maps and describing the spatial distribution based on a grid.The method proposed in this study has been evaluated by some scale topographic maps. The results indicate that the extraction precise of this method can reach more than 70% for water and mountain areas and can also describe the spatial distribution for the features with larger map areas.\n", "venue": "Abstracts of the ICA", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 1, "ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "c69ef10cb71cf4909993d1b509b474ef3730a575", "url": "https://www.semanticscholar.org/paper/c69ef10cb71cf4909993d1b509b474ef3730a575", "title": "Special section on mining knowledge from scientific data", "abstract": "The past two decades have witnessed the rapid growth of scientific publications in all areas of research. Easier access to published literature (open access, arxiv preprints, etc.) along with the recent development in computational methods, has provided researchers with a productive platform to study vast amounts of scholarly data. Scholarly data mining has thus made it possible to do \u201cresearch about research!\u201d It plays a vital role in scientometrics, bibliometrics, webometrics, and altmetrics, which require applying sophisticated algorithms to curate and derive useful insights from scholarly data. Moreover, the knowledge extracted from the scientific data can help in several decision-making processes such as policy making for fund disbursement, identifying research gap in a department and recruiting faculties to fill up the gap, speculating upcoming research areas, etc. On the other hand, the increasing popularity and use of these metrics as a measure of the quality of research output, for determining university rankings, and in decision making (tenure and recruitment decisions), has also given rise to objectionable practices to artificially boost these measures (self-citations, citation-cliques, etc.). Given that, is it always right to consider these metrics as a reliable proxy of research quality? How should decision and policymakers use these metrics to account for such malpractices? This special section aims to bring together the latest groundbreaking research on issues related to knowledge extraction and deriving insights from scientific data. Of special interest is the role these metrics play in policy and decision making \u2013 both positives and negatives. We welcomed both theoretical and empirical research, and case studies that lead to the development of novel algorithms, tool, techniques, metrics, decisions and measurements related to scholarly data. A total of 12 papers were submitted to this special section. All the submitted papers underwent a rigorous review process. Each paper was reviewed by at least two reviewers. Every paper went through at least two rounds of revisions. The contributions of the accepted papers are briefly summarized below. Outlier detection in data mining is a major research agenda. However, when it comes to scientometric research, such outliers indicate malpractices in scientific research, resulting in issues such as citation cartels and citation stacking. Chakraborty et al. (2020) defined a diverse feature set that can identify such cases of extreme outliers and reasoned them. They also showed the effect of such outlier behaviour on the bibliographic metrics such as h-index and impact factor. Madisetty et al. (2020) proposed a tool to extract inline mathematical expressions from scientific articles. This is a major problem in scientific document processing as mathematic systems often act as a bottleneck due to their cryptic symbols that a parse is unable to extract. The authors proposed two models the first one uses conditional random field using hand-crafted features and the second one uses bidirectional LSTM. This work contributes to building a real-world tool or can act as a plugin of a scientific document parser. Document classification is always an important problem in text processing. In scientific data mining, document classification is needed to categorize scientific papers into topics, keywords, etc. Masmoudi et al. (2020) proposed a novel hierarchical document classification approach using limited labelled data. They utilized the co-training paradigm to exploit content and bibliographic coupling information as two distinct papers' views. A massive unlabelled data was utilized during co-training. We hope that this special section will provide insights, analysis, and understanding about \u201cscientific research\u201d and help in building the foundation for future research and development in scientometrics. We sincerely thank the Editor-in-Chief of this journal, Dr Jon G. Hall for accepting our request to organize the special section with the Expert Systems journal. We would also like to thank the entire editorial team of the journal, the authors who submitted their valuable research to this special section and the reviews for their support for timely evaluations and comments.", "venue": "Expert Syst. J. Knowl. Eng.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "tp", "ke"], "mention_counts": {"ke": 2, "tp": 1}, "nlp_mention_counts": {"ke": 2, "tp": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "4d374c310e8d5c967f47747674b33de3fe332a5e", "url": "https://www.semanticscholar.org/paper/4d374c310e8d5c967f47747674b33de3fe332a5e", "title": "Extracting knowledge from diagnostic databases", "abstract": "The use of natural language processing and machine learning techniques to help interpret, characterize, and standardize data, thereby enhancing the extraction of knowledge from diagnostic databases, is discussed. In particular Lexfix, a vocabulary correction and standardization system, has been designed to improve keyword-based retrieval on free-form text fields in GM's Technical Assistance System (TAS) database, which contains about 300000 cases of vehicle symptoms and repair information. Also implemented was a natural language parser called TASLink, designed to interpret various kinds of ill-formed English, particularly free-form descriptions of vehicle faults. Inferule, an inductive machine-learning system that infers diagnostic rules from database cases containing information about vehicle symptoms and their solutions, is also described.<<ETX>>", "venue": "IEEE Expert", "citationCount": 18, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "ke"], "mention_counts": {"nlp": 1, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "5af6d25908902515a5c6d010ede65c1e001fff3f", "url": "https://www.semanticscholar.org/paper/5af6d25908902515a5c6d010ede65c1e001fff3f", "title": "Ontology-based workflow extraction from texts using word sense disambiguation", "abstract": "This paper introduces a method for automatic workflow extraction from texts using Process-Oriented Case-Based Reasoning (POCBR). While the current workflow management systems implement mostly different complicated graphical tasks based on advanced distributed solutions (e.g. cloud computing and grid computation), workflow knowledge acquisition from texts using case-based reasoning represents more expressive and semantic cases representations. We propose in this context, an ontology-based workflow extraction framework to acquire processual knowledge from texts. Our methodology extends classic NLP techniques to extract and disambiguate tasks in texts. Using a graph-based representation of workflows and a domain ontology, our extraction process uses a context-based approach to recognize workflow components : data and control flows. We applied our framework in a technical domain in bioinformatics : i.e. phylogenetic analyses. An evaluation based on workflow semantic similarities on a gold standard proves that our approach provides promising results in the process extraction domain. Both data and implementation of our framework are available in : http://labo.bioinfo.uqam.ca/tgrowler.", "venue": "bioRxiv", "citationCount": 4, "fieldsOfStudy": ["Biology", "Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "wsd"], "mention_counts": {"nlp": 1, "wsd": 1, "onto": 3}, "nlp_mention_counts": {"nlp": 1, "wsd": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "b60a2d982c1fe2f17a0f14db849bc9f0da1953cc", "url": "https://www.semanticscholar.org/paper/b60a2d982c1fe2f17a0f14db849bc9f0da1953cc", "title": "Automated Extraction of Causal Relations from Text for Teaching Surgical Concepts", "abstract": "Effective teaching of surgical decision making requires providing students with a deep understanding of the domain so that they have the ability to make decisions in novel situations. This means providing them with a thorough understanding of causal relations between actions and their possible effects in the context of various states of the patient as well as previous actions. Intelligent tutoring systems to teach surgical decision making thus require such domain knowledge, but there are currently no medical ontologies that encompass it. While it is possible to engineer the needed ontologies by hand, this requires a large effort for every new domain to be covered. In this paper we explore the possibility of automatically extracting causal relations from textbooks on surgery. Specifically, we adapt the spaCy NLP tool for this task and apply it to a collection of fifteen textbooks on endodontic root canal treatment, which is one of the most challenging areas of dental surgery. Since the main purpose is to extract knowledge for teaching, we focus on actions that can lead to surgical mishaps. We evaluate the precision and recall of the extracted relations using a gold standard prepared by a pair of dental surgeons.", "venue": "2020 IEEE International Conference on Healthcare Informatics (ICHI)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ke", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "821ee8258efbfa17ab41713674bfdddaea3a9846", "url": "https://www.semanticscholar.org/paper/821ee8258efbfa17ab41713674bfdddaea3a9846", "title": "AutoKG - An Automotive Domain Knowledge Graph for Software Testing: A position paper", "abstract": "Industries have a significant amount of data in semi-structured and unstructured formats which are typically captured in text documents, spreadsheets, images, etc. This is especially the case with the software description documents used by domain experts in the automotive domain to perform tasks at various phases of the Software Development Life Cycle (SDLC). In this paper, we propose an end-to-end pipeline to extract an Automotive Knowledge Graph (AutoKG) from textual data using Natural Language Processing (NLP) techniques with the application of automatic test case generation. The proposed pipeline primarily consists of the following components: 1) AutoOntology, an ontology that has been derived by analyzing several industry scale automotive domain software systems, 2) AutoRE, a Relation Extraction (RE) model to extract triplets from various sentence types typically found in the automotive domain, and 3) AutoVec, a neural embedding based algorithm for triplet matching and context-based search. We demonstrate the pipeline with an application of automatic test case generation from requirements using AutoKG.", "venue": "2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "kg", "nlp", "kg"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "059c33fb6c7e4c9293b96c1b3a46403fdd5c4c3a", "url": "https://www.semanticscholar.org/paper/059c33fb6c7e4c9293b96c1b3a46403fdd5c4c3a", "title": "Interpretability of BERT Latent Space through Knowledge Graphs", "abstract": "The advent of pretrained language have renovated the ways of handling natural languages, improving the quality of systems that rely on them. BERT played a crucial role in revolutionizing the Natural Language Processing (NLP) area. However, the deep learning framework it implements lacks interpretability. Thus, recent research efforts aimed to explain what BERT learns from the text sources exploited to pre-train its linguistic model. In this paper, we analyze the latent vector space resulting from the BERT context-aware word embeddings. We focus on assessing whether regions of the BERT vector space hold an explicit meaning attributable to a Knowledge Graph (KG). First, we prove the existence of explicitly meaningful areas through the Link Prediction (LP) task. Then, we demonstrate these regions being linked to explicit ontology concepts of a KG by learning classification patterns. To the best of our knowledge, this is the first attempt at interpreting the BERT learned linguistic knowledge through a KG relying on its pretrained context-aware word embeddings.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "onto", "nlp", "kg"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "4ca331ed6853d8bface7564c55efb9f1e66d719a", "url": "https://www.semanticscholar.org/paper/4ca331ed6853d8bface7564c55efb9f1e66d719a", "title": "Text Analysis and Knowledge Extraction", "abstract": "i. Introduction The study of text understanding and knowlegde extraction has been actively done by many researchers. The authors also studied a method of structured information extraction from texts without a global text analysis. The method is available for a comparatively sbort text such as a patent claim clause and an abstract of a technical paper. This paper describes tile outline of a method of knowledge extraction from a longer text which needs a global tex analysis. The kinds of texts ~e expository texts ~) or explanation texts-'. Expository texts described here mean those which have various hierarchical headings such as a title, a heading of each section and sometimes an abstract. In this deEinJtion, most of texts, including technical papers reports and newspapers, are expository. Texts of this kind disclose the main knowledge in a top-down manner and show not only the location of an attribute value in a text but also severn[ key points of the content. This property of expository texts contrasts with that of novels and stories in which an unexpected development of the plot is preferred. This paper pays attention to such characteristics of expository texts and describes a method of anal yzing texts by referring to information contained in the intersentential relations and the headings of texts and then extracting requested knowledge such as a summary from texts in an efficient way.", "venue": "COLING", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ie"], "mention_counts": {"ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "68cda035af2b900d5155fd4e54cb09bf0140bc7b", "url": "https://www.semanticscholar.org/paper/68cda035af2b900d5155fd4e54cb09bf0140bc7b", "title": "A knowledge model for analysis and simulation of regulatory networks", "abstract": "MOTIVATION\nIn order to aid in hypothesis-driven experimental gene discovery, we are designing a computer application for the automatic retrieval of signal transduction data from electronic versions of scientific publications using natural language processing (NLP) techniques, as well as for visualizing and editing representations of regulatory systems. These systems describe both signal transduction and biochemical pathways within complex multicellular organisms, yeast, and bacteria. This computer application in turn requires the development of a domain-specific ontology, or knowledge model.\n\n\nRESULTS\nWe introduce an ontological model for the representation of biological knowledge related to regulatory networks in vertebrates. We outline a taxonomy of the concepts, define their 'whole-to-part' relationships, describe the properties of major concepts, and outline a set of the most important axioms. The ontology is partially realized in a computer system designed to aid researchers in biology and medicine in visualizing and editing a representation of a signal transduction system.", "venue": "Bioinform.", "citationCount": 80, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "9b1e8c86c8ecace7d0dffb77d49dc41c1ba7425a", "url": "https://www.semanticscholar.org/paper/9b1e8c86c8ecace7d0dffb77d49dc41c1ba7425a", "title": "A Large DataBase of Hypernymy Relations Extracted from the Web.", "abstract": "Hypernymy relations (those where an hyponym term shares a \u201cisa\u201d relationship with his hypernym) play a key role for many Natural Language Processing (NLP) tasks, e.g. ontology learning, automatically building or extending knowledge bases, or word sense disambiguation and induction. In fact, such relations may provide the basis for the construction of more complex structures such as taxonomies, or be used as effective background knowledge for many word understanding applications. We present a publicly available database containing more than 400 million hypernymy relations we extracted from the CommonCrawl web corpus. We describe the infrastructure we developed to iterate over the web corpus for extracting the hypernymy relations and store them effectively into a large database. This collection of relations represents a rich source of knowledge and may be useful for many researchers. We offer the tuple dataset for public download and an Application Programming Interface (API) to help other researchers programmatically query the database.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 100, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "kg", "wsd"], "mention_counts": {"nlp": 2, "wsd": 1, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2, "wsd": 1}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "ae270781a06f20fdf08cfa4747a7f51db36b142f", "url": "https://www.semanticscholar.org/paper/ae270781a06f20fdf08cfa4747a7f51db36b142f", "title": "The distinction between linguistic and conceptual semantics in medical terminology and its implication for NLP-based knowledge acquisition.", "abstract": "Natural language understanding systems have to exploit various kinds of knowledge in order to represent the meaning behind texts. Getting this knowledge in place is often such a huge enterprise that it is tempting to look for systems that can discover such knowledge automatically. We describe how the distinction between conceptual and linguistic semantics may assist in reaching this objective, provided that distinguishing between them is not done too rigorously. We present several examples to support this view and argue that in a multilingual environment, linguistic ontologies should be designed as interfaces between domain conceptualizations and linguistic knowledge bases.", "venue": "Methods of Information in Medicine", "citationCount": 48, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["kg", "nlu", "onto", "nlp", "kg"], "mention_counts": {"nlp": 1, "kg": 2, "onto": 1, "nlu": 1}, "nlp_mention_counts": {"nlp": 1, "nlu": 1}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "cc5ebaf46c35386923a775808e1aa0dee9f1a686", "url": "https://www.semanticscholar.org/paper/cc5ebaf46c35386923a775808e1aa0dee9f1a686", "title": "A Question and Answering System for Management of Cloud Service Level Agreements", "abstract": "One of the key challenges faced by consumers is to efficiently manage and monitor the quality of cloud services. To manage service performance, consumers have to validate rules embedded in cloud legal contracts, such as Service Level Agreements (SLA) and Privacy Policies, that are available as text documents. Currently this analysis requires significant time and manual labor and is thus inefficient. We propose a cognitive assistant that can be used to manage cloud legal documents by automatically extracting knowledge (terms, rules, constraints) from them and reasoning over it to validate service performance. In this paper, we present this Question and Answering (Q&A) system that can be used to analyze and obtain information from the SLA documents. We have created a knowledgebase of Cloud SLAs from various providers which forms the underlying repository of our Q&A system. We utilized techniques from natural language processing and semantic web (RDF, SPARQL and Fuseki server) to build our framework. We also present sample queries on how a consumer can compute metrics such as service credit.", "venue": "2017 IEEE 10th International Conference on Cloud Computing (CLOUD)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "sw", "rdf"], "mention_counts": {"nlp": 1, "sw": 1, "ke": 1, "rdf": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "sw": 1, "rdf": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "633a9355f0bc8553c24e22f69c73d9abd3bf7452", "url": "https://www.semanticscholar.org/paper/633a9355f0bc8553c24e22f69c73d9abd3bf7452", "title": "Knowledge Extraction and Prediction from Behavior Science Randomized Controlled Trials: A Case Study in Smoking Cessation", "abstract": "Due to the fast pace at which randomized controlled trials are published in the health domain, researchers, consultants and policymakers would benefit from more automatic ways to process them by both extracting relevant information and automating the meta-analysis processes. In this paper, we present a novel methodology based on natural language processing and reasoning models to 1) extract relevant information from RCTs and 2) predict potential outcome values on novel scenarios, given the extracted knowledge, in the domain of behavior change for smoking cessation.", "venue": "AMIA", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "nlp"], "mention_counts": {"nlp": 1, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "26210e645b5261da7d9ab5a9c2cab9306ebd3777", "url": "https://www.semanticscholar.org/paper/26210e645b5261da7d9ab5a9c2cab9306ebd3777", "title": "Extracting knowledge from technological research papers in application of IoT", "abstract": "In this paper, we have extracted knowledge about IoT based on semantic analysis of corpus data which is constructed from IoT IEEE survey papers that were published recently. For the basic understanding of research tendency, common biblio-metric approach such as tf-idf is introduced. Since 2009, the number of publication of survey papers related to IoT has been increased dramatically and there are about 6 to 7 application areas where IoT would introduce an innovative usage of network and information technologies. Upon these basic understanding, we try to construct method to extract `knowledge' from those documents. We have introduced inference rules to the semantic relationship in sentences. Each sentence and consisting words are indexed as RDF primary nodes and are stored in triple-data-store. We have added several inference rules by looking at meaningful words and sentences. In this sense, the method is not fully machine-oriented; we applied heuristic knowledge by reading sentences and discourses about the technological issues of those survey papers. The result shows deeper understanding of an issue described in large amounts of documents in a short period of time. It is possible to apply this method for different area of expertise. This analysis has been pursued in a context of foresight activity in science and technology policy.", "venue": "Portland International Conference on Management of Engineering and Technology", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "ke", "ke"], "mention_counts": {"ke": 2, "rdf": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "rdf": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "0c4231cba584d3531a561564c5f50c77aa4d78d9", "url": "https://www.semanticscholar.org/paper/0c4231cba584d3531a561564c5f50c77aa4d78d9", "title": "Towards Controlled Natural Language for Semantic Annotation", "abstract": "Richly interlinked metadata constitute the foundation of the Semantic Web. Manual semantic annotation is a labor intensive task requiring training in formal ontological descriptions for the otherwise non-expert user. Although automatic annotation tools attempt to ease this knowledge acquisition barrier, their development often requires access to specialists in Natural Language Processing NLP. This challenges researchers to develop user-friendly annotation environments. Controlled Natural Languages CNLs offer an incentive to the novice user to annotate, while simultaneously authoring his/her respective documents in a user-friendly manner. CNLs have been successfully applied to ontology authoring, but little research has focused on their application to semantic annotation. This paper describes two novel approaches to semantic annotation, which permit non-expert users to simultaneously author and annotate meeting minutes using CNL. Finally, this work provides empirical evidence that for certain scenarios applying CNLs for semantic annotation can be more user friendly than a standard manual semantic annotation tool.", "venue": "Int. J. Semantic Web Inf. Syst.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "sw"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "f26613fa000a2e6879e15de57534c59d00604fa4", "url": "https://www.semanticscholar.org/paper/f26613fa000a2e6879e15de57534c59d00604fa4", "title": "Subjective Bayes Method for Word Semantic Similarity Measurement", "abstract": "Measuring semantic similarity between words is a classical problem in nature language processing, the result of which can promote many applications such as machine translation, word sense disambiguation, ontology mapping, computational linguistics, etc. This paper combines knowledge-based methods with statistical methods in measuring words similarity, the novel aspect of which is that subjective Bayes method is employed. Firstly, extract evidences based on Word Net, secondly, analyze reasonableness of candidate evidence using scatter plot, thirdly, generate sufficiency measure by statistics and piecewise linear interpolation technique, fourthly, obtain comprehensive posteriori by integrating uncertainty reasoning with conclusion uncertainty synthetic strategy, finally, we quantify word semantic similarity. On data set R&G (65), we conducted experiment through 5-fold cross validation, and the correlation of our experimental results with human judgment is 0.912, with 0.4% improvements over existing best practice, which show that using subjective Bayes method to measure word semantic similarity is reasonable and effective.", "venue": "2013 IEEE 13th International Conference on Data Mining Workshops", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "nlp", "wsd", "kg"], "mention_counts": {"onto": 1, "nlp": 1, "mt": 1, "kg": 1, "wsd": 1}, "nlp_mention_counts": {"nlp": 1, "wsd": 1, "mt": 1}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "fdb4468cee468d06506af68c6e5c81d3be234d05", "url": "https://www.semanticscholar.org/paper/fdb4468cee468d06506af68c6e5c81d3be234d05", "title": "The Penman Project on Knowledge-Based Machine Translation", "abstract": "The joint development, together with the ULTRA project at New Mexico State University and the Center for Machine Translation at Carnegie Mellon University, of an integrated knowledge-based machine-aided translation system called PANGLOSS. The ISI-specific work includes the development of English sentence generation and sentence planning capabilities and the construction of an Ontology of concepts to act as the semantic lexicon for all modules of the system as a whole. In addition, we continue to enhance Penman's existing generation technology, to collect and develop ancillary knowledge sources and software (such as grammars or bilingual dictionaries and lexicons for German, Japanese, Spanish, and Chinese), and to maintain and distribute Penman.", "venue": "HLT", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "mt", "kg", "kg", "onto"], "mention_counts": {"kg": 2, "onto": 1, "mt": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "ec12ba0a4a53822497de8d4a5e9a23db4d5e522e", "url": "https://www.semanticscholar.org/paper/ec12ba0a4a53822497de8d4a5e9a23db4d5e522e", "title": "Ontology-based Technical Text Annotation", "abstract": "Powerful tools could help users explore and maintain domain specific documentations, provided that documents have been semantically annotated. For that, the annotations must be sufficiently specialized and rich, relying on some explicit semantic model, usually an ontology, that repre- sents the semantics of the target domain. In this paper, we learn to annotate biomedical scientific publications with respect to a Gene Regulation Ontology. We devise a two-step approach to an- notate semantic events and relations. The first step is recast as a text segmentation and labeling problem and solved using machine translation tools and a CRF, the second as multi-class classi- fication. We evaluate the approach on the BioNLP-GRO benchmark, achieving an average 61% F-measure on the event detection by itself and 50% F-measure on biological relation annotation. This suggests that human annotators can be supported in domain specific semantic annotation tasks. Under different experimental settings, we also conclude some interesting observations: (1) For event detection and compared to classical time-consuming sequence labeling approach, the newly proposed machine translation based method performed equally well but with much less computation resource required. (2) A highly domain specific part of the task, namely proteins and transcription factors detection, is best performed by domain aware tools, which can be used separately as an initial step of the pipeline.", "venue": "COLING 2014", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "onto", "onto", "mt", "onto"], "mention_counts": {"onto": 3, "mt": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "3c59619603bb710f9aebb0f1cb0f437c8165c859", "url": "https://www.semanticscholar.org/paper/3c59619603bb710f9aebb0f1cb0f437c8165c859", "title": "Content Selection From Semantic Web Data", "abstract": "So far, there has been little success in Natural Language Generation in coming up with general models of the content selection process. Nonetheless, there has been some work on content selection that employ Machine learning or heuristic search. On the other side, there is a clear tendency in NLG towards the use of resources encoded in standard Semantic Web representation formats. For these reasons, we believe that time has come to propose an initial challenge on content selection from Semantic Web data. In this paper, we briefly outline the idea and plan for the execution of this task.", "venue": "International Conference on Natural Language Generation", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlg", "sw", "nlg", "sw"], "mention_counts": {"sw": 3, "nlg": 2}, "nlp_mention_counts": {"nlg": 2}, "ld_mention_counts": {"sw": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "cd287288c9fbaa97cbd3f4a070e9552592626fa5", "url": "https://www.semanticscholar.org/paper/cd287288c9fbaa97cbd3f4a070e9552592626fa5", "title": "Rare Disease Identification from Clinical Notes with Ontologies and Weak Supervision", "abstract": "The identification of rare diseases from clinical notes with Natural Language Processing (NLP) is challenging due to the few cases available for machine learning and the need of data annotation from clinical experts. We propose a method using ontologies and weak supervision. The approach includes two steps: (i) Text-to-UMLS, linking text mentions to concepts in Unified Medical Language System (UMLS), with a named entity linking tool (e.g. SemEHR) and weak supervision based on customised rules and Bidirectional Encoder Representations from Transformers (BERT) based contextual representations, and (ii) UMLS-to-ORDO, matching UMLS concepts to rare diseases in Orphanet Rare Disease Ontology (ORDO). Using MIMIC-III US intensive care discharge summaries as a case study, we show that the Text-to-UMLS process can be greatly improved with weak supervision, without any annotated data from domain experts. Our analysis shows that the overall pipeline processing discharge summaries can surface rare disease cases, which are mostly uncaptured in manual ICD codes of the hospital admissions.", "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society", "citationCount": 6, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "onto", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "93afcfacc69666365e5cb9f960caed0f6f3763d0", "url": "https://www.semanticscholar.org/paper/93afcfacc69666365e5cb9f960caed0f6f3763d0", "title": "Using Linked Data for Better Navigation in Summaries of Product Characteristics", "abstract": "Summaries of product characteristics (SPCs) serve as a basic source of information for physicians about medicinal products. SPC is attached to every single registered medicinal product and contains large amount of valuable data in natural language. In this paper we deal with natural language processing (NLP), annotation and Linked Data representation of SPCs. Moreover, we also use the annotations for acquisition of new information about, e.g., interactions. A web application for browsing Linked Data representation of SPCs has been developed.", "venue": "Workshop on Semantic Web Applications and Tools for Life Sciences", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "nlp", "nlp", "ld", "ld"], "mention_counts": {"ld": 3, "nlp": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"ld": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "318ae285b92fa59cde733fae22c27cb6da099910", "url": "https://www.semanticscholar.org/paper/318ae285b92fa59cde733fae22c27cb6da099910", "title": "An Experimental Evaluation of Content-based Recommendation Systems: Can Linked Data and BERT Help?", "abstract": "Content-Based Recommendation Systems suggest items (e.g., articles, products, objects, services, or places) that are relevant to the user based on the features describing the items. In many content-based recommendation systems we can find, along with discrete attributes, textual features (e.g., text summaries or comments) obtained from web pages, news articles, etc. Traditionally, to enable its exploitation, the textual information of items is represented by using basic information retrieval models (such as the vector space model), which do not take into account natural language challenges involving the semantics of the words (synonymy, polysemy and hiperonymy, etc.) or language understanding. Other solutions try to exploit those semantics. In this paper, we present an experimental evaluation where we compare several recommendation approaches, including a content-based recommender based on vector space models, a deep learning and content-based recommendation approach, and a semantic-aware content-based recommendation model. This last approach exploits textual features of items obtained from the Linked Open Data (LOD) and BERT (Bidirectional Encoder Representations from Transformers) for language modelling. Deep Learning transformers are achieving good results in different NLP (Natural Language Processing) problems, but using them to build content-based recommendation systems has not been explored in depth so far. Our experimental results, focused on the domain of movie recommendations, show that a approach based on the use of BERT can provide good results if enough training data are available.", "venue": "2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "lod", "lod", "nlp", "nlp"], "mention_counts": {"ld": 1, "nlp": 2, "lod": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"ld": 1, "lod": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "00c2d55702008f4759f2f3f5515ee1fa9b70700f", "url": "https://www.semanticscholar.org/paper/00c2d55702008f4759f2f3f5515ee1fa9b70700f", "title": "Extending DBpedia with Wikipedia List Pages", "abstract": "Thanks to its wide coverage and general-purpose ontology, DBpedia is a prominent dataset in the Linked Open Data cloud. DBpedia's content is harvested from Wikipedia's infoboxes, based on manually created mappings. In this paper, we explore the use of a promising source of knowledge for extending DBpedia, i.e., Wikipedia's list pages. We discuss how a combination of frequent pattern mining and natural language processing (NLP) methods can be leveraged in order to extend both the DBpedia ontology, as well as the instance information in DBpedia. We provide an illustrative example to show the potential impact of our approach and discuss its main challenges.", "venue": "NLP-DBPEDIA@ISWC", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "lod", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "lod": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"lod": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "61ad65d348eea502bfb3ff8918a45994e82d0c01", "url": "https://www.semanticscholar.org/paper/61ad65d348eea502bfb3ff8918a45994e82d0c01", "title": "A Scalable Approach for Computing Semantic Relatedness using Semantic Web Data", "abstract": "Computing semantic relatedness is an essential operation for many natural language processing (NLP) tasks, such as Entity Linking (EL) and Question Answering (QA). It is still challenging to find a scalable approach to compute the semantic relatedness using Semantic Web data. Hence, we present for the first time an approach to pre-compute the semantic relatedness between the instances, relations, and classes of an ontology, such that they can be used in real-time applications.", "venue": "Web Intelligence, Mining and Semantics", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlp", "sw", "onto", "nlp"], "mention_counts": {"nlp": 2, "sw": 2, "onto": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "911f30e10acdfd0e5bd7f452b0e650e015a3d557", "url": "https://www.semanticscholar.org/paper/911f30e10acdfd0e5bd7f452b0e650e015a3d557", "title": "A self-learning approach to improving service quality in outsourcing of engineering design using operational data", "abstract": "Managing service quality in outsourcing requires a holistic approach to managing knowledge. This need is more pronounced in case of outsourcing of high-end tasks such as engineering designs. As complex tasks are carried out large amounts of multi-structured transactional data are captured during service delivery, more often appearing as text. This qualitative operational data has rich knowledge embedded in it. This paper aims to demonstrate a way of extracting knowledge from such operational data for improving service quality. The study uses simulation as a method of inductive research. Simulation model of a self-learning system for extracting knowledge from operational data are created. The proposed artificial intelligence system integrates natural language processing and rule-based reasoning for knowledge creation. Finally, with a view to demonstrate the potential of the proposed system, a real prototype industry application is described.", "venue": "Int. J. Comput. Appl. Technol.", "citationCount": 2, "fieldsOfStudy": ["Engineering", "Computer Science"], "mentions": ["ke", "nlp", "ke"], "mention_counts": {"nlp": 1, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "a424e8af5f4480c681e015bbc01f53fa27cc4e77", "url": "https://www.semanticscholar.org/paper/a424e8af5f4480c681e015bbc01f53fa27cc4e77", "title": "Identity of Long-tail Entities in Text", "abstract": "The digital era has generated a huge amount of data on the identities (profiles) of people, organizations and other entities in a digital format, largely consisting of textual documents such as news articles, encyclopedias, personal websites, books, and social media. Identity has thus been transformed from a philosophical to a societal issue, one requiring robust computational tools to determine entity identity in text. Computational systems developed to establish identity in text often struggle with long-tail cases. This book investigates how Natural Language Processing (NLP) techniques for establishing the identity of long-tail entities \u2013 which are all infrequent in communication, hardly represented in knowledge bases, and potentially very ambiguous \u2013 can be improved through the use of background knowledge. Topics covered include: distinguishing tail entities from head entities; assessing whether current evaluation datasets and metrics are representative for long-tail cases; improving evaluation of long-tail cases; accessing and enriching knowledge on long-tail entities in the Linked Open Data cloud; and investigating the added value of background knowledge (\u201cprofiling\u201d) models for establishing the identity of NIL entities. Providing novel insights into an under-explored and difficult NLP challenge, the book will be of interest to all those working in the field of entity identification in text.", "venue": "Studies on the Semantic Web", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["lod", "kg", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 3, "kg": 1, "lod": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 1, "lod": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "aab79c2d93e9d4463cdd3ff94571d2c15c31fb8a", "url": "https://www.semanticscholar.org/paper/aab79c2d93e9d4463cdd3ff94571d2c15c31fb8a", "title": "Research on Automatic Construction of Medical Ontology", "abstract": "Using the techniques and theories of natural language processing (NLP), this paper puts forward threads and ways to build automatically domain ontologies based on text content. Through restructuring and utilizing the generally-acknowledged domain knowledge in medical domain, this paper also constructs a concept system of the multidimensional model of modern medicine towards clinic medicine, realizing the automatic construction and acquisition of modern medical knowledge specification system. This further provides theoretical foundation and technical support for automatic construction of professional domain ontologies.", "venue": "2010 International Conference on Biomedical Engineering and Computer Science", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "7deebb8cb68c0f0a4fa82ade4237d27ed0ec3953", "url": "https://www.semanticscholar.org/paper/7deebb8cb68c0f0a4fa82ade4237d27ed0ec3953", "title": "Source authoring for multilingual generation of personalised object descriptions", "abstract": "We present the source authoring facilities of a natural language generation system that produces personalised descriptions of objects in multiple natural languages starting from language-independent symbolic information in ontologies and databases as well as pieces of canned text. The system has been tested in applications ranging from museum exhibitions to presentations of computer equipment for sale. We discuss the architecture of the overall system, the resources that the authors manipulate, the functionality of the authoring facilities, the system's personalisation mechanisms, and how they relate to source authoring. A usability evaluation of the authoring facilities is also presented, followed by more recent work on reusing information extracted from existing databases and documents, and supporting the OWL ontology specification language.", "venue": "Natural Language Engineering", "citationCount": 39, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "nlg", "onto", "onto", "onto"], "mention_counts": {"nlg": 1, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlg": 1, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "0d8868ee06ec942803a040e141e4feea9328aad9", "url": "https://www.semanticscholar.org/paper/0d8868ee06ec942803a040e141e4feea9328aad9", "title": "A model for generating Arabic text from semantic representation", "abstract": "Text Generation is a challenging task in Natural Language Processing (NLP). Its purpose is to generate grammatically correct text from machine representation source such as a knowledge base. One of the most recent semantic representation is Rich Semantic Graph (RSG). It exploits not only the semantic representation techniques but also the Language structure and writing styles. Our work is a part of an ongoing research to create an abstractive summary for a single input document in the Arabic Language. The abstractive summary is generated through three modules; converting the input Arabic text into an RSG, then performing Graph Reduction, and finally generating the summarized text from the reduced graph. This is achieved with the aid of a domain Ontology. In this paper, we are illustrating the architecture of the third module, which works on generating Arabic text from RSG using Ontology.", "venue": "2015 11th International Computer Engineering Conference (ICENCO)", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto", "kg"], "mention_counts": {"nlp": 2, "onto": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "c44e0fcca75aa5437beb5ddbefb8729b7c13e02a", "url": "https://www.semanticscholar.org/paper/c44e0fcca75aa5437beb5ddbefb8729b7c13e02a", "title": "A Review on Sentiment Analysis Techniques and Applications", "abstract": ". Nowadays, what user think is the most difficult and complicated task handled by organizations. The way to idetify the attitude of the speaker or a writer on some topics is to use sentiment analysis. The use of sentiment analysis is to identify user\u2019s opinion towards some topics whether it is positive or negative. This paper presents the techniques used by previous researchers in sentiment analysis which are Machine Learning and Natural Language Processing (NLP) in solving the classification task. The comparison among these two main approaches reveals that Machine Learning techniques can solve classification task with reasonable success and with very high accuracy compared to NLP-based techniques but it is depending on the training and test data with respect to the domain. This paper also presents the use of ontology in sentiment analysis that can help in achieving more high accuracy for the classification task. this presents a method of ontology-based sentiment classification to classify and analyze online product reviews of In this research,", "venue": "IOP Conference Series: Materials Science and Engineering", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "eaadda168c52ce07c744a515bbd53247e6e99ea4", "url": "https://www.semanticscholar.org/paper/eaadda168c52ce07c744a515bbd53247e6e99ea4", "title": "Ontology-Based Natural Language Processing for In-store Shopping Situations", "abstract": "Natural Language communication between customers and products within in-store shopping environments enables new forms of product interfaces and an improved filtering and intuitive presentation of product information. In this article, we describe how customer's access to product information at the point of sale can be improved through the use of dialogue systems and heterogeneous web-based representations of product information based on formal ontologies within in-store shopping environments. After considering specific requirements of in-store shopping environments on dialogue systems, we present the model of a Conversational Recommendation Agent (CoRA), a domain-specific dialogue system, which realizes an ontology-based Natural Language Processing system for shopping situations.", "venue": "International Computer Science Conference", "citationCount": 24, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "02c3b3c11c5c175752d195737b045d6425e9c00c", "url": "https://www.semanticscholar.org/paper/02c3b3c11c5c175752d195737b045d6425e9c00c", "title": "Multimedia Reasoning with Natural Language Support", "abstract": "In this paper we present an approach that combines multimedia reasoning and natural language processing for the semantic integration of automatic and manual image annotations based on domain ontologies. We discuss how to apply natural language processing to transform natural language descriptions and queries into an ontological representation that allows users to formulate formal semantics in an intuitive manner, without the need to cope with complex ontological structures and unwieldy user interfaces. Illustrative experimental examples demonstrate the added value.", "venue": "International Conference on Semantic Computing (ICSC 2007)", "citationCount": 19, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "4dd1e354197538913f1a5664bd694cb87d8ba2d2", "url": "https://www.semanticscholar.org/paper/4dd1e354197538913f1a5664bd694cb87d8ba2d2", "title": "Using Natural Language Processing, LocusLink and the Gene Ontology to Compare OMIM to MEDLINE", "abstract": "Researchers in the biomedical and molecular biology fields are faced with a wide variety of information sources. These are presented in the form of images, free text, and structured data files that include medical records, gene and protein sequence data, and whole genome microarray data, all gathered from a variety of experimental organisms and clinical subjects. The need to organize and relate this information, particularly concerning genes, has motivated the development of resources, such as the Unified Medical Language System, Gene Ontology, LocusLink, and the Online Inheritance In Man (OMIM) database. We describe a natural language processing application to extract information on genes from unstructured text and discuss ways to integrate this information with some of the available online resources.", "venue": "HLT-NAACL 2004", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "fb1bdf035b004df831909a04be0488cddc60fa05", "url": "https://www.semanticscholar.org/paper/fb1bdf035b004df831909a04be0488cddc60fa05", "title": "Interleaving ontology-based reasoning and Natural Language Processing for character identification in folktales", "abstract": "We propose a system for identifying literary characters from folktales. The process of extracting characters from free texts is guided by an ontology that encodes the knowledge of the folktale domain. We present how the ontology works with Natural Language Processing in GATE to increase the accuracy of character recognition. We validated the solution against various folktales.", "venue": "2014 IEEE 10th International Conference on Intelligent Computer Communication and Processing (ICCP)", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "ea97fce17e26f6ce6cb0b3209c5728dc5abbdff6", "url": "https://www.semanticscholar.org/paper/ea97fce17e26f6ce6cb0b3209c5728dc5abbdff6", "title": "Galen : a third generation terminology tool to support a multipurpose national coding system for surgical procedures", "abstract": "Generalised architecture for languages, encyclopedia and nomenclatures in medicine (GALEN) has developed a new generation of terminology tools based on a language independent model describing the semantics and allowing computer processing and multiple reuses as well as natural language understanding systems applications to facilitate the sharing and maintaining of consistent medical knowledge. During the European Union 4 Th. framework program project GALEN-IN-USE and later on within two contracts with the national health authorities we applied the modelling and the tools to the development of a new multipurpose coding system for surgical procedures named CCAM in a minority language country, France. On one hand, we contributed to a language independent knowledge repository and multilingual semantic dictionaries for multicultural Europe. On the other hand, we support the traditional process for creating a new coding system in medicine which is very much labour consuming by artificial intelligence tools using a medically oriented recursive ontology and natural language processing. We used an integrated software named CLAW (for classification workbench) to process French professional medical language rubrics produced by the national colleges of surgeons domain experts into intermediate dissections and to the Grail reference ontology model representation. From this language independent concept model representation, on one hand, we generate with the LNAT natural language generator controlled French natural language to support the finalization of the linguistic labels (first generation) in relation with the meanings of the conceptual system structure. On the other hand, the Claw classification manager proves to be very powerful to retrieve the initial domain experts rubrics list with different categories of concepts (second generation) within a semantic structured representation (third generation) bridge to the electronic patient record detailed terminology.", "venue": "Medical Informatics Europe", "citationCount": 37, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlp", "nlu", "onto", "nlg"], "mention_counts": {"nlp": 1, "nlg": 1, "onto": 2, "nlu": 1}, "nlp_mention_counts": {"nlp": 1, "nlg": 1, "nlu": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "8252f3d7dfcd2132b7ce654aac34e43f9605e90c", "url": "https://www.semanticscholar.org/paper/8252f3d7dfcd2132b7ce654aac34e43f9605e90c", "title": "SNOMEDtxt: Natural Language Generation from SNOMED Ontology", "abstract": "SNOMED Clinical Terms (SNOMED CT) defines over 70,000 diseases, including many rare ones. Meanwhile, descriptions of rare conditions are missing from online educational resources. SNOMEDtxt converts ontological concept definitions and relations contained in SNOMED CT into narrative disease descriptions using Natural Language Generation techniques. Generated text is evaluated using both computational methods and clinician and lay user feedback. User evaluations indicate that lay people prefer generated text to the original SNOMED content, find it more informative, and understand it significantly better. This method promises to improve access to clinical knowledge for patients and the medical community and to assist in ontology auditing through natural language descriptions.", "venue": "MedInfo", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlg", "onto", "onto", "onto", "nlg"], "mention_counts": {"nlg": 2, "onto": 3}, "nlp_mention_counts": {"nlg": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "dc472d2a97eecde27928fa51ea47170b8d76a896", "url": "https://www.semanticscholar.org/paper/dc472d2a97eecde27928fa51ea47170b8d76a896", "title": "Sentiment Classification Based on Ontology and SVM Classifier", "abstract": "There are a lot of text documents on the Web which contain opinions or sentiments about an object such as software reviews, product reviews, movies reviews, music reviews, and book reviews etc. Opinion mining or sentiment classification aim to extract the features on which the reviewers express their opinions and determine they are positive or negative. In this paper we proposed an ontology based combination approach to enhance the existing approaches of the sentiment classification. We also used the supervised learning techniques for classification of the sentiments in the software reviews. This paper proposed the combination of using Natural Language Processing techniques (NLP), ontology based on Formal Concept Analysis (FCA) design, and Support Vector Machine (SVM) for classifying the software reviews are positive, negative or neutral.", "venue": "2010 Second International Conference on Communication Software and Networks", "citationCount": 36, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "521786983f4ec2df6739da23b5b4d049987de626", "url": "https://www.semanticscholar.org/paper/521786983f4ec2df6739da23b5b4d049987de626", "title": "Information retrieval in falktales using natural language processing", "abstract": "Our aim is to extract information about literary characters in unstructured texts. We employ natural language processing and reasoning on domain ontologies. The first task is to identify the main characters and the parts of the story where these characters are described or act. We illustrate the system in a scenario in the folktale domain. The system relies on a folktale ontology that we have developed based Propp's model for folktales morphology.", "venue": "International Conference on Computational Photography", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "2624cbc6f2a8f7a3c6a97dc195ba61f1e8fea89f", "url": "https://www.semanticscholar.org/paper/2624cbc6f2a8f7a3c6a97dc195ba61f1e8fea89f", "title": "Automatic Extraction and Post-coordination of Spatial Relations in Consumer Language", "abstract": "To incorporate ontological concepts in natural language processing (NLP) it is often necessary to combine simple concepts into complex concepts (post-coordination). This is especially true in consumer language, where a more limited vocabulary forces consumers to utilize highly productive language that is almost impossible to pre-coordinate in an ontology. Our work focuses on recognizing an important case for post-coordination in natural language: spatial relations between disorders and anatomical structures. Consumers typically utilize such spatial relations when describing symptoms. We describe an annotated corpus of 2,000 sentences with 1,300 spatial relations, and a second corpus of 500 of these relations manually normalized to UMLS concepts. We use machine learning techniques to recognize these relations, obtaining good performance. Further, we experiment with methods to normalize the relations to an existing ontology. This two-step process is analogous to the combination of concept recognition and normalization, and achieves comparable results.", "venue": "AMIA", "citationCount": 8, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "onto", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "e8bbfe184af4a83cf08fb385c0952643c25dec45", "url": "https://www.semanticscholar.org/paper/e8bbfe184af4a83cf08fb385c0952643c25dec45", "title": "Mixing semantic networks and conceptual vectors: the case hyperonymy", "abstract": "In this paper, we focus on lexical semantics, a key issue in natural language processing (NLP) that tends to converge with conceptual knowledge representation (KR) and ontologies. When ontological representation is needed, hyperonymy, the closest approximation to the is-a relation, is at stake. In this paper we describe the principles of our vector model (CVM: conceptual vector model), and show how to account for hyperonymy within the vector-based frame for semantics. We show how hyperonymy diverges from is-a and what measures are more accurate for hyperonymy representation. Our demonstration results in initiating a \"cooperation\" process between semantic networks and conceptual vectors. Text automatic rewriting or enhancing, ontology mapping with natural language expressions, are examples of applications that can be derived from the function we define in this paper.", "venue": "The Second IEEE International Conference on Cognitive Informatics, 2003. Proceedings.", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "54d7330c0820350b92b48e5125cb5e8b61da4aa5", "url": "https://www.semanticscholar.org/paper/54d7330c0820350b92b48e5125cb5e8b61da4aa5", "title": "Improving precision in concept normalization", "abstract": "Most natural language processing applications exhibit a trade-off between precision and recall. In some use cases for natural language processing, there are reasons to prefer to tilt that trade-off toward high precision. Relying on the Zipfian distribution of false positive results, we describe a strategy for increasing precision, using a variety of both pre-processing and post-processing methods. They draw on both knowledge-based and frequentist approaches to modeling language. Based on an existing high-performance biomedical concept recognition pipeline and a previously published manually annotated corpus, we apply this hybrid rationalist/empiricist strategy to concept normalization for eight different ontologies. Which approaches did and did not improve precision varied widely between the ontologies.", "venue": "Pacific Symposium on Biocomputing", "citationCount": 5, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlp", "kg", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "3545fc29cae1648615788a3a35298956c04c0109", "url": "https://www.semanticscholar.org/paper/3545fc29cae1648615788a3a35298956c04c0109", "title": "Jointly learning word embeddings using a corpus and a knowledge base", "abstract": "Methods for representing the meaning of words in vector spaces purely using the information distributed in text corpora have proved to be very valuable in various text mining and natural language processing (NLP) tasks. However, these methods still disregard the valuable semantic relational structure between words in co-occurring contexts. These beneficial semantic relational structures are contained in manually-created knowledge bases (KBs) such as ontologies and semantic lexicons, where the meanings of words are represented by defining the various relationships that exist among those words. We combine the knowledge in both a corpus and a KB to learn better word embeddings. Specifically, we propose a joint word representation learning method that uses the knowledge in the KBs, and simultaneously predicts the co-occurrences of two words in a corpus context. In particular, we use the corpus to define our objective function subject to the relational constrains derived from the KB. We further utilise the corpus co-occurrence statistics to propose two novel approaches, Nearest Neighbour Expansion (NNE) and Hedged Nearest Neighbour Expansion (HNE), that dynamically expand the KB and therefore derive more constraints that guide the optimisation process. Our experimental results over a wide-range of benchmark tasks demonstrate that the proposed method statistically significantly improves the accuracy of the word embeddings learnt. It outperforms a corpus-only baseline and reports an improvement of a number of previously proposed methods that incorporate corpora and KBs in both semantic similarity prediction and word analogy detection tasks.", "venue": "PLoS ONE", "citationCount": 26, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "kg", "nlp", "onto", "kg"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "c28813d85246e3b3f748400c6b4099eccae2756b", "url": "https://www.semanticscholar.org/paper/c28813d85246e3b3f748400c6b4099eccae2756b", "title": "Using Wikipedia for Extracting Hierarchy and Building Geo-Ontology", "abstract": "Purpose \u2013 This paper aims to serves two main purposes: First, it seeks to provide an overview of the location hierarchy from the highest divisions (continents) to the lowest divisions (wards, villages) in reality and in the Wikipedia pages. Secondly, it aims to introduce an approach to building a geographical ontology from Wikipedia.Design/methodology/approach \u2013 The paper first reviews existing applications which extract information from Wikipedia and use it as a data resource to develop natural language processing tools. The paper also reviews the structure of Wikipedia pages which show the location's information. Based on the analysis, the paper then proposes an approach to extract location hierarchy as well as geographical characteristics for the geo\u2010ontology. The approach also rebuilds the relations between locations in the ontology.Findings \u2013 Existing location name systems are mainly based on probabilistic locations, which are mined from the data and they lack the administrative relations between loc...", "venue": "Int. J. Web Inf. Syst.", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "nlp", "onto"], "mention_counts": {"nlp": 1, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "c0f6249ef92a847f93df00c8b853319e8317e8c2", "url": "https://www.semanticscholar.org/paper/c0f6249ef92a847f93df00c8b853319e8317e8c2", "title": "Use of ontology-to-text relation for creating semantic annotation", "abstract": "The paper is focused on the problem of creating new semantic annotations of multimedia objects stored in a digital library. In most cases creating such annotations is a time-consuming and expensive process when it is carried out manually by domain experts. This cumbersome process could be facilitated if a part of the information used for annotating is available in form of natural language texts. In such cases it is possible to apply some Natural Language Processing techniques for extracting needed information and then convert it into desired semantic annotations. The paper describes an approach for solving the task of ontological terms recognition within Bulgarian texts by application of a Natural Language Processing technique called \"Ontology-to-Text Relation\". All preliminary steps to prepare needed resources for implementation of this technique are discussed.", "venue": "CompSysTech '12", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "52617c9fea695118e5e80535fe362bd9b5ed8d99", "url": "https://www.semanticscholar.org/paper/52617c9fea695118e5e80535fe362bd9b5ed8d99", "title": "Converting a Historical Architecture Encyclopedia into a Semantic Knowledge Base", "abstract": "Digitizing a historical document using ontologies and natural language processing techniques can transform it from arcane text to a useful knowledge base.The Handbook on Architecture (Handbuch der Architektur) was perhaps one of the most ambitious publishing projects ever. Like a 19thcentury Wikipedia, it attempted nothing less than a full account of all architectural knowledge available at the time, both past and present. It covers topics from Greek temples to contemporary hospitals and universities; from the design of individual construction elements such as window sills to large-scale town planning; from physics to design; from planning to construction. It also discusses architectural history and styles and a multitude of other topics, such as building conception, statics, and interior design.Not surprisingly, this project took longer than planned. The encyclopedia's first volume was partly published in 1880, and over the next 63 years more than 100 architects worked on what would become more than 140 individual publications with over 25,000 pages. One important insight of our work is that targeted text analysis support, already available today, can easily be integrated into common desktop tools to support users for their task at hand. While NLP techniques are far from perfect or comprehensive, they can already deliver knowledge discovery support that goes significantly beyond the currently used approach of full-text search and information retrieval.", "venue": "IEEE Intelligent Systems", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "nlp", "onto", "kg"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "adcfe6fc2e10f89810c63abdb4e00102a1271547", "url": "https://www.semanticscholar.org/paper/adcfe6fc2e10f89810c63abdb4e00102a1271547", "title": "Computational Intelligence Model for Code Generation from Natural Language Problem Statement", "abstract": "Computers have become an integral part of the scientific world. The real-life problems are dealt with an algorithmic approach. Algorithms being independent of programming language, they can be developed using any natural spoken language that a person is comfortable with. However, the problem lies in implementing it. The computational intelligence model proposed in this paper approaches such problem by carrying out mapping at Semantic level using Natural Language Processing and ontology and applying Ontology Matching techniques to derive an automatic translator of natural language problem statement into the artificial language (here Java). The intermediate steps of translation are processed by using corpus of English for developing some techniques for mapping linguistic constructs to programming structures. The modern NLP techniques can make possible the conversion of natural language statements to a programming language. Overall, this paper proposes a knowledge-based expert system which makes use of facts and rules to build the solution.", "venue": "2018 Fourth International Conference on Computing Communication Control and Automation (ICCUBEA)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "kg": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "b8cbc702f4413a52bb5eb91c21f148f0a25e017a", "url": "https://www.semanticscholar.org/paper/b8cbc702f4413a52bb5eb91c21f148f0a25e017a", "title": "Towards Knowledge Acquisition of Metadata on AI Progress", "abstract": "We propose an ontology to help AI researchers keep track of the scholarly progress of AI related tasks such as natural language processing and computer vision. We first define the core entities and relations in the proposed Machine Learning Progress Ontology (MLPO). Then we describe how to use the techniques in natural language processing to construct a Machine Learning Progress Knowledge Base (MPKB) that can support various downstream tasks.", "venue": "International Workshop on the Semantic Web", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "47c6a376cc3680f3edf0a2e6c386749416fc17bc", "url": "https://www.semanticscholar.org/paper/47c6a376cc3680f3edf0a2e6c386749416fc17bc", "title": "Biomedical Question Answering using Extreme Multi-Label Classification and Ontologies in the Multilingual Panorama", "abstract": "Deep learning models achieve state-of-the-art results in Natural Language Processing (NLP) tasks, such as Question Answering (QA), across different domains, mostly thanks to pre-trained language models such as BERT [1]. However, there is a lack of models designed for NLP tasks in the multilingual panorama, especially in specific domains such as the biomedical sciences, mostly due to the lack of datasets available in non-English languages. In this short paper, we propose the development of a QA system using stateof-the-art deep learning models and combining it with a deep learning Extreme Multi-Label Classification (XMLC) solution along with ontologies, in order to improve the results achieved by the model. The proposed model shall be able to answer biomedical questions in English, Spanish and Portuguese.", "venue": "SIIRH@ECIR", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "6f56d8056a025670d4315689646bc14c24d8a56f", "url": "https://www.semanticscholar.org/paper/6f56d8056a025670d4315689646bc14c24d8a56f", "title": "Application of dynamic service composition in web question answering", "abstract": "Web Question Answering (WQA) and Web Service (WS) are parallel fields in intelligent web computing. In network services, they are used widely, and they are rarely combined together. For many intelligent web applications, however, using them together tends to be important. This paper based on the study of significant techniques of WQA, WS and Hierarchical Task Network (HTN), including Natural Language Processing (NLP), Web Service Ontology and a planner of HTN, gives a proposal of WQA framework. This framework combines NLP, predicate logic, Semantic Web description and an HTN planner-SHOP2, turning question answering into a planning problem by question mapping and service planning. It addresses a transition from natural language to semantic expression, composites web services dynamically by using SHOP2, and obtains the answering that meets customers' question by executing the web services sequence.", "venue": "Int. J. Wirel. Mob. Comput.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "sw", "onto"], "mention_counts": {"nlp": 3, "sw": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 1, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "bf6a51b6384aa0fcd31826570e00ecb9bc4c943f", "url": "https://www.semanticscholar.org/paper/bf6a51b6384aa0fcd31826570e00ecb9bc4c943f", "title": "Extracting Concepts from the Software Requirements Specification Using Natural Language Processing", "abstract": "Extracting concepts from the software requirements is one of the first step on the way to automating the software development process. This task is difficult due to the ambiguity of the natural language used to express the requirements specification. The methods used so far consist mainly of statistical analysis of words and matching expressions with a specific ontology of the domain in which the planned software will be applicable. This article proposes a method and a tool to extract concepts based on a grammatical analysis of requirements written in English without the need to refer to specialized ontology. These concepts can be further expressed in the class model, which then can be the basis for the object-oriented analysis of the problem. This method uses natural language processing (NLP) techniques to recognize parts of speech and to divide sentences into phrases and also the WordNet dictionary to search for known concepts and recognize relationships between them.", "venue": "International Conference on Human System Interaction", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 3, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "f7524f9db320a89d918d0fa9326ec0c0f9097aa5", "url": "https://www.semanticscholar.org/paper/f7524f9db320a89d918d0fa9326ec0c0f9097aa5", "title": "Neural model robustness for skill routing in large-scale conversational AI systems: A design choice exploration", "abstract": "Current state-of-the-art large-scale conversational AI or intelligent digital assistant systems in industry comprises a set of components such as Automatic Speech Recognition (ASR) and Natural Language Understanding (NLU). For some of these systems that leverage a shared NLU ontology (e.g., a centralized intent/slot schema), there exists a separate skill routing component to correctly route a request to an appropriate skill, which is either a first-party or third-party application that actually executes on a user request. The skill routing component is needed as there are thousands of skills that can either subscribe to the same intent and/or subscribe to an intent under specific contextual conditions (e.g., device has a screen). Ensuring model robustness or resilience in the skill routing component is an important problem since skills may dynamically change their subscription in the ontology after the skill routing model has been deployed to production. We show how different modeling design choices impact the model robustness in the context of skill routing on a state-of-the-art commercial conversational AI system, specifically on the choices around data augmentation, model architecture, and optimization method. We show that applying data augmentation can be a very effective and practical way to drastically improve model robustness.", "venue": "ArXiv", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "nlu", "onto", "nlu", "onto"], "mention_counts": {"onto": 2, "nlu": 3}, "nlp_mention_counts": {"nlu": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "3d8d15fe57c26a17236cfe3a1579d5cb23e9e273", "url": "https://www.semanticscholar.org/paper/3d8d15fe57c26a17236cfe3a1579d5cb23e9e273", "title": "Anuj at the FinSim Task: Anuj@FINSIM\u00a1VLearning Semantic Representation of Financial Domain with Investopedia", "abstract": "Natural Language Processing and its applications are getting used in every domain, and it has become an important need to have domain specific knowledge representation in the form of ontologies, taxonomies or word embeddings like BERT. As most of these knowledge bases are generic and lack the specificity of a domain, it is very important to have semantic representation for domain separately. The FinSim 2020 shared task is colocated with the FinNLP workshop, and the challenge is to classify financial terms into their predefined classes or hypernyms. This paper explains a hybrid approach that uses various NLP, machine learning, and deep learning models to develop a financial terms classifier. Also the paper explains use of a financial domain encyclopedia called Investopedia to enrich terms for better context. The semantic representation of financial terms is a very important building block for NLP applications such as question answering, chatbot, trading applications etc.", "venue": "FINNLP", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "nlp", "kg"], "mention_counts": {"nlp": 3, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "5cadbebda3bd266560a7d24e52d5d207049c663a", "url": "https://www.semanticscholar.org/paper/5cadbebda3bd266560a7d24e52d5d207049c663a", "title": "Automated Generating Natural Language Requirements based on Domain Ontology", "abstract": "\u2014Software requirements speci\ufb01cation is undoubtedly critical for the whole software life-cycle. Nowadays, writing software requirements speci\ufb01cations primarily depends on hu- man work. Although massive studies have been proposed to fasten the process via proposing advanced elicitation and analysis techniques, it is still a time-consuming and error-prone task that needs to take domain knowledge and business information into consideration. In this paper, we propose an approach, named ReqGen , which can provide recommendations by automatically generating natural language requirements speci\ufb01cations based on certain given keywords. Speci\ufb01cally, ReqGen consists of three critical steps. First, keywords-oriented knowledge is selected from domain ontology and is injected to the basic Uni\ufb01ed pre-trained Language Model (UniLM) for domain \ufb01ne-tuning. Second, a copy mechanism is integrated to ensure the occurrence of keywords in the generated statements. Finally, a requirement syntax constrained decoding is designed to close the semantic and syntax distance between the candidate and reference speci\ufb01cations. Experiments on two public datasets from different groups and domains show that ReqGen outperforms six popular natural language generation approaches with respect to the hard constraint of keywords(phrases) inclusion, BLEU, ROUGE and syntax compliance. We believe that ReqGen can promote the ef\ufb01ciency and intelligence of specifying software requirements.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "nlg", "onto", "onto", "nlg"], "mention_counts": {"nlg": 3, "onto": 2}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "7c5610ad02c3dd2c7bcb7ebba2096f74cc21c0fb", "url": "https://www.semanticscholar.org/paper/7c5610ad02c3dd2c7bcb7ebba2096f74cc21c0fb", "title": "Enhanced learner centered pedagogical strategy: Promoting STEM with underrepresented populations", "abstract": "An efficient pedagogical strategy in an Open Social Learning environment is required for effective learning on the web. Increased collaboration among the learners and mentors makes the learning experience efficient. This paper proposes an enhanced Pedagogical strategy for searching content through the use of ontologies and the learner's profile. The proposed system makes use of ontologies and learner profiles for crawling, indexing and organizing content. The content is then filtered based on the learner and \"learner like\" learners' profile to provide a rich Open Social Learning experience. Natural Language Processing is then used to semantically categorize the learning content. It has been shown that this proposed pedagogical strategy, using the ontologies and user profiles for search, Natural Language Processing for programmatic content categorization and graph database to identify learning styles of the underrepresented learners, provides an enhanced learner centered online learning experience.", "venue": "2014 IEEE Integrated STEM Education Conference", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "5edac9b00e786a71f45bb33bdfb46d1a9e22b8ba", "url": "https://www.semanticscholar.org/paper/5edac9b00e786a71f45bb33bdfb46d1a9e22b8ba", "title": "Ontology-based Venous Thromboembolism Risk Factors Mining and Model Developing from Medical Records", "abstract": "Padua linear model is widely used for the risk assessment of venous thromboembolism (VTE), which is a common and preventable complication for inpatients. However, differences of race, genetics and environment between Western and Chinese population limit Padua model' validity in Chinese patients. Extracting VTE risk factors from unstructured medical records in Chinese hospital can help to understand VTE events and develop efficient risk assessment model. In this study, we proposed an ontology-based method to mine VTE risk factors combining natural language processing (NLP) and machine learning (ML) methods. Medical records of 3106 inpatients were processed and terms in multiple ontologies from various sections of records enriched in VTE patients were sorted automatically. Then ML methods were used to estimate terms' importance and terms within admitting diagnosis and progress notes showed better VTE prediction performance than other sections. Finally a novel VTE prediction model was built based on selected terms and showed higher AUC score (0.815) than the Padua model (0.789).", "venue": "2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)", "citationCount": 2, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "bfa6106c38a6fed9b0669b24170ef3d9f9cdc771", "url": "https://www.semanticscholar.org/paper/bfa6106c38a6fed9b0669b24170ef3d9f9cdc771", "title": "Mining service integration opportunities towards joined-up government", "abstract": "Service integration is central to joined-up government initiatives and requires information on the collaborators and the services they offer, roles of different actors, the resources required, and their goals (individual and shared). These information are largely available in unstructured forms on government portals, publications and other textural sources. This paper explores semantic text mining for extracting service-related information from such sources using Natural Language Processing techniques supported by Service-Oriented Process Ontologies. Our solution framework consists of the following steps: (1) creating domain and service-oriented process ontology, (2) extracting service-related information from textual sources based on the ontology, and finally (3) mining relationship among the services based on the extracted information in Step 2 linked with a pre-defined hierarchy of service delivery goals specifying the objective(s) to be achieved among the orchestrated services. We describe our approach to these tasks and discuss the progress of the work, our experiences and the challenges encountered so far.", "venue": "ICEGOV '11", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ie", "nlp"], "mention_counts": {"nlp": 1, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "e19f718586ecb22ffa39e46a7cbc3c0f3261bb2d", "url": "https://www.semanticscholar.org/paper/e19f718586ecb22ffa39e46a7cbc3c0f3261bb2d", "title": "FML-based linguistic classification agent for social media application", "abstract": "Fuzzy Markup Language (FML) presented by IEEE Computational Intelligence Society (CIS) has been an IEEE Standard since May 2016. It is an XML-based language for designer to easily construct the knowledge base and rule base of the developed fuzzy logic system. In this paper, we propose an FML-based linguistic classification agent and apply it to popular Chinese songs' classification in social media environment. In addition, the lyrics are retrieved from Youtube, Facebook or Google+, and then we adopt Natural Language Processing (NLP) mechanism to deal with the document preprocessing. First, the domain experts construct the classification ontology model and design related categories for the application domain. Moreover, the fuzzy concept sets are also adopted in the related categories. Then, the Chinese Knowledge Information Processing (CKIP) tool is utilized to deal with the Chinese documents of the songs. Finally, the FML-based knowledge base and rule base of the classification agent are constructed for inferring the related categories of the song. The Fujisoft robot PALRO receives the classified songs and plays the song for the desired users. Experimental results show the proposed classification agent can work correctly.", "venue": "2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "kg", "nlp", "nlp"], "mention_counts": {"nlp": 2, "kg": 2, "onto": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "ddfd89e39d0e15524f801931c8e06089a078bce3", "url": "https://www.semanticscholar.org/paper/ddfd89e39d0e15524f801931c8e06089a078bce3", "title": "Semantic Search and NLP-Based Diagnostics", "abstract": "This study considers issues in semantic representation of written texts, especially in the context of entropy-based approach to natural language processing in biomedical applications. These issues lie at the intersection of Web search methodologies, ontology studies, lexicon studies, and natural language processing. The presented in the article entropy-based methodology is aimed at enhancing search techniques and diagnostics by capturing semantic properties of written texts. The range of possible applications ranges from forensic linguistics to psychological diagnostics and evaluation. The presented case study assumes that for texts written under atypical mental conditions, the level of relative text entropy may fall below a certain threshold and the distribution of entropy across the text may show unusual patterns, thus contributing to the semantic assessment of a subject's mental state. Further processing methods potentially contributing to psychological evaluation diagnosis and ontology-based search are discussed.", "venue": "2014 IEEE 27th International Symposium on Computer-Based Medical Systems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "76fa4b0da7bfc8946b453f908265c70d45addd4f", "url": "https://www.semanticscholar.org/paper/76fa4b0da7bfc8946b453f908265c70d45addd4f", "title": "SemScribe: Natural Language Generation for Medical Reports", "abstract": "Natural language generation in the medical domain is heavily influenced by domain knowledge and genre-specific text characteristics. We present SemScribe, an implemented natural language generation system that produces doctor's letters, in particular descriptions of cardiological findings. Texts in this domain are characterized by a high density of information and a relatively telegraphic style. Domain knowledge is encoded in a medical ontology of about 80,000 concepts. The ontology is used in particular for concept generalizations during referring expression generation. Architecturally, the system is a generation pipeline that uses a corpus-informed syntactic frame approach for realizing sentences appropriate to the domain. The system reads XML documents conforming to the HL7 Clinical Document Architecture (CDA) Standard and enhances them with generated text and references to the used data elements. We conducted a first clinical trial evaluation with medical staff and report on the findings.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlg", "onto", "nlg", "nlg"], "mention_counts": {"nlg": 3, "onto": 2}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "88e98c6fcafffa3c5599190603f1eb4ebce6baa1", "url": "https://www.semanticscholar.org/paper/88e98c6fcafffa3c5599190603f1eb4ebce6baa1", "title": "Integrating Lexical Units, Synsets and Ontology in the Cornetto Database", "abstract": "Cornetto is a two-year Stevin project (project number STE05039) in which a lexical semantic database is built that combines Wordnet with Framenet-like information for Dutch. The combination of the two lexical resources (the Dutch Wordnet and the Referentie Bestand Nederlands) will result in a much richer relational database that may improve natural language processing (NLP) technologies, such as word sense-disambiguation, and language-generation systems. In addition to merging the Dutch lexicons, the database is also mapped to a formal ontology to provide a more solid semantic backbone. Since the database represents different traditions and perspectives of semantic organization, a key issue in the project is the alignment of concepts across the resources. This paper discusses our methodology to first automatically align the word meanings and secondly to manually revise the most critical cases.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 63, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto", "wsd"], "mention_counts": {"nlp": 2, "wsd": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 2, "wsd": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "e4a7ef9dff814de7ed65807bc3133c4f6f75d761", "url": "https://www.semanticscholar.org/paper/e4a7ef9dff814de7ed65807bc3133c4f6f75d761", "title": "Relation extraction with weakly supervised learning based on process-structure-property-performance reciprocity", "abstract": "ABSTRACT In this study, we develop a computer-aided material design system to represent and extract knowledge related to material design from natural language texts. A machine learning model is trained on a text corpus weakly labeled by minimal annotated relationship data (~100 labeled relationships) to extract knowledge from scientific articles. The knowledge is represented by relationships between scientific concepts, such as {annealing, grain size, strength}. The extracted relationships are represented as a knowledge graph formatted according to design charts, inspired by the process-structure-property-performance (PSPP) reciprocity. The design chart provides an intuitive effect of processes on properties and prospective processes to achieve the certain desired properties. Our system semantically searches the scientific literature and provides knowledge in the form of a design chart, and we hope it contributes more efficient developments of new materials. Graphical Abstract", "venue": "Science and Technology of Advanced Materials", "citationCount": 16, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "0b07d9fdca79821d8fd8933dad1d019b09ae73ed", "url": "https://www.semanticscholar.org/paper/0b07d9fdca79821d8fd8933dad1d019b09ae73ed", "title": "Basics of a Drug Ontology for Annotations of Clinical Narratives", "abstract": "Although the medication given to a patient during a hospital stay is fundamental for patient safety, quality assessment and economic aspects of health care services, in-patient drug prescriptions are still commonly done on paper in many institutions. In Europe and North America most of the pharmacies are computerized. This does not guarantee, however, that drug prescription information is always available in hospital information systems in a structured way. As long as the only computer-readable sources of medication information are clinical narratives such as dictated or typed discharge or outpatient letters, analysis of drug information depends on natural language processing (NLP). The performance of NLP critically depends on annotated clinical corpora. We are currently developing an annotation schema for mentions of drugs in discharge letters, of which ontological foundations are presented.", "venue": "JOWO", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 3, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "3f7e95477001c72dda694a130418061b161bdad6", "url": "https://www.semanticscholar.org/paper/3f7e95477001c72dda694a130418061b161bdad6", "title": "Named Entity Recognition in Chinese Medical Literature Using Pretraining Models", "abstract": "The medical literature contains valuable knowledge, such as the clinical symptoms, diagnosis, and treatments of a particular disease. Named Entity Recognition (NER) is the initial step in extracting this knowledge from unstructured text and presenting it as a Knowledge Graph (KG). However, the previous approaches of NER have often suffered from small-scale human-labelled training data. Furthermore, extracting knowledge from Chinese medical literature is a more complex task because there is no segmentation between Chinese characters. Recently, the pretraining models, which obtain representations with the prior semantic knowledge on large-scale unlabelled corpora, have achieved state-of-the-art results for a wide variety of Natural Language Processing (NLP) tasks. However, the capabilities of pretraining models have not been fully exploited, and applications of other pretraining models except BERT in specific domains, such as NER in Chinese medical literature, are also of interest. In this paper, we enhance the performance of NER in Chinese medical literature using pretraining models. First, we propose a method of data augmentation by replacing the words in the training set with synonyms through the Mask Language Model (MLM), which is a pretraining task. Then, we consider NER as the downstream task of the pretraining model and transfer the prior semantic knowledge obtained during pretraining to it. Finally, we conduct experiments to compare the performances of six pretraining models (BERT, BERT-WWM, BERT-WWM-EXT, ERNIE, ERNIE-tiny, and RoBERTa) in recognizing named entities from Chinese medical literature. The effects of feature extraction and fine-tuning, as well as different downstream model structures, are also explored. Experimental results demonstrate that the method of data augmentation we proposed can obtain meaningful improvements in the performance of recognition. Besides, RoBERTa-CRF achieves the highest F1-score compared with the previous methods and other pretraining models.", "venue": "Sci. Program.", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "kg", "nlp"], "mention_counts": {"nlp": 2, "kg": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "d80f44537ee1025483e78ca6069bcd2f3cfd3bb6", "url": "https://www.semanticscholar.org/paper/d80f44537ee1025483e78ca6069bcd2f3cfd3bb6", "title": "On automatic construction of based-NLP Chinese medicine ontology concept\u2019s description architacture", "abstract": "This article makes further study on data needed by automatic construction for Chinese medicinal ontologypsila concept description architecture, reconstructs and uses recognized knowledge in Chinese medicinepsilas domain by theory and technology of NLP. Based on realizing Chinese medicine knowledge description architecturepsilas automatic construction and acquiring successfully, this article use expertspsila knowledge to realize au-learning system of limited textpsilas ontology and try exploring domain ontologypsila evolution .it revolve bottleneck problems of ontology study effectively to lay the data base for Chinese knowledgepsilas dining and use. Untill now, this has been more ideal and applied method and its successful study would be provide theory gist and technology support for professional domain ontologypsilas automatic construction.", "venue": "2008 International Conference on Audio, Language and Image Processing", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "41bf21ec75c0c4f89a1a847816a823e8d1b08356", "url": "https://www.semanticscholar.org/paper/41bf21ec75c0c4f89a1a847816a823e8d1b08356", "title": "A Graph-Theoretic Framework for Semantic Distance", "abstract": "Abstract Many NLP applications entail that texts are classified based on their semantic distance (how similar or different the texts are). For example, comparing the text of a new document to that of documents of known topics can help identify the topic of the new text. Typically, a distributional distance is used to capture the implicit semantic distance between two pieces of text. However, such approaches do not take into account the semantic relations between words. In this article, we introduce an alternative method of measuring the semantic distance between texts that integrates distributional information and ontological knowledge within a network flow formalism. We first represent each text as a collection of frequency-weighted concepts within an ontology. We then make use of a network flow method which provides an efficient way of explicitly measuring the frequency-weighted ontological distance between the concepts across two texts. We evaluate our method in a variety of NLP tasks, and find that it performs well on two of three tasks. We develop a new measure of semantic coherence that enables us to account for the performance difference across the three data sets, shedding light on the properties of a data set that lends itself well to our method.", "venue": "CL", "citationCount": 22, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "8511f594b48b46975d7ec7002c9abfeb59cea683", "url": "https://www.semanticscholar.org/paper/8511f594b48b46975d7ec7002c9abfeb59cea683", "title": "A Combined Method for E-Learning Ontology Population based on NLP and User Activity Analysis", "abstract": "The paper describes a combined approach to maintaining an E-Learning ontology in dynamic and changing educational environment. The developed NLP algorithm based on morpho-syntactic patterns is applied for terminology extraction from course tasks that allows to interlink extracted terms with the instances of the system\u2019s ontology whenever some educational materials are changed. These links are used to gather statistics, evaluate quality of lectures\u2019 and tasks\u2019 materials, analyse students\u2019 answers to the tasks and detect difficult terminology of the course in general (for the teachers) and its understandability in particular (for every student).", "venue": "LILE@ISWC", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "bbf6647f36bf741bc4dff9c6a5883654f3cea569", "url": "https://www.semanticscholar.org/paper/bbf6647f36bf741bc4dff9c6a5883654f3cea569", "title": "Linking e-learning ontology concepts with NLP algorithms", "abstract": "The paper describes applying NLP algorithms to the ontology-based e-learning system. The main goal of the project is to develop a tool creating additional relations between entities based on internal analysis of object property values in the e-learning ontology. Authors present results of automated analysis of links between lecture terms and tests.", "venue": "Proceedings of 16th Conference of Open Innovations Association FRUCT", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "8d511f50397a7bbb6de13634307402aa3ec0fe6f", "url": "https://www.semanticscholar.org/paper/8d511f50397a7bbb6de13634307402aa3ec0fe6f", "title": "Ontology-based WOM extraction service from weblogs", "abstract": "In this paper, we introduce a web-based service that extracts reputations of a product from the internet. If a user inputs the product name, the service first collects articles reviewing the product from weblogs, bbs, and so on. Also, it analyzes their contents using metadata and ontologies with conventional NLP techniques. Then, it indicates the reputations (positive or negative) from the overall and several pre-defined aspects, and other related products that are the subject of much discussion in the articles. This paper illustrates two technical points regarding use of metadata and ontologies with NLP, and summarizes evaluations in a case that we applied it to a market research for a vehicle.", "venue": "ACM Symposium on Applied Computing", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "226b67a66eed88537b800edfc9639c6a436a645e", "url": "https://www.semanticscholar.org/paper/226b67a66eed88537b800edfc9639c6a436a645e", "title": "Extracting knowledge using probabilistic classifier for text mining", "abstract": "Text mining is a process of extracting knowledge from large text documents. A new probabilistic classifier for text mining is proposed in this paper. It uses ODP taxonomy and domain ontology and datasets to cluster and identify the category of the given text document. The proposed work has three steps, namely, preprocessing, rule generation and probability calculation. At the stage of preprocessing the input document is split into paragraphs and statements. In rule generation, the documents from the training set are read. In probability calculation, positive and negative weight factor is calculated. The proposed algorithm calculates the positive probability value and negative probability value for each term set or pattern identified from the document. Based on the calculated probability value the probabilistic classifier indexes the document to the concern group of the cluster.", "venue": "2013 International Conference on Pattern Recognition, Informatics and Mobile Engineering", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ke", "ke"], "mention_counts": {"ke": 2, "onto": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.4979674649614837}, {"paperId": "8102a64a0f4b353f9a9e15ef671469270fdfd1f8", "url": "https://www.semanticscholar.org/paper/8102a64a0f4b353f9a9e15ef671469270fdfd1f8", "title": "On the Application of the Cyc Ontology to Word Sense Disambiguation", "abstract": "This paper describes a novel, unsupervised method of word sense disambiguation that is wholly semantic, drawing upon a complex, rich ontology and inference engine (the Cyc system). This method goes beyond more familiar semantic closeness approaches to disambiguation that rely on string cooccurrence or relative location in a taxonomy or concept map by 1) exploiting a rich array of properties, including higher-order properties, not available in merely taxonomic (or other first-order) systems, and 2) appealing to the semantic contribution a word sense makes to the content of the target text. Experiments show that this method produces results markedly better than chance when disambiguating word senses in a corpus of topically unrelated documents.", "venue": "The Florida AI Research Society", "citationCount": 58, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "wsd", "wsd", "wsd"], "mention_counts": {"wsd": 3, "onto": 2}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.4979674649614837}, {"paperId": "f6cfcef2d3caa8b2e41d7801262ffbedaf4a8752", "url": "https://www.semanticscholar.org/paper/f6cfcef2d3caa8b2e41d7801262ffbedaf4a8752", "title": "Ontology Based Structured Representation for Domain Specific Unstructured Documents", "abstract": "Extracting information from unstructured, brief and short text composed of short phrases, incomplete sentences, unordered sequence of words and words in short form not falling into any regular syntax is a challenging task. This paper describes an approach to automatically extract information from data rich unstructured text documents based on a domain dependent ontology and populate a database. Here, we apply pattern matching in terms of keywords/constants to extract the patterns and generate a structured text representation with respect to a domain specific ontology. The approach is illustrated on one such unstructured, short and brief text -classified matrimonial advertisement. The performance analysis of the approach on this case study is presented.", "venue": "International Conference on Computational Intelligence and Multimedia Applications (ICCIMA 2007)", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "ie", "onto"], "mention_counts": {"onto": 3, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "b55f0928b72bd06a3d151e2053a70c6ea6b784ad", "url": "https://www.semanticscholar.org/paper/b55f0928b72bd06a3d151e2053a70c6ea6b784ad", "title": "Using Ontologies and the Web to Learn Lexical Semantics", "abstract": "A variety of text processing tasks require or benefit from semantic resources such as ontologies and lexicons. Creating these resources manually is tedious, time consuming, and prone to error. We present a new algorithm for using the web to determine the correct concept in an existing ontology to lexicalize previously unknown words, such as might be discovered while processing texts. A detailed empirical comparison of our algorithm with two existing algorithms (Cilibrasi & Vitanyi 2004, Maedche et al. 2002) is described, leading to insights into the sources of the algorithms' strengths and weaknesses.", "venue": "IJCAI", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "tp", "onto", "onto", "tp"], "mention_counts": {"onto": 3, "tp": 2}, "nlp_mention_counts": {"tp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "d77b5e06d85f786d3d0b59cfc0a017e78388c400", "url": "https://www.semanticscholar.org/paper/d77b5e06d85f786d3d0b59cfc0a017e78388c400", "title": "Two Sides of a Coin - Translate while Classify Multilanguage Annotations with Domain Ontology-driven Word Sense Disambiguation", "abstract": "In this paper we present an approach for the translation and classification of short texts in one step. Our work lays in the tradition of Domain-Driven Word Sense Disambiguation, though a major emphasis is given to domain ontologies as the right tool for sense-tagging and topic detection of short texts which, by their nature, are known to be reluctant to statistical treatment. We claim that in a scenario where users can annotate knowledge items using different languages, domain ontologies can prove very suitable for driving the word disambiguation and topic classification tasks. In this way, two tasks are gainfully collapsed in a single one. Although this study is still in its infancy, in what follows we are able to articulate motivations, design, workflow analysis, and concrete evolutions envisioned for our tool.", "venue": "International Conference on Agents and Artificial Intelligence", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "onto", "wsd", "onto", "onto"], "mention_counts": {"wsd": 2, "onto": 3}, "nlp_mention_counts": {"wsd": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.4979674649614837}, {"paperId": "f9b7cf56cbd3fcfc98b9d2c859da17c7d91dffb2", "url": "https://www.semanticscholar.org/paper/f9b7cf56cbd3fcfc98b9d2c859da17c7d91dffb2", "title": "Koios++: A Query-Answering System for Handwritten Input", "abstract": "In this paper we propose KOIOS++, which automatically processes natural language queries provided by handwritten input. The system integrates several recent achievements in the area of handwriting recognition, natural language processing, information retrieval, and human computer interaction. It uses a knowledge base described by the resource description framework (RDF). Our generic approach first generates a lexicon as background information for the handwritten text recognition. After recognizing a handwritten query, several output hypotheses are sent to a natural language processing system in order to generate a structured query (SPARQL query). Subsequently, the query is applied to the given knowledge base and a result graph visualizes the retrieved information. At all stages, the user can easily adjust the intermediate results if there is any undesired outcome. The system is implemented as a web-service and therefore works for handwritten input on digital paper as well as on input on Pen-enabled interactive surfaces. Furthermore, we build on the generic RDF-representation of semantic knowledge which is also used by the linked open data (LOD) initiative. As such, our system works well in various scenarios. We have implemented prototypes for querying company knowledge bases, the DBPedia1, the DBLP computer science bibliography2, and a knowledge base of the DAS 2012.", "venue": "2012 10th IAPR International Workshop on Document Analysis Systems", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "lod", "nlp", "nlp", "kg", "nlp", "lod", "rdf", "kg", "rdf", "rdf"], "mention_counts": {"nlp": 3, "kg": 4, "lod": 2, "rdf": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 4, "lod": 2, "rdf": 3}, "relevance_score": 0.4910068950189542}, {"paperId": "8ac0dd1d23e0514bc51fb9de8f654ed46ec25e9e", "url": "https://www.semanticscholar.org/paper/8ac0dd1d23e0514bc51fb9de8f654ed46ec25e9e", "title": "Decision trees in automatic ontology matching", "abstract": "The semantic web progress gave rise to a growing number of ontologies in various fields. In order to allow knowledge reuse, ontology matching is an interesting option. In this paper, we propose an ontology matching system that performs class, property and instance matching. This latter is usually achieved by means of Natural Language Processing NLP techniques, which are context dependent. To avoid the limits of NLP, we use a decision tree-based instance matching scheme. Decision tree is one of the most widely used learning algorithms for inductive inference and classification. Our system works on OWL-DL ontologies, that is ontologies expressed in the Description Logic version of the Ontology Web Language. This ensures the maximum of expressiveness, completeness and decidability. Our approach is tested with the benchmark and conference tracks of the OAEI'2015 campaign. It shows very promising results since it outperforms other matching systems in most of the test cases.", "venue": "Int. J. Metadata Semant. Ontologies", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "sw", "onto", "onto", "nlp", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 3, "sw": 1, "onto": 8}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 1, "onto": 8}, "relevance_score": 0.4910068950189542}, {"paperId": "df2877d90a017d480a15ea9f788647a1cd29f91b", "url": "https://www.semanticscholar.org/paper/df2877d90a017d480a15ea9f788647a1cd29f91b", "title": "Methods and Models of Intellectual Processing of Texts for Building Ontologies of Software for Medical Terms Identification in Content Classification", "abstract": "The article investigates the problem of automated development of basic ontology. A method, algorithm and means for extracting knowledge from natural text are proposed. It is shown that such an algorithm should be multistage and include a hierarchical multi-level procedure for recognizing concepts, relationships, predicates and rules, which are introduced as a result of ontology. The analysis of the subject area is the search and analysis of various information systems analogues. The analysis of methods and criteria of information systems is carried out. The analysis of the system and its functionality is presented. This paper examines methods and models of intellectual text processing, the results of which are intended to build software ontologies, and are used during Ontology Learning, when it is necessary to improve, extend, modify an existing ontology model, or build ontology from basic ontology, having only textcollection collections as sources of knowledge. In the latter case, the task is particularly complex and requires the use of the full range of text mining methods (Text Mining as TM). The paper deals with the solution of TM problems at different stages of the PMT processing: obtaining information (identifying entities concepts and terms, their properties, facts, events, establishing relationships between entities, in particular associative ones), categorization, and clustering, semantic annotation. Below we will consider the tools of automated analysis of natural language and software products implemented on their basis for filling the system. Copyright \u00a9 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0) 2019 IDDM Workshops.", "venue": "International Workshop on Informatics & Data-Driven Medicine", "citationCount": 25, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ke", "onto", "onto", "onto", "tp", "onto", "onto", "onto", "tp"], "mention_counts": {"ke": 1, "onto": 8, "tp": 2}, "nlp_mention_counts": {"ke": 1, "tp": 2}, "ld_mention_counts": {"ke": 1, "onto": 8}, "relevance_score": 0.4910068950189542}, {"paperId": "e32d3e4eb5a615737ef5d0ef1555acff8dce4e32", "url": "https://www.semanticscholar.org/paper/e32d3e4eb5a615737ef5d0ef1555acff8dce4e32", "title": "On Moving On On Ontologies", "abstract": "This paper discusses the principles that should govern the construction of two components of a system for natural language generation (NLG): (1) the ontology or, rather, as the paper argues, the 'ontological' aspects of a belief system -and (2) the semantic representation of noun senses. It is an interesting fact that many ontologies bear a striking resemblance to a system network, as used in systemic functional grammar (SFG). Furthermore, two major current research efforts in the field of ontologybuilding are designed to run with a SFG generator: Pangloss, where the generator is Penman, and COMMUNAL, where the generator is GENESYS. It is therefore important to establish a principled approach to the 'division of labour' between the ontology and the equivalent aspects of the model of language here a system network for the 'meaning potential' of English nouns. (However, the general principles should be relevant to ANY model of language.) The paper summafises (a)the purposes and (b)the structure of (1) a system network for noun senses and (2) the equivalent ontology (based on what we in the COMMUNAL Project judge is required in the next generation of belief systems for NLG). Examples are given of current work on the relevant system network and, more briefly, of the equivalent ontological aspects of the belief system. In particular, reasons are given why it would be inappropriate to give a primary place to the 'mass' vs. 'count' distinction in an 'interlingua' ontology and even, surprising though it may seem, in a language-specific semantics for English. Finally, it turns out that, in the new perspective presented here, there is no 'component' of the belief system that is 'the ontology', and the reasons for this apparently anomalous position are given.", "venue": "INLG", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "onto", "onto", "nlg", "onto", "onto", "nlg", "onto", "onto", "onto", "onto"], "mention_counts": {"nlg": 3, "onto": 9}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"onto": 9}, "relevance_score": 0.4910068950189542}, {"paperId": "36ca7b054a964989e08c9b1e05f2e838abd7a1e8", "url": "https://www.semanticscholar.org/paper/36ca7b054a964989e08c9b1e05f2e838abd7a1e8", "title": "Kleo: A Bootstrapping Learning-by-Reading System", "abstract": "KLEO is a bootstrapping learning-by-reading system that builds a knowledge base in a fully automated way by reading texts for a domain. KLEO\u2019s initial knowledge base is a small knowledge base that consists of domain independent knowledge and KLEOexpands the knowledge base with the information extracted from texts. A key facility in KLEO is knowledge integration which combines new information gleaned from individual sentences of the texts, along with prior knowledge, to form a comprehensive and computationally useful knowledge base. This paper introduces the architecture of KLEO, especially the knowledge integration facility, and presents our evaluation plan. The knowledge acquisition bottleneck has been the major obstacle to building large-scale knowledge bases. Despite enormous past efforts, it is still costly and tedious to build knowledge bases manually. As a solution to this problem, a new approach has been gaining much attention due to the advance of natural language processing and the proliferation of texts on the Internet. The approach is to construct a knowledge base with knowledge extracted from texts. KLEO 1 is a such Learning-by-Readingsystem which op", "venue": "AAAI Spring Symposium: Learning by Reading and Learning to Read", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ie", "kg", "kg", "kg", "kg", "kg", "kg", "nlp", "kg"], "mention_counts": {"nlp": 1, "kg": 8, "ke": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 8, "ke": 1}, "relevance_score": 0.4910068950189542}, {"paperId": "49c079c47b2fdcdd4e0d9e694c87c59ca3d849c6", "url": "https://www.semanticscholar.org/paper/49c079c47b2fdcdd4e0d9e694c87c59ca3d849c6", "title": "Learning Well-Founded Ontologies through Word Sense Disambiguation", "abstract": "Foundational Ontologies help maintaining and expanding ontologies expressivity power, thus enabling them to be more precise and free of ambiguities. The use of modeling languages based on these ontologies, such as OntoUML, requires not only the modeler's experience regarding such languages, but also a good understanding about the domain being modeled. Aiming to facilitate, or even enable the modeling of complex domains, several techniques have been proposed in order to automatically generate ontologies from texts. However, none is able to generate well-founded ontologies (which are constructed based on Foundational Ontologies). Moreover, an important issue on learning from text is how to distinguish among different meanings of a word, which impacts on concepts expressed by the ontologies. Therefore, techniques for word sense disambiguation must be considered. This paper proposes a technique for automatically learn well-founded ontologies described in OntoUML through word sense disambiguation.", "venue": "2013 Brazilian Conference on Intelligent Systems", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "wsd", "onto", "wsd", "onto", "onto", "wsd", "onto", "onto", "onto"], "mention_counts": {"wsd": 3, "onto": 9}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"onto": 9}, "relevance_score": 0.4910068950189542}, {"paperId": "5937450fda276e43b19de29456a323f6556f68f6", "url": "https://www.semanticscholar.org/paper/5937450fda276e43b19de29456a323f6556f68f6", "title": "Machine Translation and Automated Analysis of the Sumerian Language", "abstract": "This paper presents a newly funded international project for machine translation and automated analysis of ancient cuneiform languages where NLP specialists and Assyriologists collaborate to create an information retrieval system for Sumerian. This research is conceived in response to the need to translate large numbers of administrative texts that are only available in transcription, in order to make them accessible to a wider audience. The methodology includes creation of a specialized NLP pipeline and also the use of linguistic linked open data to increase access to the results.", "venue": "LaTeCH@ACL", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["llod", "mt", "nlp", "nlp", "mt", "lod"], "mention_counts": {"nlp": 2, "llod": 1, "lod": 1, "mt": 2}, "nlp_mention_counts": {"nlp": 2, "mt": 2}, "ld_mention_counts": {"llod": 1, "lod": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "0870ea550fda6773d2430bac840b7dcd05b78386", "url": "https://www.semanticscholar.org/paper/0870ea550fda6773d2430bac840b7dcd05b78386", "title": "A Machine Reader for the Semantic Web", "abstract": "FRED is a machine reading tool for converting text into internally well-connected and quality linked-data-ready ontologies in web-service-acceptable time. It implements a novel approach for ontology design from natural language sentences, combining Discourse Representation Theory (DRT), linguistic frame semantics, and Ontology Design Patterns (ODP). The current version of the tool includes Earmark-based markup, and enrichment with word sense disambiguation (WSD) and named entity resolution (NER) off-the-shelf components.", "venue": "SEMWEB", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "wsd", "sw", "onto", "onto", "ld"], "mention_counts": {"ld": 1, "sw": 1, "wsd": 2, "onto": 2}, "nlp_mention_counts": {"wsd": 2}, "ld_mention_counts": {"ld": 1, "sw": 1, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "2b33c250faa413d4089fb3ae7680be52d9e6032c", "url": "https://www.semanticscholar.org/paper/2b33c250faa413d4089fb3ae7680be52d9e6032c", "title": "XLike: cross-lingual knowledge extraction", "abstract": "The goal of the XLike project is to develop technology to monitor and aggregate knowledge that is currently spread across mainstream and social media, and to enable cross-lingual services for publishers, media monitoring and business intelligence. The aim is to combine scientific insights from several scientific areas to contribute in the area of cross-lingual text understanding. By combining modern computational linguistics, machine translation, machine learning, text mining and semantic technologies we plan to deal with the following two key open research problems: (1) to extract and integrate formal knowledge from multilingual texts with cross-lingual knowledge bases, and; (2) to adapt linguistic techniques and crowdsourcing to deal with irregularities in informal language used primarily in social media. The developed technology will be language-agnostic, while within the project we specifically address English, German, Spanish, Chinese as major world languages and Catalan, Slovenian and Croatian as minority languages. Knowledge resources from Linked Open Data cloud (e.g. Wikipedia, DBpedia, Wordnets etc.) will be used with special focus on general common sense knowledge base CycKB, that will be used as Interlingua. A number of different methods to translate from natural language to the selected formal language that serves as our Interlingua are being explored, among others also SMT. For languages where no required linguistic resources are available, we use SMT systems trained from parallel or comparable corpora (e.g. drawn from the Wikipedia) to come up with the Interlingua representation.", "venue": "EAMT", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "kg", "kg", "ke", "lod"], "mention_counts": {"kg": 2, "lod": 1, "ke": 1, "mt": 1}, "nlp_mention_counts": {"ke": 1, "mt": 1}, "ld_mention_counts": {"kg": 2, "lod": 1, "ke": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "04c1c1bb97091113e27539ee0975bba46df0e698", "url": "https://www.semanticscholar.org/paper/04c1c1bb97091113e27539ee0975bba46df0e698", "title": "Improvements in Information Extraction in Legal Text by Active Learning", "abstract": "Managing licensing information and data rights is becoming a crucial issue in the Linked (Open) Data scenario. An open problem in this scenario is how to associate machine-readable licenses specifications to the data, so that automated approaches to treat such information can be fruitfully exploited to avoid data misuse. This means that we need a way to automatically extract from a natural language document specifying a certain license a machine-readable description of the terms of use and reuse identified in such license. Ontology-based Information Extraction is crucial to translate natural language documents into Linked Data. This connection supports consumers in navigating documents and semantically related data. However , the performances of automated information extraction systems are far from being perfect, and rely heavily on human intervention, either to create heuristics, to annotate examples for inferring models, or to interpret or validate patterns emerging from data. In this paper, we apply different Active Learning strategies to Information Extraction (IE) from licenses in English, with highly repetitive text, few annotated or unannotated examples available, and very fine precision needed. We show that the most popular approach to active learning, i.e., uncertainty sampling for instance selection, does not provide a good performance in this setting. We show that we can obtain a similar effect to that of density-based methods using uncertainty sampling , by just reversing the ranking criterion, and choosing the most certain instead of the most uncertain instances.", "venue": "JURIX", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "ld", "ie", "ie"], "mention_counts": {"ld": 1, "onto": 1, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"ld": 1, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "eaa4c1830012b627b74658069acfbfedb6d23705", "url": "https://www.semanticscholar.org/paper/eaa4c1830012b627b74658069acfbfedb6d23705", "title": "Up-cycling Data for Natural Language Generation", "abstract": "Museums and other cultural heritage institutions have large databases of information about the objects in their collections, and existing Natural Language Generation (NLG) systems can generate fluent and adaptive texts for visitors, given appropriate input data, but there is typically a large amount of expert human effort required to bridge the gap between the available and the required data. We describe automatic processes which aim to significantly reduce the need for expert input during the conversion and up-cycling process. We detail domain-independent techniques for processing and enhancing data into a format which allows an existing NLG system to create adaptive texts. First we normalize the dates and names which occur in the data, and we link to the Semantic Web to add extra object descriptions. Then we use Semantic Web queries combined with a wide coverage grammar of English to extract relations which can be used to express the content of database fields in language accessible to a general user. As our test domain we use a database from the Edinburgh Musical Instrument Museum.", "venue": "LREC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "nlg", "nlg", "sw", "nlg", "sw"], "mention_counts": {"sw": 2, "nlg": 4}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"sw": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "b248cdf870da2da5bf659b49c094853f9aa0c72c", "url": "https://www.semanticscholar.org/paper/b248cdf870da2da5bf659b49c094853f9aa0c72c", "title": "How the Multilingual Semantic Web can meet the Multilingual Web", "abstract": "The success of the Web is not based on technology. It is rather based on the availability of tooling to create web content, the fast number of content creators providing content, and finally the users who eagerly \u201cdigest\u201d the content and are willing to pay for it, being part of various business models. Not only the Web in general, but also the Multilingual Web is growing. More and more content is being produced in languages other than English; more and more users want to use their mother tongue on the Web. Unfortunately this growth is not without undesired side effects. \u201cIf a language is not on the Web, it doesn\u2019t exist\u201d \u2013 this phrase 1 expresses the fear of \u201cdigital extinction\u201d, faced especially by smaller language communities. To support the Multilingual Web, language technology can play a crucial role: the machine translation of the English Wikipedia articles into Thai is just one example how massive content creation can rely on language technology. The outcome is of course not perfect, and only with human post-editing the result is really useful. What does all this tell us about the Multilingual Semantic Web (MLSW)? First, like with the Web itself, the availability of standardized technological blocks is a pre-requisite for wide adoption of the MLSW. However, this is not enough. Easy to use tooling to create and to work with RDF based resources is inevitable to lower the barriers for the ordinary content creator. There should be no difference in working with the MLSW compared to editing an HTML web page or setting up a blog. Second, although the technical infrastructure of the MLSW is given via RDF based building blocks, MLSW resources are rare. Studies 2 reveal that human readable descriptions even in English are hardly available; for other languages or links between languages in the MLSW the situation is even worse. Third and finally, like for the human readable Web, the application of language technologies can help to create resources for the MLSW, e.g. via the creation of multi-language labels via machine translation. But also like with translation of ordinary Web pages, such approaches need human intervention to assure a certain level of quality.", "venue": "MSW", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "mt", "mt", "sw", "sw", "rdf"], "mention_counts": {"mt": 2, "sw": 2, "rdf": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"sw": 2, "rdf": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "9076c255ba15f56205f5029e2fa7ab0183d1ca3d", "url": "https://www.semanticscholar.org/paper/9076c255ba15f56205f5029e2fa7ab0183d1ca3d", "title": "Single Concatenated Input is Better than Indenpendent Multiple-input for CNNs to Predict Chemical-induced Disease Relation from Literature", "abstract": "Chemical compounds (drugs) and diseases are among top searched keywords on the PubMed database of biomedical literature by biomedical researchers all over the world (according to a study in 2009). Working with PubMed is essential for researchers to get insights into drugs\u2019 side effects (chemical-induced disease relations (CDR), which is essential for drug safety and toxicity. It is, however, a catastrophic burden for them as PubMed is a huge database of unstructured texts, growing steadily very fast (~28 millions scientific articles currently, approximately two deposited per minute). As a result, biomedical text mining has been empirically demonstrated its great implications in biomedical research communities. Biomedical text has its own distinct challenging properties, attracting much attetion from natural language processing communities. A large-scale study recently in 2018 showed that incorporating information into indenpendent multiple-input layers outperforms concatenating them into a single input layer (for biLSTM), producing better performance when compared to state-of-the-art CDR classifying models. This paper demonstrates that for a CNN it is vice-versa, in which concatenation is better for CDR classification. To this end, we develop a CNN based model with multiple input concatenated for CDR classification. Experimental results on the benchmark dataset demonstrate its outperformance over other recent state-of-the-art CDR classification models. \nKeywords: \nChemical disease relation prediction, Convolutional neural network, Biomedical text mining \nReferences \n[1] Paul SM, S. Mytelka, C.T. Dunwiddie, C.C. Persinger, B.H. Munos, S.R. Lindborg, A.L. Schacht, How to improve R&D productivity: The pharmaceutical industry's grand challenge, Nat Rev Drug Discov. 9(3) (2010) 203-14. https://doi.org/10.1038/nrd3078. \n[2] J.A. DiMasi, New drug development in the United States from 1963 to 1999, Clinical pharmacology and therapeutics 69 (2001) 286-296. https://doi.org/10.1067/mcp.2001.115132. \n[3] C.P. Adams, V. Van Brantner, Estimating the cost of new drug development: Is it really $802 million? Health Affairs 25 (2006) 420-428. https://doi.org/10.1377/hlthaff.25.2.420. \n[4] R.I. Do\u011fan, G.C. Murray, A. N\u00e9v\u00e9ol et al., \"Understanding PubMed user search behavior through log analysis\", Oxford Database, 2009. \n[5] G.K. Savova, J.J. Masanz, P.V. Ogren et al., \"Mayo clinical text analysis and knowledge extraction system (cTAKES): Architecture, component evaluation and applications\", Journal of the American Medical Informatics Association, 2010. \n[6] T.C. Wiegers, A.P. Davis, C.J. Mattingly, Collaborative biocuration-text mining development task for document prioritization for curation, Database 22 (2012) pp. bas037. \n[7] N. Kang, B. Singh, C. Bui et al., \"Knowledge-based extraction of adverse drug events from biomedical text\", BMC Bioinformatics 15, 2014. \n[8] A. N\u00e9v\u00e9ol, R.L. Do\u011fan, Z. Lu, \"Semi-automatic semantic annotation of PubMed queries: A study on quality, Efficiency, Satisfaction\", Journal of Biomedical Informatics 44, 2011. \n[9] L. Hirschman, G.A. Burns, M. Krallinger, C. Arighi, K.B. Cohen et al., Text mining for the biocuration workflow, Database Apr 18, 2012, pp. bas020. \n[10] Wei et al., \"Overview of the BioCreative V Chemical Disease Relation (CDR) Task\", Proceedings of the Fifth BioCreative Challenge Evaluation Workshop, 2015. \n[11] P. Verga, E. Strubell, A. McCallum, Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction, In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies 1 (2018) 872-884. \n[12] Y. Shen, X. Huang, Attention-based convolutional neural network for semantic relation extraction, In: Proceedings of COLING 2016, the Twenty-sixth International Conference on Computational Linguistics: Technical Papers, The COLING 2016 Organizing Committee, Osaka, Japan, 2016, pp. 2526-2536. \n[13] Y. Peng, Z. Lu, Deep learning for extracting protein-protein interactions from biomedical literature, In: Proceedings of the BioNLP 2017 Workshop, Association for Computational Linguistics, Vancouver, Canada, 2016, pp. 29-38. \n[14] S. Liu, F. Shen, R. Komandur Elayavilli, Y. Wang, M. Rastegar-Mojarad, V. Chaudhary, H. Liu, Extracting chemical-protein relations using attention-based neural networks, Database, 2018. \n[15] H. Zhou, H. Deng, L. Chen, Y. Yang, C. Jia, D. Huang, Exploiting syntactic and semantics information for chemical-disease relation extraction, Database, 2016, pp. baw048. \n[16] S. Liu, B. Tang, Q. Chen et al., Drug\u2013drug interaction extraction via convolutional neural networks, Comput, Math, Methods Med, Vol (2016) 1-8. https://doi.org/10.1155/2016/6918381. \n[17] L. Wang, Z. Cao, G. De Meloet al., Relation classification via multi-level attention CNNs, In: Proceedings of the Fifty-fourth Annual Meeting of the Association for Computational Linguistics 1 (2016) 1298-1307.\u00a0 \nhttps://doi.org/10.18653/v1/P16-1123. \n[18] J. Gu, F. Sun, L. Qian et al., Chemical-induced disease relation extraction via convolutional neural network, Database (2017) 1-12. https://doi.org/10.1093/database/bax024. \n[19] H.Q. Le, D.C. Can, S.T. Vu, T.H. Dang, M.T. Pilehvar, N. Collier, Large-scale Exploration of Neural Relation Classification Architectures, In\u00a0Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 2266-2277. \n[20] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition, In Proceedings of the IEEE. 86(11) (1998) 2278-2324. \n[21] Y. Kim, Convolutional neural networks for sentence classification, ArXiv preprint arXiv:1408.5882. \n[22] C. Nagesh, Panyam, Karin Verspoor, Trevor Cohn and Kotagiri Ramamohanarao, Exploiting graph kernels for high performance biomedical relation extraction, Journal of biomedical semantics 9(1) (2018) 7. \n[23] H. Zhou, H. Deng, L. Chen, Y. Yang, C. Jia, D. Huang, Exploiting syntactic and semantics information for chemical-disease relation extraction, Database,\u00a02016.", "venue": "VNU Journal of Science: Computer Science and Communication Engineering", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "hlt", "ke", "nlp", "kg"], "mention_counts": {"nlp": 2, "kg": 1, "hlt": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 2, "hlt": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "2e9fd22b64ba73262cc537d7b7f265c8d59f4acf", "url": "https://www.semanticscholar.org/paper/2e9fd22b64ba73262cc537d7b7f265c8d59f4acf", "title": "Semantic Similarity from Natural Language and Ontology Analysis", "abstract": "Artificial Intelligence federates numerous scientific fields in the aim of developing machines able to assist human operators performing complex treatments; most of which demand high cognitive skills (e.g. learning or decision processes). Central to this quest is to give machines the ability to estimate the likeness or similarity between things in the way human beings estimate the similarity between stimuli. In this context, this book focuses on semantic measures: approaches designed for comparing semantic entities such as units of language, e.g. words, sentences, or concepts and instances defined into knowledge bases. The aim of these measures is to assess the similarity or relatedness of such semantic entities by taking into account their semantics, i.e. their meaning; intuitively, the words tea and coffee, which both refer to stimulating beverage, will be estimated to be more semantically similar than the words toffee (confection) and coffee, despite that the last pair has a higher syntactic similarity. The two state-of-the-art approaches for estimating and quantifying semantic similarities/relatedness of semantic entities are presented in detail: the first one relies on corpora analysis and is based on Natural Language Processing techniques and semantic models while the second is based on more or less formal, computer-readable and workable forms of knowledge such as semantic networks, thesauri or ontologies. Semantic measures are widely used today to compare units of language, concepts, instances or even resources indexed by them (e.g., documents, genes). They are central elements of a large variety of Natural Language Processing applications and knowledge-based treatments, and have therefore naturally been subject to intensive and interdisciplinary research efforts during last decades. Beyond a simple inventory and categorization of existing measures, the aim of this monograph is to convey novices as well as researchers of these domains toward a better understanding of semantic similarity estimation and more generally semantic measures. To this end, we propose an in-depth characterization of existing proposals by discussing their features, the assumptions on which they are based and empirical results regarding their performance in particular applications. By answering these questions and by providing a detailed discussion on the foundations of semantic measures, our aim is to give the reader key knowledge required to: (i) select the more relevant methods according to a particular usage context, (ii) understand the challenges offered to this field of study, (iii) distinguish room of improvements for state-of-the-art approaches and (iv) stimulate creativity toward the development of new approaches. In this aim, several definitions, theoretical and practical details, as well as concrete applications are presented.", "venue": "Semantic Similarity from Natural Language and Ontology Analysis", "citationCount": 145, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "onto", "nlp", "kg", "nlp"], "mention_counts": {"nlp": 2, "kg": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "02615d29ffa299766ad4281ccd14cf88a3dc2633", "url": "https://www.semanticscholar.org/paper/02615d29ffa299766ad4281ccd14cf88a3dc2633", "title": "Annotation of phenotypes using ontologies: a gold standard for the training and evaluation of natural language processing systems", "abstract": "Natural language descriptions of organismal phenotypes - a principal object of study in biology, are abundant in biological literature. Expressing these phenotypes as logical statements using formal ontologies would enable large-scale analysis on phenotypic information from diverse systems. However, considerable human effort is required to make the semantics of phenotype descriptions amenable to machine reasoning by (a) recognizing appropriate on-tological terms for entities in text and (b) stringing these terms into logical statements. Most existing Natural Language Processing tools stop at entity recognition, leaving a need for tools that can assist with both aspects of the task. The recently described Semantic CharaParser aims to meet this need. We describe the first expert-curated Gold Standard corpus for ontology-based annotation of phenotypes from the systematics literature. We use it to evaluate Semantic CharaParser\u2019s annotations and explore differences in performance between humans and machine. We use four annotation accuracy metrics that can account for both semantically identical and similar matches. We found that machine-human consistency was significantly lower than inter-curator (human\u2013human) consistency. Surprisingly, allowing curators access to external information that was not available to Semantic CharaParser did not significantly increase the similarity of their annotations to the Gold Standard nor have a significant effect on inter-curator consistency. We found that the similarity of machine annotations to the Gold Standard increased after new ontology terms relevant to the input text had been added. Evaluation by the original authors of the character descriptions indicated that the Gold Standard annotations came closer to representing their intended meaning than did either the curator or machine annotations. These findings point toward ways to better design of software to augment human curators, and the Gold Standard corpus will allow training and assessment of new tools to improve phenotype annotation accuracy at scale.", "venue": "bioRxiv", "citationCount": 21, "fieldsOfStudy": ["Biology", "Computer Science", "Medicine"], "mentions": ["nlp", "onto", "onto", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "311a8b8d070411763097259d94af628d187a20d8", "url": "https://www.semanticscholar.org/paper/311a8b8d070411763097259d94af628d187a20d8", "title": "Towards a semantic-based approach for software reusable component classification and retrieval", "abstract": "In this paper, we propose a semantic-based approach to improve software component reuse. The whole approach extends the software reusable library to the World Wide Web; overcomes the keyword-based barrier by allowing user queries in natural language; treats a software component as a service described by semantic service representation format; enhances the retrieval by semantically matching between a user query semantic representation and software component semantic descriptions against a domain ontology; and finally stores the relevant software components into a reusable repository based UDDI infrastructure. The technologies applied to achieve the goal include: Natural Language Processing, Web services, Semantic Web, Conceptual Graph, domain ontology. The research in the first phase will focus on the classification and retrieval for software reusable components. In the classification process, natural language processing and domain knowledge technologies are employed for program understanding down to code level, and Web services and Semantic Web technologies as well as Conceptual Graph are used to semantically describe/represent a component. In the retrieval process, a user query in natural language is translate into semantic representation formats in order to augment retrieval recall and precision by deploying the same semantic representation technologies on both the user query side and the component side.", "venue": "ACM-SE 42", "citationCount": 70, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "sw", "nlp", "onto", "nlp", "sw"], "mention_counts": {"nlp": 2, "sw": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "21a4f55a056dc3c92b6076ab33e6a5bc0e7abe1c", "url": "https://www.semanticscholar.org/paper/21a4f55a056dc3c92b6076ab33e6a5bc0e7abe1c", "title": "Semi-supervised instance population of an ontology using word vector embedding", "abstract": "In many modern-day systems such as information extraction and knowledge management agents, ontologies play a vital role in maintaining the concept hierarchies of the selected domain. However, ontology population has become a problematic process due to its nature of heavy coupling with manual human intervention. With the use of word embeddings in the field of natural language processing, it became a popular topic due to its ability to cope up with semantic sensitivity. Hence, in this study we propose a novel way of semi-supervised ontology population through word embeddings as the basis. We built several models including traditional benchmark models and new types of models which are based on word embeddings. Finally, we ensemble them together to come up with a synergistic model with better accuracy. We demonstrate that our ensemble model can outperform the individual models.", "venue": "International Conference on Advances in ICT for Emerging Regions", "citationCount": 20, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 4, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "2b823e7dc462328105bbb48d1010851122cd3578", "url": "https://www.semanticscholar.org/paper/2b823e7dc462328105bbb48d1010851122cd3578", "title": "AQUA: A Closed-Domain Question Answering System", "abstract": "This article describes AQUA, an experimental question answering system. AQUA combines Natural Language processing (NLP), Ontologies, Logic, and Information Retrieval technologies in a uniform framework. AQUA makes intensive use of an ontology in several parts of the question answering system. The ontology is used in the refinement of the initial query, the reasoning process and in the novel similarity algorithm. The similarity algorithm is a key feature of AQUA. It is used to find similarities between relations/concepts in the translated query and relations/concepts in the ontological structures. The similarities detected then allow the interchange of concepts or relations in a logic formula corresponding to the user query.", "venue": "Inf. Syst. Manag.", "citationCount": 20, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "800d5005e834aadf5477b5e7866a2a35f0795fbb", "url": "https://www.semanticscholar.org/paper/800d5005e834aadf5477b5e7866a2a35f0795fbb", "title": "Leveraging Reusability: Cost-Effective Lexical Acquisition for Large-Scale Ontology Translation", "abstract": "Thesauri and ontologies provide important value in facilitating access to digital archives by representing underlying principles of organization. Translation of such resources into multiple languages is an important component for providing multilingual access. However, the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation using general-domain lexical resources. In this paper, we present an efficient process for leveraging human translations when constructing domain-specific lexical resources. We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories. Our experiments demonstrate a cost-effective technique for accurate machine translation of large ontologies.", "venue": "ACL", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "onto", "mt", "onto", "onto", "onto"], "mention_counts": {"onto": 4, "mt": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "0c0e8d3f1e17d2b15d07d0a37f8a55c70b29976d", "url": "https://www.semanticscholar.org/paper/0c0e8d3f1e17d2b15d07d0a37f8a55c70b29976d", "title": "Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine Translation", "abstract": "We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators. \nWe assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains. Our models achieve an overall test accuracy ranging between 61% and 69% for the hotels domain and between 64% and 78% for restaurants domain, which compares favorably to 69% and 80% obtained for English parser trained on gold English data and a few examples from validation set. We show our approach outperforms the previous state-of-the-art methodology by more than 30% for hotels and 40% for restaurants with localized ontologies for the subset of languages tested. \nOur methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours.", "venue": "EMNLP", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "mt", "mt", "mt", "onto"], "mention_counts": {"onto": 2, "mt": 4}, "nlp_mention_counts": {"mt": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "5e8aa14dba952d93a54b7d309dc37f15e51ee9b4", "url": "https://www.semanticscholar.org/paper/5e8aa14dba952d93a54b7d309dc37f15e51ee9b4", "title": "From Tale to Speech: Ontology-based Emotion and Dialogue Annotation of Fairy Tales with a TTS Output", "abstract": "In this demo and poster paper, we describe the concept and implementation of an ontology-based storyteller for fairy tales. Its main functions are (i) annotating the tales by extracting timeline information, characters and dialogues with corresponding emotions expressed in the utterances, (ii) populating an existing ontology for fairy tales with the previously extracted information and (iii) using this ontology to generate a spoken version of the tales. \n \nCommon natural language processing technologies and resources, such as part-of-speech tagging, chunking and semantic networks have been successfully used for the implementation of the three tasks mentioned just above, including the integration of an open source text-to-speech system. The code of the system is publicly available.", "venue": "International Workshop on the Semantic Web", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 1, "onto": 4, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "61967fd07555a4aa3af10b6cc622291584221f11", "url": "https://www.semanticscholar.org/paper/61967fd07555a4aa3af10b6cc622291584221f11", "title": "RETRACTION: A New WSD approach using word ontology and concept distribution", "abstract": "This paper presents a method of word sense disambiguation that assigns a target word the sense that is most related to the senses of its neighbor words. Human languages have words that can mean different things in different contexts, and such words with multiple meanings are potentially `ambiguous'. `Word sense disambiguation' means the process of `deciding which of several meanings of a term is intended in a given context'. We explore the use of measures of relatedness between word senses based on a novel hybrid approach. First, we investigate how to express a `concept' literally and regularly. We apply set algebra to Wordnet's synsets, cooperating with Wordnet's word ontology. We establish regular rules for constructing various representations (lexical notations) of a concept using Boolean operators and word forms in various synset(s) defined in Wordnet. Thus, we establish a formal mechanism for quantifying and estimating the semantic relatedness between concepts\u2014we facilitate `concept distribution statistics' to determine the semantic relatedness between two lexically expressed concepts. Our method does not require any training in advanced. The experimental results showed good performance on Semcor, a subset of the Brown corpus. We observe that measures of semantic relatedness are useful sources of information for word sense disambiguation.", "venue": "J. Inf. Sci.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "wsd", "onto", "wsd", "onto", "wsd"], "mention_counts": {"wsd": 4, "onto": 2}, "nlp_mention_counts": {"wsd": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "f486288fcddcbf8c338eca3b6138364e02002670", "url": "https://www.semanticscholar.org/paper/f486288fcddcbf8c338eca3b6138364e02002670", "title": "Towards NLP-supported Semantic Data Management", "abstract": "The heterogeneity of data poses a great challenge when data from different sources is to be merged for one application. Solutions for this are offered, for example, by ontology-based data management (OBDM). A challenge of OBDM is the automatic creation of semantic models from datasets. This process is typically performed either data- or label-driven and always involves manual human intervention. We identified textual descriptions of data, a form of metadata, quickly to be produced and consumed by humans, as third possible basis for automatic semantic modelling. In this paper, we present, how we plan to use textual descriptions to enhance semantic data management. We will use state of the art NLP technologies to identify concepts within textual descriptions and build semantic models from this in combination with an evolving ontology. We will use automatically identified models in combination with the human data provider to automatically extend the ontology so that it learns new verified concepts over time. Finally, we will use the created ontology and automatically identified semantic models to either rate descriptions for new data sources or even to automatically generate descriptive texts that are easier to understand by the human user than formal models. We present the procedure which we plan for the ongoing research, as well as expected outcomes.", "venue": "ArXiv", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "2d90eaabf9df8474c28bc39bd6a0ad27de2ce9ab", "url": "https://www.semanticscholar.org/paper/2d90eaabf9df8474c28bc39bd6a0ad27de2ce9ab", "title": "Building Taxonomies based on Human-Machine Teaming: Cyber Security as an Example", "abstract": "Taxonomies and ontologies are handy tools in many application domains such as knowledge systematization and automatic reasoning. In the cyber security field, many researchers have proposed such taxonomies and ontologies, most of which were built based on manual work. Some researchers proposed the use of computing tools to automate the building process, but mainly on very narrow sub-areas of cyber security. Thus, there is a lack of general cyber security taxonomies and ontologies, possibly due to the difficulties of manually curating keywords and concepts for such a diverse, inter-disciplinary and dynamically evolving field. This paper presents a new human-machine teaming based process to build taxonomies, which allows human experts to work with automated natural language processing (NLP) and information retrieval (IR) tools to co-develop a taxonomy from a set of relevant textual documents. The proposed process could be generalized to support non-textual documents and to build (more complicated) ontologies as well. Using the cyber security as an example, we demonstrate how the proposed taxonomy building process has allowed us to build a general cyber security taxonomy covering a wide range of data-driven keywords (topics) with a reasonable amount of human effort.", "venue": "ARES", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "a286dabfaa6356ac0634a9ace69e0d6ac289dfb4", "url": "https://www.semanticscholar.org/paper/a286dabfaa6356ac0634a9ace69e0d6ac289dfb4", "title": "BioKB - Text mining and semantic technologies for the biomedical content discovery", "abstract": "The ever-increasing number of publicly available biomedical articles calls for automatic information extraction from digitized publications. We have implemented a pipeline which, by exploiting text mining and semantic technologies, helps researchers easily access semantic content of thousands of abstracts and full text articles. The text mining component analyzes the articles content and extracts relations between a wide variety of concepts, extending the scope from proteins, chemicals and pathologies to biological processes and molecular functions. Moreover, the relations are extracted along with the context which specifies localization of the detected events, preconditions, temporal and logic order, mutual dependency and/or exclusion. Extracted knowledge is stored in a knowledge base publicly available for both, human and machine access, via web interface and SPARQL endpoint. To address the data accessibility, reusability and interoperability, all the extracted relations are standardized using unique resource identifiers (URIs) and a custom ontology based on Genia ontology.", "venue": "Workshop on Semantic Web Applications and Tools for Life Sciences", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "ie", "kg", "onto"], "mention_counts": {"ke": 1, "onto": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 2, "kg": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "78dae5ba664ae651eeb3fe7b71827f572556d67b", "url": "https://www.semanticscholar.org/paper/78dae5ba664ae651eeb3fe7b71827f572556d67b", "title": "Markup: A Web-Based Annotation Tool Powered by Active Learning", "abstract": "Across various domains, such as health and social care, law, news, and social media, there are increasing quantities of unstructured texts being produced. These potential data sources often contain rich information that could be used for domain-specific and research purposes. However, the unstructured nature of free-text data poses a significant challenge for its utilisation due to the necessity of substantial manual intervention from domain-experts to label embedded information. Annotation tools can assist with this process by providing functionality that enables the accurate capture and transformation of unstructured texts into structured annotations, which can be used individually, or as part of larger Natural Language Processing (NLP) pipelines. We present Markup (https://www.getmarkup.com/) an open-source, web-based annotation tool that is undergoing continued development for use across all domains. Markup incorporates NLP and Active Learning (AL) technologies to enable rapid and accurate annotation using custom user configurations, predictive annotation suggestions, and automated mapping suggestions to both domain-specific ontologies, such as the Unified Medical Language System (UMLS), and custom, user-defined ontologies. We demonstrate a real-world use case of how Markup has been used in a healthcare setting to annotate structured information from unstructured clinic letters, where captured annotations were used to build and test NLP applications.", "venue": "Frontiers in Digital Health", "citationCount": 4, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["nlp", "onto", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "2717044f4c3f10392277d806c773e8c47a86edc4", "url": "https://www.semanticscholar.org/paper/2717044f4c3f10392277d806c773e8c47a86edc4", "title": "Semantic lifting of unstructured data based on NLP inference of annotations", "abstract": "The paper introduces approach to semantic lifting of unstructured data with the help of natural language processing (NLP) technologies. Our approach is based on processing the text fragments with NLP tools to tag some of the natural language words and phrases with semantic annotations. Then these inferred annotations are lifted to ontology level in the form of ontology instances that become preliminary automatic annotations of the target text fragments and can later be optionally confirmed and refined by domain experts.", "venue": "CompSysTech '12", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 4, "onto": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "4486ec29894282d1a97c3a0122836dd9beb91d0c", "url": "https://www.semanticscholar.org/paper/4486ec29894282d1a97c3a0122836dd9beb91d0c", "title": "Knowledge representation in scientific models and theirpublications: a case study", "abstract": "It is our pleasure to welcome you to the Seventh International Conference on Knowledge Capture (K-CAP2013). The K-CAP conference series provides a forum that brings together members of several research communities and practitioners who are interested in efficiently capturing knowledge from a variety of sources and in creating representations that can be useful for automated reasoning, analysis, and other forms of machine processing. Therefore, research in knowledge capture is at the intersection of areas such as knowledge engineering, machine learning, big data, natural-language processing, human-computer interaction, Artificial Intelligence, and the Semantic Web. K-CAP 2012 follows on the success of six previous conferences in 2011 (Banff, Alberta Canada), 2009 (Los Angeles, California, USA), 2007 (Whistler, British Columbia, Canada), in 2005 (Banff, Alberta, Canada), in 2003 (Sanibel Island, Florida, USA), and 2001 (Victoria, British Columbia, Canada), and of the series of Knowledge Acquisition Workshops (KAW), the first of which took place in the same location (Banff, Alberta, Canada) in 1986. The theme of this seventh edition of K-CAP is: Knowledge Capture in the Age of Massive Web Data, reflecting the times we are living in. The call for papers attracted 60 submissions from Europe, America, Asia, and Oceania. The program committee accepted 13 full papers, 4 short papers and 3 application papers, that cover a variety of topics, including ontology engineering, ontology learning, linked open data, information extraction for knowledge capture, and knowledge capture from web data including online textual and multimedia resources. In addition, this volume includes descriptions of 8 posters and demos presented at the conference. In this seventh edition of the conference we have tried to innovate in several ways Rather than only aiming at research papers, we introduced an applications track for practitioners to present application-oriented contributions. We issued a call for participation for the first K-CAP \"Datathon\", a \"hackaton\" based on Open Data. Acknowledging the worldwide economic down turn, and the pressure many research and company budgets are suffering, we set up a crowdfunding initiative to help K-CAP 2013 with extra funds (http://www.indiegogo.com/projects/k-cap- 2013). Through this initiative as well as through a standard sponsoring program, we have tried to keep the participation costs as low as possible.", "venue": "K-CAP", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "nlp", "onto", "onto", "sw", "lod"], "mention_counts": {"onto": 2, "nlp": 1, "sw": 1, "lod": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "lod": 1, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "6e74fa424bceb4660e95eeee213c1a00ac0b3403", "url": "https://www.semanticscholar.org/paper/6e74fa424bceb4660e95eeee213c1a00ac0b3403", "title": "Generating sets of related sentences from input seed features", "abstract": "The Semantic Web (SW) can provide Natural Language Generation (NLG) with technologies capable to facilitate access to structured Web data. This type of data can be useful to this research area, which aims to automatically produce human utterances, in its different subtasks, such as in the content selection or its structure. NLG has been widely applied to several fields, for instance to the generation of recommendations (Lim-Cheng et al., 2014). However, generation systems are currently designed for very specific domains (Ramos-Soto et al., 2015) and predefined purposes (Ge et al., 2015). The use of SW\u2019s technologies can facilitate the development of more flexible and domain independent systems, that could be adapted to the target audience or purposes, which would considerably advance the state of the art in NLG. The main objective of this paper is to propose a multidomain and multilingual statistical approach focused on the surface realisation stage using factored language models. Our proposed approach will be tested in the context of two different domains (fairy tales and movie reviews) and for the English and Spanish languages, in order to show its appropriateness to different non-related scenarios. The main novelty studied in this approach is the generation of related sentences (sentences with related topics) for different domains, with the aim to achieve cohesion between sentences and move forward towards the generation of coherent and cohesive texts. The approach can be flexible enough thanks to the use of an input seed feature that guides all the generation process. Within our scope, the seed feature can be seen as an abstract object that will determine how the sentence will be in terms of content. For example, this seed feature could be a phoneme, a property or a RDF triple from where the proposed approach could generate a sentence.", "venue": "WebNLG", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "nlg", "sw", "nlg", "rdf", "nlg"], "mention_counts": {"sw": 1, "nlg": 4, "rdf": 1}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"sw": 1, "rdf": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "141c1cebd29166f8156e12ad6304df4dd34c9a23", "url": "https://www.semanticscholar.org/paper/141c1cebd29166f8156e12ad6304df4dd34c9a23", "title": "Transforming and Enriching Documents for the Semantic Web", "abstract": "We suggest to employ techniques from Natural Language Processing (NLP) and Knowledge Representation (KR) to transform existing documents into documents amenable for the Semantic Web. Semantic Web documents have at least part of their semantics and pragmatics marked up explicitly in both a machine processable as well as human readable manner. XML and its related standards (XSLT, RDF, Topic Maps etc.) are the unifying platform for the tools and methodologies developed for different application scenarios.", "venue": "K\u00fcnstliche Intell.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "nlp", "sw", "nlp", "rdf"], "mention_counts": {"nlp": 2, "sw": 3, "rdf": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 3, "rdf": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "e84e378b690adea6d545b7e9bbd5949f1a6ee238", "url": "https://www.semanticscholar.org/paper/e84e378b690adea6d545b7e9bbd5949f1a6ee238", "title": "Harvesting Knowledge from Web Data and Text", "abstract": "The Web bears the potential of being the world's greatest encyclopedic source, but we are far from fully ex- ploiting this potential. Valuable scientific and cultural content is interspersed with a huge amount of noisy, low- quality, unstructured text and media. The proliferation of knowledge-sharing communities like Wikipedia and the advances in automated information extraction from Web pages give rise to an unprecedented opportunity: Can we systematically harvest facts from the Web and compile them into a comprehensive machine-readable knowledge base? Such a knowledge base would contain not only the world's entities, but also their semantic properties, and their relationships with each other. Imagine a \u201cStructured Wikipedia\u201d that has the same scale and richness as Wikipedia itself, but offers a precise and concise representation of knowledge, e.g., in the RDF format. This would enable expressive and highly precise querying, e.g., in the SPARQL language (or appropriate extensions), with additional capabilities for informative ranking of query results. The benefits from solving the above challenge would be enormous. Potential applications include 1) aformalizedmachine-readableencyclopediathatcanbequeriedwithhighprecisionlikeasemanticdatabase; 2) a key asset for disambiguating entities by supporting fast and accurate mappings of textual phrases onto named entities in the knowledge base; 3) an enabler for entity-relationship-oriented semantic search on the Web, for detecting entities and relations in Web pages and reasoning about them in expressive (probabilistic) logics; 4) a backbone for natural-language question answering that would aid in dealing with entities and their rela- tionships in answering who/where/when/ etc. questions; 5) a key asset for machine translation (e.g., English to German) and interpretation of spoken dialogs, where world knowledge provides essential context for disambiguation; 6) acatalystforacquisitionoffurtherknowledgeandlargelyautomatedmaintenanceandgrowthoftheknowl- edge base. While these application areas cover a broad, partly AI-flavored ground, the most notable one from a database perspective is semantic search: finally bringing DB methodology to Web search! For example, users (or tools on behalf of users) would be able to formulate queries about succulents that grow both in Africa and America, politicians who are also scientists or are married to singers, or flu medication that can be taken by people with high blood pressure. The search engine would return precise and concise answers: lists of entities or entity pairs (depending on the question structure), for example, Angela Merkel, Benjamin Franklin, etc., or Nicolas Sarkozy for the questions about scientists. This would be a quantum leap over today's search where an- swers are embedded if not buried in lots of result pages, and the human users would have to read them to extract entities and connect them to other entities. In this sense, the envisioned large-scale knowledge harvesting [42] from Web sources may also be viewed as machine reading [13].", "venue": "CIKM 2010", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ie", "mt", "rdf", "kg"], "mention_counts": {"kg": 3, "rdf": 1, "mt": 1, "ie": 1}, "nlp_mention_counts": {"mt": 1, "ie": 1}, "ld_mention_counts": {"kg": 3, "rdf": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "547681a50b72c14f4ec9906dd21703c9b820797e", "url": "https://www.semanticscholar.org/paper/547681a50b72c14f4ec9906dd21703c9b820797e", "title": "Internet of Things under a semantic perspective with user profiles", "abstract": "Internet of Things (IoT) is a network made up of different types of devices integrated into the Internet. For the use of IoT in cities to assist humans beings in their daily activities, it is necessary to extract and process as much information as possible and therefore, semantic web technologies are a key point. This new environment is known as the Semantic Web of Things (SWoT). The application of semantic techniques to IoT can improve interoperability, effective access to data, discovery of integration resources, reasoning and processing of knowledge extraction from data, opening up opportunities for new applications. We show an overview of the most relevant semantic technologies, focusing on SWoT and well-accepted ontologies for IoT and we stress that considering users\u2019 preferences, interests and needs are of vital importance, as a complement to the semantic information extracted. This new scenario enables the development of advanced applications and services to meet user requirements and needs, in particular for smart city applications.", "venue": "EATIS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "ie", "ke", "onto"], "mention_counts": {"sw": 2, "onto": 1, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"sw": 2, "onto": 1, "ke": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "ff2ad127a869ddde0592e524067b37b4478c39f2", "url": "https://www.semanticscholar.org/paper/ff2ad127a869ddde0592e524067b37b4478c39f2", "title": "Learning Content Patterns from Linked Data", "abstract": "Linked Data (LD) datasets (e.g., DBpedia, Freebase) are used in many knowledge extraction tasks due to the high variety of domains they cover. Unfortunately, many of these datasets do not provide a description for their properties and classes, reducing the users' freedom to understand, reuse or enrich them. This work attempts to fill part of this lack by presenting an unsupervised approach to discover syntactic patterns in the properties used in LD datasets. This approach produces a content patterns database generated from the textual data (content) of properties, which describes the syntactic structures that each property have. Our analysis enables (i) a human-understanding of syntactic patterns for properties in a LD dataset, and (ii) a structural description of properties that facilitates its reuse or extension. Results over DBpedia dataset also show that our approach enables (iii) the detection of data inconsistencies, and (iv) the validation and suggestion of new values for a property. We also outline how the resulting database can be exploited in several information extraction use cases.", "venue": "LD4IE@ISWC", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "llod", "ke", "ld", "lod"], "mention_counts": {"ld": 1, "llod": 1, "lod": 1, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ld": 1, "llod": 1, "lod": 1, "ke": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "363590adf3c02263797e4b85ec483724b85ed3eb", "url": "https://www.semanticscholar.org/paper/363590adf3c02263797e4b85ec483724b85ed3eb", "title": "OpenBiodiv: Linking Type Materials, Institutions, Locations and Taxonomic Names Extracted From\u00a0Scholarly Literature", "abstract": "OpenBiodiv is a knowledge management system containing biodiversity knowledge extracted from scholarly literature: both recently published articles in Pensoft's journals and legacy (taxon treatments extracted by Plazi) (Senderov et al. 2017). OpenBiodiv advances our understanding of the use of scientific names, collection codes and institutions within published literature by using semantic technologies, such as the conversion of XML-encoded text to RDF triples, linked via the OpenBiodiv-O onthology (Senderov et al. 2018). In this poster, we show how OpenBiodiv, currently containing more than 729 million statements, can be used to address a specific use case: finding institutions storing type material specimens of the genus Prosopistoma from various literature sources (Fig. 1). This use case is important for various groups of users: institutions, taxonomists, and curators. Answering this complex question is made possible through the application of semantic technologies within OpenBiodiv. Data extraction from taxonomic articles and treatments is enabled the utilisation of common schemas and standards into the extraction process, whereas the conversion of XML-encoded scholarly literature into Res\u043eurce Description Framework (RDF) is facilitated by OpenBiodiv-O. The code base for information extraction and data transformation is wrapped in the R packages rdf4r and ropenbio.\n The ontology allows to model the structure of research articles and treatments, as well as their corresponding metadata. Thus, OpenBiodiv-O is used to represent not only the sections of treatments but also the various entities within them, for instance geographic coordinates and institution codes within the \u201cType materials\u201d section of a treatment. Institution codes marked up within articles using the Darwin Core standard (Wieczorek et al. 2012) are mapped to GRBio's institution records. Institutions which are not present in GRBio can often be extracted from the \u201cAbbreviations\u201d section of a given article, thus utilising the power of semantic publishing workflows to discover information hidden within scholarly literature (Penev et al. 2011, Agosti and Egloff 2009). Institutional codes (abbreviations) are then mapped to the narrative section, containing the type materials information. The extraction of coordinates in the taxonomic treatment section allows to establish the location of the collection event through reverse geocoding and enables the selection of treatments linked to a specific geographic region. Modelling of the \u201cNomenclature\u201d section within OpenBiodiv-O helps to link taxonomic names, mapped to GBIF\u2019s taxonomic backbone, to their type materials, thus facilitating the discovery of materials corresponding to species from a certain higher-rank taxon.", "venue": "Biodiversity Information Science and Standards", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "rdf", "onto", "ke", "ie"], "mention_counts": {"ke": 1, "onto": 1, "rdf": 2, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 1, "rdf": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "db9f2b8dd216d048f9eec81133dd38e89b08505d", "url": "https://www.semanticscholar.org/paper/db9f2b8dd216d048f9eec81133dd38e89b08505d", "title": "Geographic information extraction using natural language processing in Wikipedia texts", "abstract": "Geographic information extracted from texts is a valuable source of location data about documents, which can be used to improve information retrieval and document indexing. Linked Data and digital gazetteers provide a large amount of data that can support the recognition of places mentioned in text. Natural Language Processing techniques, which have evolved significantly over the last years, offer tools and resources to perform named entity recognition (NER), more specifically directed towards identifying place names and relationships between places and other entities. In this work, we demonstrate the use of NER from texts, as a way to detect relationships between places that can be used to enrich an ontological gazetteer. We use a collection of Wikipedia articles as a test dataset to demonstrate the validity of this idea. Results indicate that a significant volume of place/non-place and place-place relationships can be detected using the proposed techniques.", "venue": "GEOINFO", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "nlp", "onto", "ie", "nlp", "ie"], "mention_counts": {"ld": 1, "nlp": 2, "onto": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"ld": 1, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "b37f47e58bea2ad4bfbf30ad48ba5dd398a2fdb3", "url": "https://www.semanticscholar.org/paper/b37f47e58bea2ad4bfbf30ad48ba5dd398a2fdb3", "title": "Extracci\u00f3n de Datos Enlazados desde Informaci\u00f3n No Estructurada Aplicando T\u00e9cnicas de PLN y Ontolog\u00edas", "abstract": "In this work a method for the automatic extraction of linked data from unstructured textual information in Spanish and English language is presented. The method is based on the extraction of a conceptualization from the text in form of concept map, which is transformed later in a RDF data model. In the conceptualization extraction several NLP techniques are applied and the possibility to use an ontology is offered, for increasing the capacities of information extraction from the text. Several tests using three collections of texts in Spanish and English were carried out for evaluating the proposal, with promising results in the concepts and relationships extraction and showing the benefits of the use of ontologies as external knowledge resource.", "venue": "IWSW", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "rdf", "nlp", "ld", "ie", "onto"], "mention_counts": {"onto": 2, "ld": 1, "nlp": 1, "rdf": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"ld": 1, "onto": 2, "rdf": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "51a0cb9ae6422ff09331a06f016ef6cc2ee2b9bf", "url": "https://www.semanticscholar.org/paper/51a0cb9ae6422ff09331a06f016ef6cc2ee2b9bf", "title": "A Semantic Knowledge-Based Framework for Information Extraction and Exploration", "abstract": "The availability of online documents that describe domain-specific information provides an opportunity in employing a knowledge-based approach in extracting information from web data. This research proposes a novel comprehensive semantic knowledge-based framework that helps to transform unstructured data to be easily exploited by data scientists. The resultant sematic knowledgebase is reasoned to infer new facts and classify events that might be of importance to end users. The target use case for the framework implementation was the financial domain, which represents an important class of dynamic applications that require the modelling of non-binary relations. Such complex relations are becoming increasingly common in the era of linked open data. This research in modelling and reasoning upon such relations is a further contribution of the proposed semantic framework, where non-binary relations are semantically modelled by adapting the semantic reasoning axioms to fit the intermediate resources in the N-ary relations requirements.", "venue": "Int. J. Decis. Support Syst. Technol.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["lod", "kg", "kg", "ie", "ie", "kg"], "mention_counts": {"kg": 3, "lod": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"kg": 3, "lod": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "e2e6bfc102fc896095da5b00d79099c20bcd4252", "url": "https://www.semanticscholar.org/paper/e2e6bfc102fc896095da5b00d79099c20bcd4252", "title": "Information extraction in statistics indicator tables using rule generalizations and ontology", "abstract": "The main problem of rule-based information extraction technique is that the extraction rules tend to be specifically designed for specific information or document structure; hence it cannot be directly used in another without some proper modifications. Semi-structured documents like tables present another challenge to information extraction; since there are no standards on how to design it, the structure of the tables can be varying. Statistics indicator is a source of information that use tables as a means of data presentation. Statistics indicators also have a relationship concept that must be carefully identified and extracted. Generalization rules attempt to reduce effort in the extraction rule modification process by creating extraction rules in general terms. Combined with ontology, the rules can also extract the relationship between indicators. The output of this information extraction system is a database that keeps not only the data itself but also the relationship concept between indicators.", "venue": "2016 International Conference on Information Technology Systems and Innovation (ICITSI)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "ie", "ie"], "mention_counts": {"onto": 2, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "bcfc371660dac28a50cb640a54c3379c97de4d3c", "url": "https://www.semanticscholar.org/paper/bcfc371660dac28a50cb640a54c3379c97de4d3c", "title": "Constructing a lexicon of arabic-english named entity using SMT and semantic linked data", "abstract": "Named Entity Recognition (NER) is the problem of locating and categorizing atomic entities in a given text. In this work, we used DBpedia Linked datasets and combined existing open source tools to generate from a parallel corpus a bilingual lexicon of Named Entities (NE). To annotate NE in the monolingual English corpus, we used linked data entities by mapping them to Gate Gazetteers. In order to translate entities identified by the gate tool from the English corpus, we used moses, a Statistical Machine Translation (SMT) system. The construction of the Arabic-English NE lexicon is based on the results of moses translation. Our method is fully automatic and aims to help Natural Language Processing (NLP) tasks such as, Machine Translation (MT) information retrieval, text mining and question answering. Our lexicon contains 48753 pairs of Arabic-English NE, it is freely available for use by other researchers.", "venue": "\u02dcThe \u0153international Arab journal of information technology", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "nlp", "nlp", "mt", "ld", "ld"], "mention_counts": {"ld": 2, "nlp": 2, "mt": 2}, "nlp_mention_counts": {"nlp": 2, "mt": 2}, "ld_mention_counts": {"ld": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "029129428f814fb5a3a6ce2ce2255cc7d681dccc", "url": "https://www.semanticscholar.org/paper/029129428f814fb5a3a6ce2ce2255cc7d681dccc", "title": "Exploring the Potentialities of Automatic Extraction of University Webometric Information", "abstract": "Abstract Purpose The main objective of this work is to show the potentialities of recently developed approaches for automatic knowledge extraction directly from the universities\u2019 websites. The information automatically extracted can be potentially updated with a frequency higher than once per year, and be safe from manipulations or misinterpretations. Moreover, this approach allows us flexibility in collecting indicators about the efficiency of universities\u2019 websites and their effectiveness in disseminating key contents. These new indicators can complement traditional indicators of scientific research (e.g. number of articles and number of citations) and teaching (e.g. number of students and graduates) by introducing further dimensions to allow new insights for \u201cprofiling\u201d the analyzed universities. Design/methodology/approach Webometrics relies on web mining methods and techniques to perform quantitative analyses of the web. This study implements an advanced application of the webometric approach, exploiting all the three categories of web mining: web content mining; web structure mining; web usage mining. The information to compute our indicators has been extracted from the universities\u2019 websites by using web scraping and text mining techniques. The scraped information has been stored in a NoSQL DB according to a semi-structured form to allow for retrieving information efficiently by text mining techniques. This provides increased flexibility in the design of new indicators, opening the door to new types of analyses. Some data have also been collected by means of batch interrogations of search engines (Bing, www.bing.com) or from a leading provider of Web analytics (SimilarWeb, http://www.similarweb.com). The information extracted from the Web has been combined with the University structural information taken from the European Tertiary Education Register (https://eter.joanneum.at/#/home), a database collecting information on Higher Education Institutions (HEIs) at European level. All the above was used to perform a clusterization of 79 Italian universities based on structural and digital indicators. Findings The main findings of this study concern the evaluation of the potential in digitalization of universities, in particular by presenting techniques for the automatic extraction of information from the web to build indicators of quality and impact of universities\u2019 websites. These indicators can complement traditional indicators and can be used to identify groups of universities with common features using clustering techniques working with the above indicators. Research limitations The results reported in this study refers to Italian universities only, but the approach could be extended to other university systems abroad. Practical implications The approach proposed in this study and its illustration on Italian universities show the usefulness of recently introduced automatic data extraction and web scraping approaches and its practical relevance for characterizing and profiling the activities of universities on the basis of their websites. The approach could be applied to other university systems. Originality/value This work applies for the first time to university websites some recently introduced techniques for automatic knowledge extraction based on web scraping, optical character recognition and nontrivial text mining operations (Bruni & Bianchi, 2020).", "venue": "Journal of Data and Information Science", "citationCount": 3, "fieldsOfStudy": ["Computer Science", "Engineering"], "mentions": ["ie", "ke", "ke", "ie"], "mention_counts": {"ke": 2, "ie": 2}, "nlp_mention_counts": {"ke": 2, "ie": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "b4833874af3385eec011908fbb08c4e9ff11ce35", "url": "https://www.semanticscholar.org/paper/b4833874af3385eec011908fbb08c4e9ff11ce35", "title": "Ontology-Based Protein-Protein Interactions Extraction from Literature Using the Hidden Vector State Model", "abstract": "This paper proposes a novel framework of incorporating protein-protein interactions (PPI) ontology knowledge into PPI extraction from biomedical literature in order to address the emerging challenges of deep natural language understanding. It is built upon the existing work on relation extraction using the hidden vector state (HVS) model. The HVS model belongs to the category of statistical learning methods. It can be trained directly from un-annotated data in a constrained way whilst at the same time being able to capture the underlying named entity relationships. However, it is difficult to incorporate background knowledge or non-local information into the HVS model. This paper proposes to represent the HVS model as a conditionally trained undirected graphical model in which non-local features derived from PPI ontology through inference would be easily incorporated. The seamless fusion of ontology inference with statistical learning produces a new paradigm to information extraction.", "venue": "2008 IEEE International Conference on Data Mining Workshops", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "onto", "nlu"], "mention_counts": {"onto": 4, "nlu": 1, "ie": 1}, "nlp_mention_counts": {"nlu": 1, "ie": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "eb8cb35a624609563f0f02ff17246da7174aadbc", "url": "https://www.semanticscholar.org/paper/eb8cb35a624609563f0f02ff17246da7174aadbc", "title": "Enhancing Open Data Knowledge by Extracting Tabular Data from Text Images", "abstract": "Open data published by public institutions are one of the most important resources available online. Using this public information, decision makers can improve the lives of citizens. Unfortunately, most of the times these open data are published as files, some of them not being easily processable such as scanned pdf files. In this paper we present an algorithm which enhances nowadays knowledge by extracting tabular data from scanned pdf documents in an efficient way. The proposed workflow consists of several distinct steps: first the pdf documents are converted into images, subsequently images are preprocessed using specific processing techniques. The final steps imply running an adaptive binarization of the images, recognizing the structure of the tables, applying Optical Character Recognition (OCR) on each cell of the detected tables and exporting them as csv. After testing the proposed method on several low quality scanned pdf documents, it turned out that our methodology performs alike dedicated OCR paid software and we have integrated this algorithm as a service in our platform that converts open data in Linked Open Data.", "venue": "DATA", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["llod", "ke", "lod", "ke"], "mention_counts": {"llod": 1, "lod": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"llod": 1, "lod": 1, "ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "783159fe3e47579508b979192e0589bef9bdb233", "url": "https://www.semanticscholar.org/paper/783159fe3e47579508b979192e0589bef9bdb233", "title": "Presumptive Detection of Cyberbullying on Twitter through Natural Language Processing and Machine Learning in the Spanish Language", "abstract": "Nowadays, the constant development of information and communication technologies (ICTs) has changed the inter-personal interaction, allowing to transfer real experiences to a virtualized medium such as Internet. In this sense, although the space-time barriers of traditional communication are broken and social relationships are strengthened, problems related to adverse behaviors may arise. Bullying, defined as an act that threatens a person\u2019s holistic well-being, becomes cyberbullying when it is done over Internet, causing anxiety problems, depression and even suicide attempts. For this reason, it is essential to detect this type of behaviour in time. This research deploys a Spanish cyberbullying prevention system (SPC), which relies on Natural Language Processing (NLP) methods and different machine learning techniques (Naive Bayes, Support Vector Machine and Logistic Regression), using Twitter as the basis for the extraction of knowledge bases or corpus. Several precision metrics and variable corpus sizes are used for the training. The learning results reach a maximum accuracy of 93%, verified through the application of three study cases.", "venue": "CHILEAN Conference on Electrical, Electronics Engineering, Information and Communication Technologies", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "ke", "nlp", "nlp"], "mention_counts": {"nlp": 3, "kg": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "2df446c736f508301e8f783111c29919b5cf9ab6", "url": "https://www.semanticscholar.org/paper/2df446c736f508301e8f783111c29919b5cf9ab6", "title": "Drug Disease Relation Extraction from Biomedical Literature Using NLP and Machine Learning", "abstract": "Extracting the relations between medical concepts is very valuable in the medical domain. Scientists need to extract relevant information and semantic relations between medical concepts, including protein and protein, gene and protein, drug and drug, and drug and disease. These relations can be extracted from biomedical literature available on various databases. This study examines the extraction of semantic relations that can occur between diseases and drugs. Findings will help specialists make good decisions when administering a medication to a patient and will allow them to continuously be up to date in their field. The objective of this work is to identify different features related to drugs and diseases from medical texts by applying Natural Language Processing (NLP) techniques and UMLS ontology. The Support Vector Machine classifier uses these features to extract valuable semantic relationships among text entities. The contributing factor of this research is the combination of the strength of a suggested NLP technique, which takes advantage of UMLS ontology and enables the extraction of correct and adequate features (frequency features, lexical features, morphological features, syntactic features, and semantic features), and Support Vector Machines with polynomial kernel function. These features are manipulated to pinpoint the relations between drug and disease. The proposed approach was evaluated using a standard corpus extracted from MEDLINE. The finding considerably improves the performance and outperforms similar works, especially the f-score for the most important relation \u201ccure,\u201d which is equal to 98.19%. The accuracy percentage is better than those in all the existing works for all the relations.", "venue": "Mobile Information Systems", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "f03814633009b1908c18acd68d49a85f57275772", "url": "https://www.semanticscholar.org/paper/f03814633009b1908c18acd68d49a85f57275772", "title": "Categorizing sentence structures for phrase level morphological analyzer for English to Hindi RBMT", "abstract": "Algorithms for morphological analyzers have evolved majorly around words. Since writing styles are changing due to impact of languages on each other, higher version of morphological analyzers are desired for various NLP systems such as Machine Translation, Knowledge Extraction, Information Retrieval, etc. Often word level morphological analyzers adhere to language grammars and knowledge set pertaining to GNP and dictionary. Some algorithms use phrasal dictionaries also. But, impact of languages on each other leads to changes in GNP, grammatical and phrasal usage of words. General morph algorithms cannot deal with impact of such usage of words or phrases. Therefore new generation of morph analyzers are desired to handle cross lingual impact. In this paper, methodology for English language morphological analyzer is proposed for interpretation of phrases and group of words to derive knowledge in Hindi for tourism domain. The methodology, although general, is oriented towards Machine Translation. Proposed methodology is based on creation of knowledge base for morph analyzers using formulations of FST and RTN. Using this methodology, ten categories of phrasal structures in sentences have been identified which when used in MA of RBMT would improve the functional efficiency of MT in producing correct translation.", "venue": "2015 International Conference on Cognitive Computing and Information Processing(CCIP)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "mt", "ke", "kg", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "kg": 1, "mt": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "mt": 2}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "ae19a905e599c4ad3e4fa49b7fb1bbf60b5cb02d", "url": "https://www.semanticscholar.org/paper/ae19a905e599c4ad3e4fa49b7fb1bbf60b5cb02d", "title": "Ontology-Based Interactive Information Extraction From Scientific Abstracts", "abstract": "Over recent years, there has been a growing interest in extracting information automatically or semi-automatically from the scientific literature. This paper describes a novel ontology-based interactive information extraction (OBIIE) framework and a specific OBIIE system. We describe how this system enables life scientists to make ad hoc queries similar to using a standard search engine, but where the results are obtained in a database format similar to a pre-programmed information extraction engine. We present a case study in which the system was evaluated for extracting co-factors from EMBASE and MEDLINE.", "venue": "Comparative and Functional Genomics", "citationCount": 38, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "ie", "ie"], "mention_counts": {"onto": 2, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "9ef0ff09a144f5fba93c822ae29861e5a6327f8e", "url": "https://www.semanticscholar.org/paper/9ef0ff09a144f5fba93c822ae29861e5a6327f8e", "title": "Bioie: Retargetable Information Extraction and Ontological Annotation of Biological Interactions from the Literature", "abstract": "The need for extracting general biological interactions of arbitrary types from the rapidly growing volume of the biomedical literature is drawing increased attention, while the need for this much diversity also requires both a robust treatment of complex linguistic phenomena and a method to consistently characterize the results. We present a biomedical information extraction system, BioIE, to address both of these needs by utilizing a full-fledged English grammar formalism, or a combinatory categorial grammar, and by annotating the results with the terms of Gene Ontology, which provides a common and controlled vocabulary. BioIE deals with complex linguistic phenomena such as coordination, relative structures, acronyms, appositive structures, and anaphoric expressions. In order to deal with real-world syntactic variations of ontological terms, BioIE utilizes the syntactic dependencies between words in sentences as well, based on the observation that the component words in an ontological term usually appear in a sentence with known patterns of syntactic dependencies.", "venue": "J. Bioinform. Comput. Biol.", "citationCount": 35, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "onto", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 4, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "3a45ebaceb3c963305a9f5e01f0cde855e9181d6", "url": "https://www.semanticscholar.org/paper/3a45ebaceb3c963305a9f5e01f0cde855e9181d6", "title": "A Logic-Based Approach to Semantic Information Extraction", "abstract": "Recognizing and extracting meaningful information from unstructured documents, taking into account their semantics, is an important problem in the field of information and knowledge management. In this paper we describe a novel logic-based approach to semantic information extraction, from both HTML pages and flat text documents, implemented in the H\u0131L\u03b5X system. The approach is founded on a new two-dimensional representation of documents, and heavily exploits DLP an extension of disjunctive logic programming for ontology representation and reasoning, which has been recently implemented on top of the DLV system. Ontologies, representing the semantics of information to be extracted, are encoded in DLP, while the extraction patterns are expressed using regular expressions and an ad hoc two-dimensional grammar. The execution of DLP reasoning modules, encoding the H\u0131L\u03b5X grammar expressions, yields the actual extraction of information from the input document. Unlike previous systems, which are merely syntactic, H\u0131L\u03b5X combines both semantic and syntactic knowledge for a powerful information extraction.", "venue": "ICEIS", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "ie", "ie", "ie"], "mention_counts": {"onto": 2, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "3fe192aacbcadb46a8cf8286a24280f2a4aec958", "url": "https://www.semanticscholar.org/paper/3fe192aacbcadb46a8cf8286a24280f2a4aec958", "title": "ReNoun: Fact Extraction for Nominal Attributes", "abstract": "Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users\u2019 queries. However, the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts. Open information extraction tries to address this challenge, but typically assumes that facts are expressed with verb phrases, and therefore has had difficulty extracting facts for noun-based relations. We describe ReNoun, an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail. ReNoun\u2019s approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries. ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology. ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored. We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques.", "venue": "Conference on Empirical Methods in Natural Language Processing", "citationCount": 93, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "kg", "kg", "ie", "ie", "onto"], "mention_counts": {"kg": 2, "onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "6409eeedc1baebd7e652c0c184588ad0f1b51b75", "url": "https://www.semanticscholar.org/paper/6409eeedc1baebd7e652c0c184588ad0f1b51b75", "title": "An Ontology-Based Information Extraction System for bridging the configuration gap in hybrid SDN environments", "abstract": "Hybrid Software-Defined Networks (SDNs) are growing at a remarkable speed, so network administrators need to deal with the configuration of a plethora of devices including OpenFlow elements, traditional equipment, and nodes supporting both OpenFlow and traditional features. The OpenFlow Management and Configuration Protocol (OF-CONFIG) is positioned as a solid candidate for the remote configuration of OpenFlow devices, but the fact that OF-CONFIG relies on NETCONF for its transport constrains its potential considerably. Indeed, the lack of comprehensive and standardized data models has hindered the utilization of NETCONF itself in traditional networks, and will likely confine OF-CONFIG to an elementary set of configurations until the expected data models arrive. In this paper, we present a semantic-based approach that eases and automates the configuration of network devices while complementing the capabilities of OF-CONFIG and NETCONF. Our main contributions can be summarized as follows. First, we have formalized the semantics of the switch/router configuration domain using the Web Ontology Language (OWL). Second, we have developed an Ontology-Based Information Extraction (OBIE) system from the Command-Line Interface (CLI) of network devices. Third, we have defined a learning algorithm that enables automated interpretation of CLIs' configuration capabilities in heterogeneous (multi-vendor) network scenarios. The potential of our approach is demonstrated through experiments carried out on different network elements.", "venue": "IFIP/IEEE Symposium on Integrated Network Management", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto", "onto", "onto"], "mention_counts": {"onto": 4, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "89f21f2687c45d84041a8346544905e0df50f011", "url": "https://www.semanticscholar.org/paper/89f21f2687c45d84041a8346544905e0df50f011", "title": "Overview of Ontology-Based Information Extraction", "abstract": "The research on Information Extraction (IE) aims at providing more powerful information access tools to help people overcome the problem of information overloading. In this paper, the basic IE methods are summarized based on domain ontology. The ontology-driven IE model is elaborated in detail. Moreover, the features of ontology-driven IE are compared with those of document-driven IE. Then the performance index of IE system is evaluated. Finally, the developing direction of IE is discussed.", "venue": "2010 International Conference on Internet Technology and Applications", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 4, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "5c899266090734db27d5efd125bb58e79ce0e8d8", "url": "https://www.semanticscholar.org/paper/5c899266090734db27d5efd125bb58e79ce0e8d8", "title": "A Framework for Identity Resolution and Merging for Multi-source Information Extraction", "abstract": "In the context of ontology-based information extraction, identity resolution is the process of deciding whether an instance extracted from text refers to a known entity in the target domain (e.g. the ontology). We present an ontology-based framework for identity resolution which can be customized to different application domains and extraction tasks. Rules for identify resolution, which compute similarities between target and source entities based on class information and instance properties and values, can be defined for each class in the ontology. We present a case study of the application of the framework to the problem of multi-source job vacancy extraction", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie", "onto", "onto"], "mention_counts": {"onto": 4, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "8446830f3c05b97c4d12a0751c022d1ae6a5115b", "url": "https://www.semanticscholar.org/paper/8446830f3c05b97c4d12a0751c022d1ae6a5115b", "title": "Learning to Extract Symbolic Knowledge from the World Wide Web", "abstract": "The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach is to develop a trainable information extraction system that takes two inputs: an ontology defining the classes and relations of interest, and a set of training data consisting of labeled regions of hypertext representing instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system.", "venue": "AAAI/IAAI", "citationCount": 817, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "ie", "kg", "kg", "ie"], "mention_counts": {"kg": 3, "onto": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"kg": 3, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "cd2e902cc7198c993f3719b4957e191437ddb5f1", "url": "https://www.semanticscholar.org/paper/cd2e902cc7198c993f3719b4957e191437ddb5f1", "title": "Information extraction from heterogeneous sources using domain ontologies", "abstract": "The main objective of this paper is to describe the KAARE (knowledge availability, access, retrieval and extraction) system, a generic business model for knowledge extraction of semi structured and unstructured data from Web pages. The system is ontology driven and provides a set of generic tools that will enable an effective access, retrieval and filtering of information available on the World Wide Web. The interactive model is composed of five managers namely the query manager, the ontology manager, the search manager, the information manager, and the presentation manager. Each manager is responsible for carrying out the delegated tasks from which valid inferences can be made.", "venue": "Proceedings of the IEEE Symposium on Emerging Technologies, 2005.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ke", "onto", "onto"], "mention_counts": {"ke": 1, "onto": 3, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 3}, "relevance_score": 0.48737238575333663}, {"paperId": "8dc8672e67ff6c01af48e99c5b71e5e4e37f2a80", "url": "https://www.semanticscholar.org/paper/8dc8672e67ff6c01af48e99c5b71e5e4e37f2a80", "title": "Ontology-driven Information Extraction", "abstract": "Homogeneous unstructured data (HUD) are collections of unstructured documents that share common properties, such as similar layout, common file format, or common domain of values. Building on such properties, it would be desirable to automatically process HUD to access the main information through a semantic layer -- typically an ontology -- called semantic view. Hence, we propose an ontology-based approach for extracting semantically rich information from HUD, by integrating and extending recent technologies and results from the fields of classical information extraction, table recognition, ontologies, text annotation, and logic programming. Moreover, we design and implement a system, named KnowRex, that has been successfully applied to curriculum vitae in the Europass style to offer a semantic view of them, and be able, for example, to select those which exhibit required skills.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 4, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "8062bc94efd1da75d192f5e312f28e4bd70c7ec7", "url": "https://www.semanticscholar.org/paper/8062bc94efd1da75d192f5e312f28e4bd70c7ec7", "title": "Relation Extraction via Domain-aware Transfer Learning", "abstract": "Relation extraction in knowledge base construction has been researched for the last decades due to its applicability to many problems. Most classical works, such as supervised information extraction and distant supervision, focus on how to construct the knowledge base (KB) by utilizing the large number of labels or certain related KBs. However, in many real-world scenarios, the existing methods may not perform well when a new knowledge base is required but only scarce labels or few related KBs available. In this paper, we propose a novel approach called, Relation Extraction via Domain-aware Transfer Learning (ReTrans), to extract relation mentions from a given text corpus by exploring the experience from a large amount of existing KBs which may not be closely related to the target relation. We first propose to initialize the representation of relation mentions from the massive text corpus and update those representations according to existing KBs. Based on the representations of relation mentions, we investigate the contribution of each KB to the target task and propose to select useful KBs for boosting the effectiveness of the proposed approach. Based on selected KBs, we develop a novel domain-aware transfer learning framework to transfer knowledge from source domains to the target domain, aiming to infer the true relation mentions in the unstructured text corpus. Most importantly, we give the stability and generalization bound of ReTrans. Experimental results on the real world datasets well demonstrate that the effectiveness of our approach, which outperforms all the state-of-the-art baselines.", "venue": "Knowledge Discovery and Data Mining", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ie", "kg", "kg"], "mention_counts": {"kg": 3, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 3, "ke": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "2a3608b18bb218c558d09aea740986a191c8896c", "url": "https://www.semanticscholar.org/paper/2a3608b18bb218c558d09aea740986a191c8896c", "title": "Building an Annotated Corpus in the Molecular-Biology Domain", "abstract": "Corpus annotation is now a key topic for all areas of natural language processing (NLP) and information extraction (IE) which employ supervised learning. With the explosion of results in molecular-biology there is an increased need for IE to extract knowledge to support database building and to search intelligently for information in online journal collections. To support this we are building a corpus of annotated abstracts taken from National Library of Medicine's MEDLINE database. In this paper we report on this new corpus, its ontological basis, and our experience in designing the annotation scheme. Experimental results are shown for inter-annotator agreement and comments are made on methodological considerations.", "venue": "SAIC@COLING", "citationCount": 34, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "nlp", "ie", "onto"], "mention_counts": {"nlp": 2, "ke": 1, "onto": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "a9f929fbc457b05251a4bccec133d6b086977e3a", "url": "https://www.semanticscholar.org/paper/a9f929fbc457b05251a4bccec133d6b086977e3a", "title": "Information extraction from mathematical texts by means of natural language processing techniques", "abstract": "Particularly with regard to the widespread use of the internet, the increasing amount of scientific publications creates new requirements for sophisticated information retrieval systems. The discovery of semantic annotation for describing mathematical texts themselves and the structure of the observed mathematical field is an important issue supporting such information retrieval systems. A lot of good statistical approaches for finding correlations in texts exist e.g. as used by Google. mArachna follows a different approach and uses natural language processing techniques to recover all the fine-grained information snippets within mathematical texts. The extracted information is stored in knowledge bases, creating a low-level ontology of mathematics. In this article we represent our further developments in this field and the technical implementation of the mArachna prototype.", "venue": "Emme '07", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ie", "nlp", "kg", "ie", "onto"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "914b7767b82790fddb7e6fc569cab5f670e4b51f", "url": "https://www.semanticscholar.org/paper/914b7767b82790fddb7e6fc569cab5f670e4b51f", "title": "Collective Ontology-based Information Extraction using Probabilistic Graphical Models", "abstract": "Information Extraction (IE) is a process of extracting structured data from unstructured sources. It roughly consists of subtasks named entity recognition, relation extraction and coreference resolution. Researchers have primarily focused just on one subtask or their combination in a pipeline. In this paper we introduce an intelligent collective IE system combining all three subtasks by employing conditional random fields. The usage of same learning model enables us to easily communicate between iterations on the fly and to correct errors during iterative process execution. In addition to the architecture we introduce novel semantic and collective feature functions. The system\u2019s output is labelled according to an ontology and new instances are automatically created during runtime. The ontology as a schema encodes a set of constraints, defines optional manual rules or patterns and with instances provides semantic gazetteer lists. The proposed framework is being developed during ongoing PhD research. It\u2019s main contributions are intelligent iterative interconnection of the selected subtasks, extensive use of context-specific features and parameterless system that can be guided by an ontology. Some preliminary results combining just two subtasks already show promising results over traditional approaches.", "venue": "CAiSE", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie", "onto", "onto"], "mention_counts": {"onto": 4, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "a8e351104df319c9d6293d5c7ff3cc4154efbb40", "url": "https://www.semanticscholar.org/paper/a8e351104df319c9d6293d5c7ff3cc4154efbb40", "title": "Biomedical Ontologies and Text Mining for Biomedicine and Healthcare: A Survey", "abstract": "In this survey paper, we discuss biomedical ontologies and major text mining techniques applied to biomedicine and healthcare. Biomedical ontologies such as UMLS are currently being adopted in text mining approaches because they provide domain knowledge for text mining approaches. In addition, biomedical ontologies enable us to resolve many linguistic problems when text mining approaches handle biomedical literature. As the first example of text mining, document clustering is surveyed. Because a document set is normally multiple-topic, text mining approaches use document clustering as a preprocessing step to group similar documents. Additionally, document clustering is able to inform the biomedical literature searches required for the practice of evidence-based medicine. We introduce Swanson's UnDiscovered Public Knowledge (UDPK) model to generate biomedical hypotheses from biomedical literature such as MEDLINE by discovering novel connections among logically-related biomedical concepts. Another important area of text mining is document classification. Document classification is a valuable tool for biomedical tasks that involve large amounts of text. We survey well-known classification techniques in biomedicine. As the last example of text mining in biomedicine and healthcare, we survey information extraction. Information extraction is the process of scanning text for information relevant to some interest, including extracting entities, relations, and events. We also address techniques and issues of evaluating text mining applications in biomedicine and healthcare.", "venue": "Journal of Computing Science and Engineering", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 4, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "ba6ca2c36cc5f9d4628da5e0fbab76e59349b17b", "url": "https://www.semanticscholar.org/paper/ba6ca2c36cc5f9d4628da5e0fbab76e59349b17b", "title": "Semantic Information Retrieval Using Ontology In University Domain", "abstract": "Today's conventional search engines hardly do provide the essential content relevant to the user's search query. This is because the context and semantics of the request made by the user is not analyzed to the full extent. So here the need for a semantic web search arises. SWS is upcoming in the area of web search which combines Natural Language Processing and Artificial Intelligence. The objective of the work done here is to design, develop and implement a semantic search engine- SIEU(Semantic Information Extraction in University Domain) confined to the university domain. SIEU uses ontology as a knowledge base for the information retrieval process. It is not just a mere keyword search. It is one layer above what Google or any other search engines retrieve by analyzing just the keywords. Here the query is analyzed both syntactically and semantically. The developed system retrieves the web results more relevant to the user query through keyword expansion. The results obtained here will be accurate enough to satisfy the request made by the user. The level of accuracy will be enhanced since the query is analyzed semantically. The system will be of great use to the developers and researchers who work on web. The Google results are re-ranked and optimized for providing the relevant links. For ranking an algorithm has been applied which fetches more apt results for the user query.", "venue": "ArXiv", "citationCount": 25, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "ie", "sw", "onto", "kg"], "mention_counts": {"onto": 2, "nlp": 1, "sw": 1, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 2, "kg": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "1754696d0a339a134ca4290a0000609b43c73cb8", "url": "https://www.semanticscholar.org/paper/1754696d0a339a134ca4290a0000609b43c73cb8", "title": "Using domain specific generated rules for automatic ontology population", "abstract": "This article proposes a process for automatic population of ontologies from text that applies natural language processing and information extraction techniques to acquire and classify ontology instances. The work is part of HERMES, an FCT/CAPES research project looking for techniques and tools for automating the process of ontology learning and population. Two experiments using a legal and a tourism corpora were conducted in order to evaluate it. The results indicate that our approach can extract and classify instances with high effectiveness with the additional advantage of domain independence.", "venue": "International Conference on Intelligent Systems Design and Applications", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 1, "onto": 4, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "e5c3033f6cb911cfc014ee71bbe8873ddf392a64", "url": "https://www.semanticscholar.org/paper/e5c3033f6cb911cfc014ee71bbe8873ddf392a64", "title": "The Research of the Semantic Search Engine Based on the Ontology", "abstract": "In order to solve the problems of the low query precision and the shortness in understanding user's query intention that occur in traditional search engine, a framework of semantic search engine based on ontology is brought forwards. It need to extract information after the information crawled by the spider, and an algorithm of information extraction based on ontology is proposed. By using semantic reasoning which based on ontology, it helps the search engine to understand user's query intention. A prototype of search engine is developed by using of lucene, and the search result is better than that of common search engine.", "venue": "2007 International Conference on Wireless Communications, Networking and Mobile Computing", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ie", "ie", "onto"], "mention_counts": {"onto": 4, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "652249c4f77cfebc4de9e1f565b1c5d525e5ab9c", "url": "https://www.semanticscholar.org/paper/652249c4f77cfebc4de9e1f565b1c5d525e5ab9c", "title": "A Workflow for Mutation Extraction and Structure Annotation", "abstract": "Rich information on point mutation studies is scattered across heterogeneous data sources. This paper presents an automated workflow for mining mutation annotations from full-text biomedical literature using natural language processing (NLP) techniques as well as for their subsequent reuse in protein structure annotation and visualization. This system, called mSTRAP (Mutation extraction and STRucture Annotation Pipeline), is designed for both information aggregation and subsequent brokerage of the mutation annotations. It facilitates the coordination of semantically related information from a series of text mining and sequence analysis steps into a formal OWL-DL ontology. The ontology is designed to support application-specific data management of sequence, structure, and literature annotations that are populated as instances of object and data type properties. mSTRAPviz is a subsystem that facilitates the brokerage of structure information and the associated mutations for visualization. For mutated sequences without any corresponding structure available in the Protein Data Bank (PDB), an automated pipeline for homology modeling is developed to generate the theoretical model. With mSTRAP, we demonstrate a workable system that can facilitate automation of the workflow for the retrieval, extraction, processing, and visualization of mutation annotations -- tasks which are well known to be tedious, time-consuming, complex, and error-prone. The ontology and visualization tool are available at (http://datam.i2r.a-star.edu.sg/mstrap).", "venue": "J. Bioinform. Comput. Biol.", "citationCount": 23, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "onto", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "a394c10528400134c755b46918529ae4ddb4bf84", "url": "https://www.semanticscholar.org/paper/a394c10528400134c755b46918529ae4ddb4bf84", "title": "The Medical Semantic Web: Opportunities and Issues", "abstract": "In this paper, the need for the right information for patients with chronic diseases is elaborated, followed by some scenarios of how the semantic web can be utilised to retrieve useful and precise information by stakeholders. In previous work, the author has demonstrated the automation of knowledge acquisition from the current web is becoming an important step towards this goal. The aim was twofold; first to learn what types of information exist in chronic disease-related websites, and secondly how to extract and structure such information into machine understandable form. It has been shown that these websites exhibit many common concepts which resulted in the construction of the ontology to guide in extracting information for new unseen websites. Also, the study has resulted in the development of a platform for information extraction that utilises the ontology. Continuous work has opened many issues which are disussed in this paper. While further work is still needed, the experiments to date have shown encouraging results.", "venue": "International Journal of Information Technology and Web Engineering", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ie", "ie", "onto", "onto", "sw"], "mention_counts": {"sw": 2, "onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"sw": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "4a7bcc2422a4e74dce8ae7be6e1e917b1ce7c3b4", "url": "https://www.semanticscholar.org/paper/4a7bcc2422a4e74dce8ae7be6e1e917b1ce7c3b4", "title": "Improving the formal concept analysis algorithm to construct domain ontology", "abstract": "Although with different purposes, both Domain Ontology and Formal Concept Analysis (FCA) are models used to present modeling concepts. FCA uses a lattice in mathematics to present concepts based on objects and attributes, whereas domain Ontology also presents concepts and is used in various areas, e.g., biology, information retrieval, information extraction, etc. In this article we propose a method to build Domain Ontology based on FCA in order to support the information extraction task on a specified domain.", "venue": "2012 Fourth International Conference on Knowledge and Systems Engineering", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ie", "ie", "onto"], "mention_counts": {"onto": 4, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "1b41b5436257f405646157c08aee3b9fc9f332e9", "url": "https://www.semanticscholar.org/paper/1b41b5436257f405646157c08aee3b9fc9f332e9", "title": "An Ontological Model for Map Data in Automotive Systems", "abstract": "Digital map data is an important source of information for the perception of the environment around cars for advanced driver assistance functions. These functions use map data to acquire information about the road infrastructure beyond the visual horizon of the driver. Embedded software components in today's cars typically use code-based processing of the map data to offer this support to advanced driver assistance functions, but the complexity of automotive systems continues to grow towards the realization of autonomous driving. To facilitate the representation and extraction of knowledge, we explore the feasibility of using ontologies for modelling and processing the map data in cars. We describe the challenges of adequately modelling the knowledge and present a proof of concept implementation that is used in a PC-based simulation to evaluate the knowledge extraction capabilities of this approach considering the requirements of representative advanced driver assistance functions.", "venue": "International Conference on Artificial Intelligence and Knowledge Engineering", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ke", "ke"], "mention_counts": {"ke": 2, "onto": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "51bc21e6c9ae4cae7cec621e477696f97c394a3d", "url": "https://www.semanticscholar.org/paper/51bc21e6c9ae4cae7cec621e477696f97c394a3d", "title": "Inflection-Tolerant Ontology-Based Named Entity Recognition for Real-Time Applications", "abstract": "A growing number of applications users daily interact with have to operate in (near) real-time: chatbots, digital companions, knowledge work support systems -- just to name a few. To perform the services desired by the user, these systems have to analyze user activity logs or explicit user input extremely fast. In particular, text content (e.g. in form of text snippets) needs to be processed in an information extraction task. Regarding the aforementioned temporal requirements, this has to be accomplished in just a few milliseconds, which limits the number of methods that can be applied. Practically, only very fast methods remain, which on the other hand deliver worse results than slower but more sophisticated Natural Language Processing (NLP) pipelines. In this paper, we investigate and propose methods for real-time capable Named Entity Recognition (NER). As a first improvement step we address are word variations induced by inflection, for example present in the German language. Our approach is ontology-based and makes use of several language information sources like Wiktionary. We evaluated it using the German Wikipedia (about 9.4B characters), for which the whole NER process took considerably less than an hour. Since precision and recall are higher than with comparably fast methods, we conclude that the quality gap between high speed methods and sophisticated NLP pipelines can be narrowed a bit more without losing too much runtime performance.", "venue": "LDK", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "nlp", "ie", "onto"], "mention_counts": {"nlp": 3, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "3c94fb5e76e968e5a8337f7886ec6c711d7111b7", "url": "https://www.semanticscholar.org/paper/3c94fb5e76e968e5a8337f7886ec6c711d7111b7", "title": "Research on Intelligent Searching of Agricultural Information Based on Ontology", "abstract": "The development of network technology provides people with a convenient means of searching information, but to search agricultural information from the network need rapid and accurate assistance. The information searching tool that is designed theoretically by ontology is proposed, and the establishment and application of distributed data acuisition model in the web is used to collect related topics on the extraction of knowledge, through the establishment of searching and reasoning model, the extracted knowledge is classified by the intended users, then achieve the semantic searching, and is classified automatically by neural network. Result has shown that the technique can improve greatly the accuracy and reliability of agriculture information searching.", "venue": "2012 International Conference on Computer Science and Service System", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "ke", "onto"], "mention_counts": {"ke": 2, "onto": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "436e162d5ba4c159e35784bc8592b6ba4f80e258", "url": "https://www.semanticscholar.org/paper/436e162d5ba4c159e35784bc8592b6ba4f80e258", "title": "Knowledge extraction from WebPages", "abstract": "This article presents a system to extract Knowledge from webpages by producing semantic annotations. taking into account semantic information from the domain to annotate an element in a webpage implies solving two problems : (1) identifying the syntactic structure of this element in the webpage and (2) identifying the most specific concept (in terms of subsumption) of the ontology that will be used to annotate this element. Our approach relies on a wrapper-based machine learning algorithm combined with reasoning making use of the formal structure of the ontology.", "venue": "SemAnnot@ISWC", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ke", "onto", "ke"], "mention_counts": {"ke": 2, "onto": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "cc72a45679f60fc30328cc4a245d35706b8c4b03", "url": "https://www.semanticscholar.org/paper/cc72a45679f60fc30328cc4a245d35706b8c4b03", "title": "An innovative solution for breast cancer textual big data analysis", "abstract": "The digitalization of stored information in hospitals now allows for the exploitation of medical data in text format, as electronic health records (EHRs), initially gathered for other purposes than epidemiology. Manual search and analysis operations on such data become tedious. In recent years, the use of natural language processing (NLP) tools was highlighted to automatize the extraction of information contained in EHRs, structure it and perform statistical analysis on this structured information. The main difficulties with the existing approaches is the requirement of synonyms or ontology dictionaries, that are mostly available in English only and do not include local or custom notations. In this work, a team composed of oncologists as domain experts and data scientists develop a custom NLP-based system to process and structure textual clinical reports of patients suffering from breast cancer. The tool relies on the combination of standard text mining techniques and an advanced synonym detection method. It allows for a global analysis by retrieval of indicators such as medical history, tumor characteristics, therapeutic responses, recurrences and prognosis. The versatility of the method allows to obtain easily new indicators, thus opening up the way for retrospective studies with a substantial reduction of the amount of manual work. With no need for biomedical annotators or pre-defined ontologies, this language-agnostic method reached an good extraction accuracy for several concepts of interest, according to a comparison with a manually structured file, without requiring any existing corpus with local or new notations.", "venue": "ArXiv", "citationCount": 3, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["nlp", "ie", "onto", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 3, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "0c9fe65f9130a01ec734556f89902e1b082f0316", "url": "https://www.semanticscholar.org/paper/0c9fe65f9130a01ec734556f89902e1b082f0316", "title": "Domain Specific Knowledge Graphs as a Service to the Public: Powering Social-Impact Funding in the US", "abstract": "Web and mobile technologies enable ubiquitous access to information. Yet, it is getting harder, even for subject matter experts, to quickly identify quality, trustworthy, and reliable content available online through search engines powered by advanced knowledge graphs. This paper explores the practical applications of Domain Specific Knowledge Graphs that allow for the extraction of information from trusted published and unpublished sources, to map the extracted information to an ontology defined in collaboration with sector experts, and to enable the public to go from single queries into ongoing conversations meeting their knowledge needs reliably. We focused on Social-Impact Funding, an area of need for over one million nonprofit organizations, foundations, government entities, social entrepreneurs, impact investors, and academic institutions in the US.", "venue": "Knowledge Discovery and Data Mining", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "kg", "kg", "kg"], "mention_counts": {"kg": 3, "onto": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"kg": 3, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "8c18254747bd0eeee36d2305299924a9c4e7e5c6", "url": "https://www.semanticscholar.org/paper/8c18254747bd0eeee36d2305299924a9c4e7e5c6", "title": "Cost-Effective Information Extraction from Lists in OCRed Historical Documents", "abstract": "To work well, machine-learning-based approaches to information extraction and ontology population often require a large number of manually selected and annotated examples. In this paper, we propose ListReader which provides a way to train the structure and parameters of a Hidden Markov Model (HMM) without requiring any labeled training data. The induced HMM is a wrapper---a function that hides within it the complexities of low-level processing---in ListReader's case the complexities of information extraction from OCRed historical documents. The HMM wrapper is capable of recognizing lists of records in text documents and associating subsets of identical fields across related record templates. The algorithmic training method we employ is based on a novel unsupervised active grammar-induction framework. The training produces an HMM wrapper and uses an efficient active sampling process to complete the mapping from wrapper to ontology by requesting annotations from a user for automatically-selected examples. We measure performance of the final HMM in terms of F-measure of extracted information and manual annotation cost and show that ListReader learns faster and better than a state-of-the-art baseline and an alternate version of ListReader that induces a regular-expression wrapper.", "venue": "HIP@ICDAR", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "ie", "ie", "onto"], "mention_counts": {"onto": 2, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "5bac7d84ba21969ea69649d3349086f76f6ea5c4", "url": "https://www.semanticscholar.org/paper/5bac7d84ba21969ea69649d3349086f76f6ea5c4", "title": "Towards integration of ontology and text-extracted data for event coreference reasoning", "abstract": "Recently, systems for automatic extraction of semantic information about events from large textual resources have been made available. These tools generate RDF datasets about the events described in the texts, enabling logical reasoning over the extracted information.. Ontological reasoning can be exploited to implement tasks that improve the quality of the extracted information, as, for example in event coreference (i.e., recognizing whether two textual descriptions refer to the same event). Starting from the observation that state of the art tools for event coreference do not exploit ontological information, in this paper, we propose a method to enrich event coreference detection on text-extracted event data by semantic-based rule reasoning.", "venue": "SAC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "onto", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 3, "rdf": 1, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 3, "rdf": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "469bdf68c8a4aea4a7ee3c0baff2c8609347c94e", "url": "https://www.semanticscholar.org/paper/469bdf68c8a4aea4a7ee3c0baff2c8609347c94e", "title": "Impact of Size, Location, Symptomatic-Nature and Gender on the Rupture of Saccular Intracranial Aneurysms", "abstract": "Ruptured intracranial aneurysms are associated with a high rate of mortality and disability due to the difficulty in predicting the rupture and complexity of the condition itself. Clinical narratives such as progress summaries and radiological reports, etc. contain key biomarkers, medical signs, and symptoms. By applying ontology-based information extraction on clinical narratives to extract important evidences and subsequently using machine learning can help to make decision support tools for complex decision making such as prediction of aneurysm rupture. According to best of our knowledge, there doesn't exist any work to extract clinical features from clinical narratives to predict the rupture of intracranial/Brain aneurysms (BA). While no single factor individually contributes to the risk of rupture of a BA, it is important to consider the combined impact of these aspects to understand the rupture probability of the aneurysm. In this paper, we explore the impact of size as a relative factor in saccular aneurysms with respect to location, gender and symptomatic/asymptomatic aspects of BA. Our study involves descriptive and inferential statistical data analysis on features extracted from retrospective electronic health records (EHRs) using natural language processing (NLP) and ontology-based information extraction techniques. Our analysis shows that size alone is not the sole contributor for rupture but the combination of size, location and patient's gender can influence aneurysm ruptures. Our results also show interesting insight that on same vasculature location, the average size of ruptured aneurysms for females is always smaller than that of males.", "venue": "2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlp", "ie", "ie", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 2, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "ea3a17aa0a006e4d8f48db42d36a9355c4c99a56", "url": "https://www.semanticscholar.org/paper/ea3a17aa0a006e4d8f48db42d36a9355c4c99a56", "title": "Distributed knowledge extracted by a mas using ontology alignment methods", "abstract": "Nowadays new product development involves different types of actors (technician, managers, board of directors) which must be able to share knowledge, experiences and work together efficiently. Each actor has a professional specialty and uses one or several software tools (CAO, project management tools, PLM tools ...) dedicated to her specific activities. Each of these software tools produces different information sources (databases, XML files, text files) which are distributed through the enterprise network. In this paper, we present the design of a multi-agent software architecture that allows the capitalization of distributed and heterogeneous knowledge. We then describe how the agents assist users to create domain ontologies matching with professional software tools databases and how the agent build semantic queries to extract knowledge.", "venue": "International Conference Communication and Information Systems", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ke", "onto", "ke"], "mention_counts": {"ke": 2, "onto": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "8229620844d616da551020d5fcf736b07181c44a", "url": "https://www.semanticscholar.org/paper/8229620844d616da551020d5fcf736b07181c44a", "title": "Semantic E-Ink: Knowledge-Based Assistance for Making Mental Models Explicit", "abstract": "In this paper we describe a system which assists knowledge workers in making notes of their thoughts and transferring them to the computer. Our system processes handwritten notes written down with a digital pen. These notes are processed in order to recognize and understand their meaning. To realize such systems, two novel processing stages are proposed for the first time in literature. The first stage is the inclusion of knowledge bases into the Handwriting Recognition (HWR) process, where we make use of a person's mental model. The second stage is the transition from pure HWR to understanding of the handwritten notes, i.e. the system extracts knowledge in form of ontologies. For both novel approaches we performed a set of experiments on various data. With the proposed techniques, the recognition rate of the HWR system as well as the performance of the information extraction system are significantly increased.", "venue": "Int. J. Pattern Recognit. Artif. Intell.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "ie", "kg", "kg"], "mention_counts": {"ke": 1, "onto": 1, "kg": 2, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 1, "kg": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "92b0ec1900ff0bfa1a8d54aca17e6a93732da750", "url": "https://www.semanticscholar.org/paper/92b0ec1900ff0bfa1a8d54aca17e6a93732da750", "title": "Algorithm for Population of Object Property Assertions Derived from Telecom Contact Centre Product Support Documentation", "abstract": "Relay of information from technical documentation by contact center workers to assist clients is limited by industry standard storage formats and query mechanisms. Here we present and evaluate a new methodology for processing technical documents and tagging them against a Telecom Hardware domain ontology. We deploy classical ontological NLP approaches to extract information from both text segments and tables, identifying text segments, named entities and relations between named entities described by an existing T-Box. We describe a method for scoring candidate object property assertions derived from text before populating the Telecom Hardware ontology. In our algorithm we leverage customized gazetteer lists, including lists specific to object property synonyms, and use functions of distance between co-occurring terms to score candidate A-box object property assertions. We review the performance of this approach with a use case involving Tier 1 and Tier 2 call centre agents using a visual query tool, Top Braid Live, to interrogate the instantiated Telecom Hardware ontology for information relevant to the needs of clients.", "venue": "2011 IEEE Workshops of International Conference on Advanced Information Networking and Applications", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 4, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "29cfbc7ca272be88dd219a7cc3fd1da362b4b2b1", "url": "https://www.semanticscholar.org/paper/29cfbc7ca272be88dd219a7cc3fd1da362b4b2b1", "title": "26th IEEE International Conference on Tools with Artificial Intelligence, ICTAI 2014, Limassol, Cyprus, November 10-12, 2014", "abstract": "AI Foundations Evolutionary computing, Bayesian and Neural Networks Decision/Utility Theory and Decision Optimization Search, SAT, and CSP Description Logic and Ontologies AI in Domain Specific Applications AI in Natural Language Processing and Understanding AI in Computational Biology, Medicine and Biomedical Applications AI in WWW, Communication, Social Networking, Recommender Systems, Games and E-Commerce AI in Finance and Risk Management AI in Computer Systems AI in Robotics, Computer Vision and Games AI in Software Engineering, Real-Time and Embedded Applications, and Sensor Networks AI in Cloud Computing, Data-Intensive Applications and Online/Streaming and Multimedia Systems AI in Web search and Information Retrieval AI in Computer Security, Data Privacy, and Information Assurance AI in Data Analytics and Big Data Visualization Analytics for Big Data Computational Modeling for Big Data Large-scale Recommendation and Social Media Systems Cloud/Grid/Stream Data Mining for Big Velocity Data Semantic-based Big Data Mining Machine Learning and Data Mining Data pre-processing, reduction and feature selection Learning Graphical Models and Complex Networks Active, Cost-Sensitive, Semi-Supervised, Multi-Instance, Multi-Label and Multi-Task Learning Transfer/Adaptive, Rational and Structured Learning Preference/Ranking, Ensemble, and Reinforcement Learning Knowledge Representation, Reasoning and Cognitive Modelling Knowledge Representation, Reasoning Knowledge Extraction, Management and Sharing Case-Based Reasoning and Knowledge-based Systems Cognitive Modelling and Semantic Web AI and Decision Systems Decision Guidance and Support Systems Optimization-based recommender systems Group, distributed, and collaborative decisions Crowd-sourcing and collective intelligence decision making Strategic, tactical and operational level decisions Decision making in social and mobile networks Uncertainty in AI Uncertainty and Fuzziness Representation and Reasoning Approximate/Exact Probabilistic Inference Knowledge Discovery and Data Mining for Uncertain Data AI Tools for Manufacturing Innovation AI in manufacturing performance evaluation AI in manufacturing decision and data analytics AI in modeling, simulation and optimization of manufacturing systems AI in additive manufacturing AI in manufacturing control and planning ICTAI ICTAI", "venue": "ICTAI", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlp", "kg", "ke", "onto"], "mention_counts": {"onto": 1, "nlp": 1, "sw": 1, "ke": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"sw": 1, "onto": 1, "kg": 1, "ke": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "61b6a133f26384655618f935af40803f5cdd532d", "url": "https://www.semanticscholar.org/paper/61b6a133f26384655618f935af40803f5cdd532d", "title": "Biomedical Informatics Techniques for Processing and Analyzing Web Blogs of Military Service Members", "abstract": "Introduction Web logs (\u201cblogs\u201d) have become a popular mechanism for people to express their daily thoughts, feelings, and emotions. Many of these expressions contain health care-related themes, both physical and mental, similar to information discussed during a clinical interview or medical consultation. Thus, some of the information contained in blogs might be important for health care research, especially in mental health where stress-related conditions may be difficult and expensive to diagnose and where early recognition is often key to successful treatment. In the field of biomedical informatics, techniques such as information retrieval (IR) and natural language processing (NLP) are often used to unlock information contained in free-text notes. These methods might assist the clinical research community to better understand feelings and emotions post deployment and the burden of symptoms of stress among US military service members. Methods In total, 90 military blog posts describing deployment situations and 60 control posts of Operation Enduring Freedom/Operation Iraqi Freedom (OEF/OIF) were collected. After \u201cstop\u201d word exclusion and stemming, a \u201cbag-of-words\u201d representation and term weighting was performed, and the most relevant words were manually selected out of the high-weight words. A pilot ontology was created using Collaborative Prot\u00e9g\u00e9, a knowledge management application. The word lists and the ontology were then used within General Architecture for Text Engineering (GATE), an NLP framework, to create an automated pipeline for recognition and analysis of blogs related to combat exposure. An independent expert opinion was used to create a reference standard and evaluate the results of the GATE pipeline. Results The 2 dimensions of combat exposure descriptors identified were: words dealing with physical exposure and the soldiers\u2019 emotional reactions to it. GATE pipeline was able to retrieve blog texts describing combat exposure with precision 0.9, recall 0.75, and F-score 0.82. Discussion Natural language processing and automated information retrieval might potentially provide valuable tools for retrieving and analyzing military blog posts and uncovering military service members\u2019 emotions and experiences of combat exposure.", "venue": "Journal of medical Internet research", "citationCount": 30, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 4, "onto": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "7c252ab8f84776f20d450a034271e2dd550270e7", "url": "https://www.semanticscholar.org/paper/7c252ab8f84776f20d450a034271e2dd550270e7", "title": "An Incremental Process Mining Approach to Extract Knowledge from Legacy Systems", "abstract": "Several approaches have already been proposed to extract both business processes and business rules from a legacy source code. These approaches consider static source code analysis for the extraction procedure. However, business processes have components that can not be directly extracted by static analysis (i.e., participants, responsibilities, and concurrent activities). Moreover, well-known static analysis algorithms do not support the incremental extraction of information from the legacy code. Large legacy systems can benefit from an incremental analysis strategy in order to provide iterative information extraction as well as to achieve partial results much earlier. This paper discusses a new approach for business knowledge extraction from legacy systems. The approach considers an incremental process mining technique to extract business process structures and the business rules associated to it. Discovery results can be used in various ways by business analysts and software architects, e.g. documentation of legacy systems or for re-engineering purposes.", "venue": "IEEE International Enterprise Distributed Object Computing Conference", "citationCount": 30, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ie", "ke"], "mention_counts": {"ke": 2, "ie": 2}, "nlp_mention_counts": {"ke": 2, "ie": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "d78991a98d4c41b8b111189bab744e970822d555", "url": "https://www.semanticscholar.org/paper/d78991a98d4c41b8b111189bab744e970822d555", "title": "Curation and annotation of planarian gene expression patterns with segmented reference morphologies", "abstract": "MOTIVATION\nMorphological and genetic spatial data from functional experiments based on genetic, surgical, and pharmacological perturbations are being produced at an extraordinary pace in developmental and regenerative biology. However, our ability to extract knowledge from these large datasets are hindered due to the lack of formalization methods and tools able to unambiguously describe, centralize, and interpret them. Formalizing spatial phenotypes and gene expression patterns is especially challenging in organisms with highly variable morphologies such as planarian worms, which due to their extraordinary regenerative capability can experimentally result in phenotypes with almost any combination of body regions or parts.\n\n\nRESULTS\nHere we present a computational methodology and mathematical formalism to encode and curate the morphological outcomes and gene expression patterns in planaria. Worm morphologies are encoded with mathematical graphs based on anatomical ontology terms to automatically generate reference morphologies. Gene expression patterns are registered to these standard reference morphologies, which can then be annotated automatically with anatomical ontology terms by analyzing the spatial expression patterns and their textual descriptions. This methodology enables the curation and annotation of complex experimental morphologies together with their gene expression patterns in a centralized standardized dataset, paving the way for the extraction of knowledge and reverse-engineering of the much sought-after mechanistic models in planaria and other regenerative organisms.\n\n\nAVAILABILITY\nWe implemented this methodology in a user-friendly graphical software tool, PlanGexQ, freely available together with the data in the manuscript at https://lobolab.umbc.edu/plangexq.", "venue": "Bioinform.", "citationCount": 4, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "onto", "onto"], "mention_counts": {"ke": 2, "onto": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "4732499ab45dbcb092631050d596bbf4533ee869", "url": "https://www.semanticscholar.org/paper/4732499ab45dbcb092631050d596bbf4533ee869", "title": "BIOS: An Algorithmically Generated Biomedical Knowledge Graph", "abstract": "Biomedical knowledge graphs (BioMedKGs) are essential infrastructures for biomedical and healthcare big data and artificial intelligence (AI), facilitating natural language processing, model development, and data exchange. For decades, these knowledge graphs have been developed via expert curation; however, this method can no longer keep up with today\u2019s AI development, and a transition to algorithmically generated BioMedKGs is necessary. In this work, we introduce the Biomedical Informatics Ontology System (BIOS), the first large-scale publicly available BioMedKG generated completely by machine learning algorithms. BIOS currently contains 4.1 million concepts, 7.4 million terms in two languages, and 7.3 million relation triplets. We present the methodology for developing BIOS, including the curation of raw biomedical terms, computational identification of synonymous terms and aggregation of these terms to create concept nodes, semantic type classification of the concepts, relation identification, and biomedical machine translation. We provide statistics on the current BIOS content and perform preliminary assessments of term quality, synonym grouping, and relation extraction. The results suggest that machine learning-based BioMedKG development is a viable alternative to traditional expert curation. as the standard deep learning NER model. Our approach of using automated annotations to generate training data is a type of distant supervision NER (DS-NER), and recent studies have mainly focused on sample cleaning techniques 21\u201323 . Recent novel developments in discontinuous NER 24,25 , nested NER 26\u201328 , and Seq2Seq NER 29 have not yet been employed in the BIOS project due to concerns about engineering maturity.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "onto", "kg", "kg", "mt"], "mention_counts": {"nlp": 1, "kg": 3, "onto": 1, "mt": 1}, "nlp_mention_counts": {"nlp": 1, "mt": 1}, "ld_mention_counts": {"kg": 3, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "df3364df3a9804500c638851513bb88c63dc4704", "url": "https://www.semanticscholar.org/paper/df3364df3a9804500c638851513bb88c63dc4704", "title": "Full Syntactic Parsing for Enrichment of RDF dataset", "abstract": "RDF data extracted automatically often contain long textual literals. This paper shows how to use natural language processing techniques to automatically generate specific RDF triples from the information in the literals. We look specifically at drug indications found in the DailyMed dataset. We develop knowledge schemas to capture its information as well as precise syntactic-based methods of knowledge extraction to automatically generate instances of these schemas from textual data.", "venue": "LD4IE@ISWC", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "rdf", "nlp", "rdf", "rdf"], "mention_counts": {"nlp": 1, "ke": 1, "rdf": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "rdf": 3}, "relevance_score": 0.48737238575333663}, {"paperId": "dd7d5e67ef095edbf8750368b64bfb72e4bac9d1", "url": "https://www.semanticscholar.org/paper/dd7d5e67ef095edbf8750368b64bfb72e4bac9d1", "title": "Extracting Concepts' Relations and Users' Preferences for Personalizing Query Disambiguation", "abstract": "For most Web searching applications, queries are commonly ambiguous because words usually contain several meanings. Traditional Word Sense Disambiguation (WSD) methods use statistic models or ontology-based knowledge models to find the most appropriate sense for the ambiguous word. Since queries are usually short, the contexts of the queries may not always provide enough information for disambiguating queries. Thus, more than one interpretation may be found for one ambiguous query. In this paper, we propose a cluster-based WSD method, which finds out all appropriate interpretations for the query. Because some senses of one ambiguous word usually have very close semantic relations, we group those similar senses together for explaining the ambiguous word in one interpretation. If the cluster-based WSD method generates several contradictory interpretations for one ambiguous query, we extract users\u00e2\u20ac\u2122 preferences from clickthrough data, and determine suitable concepts or concepts\u00e2\u20ac\u2122 clusters that meet users\u00e2\u20ac\u2122 interests for explaining the ambiguous query.", "venue": "Int. J. Semantic Web Inf. Syst.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "onto", "kg", "wsd", "wsd", "wsd"], "mention_counts": {"kg": 1, "wsd": 4, "onto": 1}, "nlp_mention_counts": {"wsd": 4}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "414d8017e6364d30446b39811c6a6573d9ce71ba", "url": "https://www.semanticscholar.org/paper/414d8017e6364d30446b39811c6a6573d9ce71ba", "title": "A Semantic Web-Based Approach for Building Personalized News Services", "abstract": "This article proposes Hermes, a Semantic Web-based framework for building personalized news services. It makes use of ontologies for knowledge representation, natural language processing techniques for semantic text analysis, and semantic query languages for specifying wanted information. Hermes is supported by an implementation of the framework, the Hermes News Portal, a tool which allows users to have a personalized online access to news items. The Hermes framework and its associated implementation aim at advancing the state-of-the-art of semantic approaches for personalized news services by employing Semantic Web standards, exploiting domain information, using a word sense disambiguation procedure, and being able to express temporal constraints for the desired news items.", "venue": "Int. J. E Bus. Res.", "citationCount": 93, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "nlp", "onto", "sw", "sw", "sw"], "mention_counts": {"nlp": 1, "wsd": 1, "onto": 1, "sw": 3}, "nlp_mention_counts": {"nlp": 1, "wsd": 1}, "ld_mention_counts": {"sw": 3, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "ac8df2b74ce2407852a731e5de1baa00592b1111", "url": "https://www.semanticscholar.org/paper/ac8df2b74ce2407852a731e5de1baa00592b1111", "title": "N\u00b3 - A Collection of Datasets for Named Entity Recognition and Disambiguation in the NLP Interchange Format", "abstract": "Extracting Linked Data following the Semantic Web principle from unstructured sources has become a key challenge for scientific research. Named Entity Recognition and Disambiguation are two basic operations in this extraction process. One step towards the realization of the Semantic Web vision and the development of highly accurate tools is the availability of data for validating the quality of processes for Named Entity Recognition and Disambiguation as well as for algorithm tuning. This article presents three novel, manually curated and annotated corpora (N3). All of them are based on a free license and stored in the NLP Interchange Format to leverage the Linked Data character of our datasets.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 78, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlp", "nlp", "sw", "ld", "ld"], "mention_counts": {"nlp": 2, "sw": 2, "ld": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"ld": 2, "sw": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "51c8975d88aa66781300e8ca88272ab3112445c0", "url": "https://www.semanticscholar.org/paper/51c8975d88aa66781300e8ca88272ab3112445c0", "title": "GAIA: A Fine-grained Multimedia Knowledge Extraction System", "abstract": "We present the first comprehensive, open source multimedia knowledge extraction system that takes a massive stream of unstructured, heterogeneous multimedia data from various sources and languages as input, and creates a coherent, structured knowledge base, indexing entities, relations, and events, following a rich, fine-grained ontology. Our system, GAIA, enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos. GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation. The system is publicly available at GitHub and DockerHub, with a narrated video that documents the system.", "venue": "Annual Meeting of the Association for Computational Linguistics", "citationCount": 34, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "onto", "kg"], "mention_counts": {"ke": 2, "onto": 1, "kg": 1}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1, "kg": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "d88bbe204c9f5509115d686c153062994247609e", "url": "https://www.semanticscholar.org/paper/d88bbe204c9f5509115d686c153062994247609e", "title": "Efficient Reinforcement Learning from Demonstration via Bayesian Network-Based Knowledge Extraction", "abstract": "Reinforcement learning from demonstration (RLfD) is considered to be a promising approach to improve reinforcement learning (RL) by leveraging expert demonstrations as the additional decision-making guidance. However, most existing RLfD methods only regard demonstrations as low-level knowledge instances under a certain task. Demonstrations are generally used to either provide additional rewards or pretrain the neural network-based RL policy in a supervised manner, usually resulting in poor generalization capability and weak robustness performance. Considering that human knowledge is not only interpretable but also suitable for generalization, we propose to exploit the potential of demonstrations by extracting knowledge from them via Bayesian networks and develop a novel RLfD method called Reinforcement Learning from demonstration via Bayesian Network-based Knowledge (RLBNK). The proposed RLBNK method takes advantage of node influence with the Wasserstein distance metric (NIW) algorithm to obtain abstract concepts from demonstrations and then a Bayesian network conducts knowledge learning and inference based on the abstract data set, which will yield the coarse policy with corresponding confidence. Once the coarse policy's confidence is low, another RL-based refine module will further optimize and fine-tune the policy to form a (near) optimal hybrid policy. Experimental results show that the proposed RLBNK method improves the learning efficiency of corresponding baseline RL algorithms under both normal and sparse reward settings. Furthermore, we demonstrate that our RLBNK method delivers better generalization capability and robustness than baseline methods.", "venue": "Computational Intelligence and Neuroscience", "citationCount": 4, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "kg", "kg"], "mention_counts": {"ke": 2, "kg": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "08644f5c8eabe776a6ed52ce9ad7bab4a9be25fd", "url": "https://www.semanticscholar.org/paper/08644f5c8eabe776a6ed52ce9ad7bab4a9be25fd", "title": "Studying the History of Pre-Modern Zoology with Linked Data and Vocabularies", "abstract": "In this paper we first present the international multidisciplinary research network Zoomathia, which aims the study of the transmission of zoological knowledge from Antiquity to Middle Ages through varied resources, and considers especially textual information, including compilation literature such as encyclopaedias. We then present a preliminary work in the context of Zoomathia consisting in (i) extracting pertinent knowledge from mediaeval texts using Natural Language Processing (NLP) methods, (ii) semantically enriching semi-structured zoological data and publishing it as an RDF dataset and its vocabulary, linked to other relevant Linked Data sources, and (iii) reasoning on this linked RDF data to help epistemologists, historians and philologists in their analysis of these ancient texts.", "venue": "SW4SHD@ESWC", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "ld", "nlp", "nlp", "ld", "rdf"], "mention_counts": {"ld": 2, "nlp": 2, "rdf": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"ld": 2, "rdf": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "c3eac066c4cabd8be8f3d670a897708564fc833f", "url": "https://www.semanticscholar.org/paper/c3eac066c4cabd8be8f3d670a897708564fc833f", "title": "A Knowledge-Based Framework for Power Flow and Optimal Power Flow Analyses", "abstract": "This paper proposes the application of formal methods for knowledge discovery from large quantity of data to reduce the complexity of power flow (PF) and optimal power flow (OPF) problems. In particular, a knowledge-based paradigm for PF and OPF analyses is used to extract complex features, hidden relationships, and useful hypotheses potentially describing regularities in the problem solutions from operation data-sets. This is realized by designing a knowledge-extraction process based on principal components analysis. The structural knowledge extracted by this process is then used to project the problem equations into a domain in which these equations can be solved more effectively. In this new domain, the cardinality of the PF and OPF problem is sensibly reduced and, consequently, the problem solutions can be obtained more efficiently. The effectiveness of the proposed framework is demonstrated with numerical results obtained for realistic power networks for several operating conditions.", "venue": "IEEE Transactions on Smart Grid", "citationCount": 26, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["kg", "kg", "ke", "ke"], "mention_counts": {"kg": 2, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 2, "ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "4e0090afe628c66b0b15bdcdfc2e097c20c7e564", "url": "https://www.semanticscholar.org/paper/4e0090afe628c66b0b15bdcdfc2e097c20c7e564", "title": "An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing", "abstract": "Knowledge graph (KG) schemas can vary greatly from one domain to another. Therefore supervised approaches to graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graph-text data, while adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations. This situation calls for an approach that (1) does not need large amounts of annotated data and (2) is easy to adapt to new KG schemas. To this end, we present the first approach to fully unsupervised text generation from KGs and KG generation from text. Inspired by recent work on unsupervised machine translation, we serialize a KG as a sequence of facts and frame both tasks as sequence translation. By means of a shared sequence encoder and decoder, our model learns to map both graphs and texts into a joint semantic space and thus generalizes over different surface representations with the same meaning. We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome. Our system outperforms strong baselines for both text$\\leftrightarrow$graph tasks without any manual adaptation from one dataset to the other. In additional experiments, we investigate the impact of using different unsupervised objectives.", "venue": "Conference on Empirical Methods in Natural Language Processing", "citationCount": 27, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "mt", "kg", "kg", "kg"], "mention_counts": {"ke": 1, "kg": 3, "mt": 1}, "nlp_mention_counts": {"ke": 1, "mt": 1}, "ld_mention_counts": {"ke": 1, "kg": 3}, "relevance_score": 0.48737238575333663}, {"paperId": "89cee0ed9a6195f40c5ed565f77b6cf0b9986c9c", "url": "https://www.semanticscholar.org/paper/89cee0ed9a6195f40c5ed565f77b6cf0b9986c9c", "title": "Mobility in Unsupervised Word Embeddings for Knowledge Extraction - The Scholars' Trajectories across Research Topics", "abstract": "In the knowledge discovery field of the Big Data domain the analysis of geographic positioning and mobility information plays a key role. At the same time, in the Natural Language Processing (NLP) domain pre-trained models such as BERT and word embedding algorithms such as Word2Vec enabled a rich encoding of words that allows mapping textual data into points of an arbitrary multi-dimensional space, in which the notion of proximity reflects an association among terms or topics. The main contribution of this paper is to show how analytical tools, traditionally adopted to deal with geographic data to measure the mobility of an agent in a time interval, can also be effectively applied to extract knowledge in a semantic realm, such as a semantic space of words and topics, looking for latent trajectories that can benefit the properties of neural network latent representations. As a case study, the Scopus database was queried about works of highly cited researchers in recent years. On this basis, we performed a dynamic analysis, for measuring the Radius of Gyration as an index of the mobility of researchers across scientific topics. The semantic space is built from the automatic analysis of the paper abstracts of each author. In particular, we evaluated two different methodologies to build the semantic space and we found that Word2Vec embeddings perform better than the BERT ones for this task. Finally, The scholars\u2019 trajectories show some latent properties of this model, which also represent new scientific contributions of this work. These properties include (i) the correlation between the scientific mobility and the achievement of scientific results, measured through the H-index; (ii) differences in the behavior of researchers working in different countries and subjects; and (iii) some interesting similarities between mobility patterns in this semantic realm and those typically observed in the case of human mobility.", "venue": "Future Internet", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "ke", "ke"], "mention_counts": {"nlp": 2, "ke": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "6cde39aed01f0a5347cc3d7b7bbfedc729119686", "url": "https://www.semanticscholar.org/paper/6cde39aed01f0a5347cc3d7b7bbfedc729119686", "title": "Knowledge Extraction from Fall-Time Auto-Correlated Patterns by Using Neural Rule Based Expert System", "abstract": "In this paper, the author presents an approach for automated knowledge acquisition system using Kohonen self-organizing maps and k-means clustering. The extracted knowledge in terms of rules are used as knowledge base for a rule based expert system. For the sake of illustrating and validating the system overall architecture, a fall-time auto-correlated data patterns has been used as a learning data set. The verification of the produced knowledge based was conducted by conventional expert system.", "venue": "International Journal of Information Acquisition", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "kg"], "mention_counts": {"kg": 2, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 2, "ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "72edc19f7b5d3906fc2d2908d38312495a005afd", "url": "https://www.semanticscholar.org/paper/72edc19f7b5d3906fc2d2908d38312495a005afd", "title": "Semantically Enriched Crop Type Classification and Linked Earth Observation Data to Support the Common Agricultural Policy Monitoring", "abstract": "During the last decades, massive amounts of satellite images are becoming available that can be enriched with semantic annotations for the creation of value-added earth observation products. One challenge is to extract knowledge from the raw satellite data in an automated way and to effectively manage the extracted information in a semantic way, to allow fast and accurate decisions of spatiotemporal nature in a real operational scenario. In this work, we present a framework that combines supervised learning for crop type classification on satellite imagery time-series with semantic web and linked data technologies to assist in the implementation of rule sets by the European common agricultural policy (CAP). The framework collects georeferenced data that are available online and satellite images from the Sentinel-2 mission. We analyze image time-series that cover the entire cultivation period and link each parcel with a specific crop. On top of that, we introduce a semantic layer to facilitate a knowledge-driven management of the available information, capitalizing on ontologies for knowledge representation and semantic rules, to identify possible farmers noncompliance according to the Greening 1 (crop diversification) and SMR 1 rule (protection of waters against pollution caused by nitrates) rules of the CAP. Experiments show the effectiveness of the proposed integrated approach in three different scenarios for crop type monitoring and consistency checking for noncompliance to the CAP rules: the smart sampling of on-the-spot checks; the automatic detection of CAP's Greening 1 rule; and the automatic detection of susceptible parcels according to the CAP's SMR 1 rule.", "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "citationCount": 9, "fieldsOfStudy": ["Computer Science", "Geography"], "mentions": ["onto", "ke", "ld", "ie", "sw"], "mention_counts": {"onto": 1, "ld": 1, "ke": 1, "sw": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ld": 1, "ke": 1, "sw": 1, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "56052b7a3f45519c06957f986cd7f4a799af8013", "url": "https://www.semanticscholar.org/paper/56052b7a3f45519c06957f986cd7f4a799af8013", "title": "A knowledge-based framework for power flow and optimal power flow analyses", "abstract": "This paper proposes the application of formal methods for knowledge discovery from large quantity of data to reduce the complexity of Power Flow (PF) and Optimal Power Flow (OPF) problems. In particular, a knowledge-based paradigm for PF and OPF analyses is used to extract complex features, hidden relationships, and useful hypotheses potentially describing regularities in the problem solutions from operation data-sets. This is realized by designing a knowledge-extraction process based on Principal Components Analysis (PCA). The structural knowledge extracted by this process is then used to project the problem equations into a domain in which these equations can be solved more effectively. In this new domain, the cardinality of the PF and OPF problem is sensibly reduced and, consequently, the problem solutions can be obtained more efficiently. The effectiveness of the proposed framework is demonstrated with numerical results obtained for realistic power networks for several operating conditions.", "venue": "2017 IEEE Power & Energy Society General Meeting", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "kg"], "mention_counts": {"kg": 2, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 2, "ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "7932652e807ff997e763a026657bee83d350dc39", "url": "https://www.semanticscholar.org/paper/7932652e807ff997e763a026657bee83d350dc39", "title": "A Novel Adaptation of Information Extraction Algorithm to Process Natural Text Descriptions of Pedestrian Encounters", "abstract": "Complete understanding of pedestrian behavior still poses challenges to current automated driving technologies. This study introduces a knowledge extraction framework to better understand the human perception of pedestrian crossing behavior from the ego-vehicle perspective. Using manually annotated explanatory texts, we train an information extraction algorithm to map the unstructured descriptive text into structured data. Evaluation of the algorithm confirms a good performance in most of the key feature categories. This model can identify and extract vital cues from pedestrian encounter descriptions, which can assist in analyzing human decision-making processes to estimate the pedestrians' situational crossing intent. The knowledge extracted from such structured text data can help to holistically study interconnection among the human-used factors which is crucial for modeling pedestrian behavior.", "venue": "International Conference on Intelligent Transportation Systems", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke", "ie"], "mention_counts": {"ke": 2, "ie": 2}, "nlp_mention_counts": {"ke": 2, "ie": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "d2515e280629ee199defef93cf6e1aab358e85ad", "url": "https://www.semanticscholar.org/paper/d2515e280629ee199defef93cf6e1aab358e85ad", "title": "Demographic Market Segmentation on Short Banking Movement Descriptions Applying Natural Language Processing", "abstract": "Banking movement descriptions can be a valuable type of short texts for knowledge extraction with application in finance and social studies. Conventional research on text mining has mostly been applied to medium-sized documents. Knowledge extraction from banking movement descriptions is challenging due to the lack of meaningful textual data and their ad-hoc terminology. In this work we present a clustering analysis on short banking movement descriptions based on Natural Language Processing techniques. We exploit the knowledge in an experimental data set composed of almost 20,000 real banking transactions that have been anonymised as required by European data protection regulations. At the end, we were able to extract five distinctive user clusters with similar demographics. Our approach has potential applications in Personal Finance Management.", "venue": "2021 International Symposium on Computer Science and Intelligent Controls (ISCSIC)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "nlp", "ke"], "mention_counts": {"nlp": 2, "ke": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "c2d59f5b44114fa8167d6c26f58db939c84f3ff5", "url": "https://www.semanticscholar.org/paper/c2d59f5b44114fa8167d6c26f58db939c84f3ff5", "title": "iTextMine: integrated text-mining system for large-scale knowledge extraction from the literature", "abstract": "Abstract Numerous efforts have been made for developing text-mining tools to extract information from biomedical text automatically. They have assisted in many biological tasks, such as database curation and hypothesis generation. Text-mining tools are usually different from each other in terms of programming language, system dependency and input/output format. There are few previous works that concern the integration of different text-mining tools and their results from large-scale text processing. In this paper, we describe the iTextMine system with an automated workflow to run multiple text-mining tools on large-scale text for knowledge extraction. We employ parallel processing with dockerized text-mining tools with a standardized JSON output format and implement a text alignment algorithm to solve the text discrepancy for result integration. iTextMine presently integrates four relation extraction tools, which have been used to process all the Medline abstracts and PMC open access full-length articles. The website allows users to browse the text evidence and view integrated results for knowledge discovery through a network view. We demonstrate the utilities of iTextMine with two use cases involving the gene PTEN and breast cancer and the gene SATB1.", "venue": "Database J. Biol. Databases Curation", "citationCount": 15, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ie", "tp", "ke"], "mention_counts": {"ke": 2, "tp": 1, "ie": 1}, "nlp_mention_counts": {"ke": 2, "tp": 1, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "8f5155b5257ed14db255a2c63f8aa11e0acce256", "url": "https://www.semanticscholar.org/paper/8f5155b5257ed14db255a2c63f8aa11e0acce256", "title": "Knowledge Extraction from MEDLINE by Combining Clustering with Natural Language Processing", "abstract": "The identification of relevant predicates between co-occurring concepts in scientific literature databases like MEDLINE is crucial for using these sources for knowledge extraction, in order to obtain meaningful biomedical predications as subject-predicate-object triples. We consider the manually assigned MeSH indexing terms (main headings and subheadings) in MEDLINE records as a rich resource for extracting a broad range of domain knowledge. In this paper, we explore the combination of a clustering method for co-occurring concepts based on their related MeSH subheadings in MEDLINE with the use of SemRep, a natural language processing engine, which extracts predications from free text documents. As a result, we generated sets of clusters of co-occurring concepts and identified the most significant predicates for each cluster. The association of such predicates with the co-occurrences of the resulting clusters produces the list of predications, which were checked for relevance.", "venue": "AMIA", "citationCount": 4, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "ke", "ke", "nlp"], "mention_counts": {"nlp": 2, "ke": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "da20eeeff2fdff7278faa2f7e1cbaad398d20a73", "url": "https://www.semanticscholar.org/paper/da20eeeff2fdff7278faa2f7e1cbaad398d20a73", "title": "A Collaborative Approach for FCA-Based Knowledge Extraction", "abstract": "In this paper, we propose an approach for an FCA-based knowledge extraction process relying on the collaboration between hu- mans and machines. Evaluation of the results is performed on the lattice by experts through an interactive process where they may specify their wishes for changes using a set of predefined operations. Thus, the system then may suggest several strategies to reach their goal. In such an inter- active and iterative process, the system converges towards a knowledge model close to the experts' needs. We illustrate the process on a small preliminary experiment.", "venue": "International Conference on Concept Lattices and their Applications", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "kg"], "mention_counts": {"kg": 2, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 2, "ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "dffce32ba029298e6cb0a6af3911c8338ccd1b80", "url": "https://www.semanticscholar.org/paper/dffce32ba029298e6cb0a6af3911c8338ccd1b80", "title": "Grading Knowledge, Extracting Degree Information from Texts", "abstract": "\"Text Knowledge Extraction\" maps natural language texts onto a formal representation of the facts contained in the texts. Common text knowledge extraction methods show a severe lack of methods for understanding natural language \"degree expressions\", like \"expensive hard disk drive\" and \"good monitor\", which describe gradable properties like price and quality, respectively. However, without an adequate understanding of such degree expressions it is often impossible to grasp the central meaning of a text. \n \nThis book shows concise and comprehensive concepts for extracting degree information from natural language texts. It researches this task with regard to the three levels of (i) analysing natural language degree expressions, (ii) representing them in a terminologic framework, and (iii) inferencing on them byconstrain t propagation. On each of these three levels, the author shows that former approaches to the degree understanding problem were too simplistic, since theyignored byand large the role of the background knowledge involved. Thus, he gives a constructive verification of his central hypothesis, viz. that the proper extraction of grading knowledge relies heavilyon background grading knowledge. \n \nThis construction proceeds as follows. First, the author gives an overview of the ParseTalk information extraction system. Then, from the review of relevant linguistic literature, the author derives two distinct categories of natural language degree expressions and proposes knowledge-intensive algorithms to handle their analyses in the ParseTalk system. These methods are applied to two text domains, viz. a medical diagnosis domain and a repositoryof texts from information technologymagazines. Moreover, for inferencing the author generalizes from well-known constraint propagation mechanisms. This generalization is especiallyapt for representing and reasoning with natural language degree expressions, but it is also interesting from the point of view where it originated, viz. the field of temporal reasoning. The conclusion of the book gives an integration of all three levels of understanding showing that their coupling leads to an even more advanced -- and more efficient -- performance of the proposed mechanisms.", "venue": "Lecture Notes in Computer Science", "citationCount": 20, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "nlu", "ke", "ke"], "mention_counts": {"ke": 2, "nlu": 1, "ie": 1}, "nlp_mention_counts": {"ke": 2, "nlu": 1, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "990db46923e9a9731d1cb63e0cfc399388b41429", "url": "https://www.semanticscholar.org/paper/990db46923e9a9731d1cb63e0cfc399388b41429", "title": "Mining Concepts from Wikipedia for Ontology Construction", "abstract": "An ontology is a structured knowledgebase of concepts organized by relations among them. But concepts are usually mixed with their instances in the corpora for knowledge extraction. Concepts and their corresponding instances share similar features and are difficult to distinguish. In this paper, a novel approach is proposed to comprehensively obtain concepts with the help of definition sentences and Category Labels in Wikipedia pages. N-gram statistics and other NLP knowledge are used to help extracting appropriate concepts. The proposed method identified nearly 50,000 concepts from about 700,000 Wiki pages. The precision reaching 78.5% makes it an effective approach to mine concepts from Wikipedia for ontology construction.", "venue": "2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology", "citationCount": 26, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ke", "onto", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "onto": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 3}, "relevance_score": 0.48737238575333663}, {"paperId": "012e02fa9765dec50e6b534fe13578773fd8d6c7", "url": "https://www.semanticscholar.org/paper/012e02fa9765dec50e6b534fe13578773fd8d6c7", "title": "Discovering attribute and entity synonyms for knowledge integration and semantic web search", "abstract": "We propose the <i>Context-aware Synonym Suggestion System</i> (<i>CS</i><sup>3</sup>) which learns synonyms from text by using our NLP-based text mining framework, called <i>SemScape</i>, and also from existing evidence in the current knowledge bases (KBs). Using <i>CS</i><sup>3</sup> and our previously proposed knowledge extraction system <i>IBminer</i>, we integrate some of the publicly available knowledge bases into one of the superior quality and coverage, called <i>IKBstore</i>.", "venue": "SS@ '13", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "ke", "sw", "kg"], "mention_counts": {"nlp": 1, "sw": 1, "kg": 2, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"kg": 2, "sw": 1, "ke": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "c4ca82ca3d2df330bfad849574981707e253779a", "url": "https://www.semanticscholar.org/paper/c4ca82ca3d2df330bfad849574981707e253779a", "title": "An Embedded Tree Matching Algorithm based on Metaphorical Dependency Structure", "abstract": "Knowledge extraction plays an active role in intelligent systems. This paper proposes an embedded tree matching algorithm oriented to knowledge extraction and natural language processing in intelligent systems to extract semantic structures from discourses. The algorithm is designed and tested on Chinese metaphoric expressions to find in a given sentence all the dependency semantic relations that most probably occur in metaphors. The main process is top-down searching and bottom-up amending. Experiment results show the algorithm can expectantly find out accurate dependency relations. This work proposes a preliminary work for discovering potential metaphorical structures or other semantic structures in Chinese sentences which would be useful for intelligent systems capable of natural language understanding.", "venue": "2007 International Conference on Convergence Information Technology (ICCIT 2007)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "ke", "nlp", "ke"], "mention_counts": {"nlp": 1, "ke": 2, "nlu": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 2, "nlu": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "810f3953c11e743101c5a174687c68aefab96222", "url": "https://www.semanticscholar.org/paper/810f3953c11e743101c5a174687c68aefab96222", "title": "Performance analysis of a text processing architecture for knowledge acquisition in requirements engineering", "abstract": "This study is aimed to validate a text processing architecture for knowledge acquisition and analyze the performance of several populations under controlled validation studies by focusing on empirical methods. We report our experience by analyzing three case studies whose performance issues have been addressed by using a proposed natural-language-mapping model. We discuss the advantages and the disadvantages of an automated prototype for implementing such architectural model, compared with a by-hand one. We compare the cases running out on the prototype in order to identify the suitable features the software systems should have. The final goal of the validation process is describing an ongoing research work concerned with the definition of an approach to automate processes for knowledge extraction in requirements engineering. Some of achieved findings of the performance analysis are: (i) the approach can be applied to business-based technical documents regardless of the organizational process involved; (ii) the activities related to the domain understanding can be executed in a low-costs process; and (iii) empirical methods can be used in controlled validation studies for knowledge extraction approaches.", "venue": "EATIS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["tp", "ke", "tp", "ke"], "mention_counts": {"ke": 2, "tp": 2}, "nlp_mention_counts": {"ke": 2, "tp": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "640f0881e4ffd03dbbaaa6091895c642be70f257", "url": "https://www.semanticscholar.org/paper/640f0881e4ffd03dbbaaa6091895c642be70f257", "title": "An Interactive Knowledge-based Multi-objective Evolutionary Algorithm Framework for Practical Optimization Problems", "abstract": "\u2014Experienced users often have useful knowledge and intuition in solving real-world optimization problems. User knowledge can be formulated as inter-variable relationships to assist an optimization algorithm in \ufb01nding good solutions faster. Such inter-variable interactions can also be automatically learned from high-performing solutions discovered at intermediate iterations in an optimization run \u2013 a process called innovization . These relations, if vetted by the users, can be enforced among newly generated solutions to steer the optimization algorithm towards practically promising regions in the search space. Challenges arise for large-scale problems where the number of such variable relationships may be high. This paper proposes an interactive knowledge-based evolutionary multi-objective optimization (IK- EMO) framework that extracts hidden variable-wise relationships as knowledge from evolving high-performing solutions, shares them with users to receive feedback, and applies them back to the optimization process to improve its effectiveness. The knowledge extraction process uses a systematic and elegant graph analysis method which scales well with number of variables. The working of the proposed IK-EMO is demonstrated on three large-scale real-world engineering design problems. The simplicity and elegance of the proposed knowledge extraction process and achievement of high-performing solutions quickly indicate the power of the proposed framework. The results presented should motivate further such interaction-based optimization studies for their routine use in practice. Abstract \u2014Additional supporting materials to the main paper are provided in this supplementary document. These materials presents additional results illustrating the main concepts pre- sented in the paper.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "c02a94cc935889138d7dabe4733ca0b1d4ee16ac", "url": "https://www.semanticscholar.org/paper/c02a94cc935889138d7dabe4733ca0b1d4ee16ac", "title": "Intelligent Tools for Semantic Web Navigator Design", "abstract": "This paper describes the methods and instruments for semantic web navigator design which is a novel system providing semantic drive for Internet users. The solutions proposed rest on the statistical paradigm for knowledge extraction and the semantic presentations based on the Extended Semantic Networks (ESN) mechanism. The approach presented comprises rule-based and stochastic techniques for text processing and extracted entities and relations mapping onto the structures of the knowledge base. The work is supported by the Russian Foundation for Basic Research, grant 11-06-00476-\u00ed", "venue": "RCDL", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ke", "kg", "tp", "sw"], "mention_counts": {"sw": 2, "kg": 1, "tp": 1, "ke": 1}, "nlp_mention_counts": {"ke": 1, "tp": 1}, "ld_mention_counts": {"sw": 2, "kg": 1, "ke": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "2182776abf33a500273ed28c57cb9dad6f58c7d5", "url": "https://www.semanticscholar.org/paper/2182776abf33a500273ed28c57cb9dad6f58c7d5", "title": "OntoCovid: Aplicando SABiO para a modelagem conceitual bem fundamentada no dom\u00ednio da COVID-19. (OntoCovid: Applying SABiO to conceptual modeling well grounded in the COVID-19 domain)", "abstract": "The relevance of foundational ontologies and well-founded conceptual models is acknowledged in several contexts for making the real-world semantics of data explicit, and for reducing the impact of semantic ambiguities during the integration and manipulation of different data sources. Few domains pose such demands as urgently as the analysis and knowledge extraction from COVID-19 data. Since COVID-19 was declared a pandemic in early 2020, huge efforts from around the world provided an avalanche of data for research and analysis at an unprecedented rate. However, the coexistence of semantically divergent and non-explicit definitions for data from distinct countries and time periods that are being integrated and analyzed makes the conclusions of such analysis and the extracted knowledge potentially questionable. This work contributes to the development of a preliminary version of OntoCOVID, an ontology for the domain of COVID-19 well-founded in UFO and built using the SABiO methodology. \u00a9 2021 Copyright for this paper by its authors.", "venue": "ONTOBRAS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "onto", "onto"], "mention_counts": {"ke": 2, "onto": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "5cdfdfd3dee7e4ed91e05b231d04b9eca68b1032", "url": "https://www.semanticscholar.org/paper/5cdfdfd3dee7e4ed91e05b231d04b9eca68b1032", "title": "Populating a multilingual ontology of proper names from open sources", "abstract": "Even if proper names play a central role in natural language processing (NLP) applications they are still under-represented in lexicons, annotated corpora, and other resources dedicated to text processing.\u00a0 One of the main challenges is both the prevalence and the dynamicity of proper names. At the same time, large and regularly-updated knowledge sources containing partially-structured data, such as Wikipedia or GeoNames, are publicly available and contain large numbers of proper names. We present a method for a semi-automatic enrichment of Prolexbase, an existing multilingual ontology of proper names dedicated to natural language processing, with data extracted from these open sources in three languages: Polish, English and French. Fine-grained data extraction and integration procedures allow the user to enrich previous contents of Prolexbase with new incoming data. All data are manually validated and available under an open licence.", "venue": "J. Lang. Model.", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "nlp", "tp", "onto"], "mention_counts": {"nlp": 3, "onto": 2, "tp": 1}, "nlp_mention_counts": {"nlp": 3, "tp": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "4db580c9132b90879f7a0f96e5d8bf29901a6543", "url": "https://www.semanticscholar.org/paper/4db580c9132b90879f7a0f96e5d8bf29901a6543", "title": "Lexical Bottleneck in Machine Translation and Natural Language Processing: A Case Study", "abstract": "This paper emphasises the need to develop efficient lexical knowledge acquisition techniques in order to tackle problems related to the so-called lexical bottleneck. Bearing this in mind, a semi-automatic technique for semantic clustering and word sense disambiguation is proposed. The main principles behind this method are the extraction of knowledge on a sublanguage basis and from actual corpora. Clustering and disambiguation are carried out by means of the similarity measure Dynamic Matching. Further, the development of a domain- specific semantic ontology is also reported.", "venue": "TC", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "wsd", "ke", "mt"], "mention_counts": {"onto": 1, "nlp": 1, "ke": 1, "mt": 1, "wsd": 1}, "nlp_mention_counts": {"nlp": 1, "wsd": 1, "ke": 1, "mt": 1}, "ld_mention_counts": {"ke": 1, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "c8eed37742b5a5d6d650d058f950c592fa339b5e", "url": "https://www.semanticscholar.org/paper/c8eed37742b5a5d6d650d058f950c592fa339b5e", "title": "Combining Analogy with Language Models for Knowledge Extraction", "abstract": "Learning structured knowledge from natural language text has been a long-standing challenge. Previous work has focused on specific domains, mostly extracting knowledge about named entities (e.g. countries, companies, or persons) instead of general-purpose world knowledge (e.g. information about science or everyday objects). In this paper we combine the Companion Cognitive Architecture with the BERT Language Model to extract structured knowledge from text, with the goal of automatically inferring missing commonsense facts from an existing knowledge base. Using the principles of distant supervision, the system learns functions called query cases that map statements expressed in natural language into knowledge base relations. Afterwards, the system uses such query cases to extract structured knowledge using analogical reasoning. We run experiments on 2,679 Simple English Wikipedia articles, where the system is able to learn high precision facts about a variety of subjects from a few training examples, outperforming strong baselines.", "venue": "Conference on Automated Knowledge Base Construction", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ke", "ke"], "mention_counts": {"kg": 2, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 2, "ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "8ce0afac6f99c435a5900cf2db36a747e72ab50b", "url": "https://www.semanticscholar.org/paper/8ce0afac6f99c435a5900cf2db36a747e72ab50b", "title": "Towards a Knowledge Base of Financial Relations: Overview and Project Description", "abstract": "In this paper, we present an overview of the existing knowledge bases of financial relations and a description of our ongoing research towards building one based on unstructured data. First, we aim to create a specialized ontology of financial relations which will allow to model information related to regulatory compliance monitoring tasks. Next, we plan to harness Natural Language Processing (NLP) methods to extract financial relations from text sources such as news, press releases, etc. and to organize the derived information according to our crafted ontology. In addition to an overview of the literature in these areas, we describe our vision of an automated knowledge system for the experts of financial domains.", "venue": "2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "kg", "nlp", "nlp", "kg"], "mention_counts": {"nlp": 2, "kg": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "0729184bb3d56978a7559f825a77b98861408017", "url": "https://www.semanticscholar.org/paper/0729184bb3d56978a7559f825a77b98861408017", "title": "Building a Large Ontology for Machine Translation", "abstract": "This paper describes efforts underway to construct a large-scale ontology to support semantic processing in the PAN-GLOSS knowledge-base machine translation system. Because we are aiming at broad semantic coverage, we are focusing on automatic and semi-automatic methods of knowledge acquisition. Here we report on algorithms for merging complementary online resources, in particular the LDOCE and WordNet dictionaries. We discuss empirical results, and how these results have been incorporated into the PANGLOSS ontology.", "venue": "Human Language Technology - The Baltic Perspectiv", "citationCount": 51, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "mt", "kg", "onto", "onto"], "mention_counts": {"kg": 1, "onto": 3, "mt": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"kg": 1, "onto": 3}, "relevance_score": 0.48737238575333663}, {"paperId": "c7fce11a969e3a3b23055a8c209d3ca90999660e", "url": "https://www.semanticscholar.org/paper/c7fce11a969e3a3b23055a8c209d3ca90999660e", "title": "Semantic question answering on big data", "abstract": "This article describes a high-precision semantic question answering (SQA) engine for large datasets. We employ an RDF store to index the semantic information extracted from large document collections and a natural language to SPARQL conversion module to find desired information. In order to be able to find answers to complex questions in structured/unstructured data resources, our system produces rich semantic structures from the data resources and then transforms the extracted knowledge into an RDF representation. In order to facilitate easy access to the information stored in the RDF semantic index, our system accepts a user's natural language questions, translates them into SPARQL queries and returns a precise answer back to the user. Our improvements in performance over a regular free text search index-based question answering engine prove that SQA can benefit greatly from the addition and consumption of deep semantic information.", "venue": "SBD '16", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "rdf", "rdf", "rdf", "ie"], "mention_counts": {"ke": 1, "rdf": 3, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "rdf": 3}, "relevance_score": 0.48737238575333663}, {"paperId": "a8eb910224905d8641c17657eeecf2ed6df89ce4", "url": "https://www.semanticscholar.org/paper/a8eb910224905d8641c17657eeecf2ed6df89ce4", "title": "Graph-Theoretic Analysis of Collaborative Knowledge Bases in Natural Language Processing", "abstract": "We present a graph-theoretic analysis of the topological structures underlying the collaborative knowledge bases Wikipedia and Wiktionary, which are promising uprising resources in Natural Language Processing. We contrastively compare them to a conventional linguistic knowledge base, and address the issue of how these Social Web knowledge repositories can be best exploited within the Social-Semantic Web.", "venue": "SEMWEB", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "kg", "kg", "sw", "kg"], "mention_counts": {"nlp": 2, "sw": 1, "kg": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 3, "sw": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "053abef92471b7e8d5294d72028cb21329b7c7f5", "url": "https://www.semanticscholar.org/paper/053abef92471b7e8d5294d72028cb21329b7c7f5", "title": "MTMT in QALab-3: World History Essay Question Answering System that Utilizes Textbooks and Open Knowledge Bases", "abstract": "This paper introduces the system and its evaluation for answering world history essay questions by utilizing linked open data which assists machine translation. Since the target questions are the world history subject of the entrance examination of the University of Tokyo, most answers can be found in the Japanese world history textbooks. However, an equivalent content of high-quality English translation of the Japanese world history textbooks is not available. Therefore, we try to translate those textbooks utilizing linked open data, and using source language knowledge resource of which content is not equivalent with the target knowledge resource. The evaluation result indicates that the proposed system shows the best ROUGE-1 scores of all the end-to-end submissions [13]. The result of this paper concludes followings. 1) Simple neural translation of knowledge resource does not work for domain-specific cross-lingual question answering. 2) Linked open data is effective to find correct translation for difficult terms in machine translation process. 3) Adding source language open knowledge resource would help even if its content is not equivalent to the target knowledge resources.", "venue": "NTCIR", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["lod", "lod", "kg", "mt", "lod", "mt"], "mention_counts": {"kg": 1, "lod": 3, "mt": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"kg": 1, "lod": 3}, "relevance_score": 0.48737238575333663}, {"paperId": "795c7b3bf5357af5fb480d96828f9cac089f437d", "url": "https://www.semanticscholar.org/paper/795c7b3bf5357af5fb480d96828f9cac089f437d", "title": "A Common Concept Description of Natural Language Texts as the Foundation of Semantic Computing on the Web", "abstract": "Summary form only given. In order to intelligently process vast information on the Web, we need to make computers understand the meaning of the Web contents and manipulate them taking account of their semantics. Since text is the major medium conveying information, it is thus natural and reasonable to set it as the immediate target that the computer understands the meaning, while there are other types of media such as picture, movie, etc. Toward this direction, the activity of the semantic Web is going on. It aims to establish a standardized machine-readable description format of meta-data. However, the meta-data are only fragments of the Web contents. Unlike the semantic Web, we aim to describe the concept meaning expressed in the whole natural language texts with a common format that the computer can understand. We have designed concept description language (CDL) as a vehicle for this end, and started its standardization activity in W3C. There are several levels of the meaning of the texts, ranging from shallow level to deep one. While it is still difficult to make a consensus on how to describe the deep meaning, we think that a certain consensus can be attained on a way of describing the shallow meaning of the texts, based on the research results accumulated in the field of natural language processing such as machine translation over the last several decades. In CDL, besides lexicons, 45 relations are predefined as being necessary and sufficient for denoting every semantic relation between entities (lexicons in a simple case). These CDL relations can be used universally, while the ontologies in the semantic Web are domain dependent and thus cause some problematic situations. Current issues of CDL are, among others, an easy semi-automatic way of converting natural language texts into the CDL description, and an effective mechanism of executing semantic retrieval on the CDL database. We believe that CDL contributes to build a framework of next-generation Web which provides the foundation for a variety of semantic computing. Also, CDL may contribute to overcome the language barrier among nations.", "venue": "2008 IEEE International Conference on Sensor Networks, Ubiquitous, and Trustworthy Computing (sutc 2008)", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "sw", "sw", "mt", "nlp", "sw"], "mention_counts": {"nlp": 1, "sw": 3, "onto": 1, "mt": 1}, "nlp_mention_counts": {"nlp": 1, "mt": 1}, "ld_mention_counts": {"sw": 3, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "217a1b7452993a198664343f1273e14e60c70868", "url": "https://www.semanticscholar.org/paper/217a1b7452993a198664343f1273e14e60c70868", "title": "Mitigating the Impact of out of Vocabulary Words in a Neural-Machine-Translation-based Question Answering System", "abstract": "The diffusion of ontologies ended up in the development of rich knowledge bases featuring large volumes of information concerning multiple domains. However, the largest majority of potential users are unfamiliar with the SPARQL query language, and thus can enjoy only limited access to knowledge bases provided by predefined interfaces. Systems able to translate questions posed in natural language in SPARQL queries have the potential of overcoming this problem. In this paper, we approach this problem as a Neural Machine Translation task to implement an automatic translation of natural language questions in SPARQL queries. A distinctive feature of our deep-learning-based approach is its robustness with respect to the presence of terms (referring to individuals) that do not occur in the training set. We demonstrate the potential of our approach by presenting its results on the Monument dataset, a benchmark for Question Answering on the well-known DBpedia ontology.", "venue": "DeepOntoNLP/X-SENTIMENT@ESWC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "mt", "mt", "kg", "onto"], "mention_counts": {"kg": 2, "onto": 2, "mt": 2}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "b8f74af566cffade2de9074cb2b2024cb9655d47", "url": "https://www.semanticscholar.org/paper/b8f74af566cffade2de9074cb2b2024cb9655d47", "title": "DEVELOPMENT OF AN ONTOLOGICAL MODEL OF SYNTACTIC RULES OF SIMPLE SENTENCES OF TURKISH LANGUAGE", "abstract": "This article describes the syntactic rules of sentences in Turkish language and presented its tree components as well by means of formal grammars Chomsky. At the same time, an ontological model of syntactic rules of simple sentences of the Turkish language is constructed, taking into account its semantics. The proposed ontological models use terms from the unified metalanguage UniTurk to denote syntactic categories and concepts. The results of this work can be used to solve NLP tasks, for example, in the systems of knowledge, information retrieval, question and answering systems, in machine translation, automatic summarization of Turkish texts, as well as in the reference and training systems, moreover, in building the ontological model of syntax rules of Turkic languages and is planned to use.", "venue": "BULLETIN Series of Physics & Mathematical Sciences", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "mt", "onto", "nlp"], "mention_counts": {"nlp": 1, "onto": 4, "mt": 1}, "nlp_mention_counts": {"nlp": 1, "mt": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "66083ca3223419763596967f461c047fd3789ba8", "url": "https://www.semanticscholar.org/paper/66083ca3223419763596967f461c047fd3789ba8", "title": "Enriched semantic information processing using WordNet based on semantic relation network", "abstract": "The Semantic Web is an evolving extension of the World Wide Web in which the semantics of information and services on the web is defined, making it possible for the web to understand and satisfy the requests of people and machines to use the web content. Semantic information processing is used to construct knowledge base at the human level. The most fundamental step in semantic information processing (SIP) is to construct knowledge base (KB) at the human level. WordNet has been built to be the most systematic and as close to the human level and is being applied actively in various works. Consequently, search results corresponding to different meanings may be retrieved, making identifying relevant results inconvenient and time-consuming. It has been found that a semantic gap exists between concept pairs of WordNet and those of real world. A study on the enrichment method to build a Knowledge Base was proposed here. A rule based method using WordNet's glossaries and an inference method using axioms for WordNet relations are applied for the enrichment and an enriched WordNet (E-WordNet) is built as the result. Moreover, WSD-SemNet, a new word sense disambiguation method in which E-WordNet is applied for Semantic Information Processing.", "venue": "2012 International Conference on Computing, Electronics and Electrical Technologies (ICCEET)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "wsd", "kg", "wsd", "sw"], "mention_counts": {"kg": 3, "wsd": 2, "sw": 1}, "nlp_mention_counts": {"wsd": 2}, "ld_mention_counts": {"kg": 3, "sw": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "93b4b77a6150112f79109bde5cc9478bba69cca2", "url": "https://www.semanticscholar.org/paper/93b4b77a6150112f79109bde5cc9478bba69cca2", "title": "The Polish Cyc lexicon as a bridge between Polish language and the Semantic Web", "abstract": "In this paper we discuss the problem of building the Polish lexicon for the Cyc ontology. As the ontology is very large and complex we describe semi-automatic translation of part of it, which might be useful for tasks lying on the border between the fields of Semantic Web and Natural Language Processing. We concentrate on precise identification of lexemes, which is crucial for tasks such as natural language generation in massively inflected languages like Polish, and we also concentrate on multi-word entries, since in Cyc for every 10 concepts, 9 of them is mapped to expressions containing more than one word.", "venue": "Proceedings of the International Multiconference on Computer Science and Information Technology", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlg", "nlp", "sw", "sw", "onto"], "mention_counts": {"nlp": 1, "nlg": 1, "onto": 2, "sw": 2}, "nlp_mention_counts": {"nlp": 1, "nlg": 1}, "ld_mention_counts": {"sw": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "cad38240c1385cab9cb23624874b3bfcd1ccd67f", "url": "https://www.semanticscholar.org/paper/cad38240c1385cab9cb23624874b3bfcd1ccd67f", "title": "The Polish Cyc lexicon as a bridge between Polish language and the Semantic Web", "abstract": "In this paper we discuss the problem of building the Polish lexicon for the Cyc ontology. As the ontology is very large and complex we describe semi-automatic translation of part of it, which might be useful for tasks lying on the border between the fields of Semantic Web and Natural Language Processing. We concentrate on precise identification of lexemes, which is crucial for tasks such as natural language generation in massively inflected languages like Polish, and we also concentrate on multi-word entries, since in Cyc for every 10 concepts, 9 of them is mapped to expressions containing more than one word.", "venue": "International Multiconference on Computer Science and Information Technology", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlg", "nlp", "sw", "sw", "onto"], "mention_counts": {"nlp": 1, "nlg": 1, "onto": 2, "sw": 2}, "nlp_mention_counts": {"nlp": 1, "nlg": 1}, "ld_mention_counts": {"sw": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "9e80a1434a3c563930d927a7c9539f3d5838e80a", "url": "https://www.semanticscholar.org/paper/9e80a1434a3c563930d927a7c9539f3d5838e80a", "title": "Question answering systems: the story till the Arabic linked data", "abstract": "Question answering system (QAS) is essential to satisfy the need to query information available in various formats, including structured data (ontology, databases) or unstructured data (document, web). The QAS provides a correct response to the question asked by a user in natural language. QAS uses natural language processing (NLP) techniques to interface with the system user. In this paper, we survey various QAS such as Natural Language Interfacing to DataBases (NLIDB), ontology-based question answering and question answering systems for unstructured data. We give also statistics and analysis. This can help researchers to choose an appropriate solution to their issues. In case of insufficiency, they can propose new systems for complex queries and adapt or reuse QAS techniques for specific research issues. We give also our point of view on how can QAS deal with Arabic linked data.", "venue": "Int. J. Artif. Intell. Soft Comput.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "ld", "ld"], "mention_counts": {"ld": 2, "nlp": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"ld": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "4fe52c600e8298ab11d18c65968eb1feecc4f231", "url": "https://www.semanticscholar.org/paper/4fe52c600e8298ab11d18c65968eb1feecc4f231", "title": "Mapping Natural Language Labels to Structured Web Resources", "abstract": "Mapping natural language terms to a Web knowledge base enriches information systems without additional context, with new relations and properties from the Linked Open Data. In this paper we formally define such task, which is related to word sense disambiguation, named entity recognition and ontology matching. We provide a manually annotated dataset of labels linked to DBpedia as a gold standard for evaluation, and we use it to experiment with a number of methods, including a novel algorithm that leverages the specific characteristics of the mapping task. The empirical evidence confirms that general term mapping is a hard task, that cannot be easily solved by applying existing methods designed for related problems. However, incorporating NLP ideas such as representing the context and a proper treatment of multiword expressions can significantly boost the performance, in particular the coverage of the mapping. Our findings open up the challenge to find new ways of approaching term mapping to Web resources and bridging the gap between natural language and the Semantic Web.", "venue": "NL4AI@AI*IA", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "lod", "nlp", "sw", "wsd", "onto"], "mention_counts": {"onto": 1, "nlp": 1, "lod": 1, "sw": 1, "kg": 1, "wsd": 1}, "nlp_mention_counts": {"nlp": 1, "wsd": 1}, "ld_mention_counts": {"kg": 1, "lod": 1, "onto": 1, "sw": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "35f4f0ec4de843d7f9e68f04c1baae435106b073", "url": "https://www.semanticscholar.org/paper/35f4f0ec4de843d7f9e68f04c1baae435106b073", "title": "Auto-CORPus: A Natural Language Processing Tool for Standardizing and Reusing Biomedical Literature", "abstract": "To analyse large corpora using machine learning and other Natural Language Processing (NLP) algorithms, the corpora need to be standardised. The BioC format is a community-driven simple data structure for sharing text and annotations, however there is limited access to biomedical literature in BioC format and a lack of bioinformatics tools to convert online publication HTML formats to BioC. We present Auto-CORPus (Automated pipeline for Consistent Outputs from Research Publications), a novel NLP tool for the standardisation and conversion of publication HTML and table image files to three convenient machine-interpretable outputs to support biomedical text analytics. Firstly, Auto-CORPus can be configured to convert HTML from various publication sources to BioC. To standardise the description of heterogenous publication sections, the Information Artifact Ontology is used to annotate each section within the BioC output. Secondly, Auto-CORPus transforms publication tables to a JSON format to store, exchange and annotate table data between text analytics systems. The BioC specification does not include a data structure for representing publication table data, so we present a JSON format for sharing table content and metadata. Inline tables within full-text HTML files and linked tables within separate HTML files are processed and converted to machine-interpretable table JSON format. Finally, Auto-CORPus extracts abbreviations declared within publication text and provides an abbreviations JSON output that relates an abbreviation with the full definition. This abbreviation collection supports text mining tasks such as named entity recognition by including abbreviations unique to individual publications that are not contained within standard bio-ontologies and dictionaries. Availability The Auto-CORPus package is freely available with detailed instructions from Github at https://github.com/omicsNLP/Auto-CORPus/.", "venue": "bioRxiv", "citationCount": 3, "fieldsOfStudy": ["Biology", "Computer Science", "Medicine"], "mentions": ["onto", "nlp", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "884486a4ef91a0ffccf312bdd2546975deee17e7", "url": "https://www.semanticscholar.org/paper/884486a4ef91a0ffccf312bdd2546975deee17e7", "title": "Situating natural language understanding within experience-based design", "abstract": "Building useful systems with an ability to understand \"real\" natural language input has long been an elusive goal for Artificial Intelligence. Well-known problems such as ambiguity, indirectness, and incompleteness of natural language inputs have thwarted efforts to build natural language interfaces to intelligent systems. In this article, we report on our work on a model of understanding natural language design specifications of physical devices such as simple electrical circuits. Our system, called KA, solves the classical problems of ambiguity, incompleteness and indirectness by exploiting the knowledge and problem-solving processes in the situation of designing simple physical devices. In addition, KA acquires its knowledge structures (apart from a basic ontology of devices) from the results of its problem-solving processes. Thus, KA can be bootstrapped to understand design specifications and user feedback about new devices using the knowledge structures it acquired from similar devices designed previously.In this paper, we report on three investigations in the KA project. Our first investigation demonstrates that KA can resolve ambiguities in design specifications as well as infer unarticulated requirements using the ontology, the knowledge structures, and the problem-solving processes provided by its design situation. The second investigation shows that KA's problem-solving capabilities help ascertain the relevance of indirect design specifications, and identify unspecified relations between detailed requirements. The third investigation demonstrates the extensibility of KA's theory of natural language understanding by showing that KA can interpret user feedback as well as design requirements. Our results demonstrate that situating language understanding in problem solving, such as device design in KA, provides effective solutions to unresolved problems in natural language processing.", "venue": "Int. J. Hum. Comput. Stud.", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "nlu", "onto", "onto", "nlp", "nlu"], "mention_counts": {"nlp": 1, "onto": 2, "nlu": 3}, "nlp_mention_counts": {"nlp": 1, "nlu": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "a4094dca181aaaf0262fb78feef129f85764501c", "url": "https://www.semanticscholar.org/paper/a4094dca181aaaf0262fb78feef129f85764501c", "title": "A method of natural language understanding on SPARQL ontology query", "abstract": "In this paper, the development of a SPARQL ontology query based on Natural Language Understanding is presented. To obtain ontology knowledge conveniently, Stanford Parser for user's natural language inquiring is utilized, and query triple according to the grammar is constructed. The method greatly reduces the number of combinations compared with the key word method. Combined with user dictionary, the terms of query triple can be more accurately mapped to the ontology entities. Meanwhile scores calculation is not only considered the similarity of words' form and semantic, but also considered the ambiguity of concept, for returning to the specific concept as far as possible. Experimental results are presented to demonstrate the performance and validity of method.", "venue": "2011 9th World Congress on Intelligent Control and Automation", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlu", "nlu", "onto"], "mention_counts": {"onto": 4, "nlu": 2}, "nlp_mention_counts": {"nlu": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "d56d8283d76e2930a3665d8abcfc9755adde404c", "url": "https://www.semanticscholar.org/paper/d56d8283d76e2930a3665d8abcfc9755adde404c", "title": "An Exploratory Study on Malay Processing Tool for Acquisition of Taxonomy Using FCA", "abstract": "In an effort to develop a tool for automatic ontology building from Malay text based on formal concept analysis, an effective natural language processing (NLP) tool as the pre-processing components is needed. The goal of the study is to investigate whether existing Malay NLP tools can be a viable component to be part of the ontology learning tool. This paper discusses the outcome of the study on three NLP applications. The annotated corpus approach was adopted in this study as deemed suitable for exploratory research. Using the metrics relative subject recall and precision, the results obtained show that statistical modeling technique outperformed pola grammar technique.", "venue": "2008 Eighth International Conference on Intelligent Systems Design and Applications", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 4, "onto": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "65b058af9d9ed034a9381effd1db587925a65c67", "url": "https://www.semanticscholar.org/paper/65b058af9d9ed034a9381effd1db587925a65c67", "title": "Construction of an ontology for intelligent Arabic QA systems leveraging the Conceptual Graphs representation", "abstract": "The last decade had known a great interest in Arabic Natural Language Processing (NLP) applications. This interest is due to the prominent importance of this 6th most wide-spread language in the world with more than 350 million native speakers. Currently, some basic Arabic language challenges related to the high inflection and derivation, Part-of-Speech (PoS) tagging, and diacritical ambiguity of Arabic text are practically tamed to a great extent. However, the development of high level and intelligent applications such as Question Answering (QA) systems is still obstructed by the lacks in terms of ontologies and other semantic resources. In this paper, we present the construction of a new Arabic ontology leveraging the contents of Arabic WordNet (AWN) and Arabic VerbNet (AVN). This new resource presents the advantage to combine the high lexical coverage and semantic relations between words existing in AWN together with the formal representation of syntactic and semantic frames corresponding to verbs in AVN. The Conceptual Graphs representation was adopted in the framework of a multi-layer platform dedicated to the development of intelligent and multi-agents systems. The built ontology is used to represent key concepts in questions and documents for further semantic comparison. Experiments conducted in the context of the QA task show a promising coverage with respect to the processed questions and passages. The obtained results also highlight an improvement in the performance of Arabic QA regarding the [email\u00a0protected] measure.", "venue": "Journal of Intelligent & Fuzzy Systems", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "a93333718a381eaa4428612128fdb8be98c40a22", "url": "https://www.semanticscholar.org/paper/a93333718a381eaa4428612128fdb8be98c40a22", "title": "A bottom-up, knowledge-aware approach to integrating and querying web data services", "abstract": "As a wealth of data services is becoming available on the Web, building and querying Web applications that effectively integrate their content is increasingly important. However, schema integration and ontology matching with the aim of registering data services often requires a knowledge-intensive, tedious, and error-prone manual process.\n We tackle this issue by presenting a bottom-up, semi-automatic service registration process that refers to an external knowledge base and uses simple text processing techniques in order to minimize and possibly avoid the contribution of domain experts in the annotation of data services. The first by-product of this process is a representation of the domain of data services as an entity-relationship diagram, whose entities are named after concepts of the external knowledge base matching service terminology rather than being manually created to accommodate an application-specific ontology. Second, a three-layer annotation of service semantics (service interfaces, access patterns, service marts) describing how services \u201cplay\u201d with such domain elements is also automatically constructed at registration time. When evaluated against heterogeneous existing data services and with a synthetic service dataset constructed using Google Fusion Tables, the approach yields good results in terms of data representation accuracy.\n We subsequently demonstrate that natural language processing methods can be used to decompose and match simple queries to the data services represented in three layers according to the preceding methodology with satisfactory results. We show how semantic annotations are used at query time to convert the user's request into an executable logical query. Globally, our findings show that the proposed registration method is effective in creating a uniform semantic representation of data services, suitable for building Web applications and answering search queries.", "venue": "TWEB", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "tp", "onto", "onto", "kg"], "mention_counts": {"nlp": 1, "kg": 2, "onto": 2, "tp": 1}, "nlp_mention_counts": {"nlp": 1, "tp": 1}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "aa30d3e4c242b6fc39aa8dabd8c1635f7b771e00", "url": "https://www.semanticscholar.org/paper/aa30d3e4c242b6fc39aa8dabd8c1635f7b771e00", "title": "Enhancing Knowledge Representations by Ontological Relations", "abstract": "Several medical natural language processing (NLP) systems currently base on ontologies that provide the domain knowledge. But, relationships between concepts defined in ontologies as well as relations predefined in a semantic network are widely unused in this context. The objective of this paper is to analyse potentials of using ontological relations to produce correct semantic structures for a medical document automatically and to ameliorate and enrich these structures. Knowledge representations to unstructured medical narratives are generated by means of the method SeReMeD. This approach is based on semantic transformation rules for mapping syntactic information to semantic roles. Contextual relations expressed in natural language are automatically identified and represented in the generated structures. To achieve additional semantic relationships between concepts, the UMLS Medical Semantic Network and relationships between concepts predefined in the UMLS Metathesaurus are used to support the structuring process of SeReMeD. First results show that these relations can enhance and ameliorate the automatically generated semantic structures.", "venue": "MIE", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "onto", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "f20af2efecee35213ac1045af8ce4891c4ff2fd7", "url": "https://www.semanticscholar.org/paper/f20af2efecee35213ac1045af8ce4891c4ff2fd7", "title": "A Semantic VSM-Based Recommender System", "abstract": "Online forums enable users to discuss together around various topics. One of the serious problems of these environments is high volume of discussions and thus information overload problem. Unfortunately without considering the users interests, traditional Information Retrieval (IR) techniques are not able to solve the problem. Therefore, employment of a Recommender System (RS) that could suggest favorite's topics of users according to their tastes could increases the dynamism of forum and prevent the users from duplicate posts. In addition, consideration of semantics can be useful for increasing the performance of IR based RS. Our goal is study of impact of ontology and data mining techniques on improving of content-based RS. For this purpose, at first, three type of ontologies will be constructed from the domain corpus with utilization of text mining, Natural Language Processing (NLP) and Wordnet and then they will be used as an input in two kind of RS: one, fully ontology-based and one with enriching the user profile vector with ontology in vector space model (VSM) (proposed method). Afterward the results will be compared with the simple VSM based RS. Given results show that the proposed RS presents the highest performance", "venue": "ArXiv", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "950b8c95a28499a7cc577a8e0716c0a8bf4c75a7", "url": "https://www.semanticscholar.org/paper/950b8c95a28499a7cc577a8e0716c0a8bf4c75a7", "title": "Using ontology network structure in text mining.", "abstract": "Statistical text mining treats documents as bags of words, with a focus on term frequencies within documents and across document collections. Unlike natural language processing (NLP) techniques that rely on an engineered vocabulary or a full-featured ontology, statistical approaches do not make use of domain-specific knowledge. The freedom from biases can be an advantage, but at the cost of ignoring potentially valuable knowledge. The approach proposed here investigates a hybrid strategy based on computing graph measures of term importance over an entire ontology and injecting the measures into the statistical text mining process. As a starting point, we adapt existing search engine algorithms such as PageRank and HITS to determine term importance within an ontology graph. The graph-theoretic approach is evaluated using a smoking data set from the i2b2 National Center for Biomedical Computing, cast as a simple binary classification task for categorizing smoking-related documents, demonstrating consistent improvements in accuracy.", "venue": "AMIA ... Annual Symposium proceedings. AMIA Symposium", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "onto", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "db8ba9131a190418472a04c5799c3711da181a66", "url": "https://www.semanticscholar.org/paper/db8ba9131a190418472a04c5799c3711da181a66", "title": "Robots with language: Multi-label visual recognition using NLP", "abstract": "There has been a recent interest in utilizing contextual knowledge to improve multi-label visual recognition for intelligent agents like robots. Natural Language Processing (NLP) can give us labels, the correlation of labels, and the ontological knowledge about them, so we can automate the acquisition of contextual knowledge. In this paper we show how to use tools from NLP in conjunction with Vision to improve visual recognition. There are two major approaches: First, different language databases organize words according to various semantic concepts. Using these, we can build special purpose databases that can predict the labels involved given a certain context. Here we build a knowledge base for the purpose of describing common daily activities. Second, statistical language tools can provide the correlations of different labels. We show a way to learn a language model from large corpus data that exploits these correlations and propose a general optimization scheme to integrate the language model into the system. Experiments conducted on three multi-label everyday recognition tasks support the effectiveness and efficiency of our approach, with significant gains in recognition accuracies when correlation information is used.", "venue": "2013 IEEE International Conference on Robotics and Automation", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "nlp", "kg", "nlp"], "mention_counts": {"nlp": 4, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "56eaa25b11416a294e1339ed9facffaa22fdd8a1", "url": "https://www.semanticscholar.org/paper/56eaa25b11416a294e1339ed9facffaa22fdd8a1", "title": "A Semi-Automated Approach for Multilingual Terminology Matching: Mapping the French Version of the ICD-10 to the ICD-10 CM", "abstract": "The aim of this study was to develop a simple method to map the French International Statistical Classification of Diseases and Related Health Problems, 10th revision (ICD-10) with the International Classification of Diseases, 10th Revision, Clinical Modification (ICD-10 CM). We sought to map these terminologies forward (ICD-10 to ICD-10 CM) and backward (ICD-10 CM to ICD-10) and to assess the accuracy of these two mappings. We used several terminology resources such as the Unified Medical Language System (UMLS) Metathesaurus, Bioportal, the latest version available of the French ICD-10 and several official mapping files between different versions of the ICD-10. We first retrieved existing partial mapping between the ICD-10 and the ICD-10 CM. Then, we automatically matched the ICD-10 with the ICD-10-CM, using our different reference mapping files. Finally, we used manual review and natural language processing (NLP) to match labels between the two terminologies. We assessed the accuracy of both methods with a manual review of a random dataset from the results files. The overall matching was between 94.2 and 100%. The backward mapping was better than the forward one, especially regarding exact matches. In both cases, the NLP step was highly accurate. When there are no available experts from the ontology or NLP fields for multi-lingual ontology matching, this simple approach enables secondary reuse of Electronic Health Records (EHR) and billing data for research purposes in an international context.", "venue": "MIE", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "nlp", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 4, "onto": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "d0d3772990f21d71fc7645ef66552597387a7a2b", "url": "https://www.semanticscholar.org/paper/d0d3772990f21d71fc7645ef66552597387a7a2b", "title": "Automating web Table Columns to Knowledge Base Mapping using Translation Embedding", "abstract": "Extracting semantic information from web tables is a heavily researched area in natural language processing (NLP) and knowledge base augmentation. Identifying the kind of information each table column contains makes it easy for computers to read, understand and use the information in a table. Mapping the columns to concepts from an ontology solves this issue. This paper introduces Col2Pedia, a novel supervised learning based approach to map web table columns to concepts from a knowledge base, automatically. Col2Pedia leverages the behaviour of columns of a table when taken as a pair to infer mappings for these columns with high precision and recall.", "venue": "International Computer Science Conference", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "kg", "nlp", "kg", "onto"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 3, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "e395e30945b87b758aad388a27b56843f264e353", "url": "https://www.semanticscholar.org/paper/e395e30945b87b758aad388a27b56843f264e353", "title": "\u0410\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043f\u043e\u0440\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u0444\u0440\u0430\u0437 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u043f\u043e OWL-\u043c\u043e\u0434\u0435\u043b\u0438, \u0441\u0435\u043c\u0430\u043d\u0442\u0438\u043a\u0435 \u0438 \u043f\u0440\u0430\u0433\u043c\u0430\u0442\u0438\u043a\u0435 (Automatic Natural Language Generation Using an OWL Model, Semantics and Pragmatics)", "abstract": "This paper is focused on the problem of agreement between the ontology of a user and of a computer program. The natural language generation algorithms which use an OWL model, semantics and pragramtics have been studied. The present solution can be used to increase the interaction efficency between users and virtual assistants on websites specialized on selling goods and services.", "venue": "AIST", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlg", "onto", "nlg"], "mention_counts": {"nlg": 2, "onto": 4}, "nlp_mention_counts": {"nlg": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "b0cd891aaa09d1fb0e26fe3f54deccafe76bd5b3", "url": "https://www.semanticscholar.org/paper/b0cd891aaa09d1fb0e26fe3f54deccafe76bd5b3", "title": "Ontology-based Context Aware Recommender System Application for Tourism", "abstract": "In this work a novel recommender system (RS) for Tourism is presented. The RS is context aware as is now the rule in the state-of-the-art for recommender systems and works on top of a tourism ontology which is used to group the different items being offered. The presented RS mixes different types of recommenders creating an ensemble which changes on the basis of the RS\u2019s maturity. Starting from simple content-based recommendations and iteratively adding popularity, demographic and collaborative filtering methods as rating density and user cardinality increases. The result is a RS that mutates during its lifetime and uses a tourism ontology and natural language processing (NLP) to correctly bin the items to specific item categories and meta categories in the ontology. This item classification facilitates the association between user preferences and items, as well as allowing to better classify and group the items being offered, which in turn is particularly useful for context-aware filtering.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "1049f45342fc970bdb34dc3213eaebb41544e9bc", "url": "https://www.semanticscholar.org/paper/1049f45342fc970bdb34dc3213eaebb41544e9bc", "title": "Graph-based Interactive Data Federation System for Heterogeneous Data Retrieval and Analytics", "abstract": "Given the increasing number of heterogeneous data stored in relational databases, file systems or cloud environment, it needs to be easily accessed and semantically connected for further data analytic. The potential of data federation is largely untapped, this paper presents an interactive data federation system (https://vimeo.com/319473546) by applying large-scale techniques including heterogeneous data federation, natural language processing, association rules and semantic web to perform data retrieval and analytics on social network data. The system first creates a Virtual Database (VDB) to virtually integrate data from multiple data sources. Next, a RDF generator is built to unify data, together with SPARQL queries, to support semantic data search over the processed text data by natural language processing (NLP). Association rule analysis is used to discover the patterns and recognize the most important co-occurrences of variables from multiple data sources. The system demonstrates how it facilitates interactive data analytic towards different application scenarios (e.g., sentiment analysis, privacy-concern analysis, community detection).", "venue": "The Web Conference", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "rdf", "nlp", "sw", "tp"], "mention_counts": {"nlp": 3, "sw": 1, "tp": 1, "rdf": 1}, "nlp_mention_counts": {"nlp": 3, "tp": 1}, "ld_mention_counts": {"sw": 1, "rdf": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "c724ddb0ce087b679d411b55a2d8867d4d3b563b", "url": "https://www.semanticscholar.org/paper/c724ddb0ce087b679d411b55a2d8867d4d3b563b", "title": "Automatic Evaluation and Composition of NLP Pipelines with Web Services", "abstract": "We describe the innovative use of describing an existing natural language \u201cpipeline\u201d using the Semantic Web, and focus on how the performance and results of the components may be described. Earlier work has shown how NLP Web Services can be automatically composed via Semantic Web Service composition, and once the results of NLP components can be stored directly, they can also be used to direct the composition, leading to advances in the sharing and evaluation of NLP resources.", "venue": "LREC", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "sw", "nlp", "nlp", "sw"], "mention_counts": {"nlp": 4, "sw": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"sw": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "f16cc702b2467ad6827336decfd0825cc7816862", "url": "https://www.semanticscholar.org/paper/f16cc702b2467ad6827336decfd0825cc7816862", "title": "Application Prospect of Knowledge Graph Technology in Knowledge Management of Oil and Gas Exploration and Development", "abstract": "A large number of research reports have been produced during the exploration and development of oil and gas. Traditional relational database-based information management systems and keyword-based information retrieval systems cannot effectively analyze, organize, and utilize the knowledge in these research reports. knowledge graph use machine learning, natural language processing, semantic search and other technologies to extract knowledge from multi-source heterogeneous knowledge carriers and build a graphical knowledge base. Knowledge management systems designed for oil and gas exploration and development, semantic search and knowledge Push, smart question and answer, analogy and intelligent prediction.", "venue": "2019 2nd International Conference on Artificial Intelligence and Big Data (ICAIBD)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "kg", "nlp", "kg"], "mention_counts": {"nlp": 1, "kg": 3, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"kg": 3, "ke": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "6394632141da7b04eab444186c96aa95dbc337b0", "url": "https://www.semanticscholar.org/paper/6394632141da7b04eab444186c96aa95dbc337b0", "title": "Natural language query handling using extended knowledge provider system", "abstract": "Extraction of knowledge data from knowledge database using natural language query is a difficult task. Different types of natural language processing (NLP) techniques have been developed to handle this knowledge data extraction task. This paper proposes an automated query-response model termed Extended Automated Knowledge Provider System (EAKPS) that can manage various types of natural language queries from user. The EAKPS uses combination based technique and it can handle assertive, interrogative, imperative, compound and complex type query sentences. The algorithm of EAKPS generates structure query language (SQL) for each natural language query to extract knowledge data from the knowledge database resident within the EAKPS. Extraction of noun or noun phrases is another issue in natural language query processing. Most of the times, determiner, preposition and conjunction are prefixed to a noun or noun phrase and it is difficult to identify the noun/noun phrase with prefix during query processing. The proposed system is able to identify these prefixes and extract exact noun or noun phrases from natural language queries without any manual intervention.", "venue": "Int. J. Knowl. Based Intell. Eng. Syst.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "ke", "nlp"], "mention_counts": {"nlp": 2, "ke": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 2}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.48737238575333663}, {"paperId": "3788e4948c0f6c1b0ed017440e8429ebc9620d48", "url": "https://www.semanticscholar.org/paper/3788e4948c0f6c1b0ed017440e8429ebc9620d48", "title": "An Improved Word Similarity Measure For Ontological Context", "abstract": "For various Natural Language Processing (NLP) use cases, it is desirable to know the significance of the text. Various methods based on path, corpus and knowledge measures are used to find out the similarity among words. Different word similarity approaches are analysed in this paper. The widely accepted approach is Wu and Palmer similarity measure. But it has a major disadvantage. It produces less similarity score for those pair of words which are in same hierarchy in the ontology, whereas according to the contextual meaning, these words are more connected and so should have more similarity score. This paper lists the shortcomings of Wu and Palmer formula and presents a remodelled formula to improve the scores of such pair of words. The remodelled formula uses logarithm bringing the depth of the words in the ontology under a uniform scale. wup- Wu and Palmer, LCS- Least Common Subsumer(the most clearly identified concept or word which is an ancestor of two words in the ontology), Simremodelled - Remodelled Wu and Palmer formula", "venue": "2019 International Conference on Advances in Computing, Communication and Control (ICAC3)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "f84ac621bba75e09c9b5ef35156cb46357200e81", "url": "https://www.semanticscholar.org/paper/f84ac621bba75e09c9b5ef35156cb46357200e81", "title": "Complete and Consistent Annotation of WordNet using the Top Concept Ontology", "abstract": "This paper presents the complete and consistent ontological annotation of the nominal part of WordNet. The annotation has been carried out using the semantic features defined in the EuroWordNet Top Concept Ontology and made available to the NLP community. Up to now only an initial core set of 1,024 synsets, the so-called Base Concepts, was ontologized in such a way. The work has been achieved by following a methodology based on an iterative and incremental expansion of the initial labeling through the hierarchy while setting inheritance blockage points. Since this labeling has been set on the EuroWordNet\u0092s Interlingual Index (ILI), it can be also used to populate any other wordnet linked to it through a simple porting process. This feature-annotated WordNet is intended to be useful for a large number of semantic NLP tasks and for testing for the first time componential analysis on real environments. Moreover, the quantitative analysis of the work shows that more than 40% of the nominal part of WordNet is involved in structure errors or inadequacies.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 51, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "cfd6fadb25a0db9a05d776a6fb24264f322b1b98", "url": "https://www.semanticscholar.org/paper/cfd6fadb25a0db9a05d776a6fb24264f322b1b98", "title": "Using WordNet for Building WordNets", "abstract": "This paper summar ises a set of methodologies and techniques for the fast construction of multilingual WordNets. The English WordNet is used in this approach as a backbone for Catalan and Spanish WordNets and as a lexical knowledge resource for several subtasks. 1 Motivation and Introduction One of the main issues in last years as regards NLP activit ies is the i nc r ea s ing ly fast development of generic language resources. A lot of such resources, including both software and l ingware items (lexicons, lexical databases, grammars, corpora marked in several ways) have been made available for research a n d industrial applications. Special interest presents, for knowledge-based NLP tasks, the availability of wide coverage ontologies. Most known ontologies (as GUM, CYC, ONTOS, MICROKOSMOS, EDR or WORDNET, see [Gomez 98] for an extensive survey) defe r in great extent on several characteristics (e.g. broad coverage vs. domain specific, lexicaUy oriented vs. conceptually oriented, granularity, kind of information placed in nodes, kind of relations, way of building, etc.). It is clear, however, that for a wide range of applications, WordNet (WN) [Miller 90] as become a de-facto standard. The success of WordNet has determined the emergence of several projects that aim the construction of WordNets for other languages than English (e.g., [Hamp & Feldweg 97], [Artale et al. 97]) or to develop multilingual WordNets (the mos t important project in this line is EuroWordNet (EWN)I). lhttp://www.let.uva.rd/~ewn/The aim of EWN vroject is to braid a multi.lingual database with WordN'ets for several european languages (in the first phase, Dutch, Italian and Spanish in addltion to English). The construction of a WN for a language Lg (LgWN) can be tackled in d i f fe ren t ways according to the lexical sources available. Of course the manual construction can be undertaken quite straightforwardly and leads to the best results in terms of accuracy, but has the important drawback of its cost. So, other approaches have been carried out taking profi t of available resources in fully automatic or semi-automatic ways. Which are these lexical resources? Basically four kinds of resources have been used: 1) English WN (EnWN0, as an initial skeleton for trying to attach the words of Lg to it, 2) a l ready existing taxonomies of Lg (both at word and at sense level), 3) bilingual (English and Lg) and 4) monolingual (Lg) dictionaries. All the approaches using EnWN as skeleton are based on the assumption of a close conceptual similarity between English and Lg, in such a way that most of the structure (relations) in EnWN could be maintained for LgWN. In the case of bilingual dictionaries the usual approach is to try to link the English counterpart of entries to synsets in EnWN and to assume that the entry can be ]inked to the same synset. Monolingual dictionaries have been used basically as a source for extracting taxonomic (hypemym) links between words (or senses [Bruce & Guthrie 92], [Rigau et al. 97]) and in lower extent for extracting other kinds of semantic relations [Richardson 97] (e.g. meronymic links). Once a taxonomy of Lg (already existing or built from a monolingual MILD) is available, the task can consist of 1) enriching the taxonomic structure with other semantic links (manually or automatically), as is the case of bu i ld ing individual WNs, or 2) merging this structure with other already existing ontologies (as EnWN or EWN). This paper presents our approach to the construction of WNs for two languages, Spanish and Catalan, and linking the first one to EWN. We have developed a methodology that uses as core source EnWN 2. The methodology implies 1) 2We have used WordNet 1.5. 65", "venue": "WordNet@ACL/COLING", "citationCount": 75, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "kg", "nlp"], "mention_counts": {"nlp": 2, "onto": 3, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 3}, "relevance_score": 0.48737238575333663}, {"paperId": "496f08b4e29351852dc46dbbd745e6fa85f489f7", "url": "https://www.semanticscholar.org/paper/496f08b4e29351852dc46dbbd745e6fa85f489f7", "title": "When Conset Meets Synset: A Preliminary Survey of an Ontological Lexical Resource Based on Chinese Characters", "abstract": "This paper describes an on-going project concerning with an ontological lexical resource based on the abundant conceptual information grounded on Chinese characters. The ultimate goal of this project is set to construct a cognitively sound and computationally effective character-grounded machine-understandable resource. \n \nPhilosophically, Chinese ideogram has its ontological status, but its applicability to the NLP task has not been expressed explicitly in terms of language resource. We thus propose the first attempt to locate Chinese characters within the context of ontology. Having the primary success in applying it to some NLP tasks, we believe that the construction of this knowledge resource will shed new light on theoretical setting as well as the construction of Chinese lexical semantic resources.", "venue": "ACL", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "e658940fe2921692824f36bd9e84c37d22cf5782", "url": "https://www.semanticscholar.org/paper/e658940fe2921692824f36bd9e84c37d22cf5782", "title": "Onto-based sentiment classification using machine learning techniques", "abstract": "Sentiment analysis is a methodology used to analyse the emotion or view of an individual to a situation or topic. In present scenario, Social media is the source for the collection of individual's feedbacks, user's emotions, reviews and personal experiences which lead to a need for efficient mining of the text to derive knowledge. An optimal classification of text based on emotion is an unsolved problem in text mining. To extract knowledge from text many machine learning tools and techniques were proposed. An onto-based process is proposed to analyse the customer's emotion in this paper. The input emotional text that needs to be classified is given as input to the NLP and processed and an emotional ontology is created for better understanding of the semantics and relationships. When adding new instances, Ontology can be automatically classify them based on emotional relationship. The Emowords from ontology can be further classified using any of the standard machine learning techniques which definitively gives a better performance. This paper is a review of all the machine learning techniques that can be applied on the semantic analysis of sentiments.", "venue": "International Conference on Innovations in Information, Embedded and Communication Systems", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ke", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "onto": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 3}, "relevance_score": 0.48737238575333663}, {"paperId": "31fec5a1e239cb81fa4491c9935e8d319530582a", "url": "https://www.semanticscholar.org/paper/31fec5a1e239cb81fa4491c9935e8d319530582a", "title": "Learning taxonomical relations from domain texts using WordNet and word sense disambiguation", "abstract": "Learning taxonomical relations from domain texts is an important task for ontology learning from texts. We observe that rich information on taxonomical relations is available in the lexical knowledge base WordNet. However, in order to exploit the taxonomical relations in WordNet we need to tackle the difficult problem of word sense disambiguation. In this paper, we present a weighted word sense disambiguation method and show its application for learning taxonomical relations from domain texts. The experimental results indicate that using Word Net and our word sense disambiguation method achieves good accuracy and coverage for the learning task.", "venue": "IEEE International Conference on Granular Computing", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "wsd", "wsd", "wsd", "kg", "wsd"], "mention_counts": {"kg": 1, "wsd": 4, "onto": 1}, "nlp_mention_counts": {"wsd": 4}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.48737238575333663}, {"paperId": "1f7e823a32904cd46c026586d2193b0abb4de6c6", "url": "https://www.semanticscholar.org/paper/1f7e823a32904cd46c026586d2193b0abb4de6c6", "title": "Standardization of Unstructured Textual Data into Semantic Web Format", "abstract": "Analysis done on the nature of the data posted on the World Wide Web (WWW) reveal that more than 80% of the data over the WWW is in unstructured text format. Hence extracting information from text is of paramount importance both for academic and business purposes. Simultaneously, evolution of web technology led to the novel concept of Semantic Web, which is an extension of the current web in which information is given well-defined meaning, enabling computers and people to work in cooperation in a better way. Integration of voluminous, legacy text data that are unstructured and semistructured, into Semantic Web format is a challenging and daunting task for the research community. This paper is an attempt to marry the concept of Semantic Web format with unstructured text, thus to enable the computers to discover the previously unknown information, by automatically extracting information from different written resources. . . .", "venue": "IICAI", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ie", "sw", "sw", "ie", "sw"], "mention_counts": {"sw": 4, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"sw": 4}, "relevance_score": 0.48737238575333663}, {"paperId": "361ea8d125f263140de6d2c791668163e294cc92", "url": "https://www.semanticscholar.org/paper/361ea8d125f263140de6d2c791668163e294cc92", "title": "Enhancing Semantic Analysis of Pathology Reports", "abstract": "Pathology reports play an essential role in cancer treatment and research. They contain vital findings about a patient\u2019s cancer, such as cell histology and molecular markers, that are used to diagnose the type of cancer, determine treatment options, and enhance our understanding of the nature of the disease. At Roswell Park Comprehensive Cancer Center, 1 pathology reports are stored mainly as unstructured text in a relational database. 2 To find information within pathology reports, we use either string matching methods (e.g., regular expressions), or the TIES natural language processing (NLP) program. The drawback of string matching is that string variations need to be accounted for in order to find information. For instance, a search for patients whose tumors lack estrogen receptor (ER) proteins will also have to search for strings matching \u2018ER negative\u2019, \u2018estrogen receptor negative\u2019, \u2018hormone status negative\u2019, and the like. TIES addresses some of this variability by mapping multiple strings to the same ontology class, but some searches consistently do not perform well. Moreover, a class identified by TIES is not linked to the formal axioms that define the class, which prevents researchers from fully leveraging the formal relations that hold between classes within an ontology. For example, the formal definition of Medullary Breast Carcinoma (C17965 5 ) in the NCI Thesaurus (NCIt) [1] includes the axiom: Disease_Mapped_To_Gene some 'BRCA1 Gene' But, this axiom is not accessible within TIES, and, thus, you are not able to query for other cancers that are also mapped to the BRCA1 gene (such as hereditary prostate carcinoma) and investigate commonalities between them. To address these shortcomings, we are developing an ontology that we currently call the \u2018Document Content Ontology\u2019 6 (DCO) to represent the terms, words, word 1 https://www.roswellpark.org 2 The database does contain some structured fields, but we find that most researchers are interested in the information contained in the unstructured text. 3 http://ties.dbmi.pitt.edu 4 TIES consistently fails to identify findings that the cells in the tumor lack estrogen-receptor proteins. 5 IRI: http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C17965 6 The name of the ontology may change! contexts, and their positions (i.e., indexes) within the documents. That is, we are using the DCO to represent the content of the document and where the content is located. It is important to make clear that while we are using an NLP program for named entity recognition (described in what follows), we are not developing an NLP program. Rather, we are augmenting the output of the NLP program so that we can more fully leverage the axioms contained within an ontology. A high-level summary of the DCO is illustrated in Figure 1. Documents contain (i.e., has part) terms, and terms, which are composed of one more words, have meanings that are specified using the semantic type, sematic label, and semantic source annotation properties to reference an ontology class. The literal value data property associates the actual data (e.g., strings) with the term, and the polarity annotation represents whether the term has a positive or negative connotation (e.g., the patient does not have breast carcinoma). In some cases, we also represent the word context: the group of words surrounding the word or words that constitute a term. Word contexts are useful in aiding NLP programs to disambiguate the sense in which a word is being used. For brevity, not all properties and classes are discussed. Full details are available at https://github.com/RoswellParkResearch/document-contentontology. We are aware that a number of other ontologies (such as the Information Artifact Ontology and Semanticscience Integrated * corresponding author Proceedings of the 9th International Conference on Biological Ontology (ICBO 2018), Corvallis, Oregon, USA 1 ICBO 2018 August 7-10, 2018 1 Ontology) have terms similar to ours. However, these ontologies carry with them metaphysical commitments, such as a document being a type of generically dependent continuant. Since we are just beginning to develop the DCO, we wish (for now) to remain agnostic concerning such commitments. At present, we are using the DCO to structure output from the Noble Coder Named Recognition Engine [4]. Noble takes a document as input and outputs a file containing information about (named) entities identified within the document as well as the associated ontology classes that specify the meanings of the named entities. For example, if the Noble program determines that some text within a document refers to ductal breast carcinoma, Noble associates this text with the NCIt class \u2018Ductal Breast Carcinoma\u2019 (C4017). We translate the output of Noble into OWL and load it along with the full ontology of association classes (which we call a named entity\u2019s semantic type) into a GraphDB semantic triple store. This allows us to simultaneously query for pathology reports having a specified named entity and the ontology for other entities related to the named entity. Figure 2 shows an example of a SPARQL query to find documents that contain a term whose semantic type references ontology terms that represent diseases that are mapped to the BRCA1 gene. This method of querying pathology reports using a named entity\u2019s semantic type as well as the ontology\u2019s formal structure provides better coverage for finding relevant pathology reports than simple named entity recognition alone. In addition to leveraging the ontologies axioms, we can also examine the word context surrounding a term. For instance, this is useful for addressing the aforementioned issue of searching pathology reports in which the cells are found to be ER negative.", "venue": "International Conference on Biomedical Ontology", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "onto", "onto", "onto", "onto", "onto", "onto", "onto", "onto", "onto", "onto", "onto", "nlp", "onto", "nlp", "nlp", "nlp", "onto", "onto", "onto", "onto", "onto", "nlp", "onto", "onto"], "mention_counts": {"nlp": 6, "onto": 19}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"onto": 19}, "relevance_score": 0.4799867828682649}, {"paperId": "0142c16a9d7dd9a196a3a6e3b75e7ca55483c7dc", "url": "https://www.semanticscholar.org/paper/0142c16a9d7dd9a196a3a6e3b75e7ca55483c7dc", "title": "An Ontology-Based Information Extraction System for Organic Farming", "abstract": "In the existing farming system, information is obtained manually, and most times, farmers act based on their discretion. Sometimes, farmers rely on information from experts and extension officers for decision making. In recent times, a lot of information systems are available with relevant information on organic farming practices; however, such information is scattered in different context, form, and media all over the internet, making their retrieval difficult. The use of ontology with the aid of a conceptual scheme makes the comprehensive and detailed formalization of any subject domain possible. This study is aimed at acquiring, storing, and providing organic farming-based information available to current and intending software developer who may wish to develop applications for farmers. It employs information extraction (IE) and ontology development techniques to develop an ontology-based information extraction (OBIE) system called ontology-based information extraction system for organic farming (OBIESOF). The knowledge base was built using prot\u00e9g\u00e9 editor; Java was used for the implementation of the ontology knowledge base with the aid of the high-level application programming language for working web ontology language application program interface (OWL API). In contrast, HermiT was used to checking the consistencies of the ontology and for submitting queries in order to verify their validity. The queries were expressed in description logic (DL) query language. The authors tested the capability of the ontology to respond to user queries by posing instances of the competency questions from DL query interface. The answers generated by the ontology were promising and serve as positive pointers to its usefulness as a knowledge repository.", "venue": "International Journal on Semantic Web and Information Systems (IJSWIS)", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "onto", "onto", "onto", "ie", "kg", "onto", "kg", "ie", "onto", "onto", "onto", "onto", "onto"], "mention_counts": {"kg": 2, "onto": 11, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"kg": 2, "onto": 11}, "relevance_score": 0.469881796585065}, {"paperId": "10a6c6051beb1cfbe8e4696683758d79613b4ea4", "url": "https://www.semanticscholar.org/paper/10a6c6051beb1cfbe8e4696683758d79613b4ea4", "title": "Natural Language Generation from Graphs", "abstract": "The Resource Description Framework (RDF) is the primary language to describe information on the Semantic Web. The deployment of semantic web search from Google and Microsoft, the Linked Open Data Community project along with the announcement of schema.org by Yahoo, Bing and Google have significantly fostered the generation of data available in RDF format. Yet the RDF is a computer representation of data and thus is hard for the non-expert user to understand. We propose a Natural Language Generation (NLG) engine to generate English text from a small RDF graph. The Natural Language Generation from Graphs (NLGG) system uses an ontology skeleton, which contains hierarchies of concepts, relationships and attributes, along with handcrafted template information as the knowledge base. We performed two experiments to evaluate NLGG. First, NLGG is tested with RDF graphs extracted from four ontologies in different domains. A Simple Verbalizer is used to compare the results. NLGG consistently outperforms the Simple Verbalizer in all the test cases. In the second experiment, we compare the effort spent to make NLGG and NaturalOWL work with the M-PIRO ontology. Results show that NLGG generates acceptable text with much smaller effort.", "venue": "Int. J. Semantic Comput.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "sw", "nlg", "rdf", "nlg", "sw", "rdf", "onto", "kg", "rdf", "onto", "lod", "onto", "nlg", "rdf", "nlg", "rdf"], "mention_counts": {"onto": 3, "sw": 2, "nlg": 4, "lod": 1, "kg": 1, "rdf": 6}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"onto": 3, "sw": 2, "lod": 1, "kg": 1, "rdf": 6}, "relevance_score": 0.469881796585065}, {"paperId": "3520891885602849643dde1b58e547e0fe79d851", "url": "https://www.semanticscholar.org/paper/3520891885602849643dde1b58e547e0fe79d851", "title": "Annotating a Low-Resource Language with LLOD Technology: Sumerian Morphology and Syntax", "abstract": "This paper describes work on the morphological and syntactic annotation of Sumerian cuneiform as a model for low resource languages in general. Cuneiform texts are invaluable sources for the study of history, languages, economy, and cultures of Ancient Mesopotamia and its surrounding regions. Assyriology, the discipline dedicated to their study, has vast research potential, but lacks the modern means for computational processing and analysis. Our project, Machine Translation and Automated Analysis of Cuneiform Languages, aims to fill this gap by bringing together corpus data, lexical data, linguistic annotations and object metadata. The project\u2019s main goal is to build a pipeline for machine translation and annotation of Sumerian Ur III administrative texts. The rich and structured data is then to be made accessible in the form of (Linguistic) Linked Open Data (LLOD), which should open them to a larger research community. Our contribution is two-fold: in terms of language technology, our work represents the first attempt to develop an integrative infrastructure for the annotation of morphology and syntax on the basis of RDF technologies and LLOD resources. With respect to Assyriology, we work towards producing the first syntactically annotated corpus of Sumerian.", "venue": "Inf.", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "mt", "llod", "lod", "llod", "rdf", "llod"], "mention_counts": {"llod": 3, "lod": 1, "mt": 2, "rdf": 1}, "nlp_mention_counts": {"mt": 2}, "ld_mention_counts": {"llod": 3, "lod": 1, "rdf": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "4f16c553b68a663585b0ee7d7a7b31d2da8e29a5", "url": "https://www.semanticscholar.org/paper/4f16c553b68a663585b0ee7d7a7b31d2da8e29a5", "title": "Surface Realisation from Knowledge-Bases", "abstract": "Natural Language Generation (NLG) is the task of automatically producing natural language text to describe information present in non-linguistic data. It involves three main subtasks: (i) selecting the relevant portion of input data; (ii) determining the words that will be used to verbalise the selected data; and (iii) mapping these words into natural language text. The latter task is known as Surface Realisation (SR). In my thesis, I study the SR task in the context of input data coming from Knowledge Bases (KB). I present two novel approaches to surface realisation from knowledge bases: a supervised approach and a weakly supervised approach. \n \nIn the first, supervised, approach, I present a corpus-based method for inducing a Feature Based Lexicalized Tree Adjoining Grammar (FB-LTAG) from a parallel corpus of text and data. The resulting grammar includes a unification based semantics and can be used by an existing surface realiser to generate sentences from test data. I show that the induced grammar is compact and generalises well over the test data yielding results that are close to those produced by a handcrafted symbolic approach and which outperform an alternative statistical approach. \n \nIn the weakly supervised approach, I explore a method for surface realisation from KB data which uses a supplied lexicon but does not require a parallel corpus. Instead, I build a corpus from heterogeneous sources of domain-related text and use it to identify possible lexicalisations of KB symbols (classes and relations) and their verbalisation patterns (frames). Based on the observations made, I build different probabilistic models which are used for selection of appropriate frames and syntax/semantics linking while verbalising KB inputs. I evaluate the output sentences and analyse the issues relevant to learning from non-parallel corpora. \n \nIn both these approaches, I use the data derived from an existing biomedical ontology as a reference input. The proposed methods are generic and can be easily adapted for input from other ontologies for which a parallel/non-parallel corpora exists.", "venue": "ACL", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "onto", "nlg", "kg", "onto", "nlg"], "mention_counts": {"kg": 3, "nlg": 2, "onto": 2}, "nlp_mention_counts": {"nlg": 2}, "ld_mention_counts": {"kg": 3, "onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "6e26010ce9b1ad5e60a0871551ce9570692fa3b6", "url": "https://www.semanticscholar.org/paper/6e26010ce9b1ad5e60a0871551ce9570692fa3b6", "title": "An algorithm to generate short sentences in natural language from linked open data based on linguistic templates", "abstract": "The generation of natural language phrases from Linked Open Data can benefit from a significant amount of information available on the internet, as well as from the existence of properties within them, which appears, mostly, in the RDF format. These properties can represent semantic relationships between concepts that might help in creating sentences in natural language. Nevertheless, research in this field tends not to use the information in RDF. We support that this is a factor that might foster the generation of more natural phrases. In this scenario, this research explores these RDF properties for the generation of natural language phrases. The short sentences generated by the algorithm implementation were evaluated regarding their fluency by linguists and native English speakers. The results show that the sentences generated are promising regarding sentence fluency.", "venue": "Int. J. Metadata Semant. Ontologies", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "lod", "nlg", "rdf", "lod", "rdf", "rdf"], "mention_counts": {"nlg": 2, "lod": 2, "rdf": 3}, "nlp_mention_counts": {"nlg": 2}, "ld_mention_counts": {"lod": 2, "rdf": 3}, "relevance_score": 0.46718541496779636}, {"paperId": "332408747e9b425d9f63cdc14658a9430d916a2a", "url": "https://www.semanticscholar.org/paper/332408747e9b425d9f63cdc14658a9430d916a2a", "title": "Design and Implementation of an Extended Corporate CRMDatabase System with Big Data Analytical Functionalities", "abstract": "The amount of open information available on-line from heterogeneous sources and domains is growing at an extremely fast pace, and constitutes an important knowledge base for the consideration of industries and companies. In this context, two relevant data providers can be highlighted: the \u201cLinked Open Data\u201d (LOD) and \u201cSocial Media\u201d (SM) paradigms. The fusion of these data sources \u2013 structured the former, and raw data the latter \u2013, along with the information contained in structured corporate databases within the organizations themselves, may unveil significant business opportunities and competitive advantage to those who are able to understand and leverage their value. In this paper, we present two complementary use cases, illustrating the potential of using the open data in the business domain. The first represents the creation of an existing and potential customer knowledge base, exploiting social and linked open data based on which any given organization might infer valuable information as a support for decision making. The second focuses on the classification of organizations and enterprises aiming at detecting potential competitors and/or allies via the analysis of the conceptual similarity between their participated projects. To this end, a solution based on the synergy of Big Data and semantic technologies will be designed and developed. The first will be used to implement the tasks of collection, data fusion and classification supported by natural language processing (NLP) techniques, whereas the latter will deal with semantic aggregation, persistence, reasoning and information retrieval, as well as with the triggering of alerts based on the semantized information.", "venue": "J. Univers. Comput. Sci.", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "lod", "nlp", "kg", "lod", "nlp", "lod"], "mention_counts": {"nlp": 2, "kg": 2, "lod": 3}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 2, "lod": 3}, "relevance_score": 0.46718541496779636}, {"paperId": "866329e7377ca7a4f1f806d3b7bda1bdd8296742", "url": "https://www.semanticscholar.org/paper/866329e7377ca7a4f1f806d3b7bda1bdd8296742", "title": "MEANING: a Roadmap to Knowledge Technologies", "abstract": "Knowledge Technologies need to extract knowledge from existing texts, which calls for advanced Human Language Technologies (HLT). Progress is being made in Natural Language Processing but there is still a long way towards Natural Language Understanding. An important step towards this goal is the development of technologies and resources that deal with concepts rather than words. The MEANING project argues that we need to solve two complementary and intermediate tasks to enable the next generation of intelligent open domain HLT application systems: Word Sense Disambiguation and large-scale enrichment of Lexical Knowledge Bases. Innovations in this area will lead to HLT with deeper understanding of texts, and immediate progress in real applications of Knowledge Technologies.", "venue": "RAODMAP@COLING", "citationCount": 48, "fieldsOfStudy": ["Computer Science"], "mentions": ["hlt", "kg", "nlp", "wsd", "ke", "nlu"], "mention_counts": {"kg": 1, "nlu": 1, "nlp": 1, "ke": 1, "hlt": 1, "wsd": 1}, "nlp_mention_counts": {"nlu": 1, "nlp": 1, "ke": 1, "hlt": 1, "wsd": 1}, "ld_mention_counts": {"kg": 1, "ke": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "0c07d55c71c9f2323e273188f2a432359d32a1c0", "url": "https://www.semanticscholar.org/paper/0c07d55c71c9f2323e273188f2a432359d32a1c0", "title": "A preliminary framework for human-agent communication in electronic negotiations", "abstract": "Electronic negotiations are business negotiations conducted via electronic means using information and communications technologies (ICT). Two dominant types of electronic negotiation systems are automated negotiation systems for software agents and negotiation support systems (NSSs) for humans. However, the integration of two types for human-agent negotiations is an important task. In this paper, an extended communication model for human-agent business negotiations is presented. For this purpose, the underlying communication models of automated negotiations and NSSs are analyzed. The extended communication model is based on a common negotiation ontology which captures the negotiation agenda and paves the way for such hybrid communication, a natural language processing (NLP) component which process natural language negotiation content, and a translator to covert human message to agent message and vice versa. NLP component has been added as an alternative approach to manual annotation of message content. Our aim is to show that using an ontology-based negotiation approach as implemented in the NSS Negoisst and NLP techniques integrated into it can make human-agent communication better.", "venue": "2011 International Conference on Information and Communication Technologies", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 5, "onto": 2}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "bc01a1a27b3a2f154d2a8cc9032f6ea957c6723b", "url": "https://www.semanticscholar.org/paper/bc01a1a27b3a2f154d2a8cc9032f6ea957c6723b", "title": "Harnessing the Expertise of 70, 000 Human Editors: Knowledge-Based Feature Generation for Text Categorization", "abstract": "Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-specific and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two significant problems in natural language processing---synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets confirm improved performance compared to the bag of words document representation.", "venue": "Journal of machine learning research", "citationCount": 86, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "kg", "onto", "kg", "wsd", "onto"], "mention_counts": {"nlp": 1, "wsd": 1, "onto": 3, "kg": 2}, "nlp_mention_counts": {"nlp": 1, "wsd": 1}, "ld_mention_counts": {"kg": 2, "onto": 3}, "relevance_score": 0.46718541496779636}, {"paperId": "0d94c48e3b4682416867dd990d468fcaed2490c5", "url": "https://www.semanticscholar.org/paper/0d94c48e3b4682416867dd990d468fcaed2490c5", "title": "Excavating grey literature: A case study on the rich indexing of archaeological documents via natural language-processing techniques and knowledge-based resources", "abstract": "Purpose \u2013 This paper sets out to discuss the use of information extraction (IE), a natural language\u2010processing (NLP) technique to assist \u201crich\u201d semantic indexing of diverse archaeological text resources. The focus of the research is to direct a semantic\u2010aware \u201crich\u201d indexing of diverse natural language resources with properties capable of satisfying information retrieval from online publications and datasets associated with the Semantic Technologies for Archaeological Resources (STAR) project.Design/methodology/approach \u2013 The paper proposes use of the English Heritage extension (CRM\u2010EH) of the standard core ontology in cultural heritage, CIDOC CRM, and exploitation of domain thesauri resources for driving and enhancing an Ontology\u2010Oriented Information Extraction process. The process of semantic indexing is based on a rule\u2010based Information Extraction technique, which is facilitated by the General Architecture of Text Engineering (GATE) toolkit and expressed by Java Annotation Pattern Engine (JAPE) rules.F...", "venue": "Aslib Proc.", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "ie", "onto", "ie", "ie", "nlp"], "mention_counts": {"nlp": 2, "kg": 1, "onto": 1, "ie": 3}, "nlp_mention_counts": {"nlp": 2, "ie": 3}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "a1bd4a82742a9dc97369aad03df835db8faae9dd", "url": "https://www.semanticscholar.org/paper/a1bd4a82742a9dc97369aad03df835db8faae9dd", "title": "Conceptual Primitive Decomposition for Knowledge Sharing via Natural Language", "abstract": "Natural language is an ideal mode of interaction and knowledge sharing between intelligent computer systems and their human users. But a major problem that natural language interaction poses is linguistic variation, or the \u201cparaphrase problem\u201d: there are a variety of ways of referring to the same idea. This is a special problem for intelligent systems in domains such as information retrieval, where a query presented in natural language is matched against an ontology or knowledge base, particularly when its representation uses a vocabulary based in natural language. This paper proposes solutions to these problems in primitive decomposition methods that represent concepts in terms of structures reflecting low-level, embodied human cognition. We argue that this type of representation system engenders richer relations between natural language expressions and knowledge structures, enabling more effective interactive knowledge sharing.", "venue": "JOWO", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "nlp", "onto", "kg", "nle", "nlg", "nll"], "mention_counts": {"onto": 1, "nlu": 1, "nll": 1, "nlp": 1, "nle": 1, "nlg": 1, "kg": 1}, "nlp_mention_counts": {"nll": 1, "nlu": 1, "nlp": 1, "nle": 1, "nlg": 1}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "7ddb89b085cc36f67906c62071acd80b4ed33fb0", "url": "https://www.semanticscholar.org/paper/7ddb89b085cc36f67906c62071acd80b4ed33fb0", "title": "Automated feedback generation for formal manufacturing rule extraction", "abstract": "Abstract Manufacturing knowledge is maintained primarily in the unstructured text in industry. To facilitate the reuse of the knowledge, previous efforts have utilized Natural Language Processing (NLP) to classify manufacturing documents or to extract structured knowledge (e.g. ontology) from manufacturing text. On the other hand, extracting more complex knowledge, such as manufacturing rule, has not been feasible in a practical scenario, as standard NLP techniques cannot address the input text that needs validation. Specifically, if the input text contains the information irrelevant to the rule-definition or semantically invalid expression, standard NLP techniques cannot selectively derive precise information for the extraction of the desired formal manufacturing rule. To address the gap, we developed the feedback generation method based on Constraint-based Modeling (CBM) coupled with NLP and domain ontology, designed to support formal manufacturing rule extraction. Specifically, the developed method identifies the necessity of input text validation based on the predefined constraints and provides the relevant feedback to help the user modify the input text, so that the desired rule can be extracted. We proved the feasibility of the method by extending the previously implemented formal rule extraction framework. The effectiveness of the method is demonstrated by enabling the extraction of correct manufacturing rules from all the cases that need input text validation, about 30% of the dataset, after modifying the input text based on the feedback. We expect the feedback generation method will contribute to the adoption of semantics-based technology in the manufacturing field, by facilitating precise knowledge acquisition from manufacturing-related documents in a practical scenario.", "venue": "Artificial Intelligence for Engineering Design, Analysis and Manufacturing", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "nlp", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 5, "onto": 2}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "a9e88be2e77a73a6f5d6c50fd4dc7a30f680ba86", "url": "https://www.semanticscholar.org/paper/a9e88be2e77a73a6f5d6c50fd4dc7a30f680ba86", "title": "Semantic Data Integration Techniques for Transforming Big Biomedical Data into Actionable Knowledge", "abstract": "FAIR principles and the Open Data initiatives have motivated the publication of large volumes of data. Specifically, in the biomedical domain, the size of the data has increased exponentially in the last decade, and with the advances in the technologies to collect and generate data, a faster growth rate is expected for the next years. The available collections of data are characterized by the dominant dimensions of big data, i.e., they are not only large in volume, but they can be also heterogeneous and present quality issues. These data complexity problems impact on the typical tasks of data management, and particularly, in the task of integrating big biomedical data sources. We tackle the problem of big data integration and present a knowledge-driven framework able to extract and integrate data collected from structured and unstructured data sources. The proposed framework resorts to Natural Language Processing techniques to extract knowledge from unstructured data and short text. Furthermore, ontologies and controlled vocabularies, e.g., UMLS, are utilized to annotate the extracted entities and relations with terms from the ontology or controlled vocabulary. The annotated data is integrated into a knowledge graph. A unified schema is used to describe the meaning of the integrated data as well as the main properties and relations. As proof of concept, we show the results of applying the proposed framework to integrate clinical records from lung cancer patients with data extracted from open data sources like Drugbank and PubMed. The created knowledge graph enables the discovery of interactions between drugs in the treatments prescribed to lung cancer patients.", "venue": "2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "onto", "kg", "onto", "kg"], "mention_counts": {"nlp": 1, "kg": 2, "onto": 2, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 2, "kg": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "dd6f0764a16decd8f2b35478d66904b30521843b", "url": "https://www.semanticscholar.org/paper/dd6f0764a16decd8f2b35478d66904b30521843b", "title": "Towards a Generation-Based Semantic Web Authoring Tool", "abstract": "Widespread use of Semantic Web technologies requires interfaces through which knowledge can be viewed and edited without deep understanding of Description Logic and formalisms like OWL and RDF. Several groups are pursuing approaches based on Controlled Natural Languages (CNLs), so that editing can be performed by typing in sentences which are automatically interpreted as statements in OWL. We suggest here a variant of this approach which relies entirely on Natural Language Generation (NLG), and propose requirements for a system that can reliably generate transparent realisations of statements in Description Logic.", "venue": "ENLG", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "sw", "sw", "nlg", "onto", "onto", "nlg"], "mention_counts": {"sw": 2, "nlg": 2, "onto": 2, "rdf": 1}, "nlp_mention_counts": {"nlg": 2}, "ld_mention_counts": {"sw": 2, "onto": 2, "rdf": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "b2fbe2c7766eaf12638a82bb7221593912826805", "url": "https://www.semanticscholar.org/paper/b2fbe2c7766eaf12638a82bb7221593912826805", "title": "RichRDF: A Tool for Enriching Food, Energy, and Water Datasets with Semantically Related Facts and Images", "abstract": "Food, energy, and water (FEW) are the key resources to sustain human life and economic growth on Earth. While there is a plethora of information related to FEW systems online, there is a lack of reliable knowledge management tools that enable easy consumption of such information. In this paper, we present a web-based tool called RichRDF with the goal of enriching exiting FEW systems with semantically related facts and images. The main features of RichRDF include (1) an entity extraction algorithm that extracts meaningful subjects from Resource Description Framework (RDF) statements using natural language processing (NLP) techniques, (2) a reliable approach to add semantic similarity scores and relationships between different RDF subjects based on ConceptNet, (3) an efficient way to use the numbers of WordNet synsets to request the associated images from ImageNet, and (4) a user friendly interface that allows users to load and convert FEW datasets to RDF and then query the RDF datasets using an existing SPARQL engine. A video highlighting the key features of RichRDF is available at https://youtu.be/vyHgh4LgKCo.", "venue": "International Workshop on the Semantic Web", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "rdf", "nlp", "nlp", "rdf", "rdf", "rdf"], "mention_counts": {"nlp": 2, "rdf": 5}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"rdf": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "640bb9aab1f7294898a629bdc291eb2d1ed5567c", "url": "https://www.semanticscholar.org/paper/640bb9aab1f7294898a629bdc291eb2d1ed5567c", "title": "Application of Chinese Natural Language Generation in Semantic Web", "abstract": "RDF is the representation of the Semantic Web. When querying RDF documents, the result is a sub-graph of RDF data model or a number of triple statements. In this paper, we apply natural language generation technique to render the result into multi-sentential text for human comprehension. We investigate the effect of discourse segmentation on the generation of anaphora and punctuation marks in Chinese. We first describe the construction of hierarchical discourse structure of message content by employing the planning concept in the field of artificial intelligence. Based on this discourse structure, we propose to segment the discourse in terms of the levels of rhetorical nodes in the hierarchy of message structure. The implementation result shows that the segmentation scheme is promising.", "venue": "ISDB", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "rdf", "rdf", "nlg", "rdf", "nlg"], "mention_counts": {"sw": 2, "nlg": 2, "rdf": 3}, "nlp_mention_counts": {"nlg": 2}, "ld_mention_counts": {"sw": 2, "rdf": 3}, "relevance_score": 0.46718541496779636}, {"paperId": "b910079a426b662179430ecad768983f94f5107a", "url": "https://www.semanticscholar.org/paper/b910079a426b662179430ecad768983f94f5107a", "title": "Applying Semantic Role Labeling and Spreading Activation Techniques for Semantic Information Retrieval", "abstract": "Semantically enhanced information retrieval (IR) is aimed at improving classical IR methods and goes way beyond plain Boolean keyword matching with the main goal of better serving implicit and ambiguous information needs. As a de-facto pre-requisite to semantic IR, different information extraction (IE) techniques are used to mine unstructured text for underlying knowledge.\u00a0 In this paper we present a method that combines both IE and IR to enable semantic search in natural language texts. First, we apply semantic role labeling (SRL) to automatically extract event-oriented information found in natural language texts to an RDF knowledge graph leveraging semantic web technology. Second, we investigate how a custom flavored graph traversal spreading activation algorithm can be employed to interpret user\u2019s information needs on top of the prior-extracted knowledge base. Finally, we present an assessment on the applicability of our method for semantically enhanced IR. An experimental evaluation on partial WikiQA dataset shows the strengths of our approach and also unveils common pitfalls that we use as guidelines to draw further work directions in the open-domain semantic search field.", "venue": "Inf. Technol. Control.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "rdf", "sw", "kg", "kg", "ke"], "mention_counts": {"ke": 1, "sw": 1, "kg": 2, "rdf": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "sw": 1, "kg": 2, "rdf": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "9096a2d5ee95110a0f9b67fba7e36741f24820d7", "url": "https://www.semanticscholar.org/paper/9096a2d5ee95110a0f9b67fba7e36741f24820d7", "title": "Semantic Knowledge Bases from Web Sources", "abstract": "The Web bears the potential of being the world's greatest encyclopedic source, but we are far from fully ex- ploiting this potential. Valuable scientific and cultural content is interspersed with a huge amount of noisy, low- quality, unstructured text and media. The proliferation of knowledge-sharing communities like Wikipedia and the advances in automated information extraction from Web pages give rise to an unprecedented opportunity: Can we systematically harvest facts from the Web and compile them into a comprehensive machine-readable knowledge base? Such a knowledge base would contain not only the world's entities, but also their semantic properties, and their relationships with each other. Imagine a \u201cStructured Wikipedia\u201d that has the same scale and richness as Wikipedia itself, but offers a precise and concise representation of knowledge, e.g., in the RDF format. This would enable expressive and highly precise querying, e.g., in the SPARQL language (or appropriate extensions), with additional capabilities for informative ranking of query results. The benefits from solving the above challenge would be enormous. Potential applications include 1) aformalizedmachine-readableencyclopediathatcanbequeriedwithhighprecisionlikeasemanticdatabase; 2) a key asset for disambiguating entities by supporting fast and accurate mappings of textual phrases onto named entities in the knowledge base; 3) an enabler for entity-relationship-oriented semantic search on the Web, for detecting entities and relations in Web pages and reasoning about them in expressive (probabilistic) logics; 4) a backbone for natural-language question answering that would aid in dealing with entities and their rela- tionships in answering who/where/when/ etc. questions; 5) a key asset for machine translation (e.g., English to German) and interpretation of spoken dialogs, where world knowledge provides essential context for disambiguation; 6) acatalystforacquisitionoffurtherknowledgeandlargelyautomatedmaintenanceandgrowthoftheknowl- edge base. While these application areas cover a broad, partly AI-flavored ground, the most notable one from a database perspective is semantic search: finally bringing DB methodology to Web search! For example, users (or tools on behalf of users) would be able to formulate queries about succulents that grow both in Africa and America, politicians who are also scientists or are married to singers, or flu medication that can be taken by people with high blood pressure. The search engine would return precise and concise answers: lists of entities or entity pairs (depending on the question structure), for example, Angela Merkel, Benjamin Franklin, etc., or Nicolas Sarkozy for the questions about scientists. This would be a quantum leap over today's search where an- swers are embedded if not buried in lots of result pages, and the human users would have to read them to extract entities and connect them to other entities. In this sense, the envisioned large-scale knowledge harvesting [42] from Web sources may also be viewed as machine reading [13].", "venue": "IJCAI 2011", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "rdf", "ie", "mt", "kg", "kg", "kg"], "mention_counts": {"mt": 1, "kg": 4, "rdf": 1, "ie": 1}, "nlp_mention_counts": {"mt": 1, "ie": 1}, "ld_mention_counts": {"kg": 4, "rdf": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "64a03695d5b78442cce69941c9cba0c1aad45e9f", "url": "https://www.semanticscholar.org/paper/64a03695d5b78442cce69941c9cba0c1aad45e9f", "title": "Aprendizaje Profundo para la Extracci\u00f3n de Aspectos: Tarea Esencial en la Creaci\u00f3n y uso de las Ontolog\u00edas (Deep Learning for the Aspects Extraction: Essential Task in the Creation and Use of Ontologies)", "abstract": "On the internet, a large amount of information is organized and related through the technologies and components that define the Semantic Web. Other data sources such as blogs, web pages and digital documents contain important information, but it is not well organized or structured. For many institutions and people, it is important to extract and organize this information automatically by using knowledge sources such as ontologies. The efficient use of these sources of knowledge is conditioned by an effective process of information extraction. This process requires performing several natural language processing tasks. One of the most important tasks is the aspect extraction or the extraction of entity characteristics, events or objects in the processed information. An efficient aspect extraction allows an effective use of ontologies. In recent years, researchers have applied models based on Deep Learning algorithms to extracting aspects with good results. This paper is focus on the study of the main results related to the application of Deep Learning techniques in the aspect extraction task. The analysis presented here will facilitate the selection of Deep Learning methods for aspect extraction, as well as the textual representation models to be used that allow obtaining good results, and thus contribute satisfactorily to the use of ontologies.", "venue": "IWSW", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "onto", "onto", "ie", "nlp", "onto", "onto"], "mention_counts": {"nlp": 1, "sw": 1, "onto": 4, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.46718541496779636}, {"paperId": "7ae938cfd964f21a8e6bc65c0102b249e4c02d0e", "url": "https://www.semanticscholar.org/paper/7ae938cfd964f21a8e6bc65c0102b249e4c02d0e", "title": "Towards Enriching Linked Open Data via Open Information Extraction", "abstract": "The descriptions of various entities on Linked Data repositories are subject to constant renewals and modifications, with respect to both the descriptions of concepts and relations and entities realizing their instantiations. Thus, the underlying ontologies have to be updated accordingly in order to reflect these changes. This paper presents a system for examining the possibilities of discovering new relations and updating/verifying existing ones for entities described in Linked Data repositories by using Open Information Extraction techniques. These are applied to web content. The process aims to the enrichment of the examined datasets and the expansion of the ontologies with newly-discovered concepts and relations. Towards this target, the paper discusses the intricacies, pitfalls and challenges present.", "venue": "KNOW@LOD", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "lod", "onto", "ld", "ie", "onto", "ld"], "mention_counts": {"ld": 2, "lod": 1, "onto": 2, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"ld": 2, "lod": 1, "onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "f355eac84c7aa4823eb55015257bf85bec3c80ae", "url": "https://www.semanticscholar.org/paper/f355eac84c7aa4823eb55015257bf85bec3c80ae", "title": "Early Steps Towards Web Scale Information Extraction with LODIE", "abstract": "Information extraction (IE) is the technique for transforming unstructured textual data into structured representation that can be understood by machines. The exponential growth of the Web generates an exceptional quantity of data for which automatic knowledge capture is essential. This work describes the methodology for web scale information extraction in the LODIE project (linked open data information extraction) and highlights results from the early experiments carried out in the initial phase of the project. LODIE aims to develop information extraction techniques able to scale at web level and adapt to user information needs. The core idea behind LODIE is the usage of linked open data, a very large-scale information resource, as a ground-breaking solution for IE, which provides invaluable annotated data on a growing number of domains. This article has two objectives. First, describing the LODIE project as a whole and depicting its general challenges and directions. Second, describing some initial steps taken towards the general solution, focusing on a specific IE subtask, wrapper induction.", "venue": "The AI Magazine", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "ie", "ie", "lod", "ie", "lod"], "mention_counts": {"lod": 2, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"lod": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "61a8655342d88908ef6a63e6984f9e9824db344b", "url": "https://www.semanticscholar.org/paper/61a8655342d88908ef6a63e6984f9e9824db344b", "title": "Web Scale Information Extraction with LODIE", "abstract": "Information Extraction (IE) is the technique for transforming unstructured textual data into structured representation that can be understood by machines. The exponential growth of the Web generates an exceptional quantity of data for which automatic knowledge capture is essential. This work describes the methodology for Web scale Information Extraction adopted by the LODIE project (Linked Open Data Information Extraction). LODIE aims to develop Information Extraction techniques able to (i) scale at web level and (ii) adapt to user information need. The core idea behind LODIE is the usage of Linked Open Data, a very large-scale information resource, as a ground-breaking solution for IE, which provides invaluable annotated data on a growing number of domains.", "venue": "AAAI Fall Symposia", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "lod", "lod", "ie", "ie", "ie"], "mention_counts": {"lod": 2, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"lod": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "248b28325517687390572ba65ef54761ecc69402", "url": "https://www.semanticscholar.org/paper/248b28325517687390572ba65ef54761ecc69402", "title": "KBPearl: A Knowledge Base Population System Supported by Joint Entity and Relation Linking", "abstract": "Nowadays, most openly available knowledge bases (KBs) are incomplete, since they are not synchronized with the emerging facts happening in the real world. Therefore, knowledge base population (KBP) from external data sources, which extracts knowledge from unstructured text to populate KBs, becomes a vital task. Recent research proposes two types of solutions that partially address this problem, but the performance of these solutions is limited. The first solution, dynamic KB construction from unstructured text, requires specifications of which predicates are of interest to the KB, which needs preliminary setups and is not suitable for an in-time population scenario. The second solution, Open Information Extraction (Open IE) from unstructured text, has limitations in producing facts that can be directly linked to the target KB without redundancy and ambiguity. In this paper, we present an end-to-end system, KBPearl, for KBP, which takes an incomplete KB and a large corpus of text as input, to (1) organize the noisy extraction from Open IE into canonicalized facts; and (2) populate the KB by joint entity and relation linking, utilizing the context knowledge of the facts and the side information inferred from the source text. We demonstrate the effectiveness and efficiency of KBPearl against the state-of-the-art techniques, through extensive experiments on real-world datasets. PVLDB Reference Format: Xueling Lin, Haoyang Li, Hao Xin, Zijian Li and Lei Chen. KBPearl: A Knowledge Base Population System Supported by Joint Entity and Relation Linking. PVLDB, 13(7): 1035-1049, 2020. DOI: https://doi.org/10.14778/3384345.3384352", "venue": "Proceedings of the VLDB Endowment", "citationCount": 26, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ie", "kg", "kg", "kg"], "mention_counts": {"ke": 1, "kg": 4, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "kg": 4}, "relevance_score": 0.46718541496779636}, {"paperId": "773321cf179b499d81dd026c95703857bf993cb8", "url": "https://www.semanticscholar.org/paper/773321cf179b499d81dd026c95703857bf993cb8", "title": "Data processing and semantics for advanced internet of things (IoT) applications: modeling, annotation, integration, and perception", "abstract": "This tutorial presents tools and techniques for effectively utilizing the Internet of Things (IoT) for building advanced applications, including the Physical-Cyber-Social (PCS) systems. The issues and challenges related to IoT, semantic data modelling, annotation, knowledge representation (e.g. modelling for constrained environments, complexity issues and time/location dependency of data), integration, analysis, and reasoning will be discussed. The tutorial will describe recent developments on creating annotation models and semantic description frameworks for IoT data (e.g. such as W3C Semantic Sensor Network ontology). A review of enabling technologies and common scenarios for IoT applications from the data and knowledge engineering point of view will be discussed. Information processing, reasoning, and knowledge extraction, along with existing solutions related to these topics will be presented. The tutorial summarizes state-of-the-art research and developments on PCS systems, IoT related ontology development, linked data, domain knowledge integration and management, querying large-scale IoT data, and AI applications for automated knowledge extraction from real world data.", "venue": "Web Intelligence, Mining and Semantics", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "ld", "onto", "ke"], "mention_counts": {"ld": 1, "ke": 2, "onto": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ld": 1, "ke": 2, "onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "9e77f918e2c52e8d714370cfe0c8e6b3ad6b8229", "url": "https://www.semanticscholar.org/paper/9e77f918e2c52e8d714370cfe0c8e6b3ad6b8229", "title": "Knowledge extraction based on linked open data for clinical documentation", "abstract": "Smart cities are becoming a reality in the near future to transform many sectors and activities in our lives. Smart cities systems such as healthcare systems will have new functionality to improve the quality of life. Electronic health records are an essential component of healthcare systems. They are valuable for medical research, but the information is recorded as unstructured free text. Knowledge extraction (KE) from unstructured text in electronic health records is a problem but still not totally resolved. KE is very challenging because medical language has ungrammatical and fragmented constructions. We have implemented a unique framework KE based on linked open data for clinical documentation (KE-LODC) that generates accurate and high quality triples transforming unstructured text from clinical documentation into well-defined and ready-to-use linked open data for diagnosis and treatment. Our framework proved to produce a large number of highly qualified triple candidates which improves the likelihood of better classification.", "venue": "Int. J. Simul. Process. Model.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["lod", "lod", "lod", "ke", "ke"], "mention_counts": {"ke": 2, "lod": 3}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "lod": 3}, "relevance_score": 0.46718541496779636}, {"paperId": "54405ac6301d8342b3d17420209f327814b1f28f", "url": "https://www.semanticscholar.org/paper/54405ac6301d8342b3d17420209f327814b1f28f", "title": "Knowledge Extraction and Application for Metal Materials Based on DBpedia", "abstract": "Linked data is developing very fast and becoming more and more important in different domains. As a relatively-comprehensive linked data set, DBpedia contains billions of triples, which involves knowledge from diverse domains. This paper aims to utilize the metal materials knowledge in DBpeida to provide more useful services for materials experts. A knowledge extraction algorithm is designed to extract metal materials knowledge from DBpedia into a local knowledge base. Then, we develop an experimental prototype for metal materials information recommendation based on semantic distance calculation. The experimental results show that the system can help users retrieve metal knowledge originated from DBpedia rapidly and conveniently.", "venue": "2014 10th International Conference on Semantics, Knowledge and Grids", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ld", "ke", "ld"], "mention_counts": {"ld": 2, "kg": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ld": 2, "kg": 1, "ke": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "64ad134fa13221ed251d3d651298c636144201db", "url": "https://www.semanticscholar.org/paper/64ad134fa13221ed251d3d651298c636144201db", "title": "Learning to Adapt Web Information Extraction Knowledge and Discovering New Attributes via a Bayesian Approach", "abstract": "This paper presents a Bayesian learning framework for adapting information extraction wrappers with new attribute discovery, reducing human effort in extracting precise information from unseen Web sites. Our approach aims at automatically adapting the information extraction knowledge previously learned from a source Web site to a new unseen site, at the same time, discovering previously unseen attributes. Two kinds of text-related clues from the source Web site are considered. The first kind of clue is obtained from the extraction pattern contained in the previously learned wrapper. The second kind of clue is derived from the previously extracted or collected items. A generative model for the generation of the site-independent content information and the site-dependent layout format of the text fragments related to attribute values contained in a Web page is designed to harness the uncertainty involved. Bayesian learning and expectation-maximization (EM) techniques are developed under the proposed generative model for identifying new training data for learning the new wrapper for new unseen sites. Previously unseen attributes together with their semantic labels can also be discovered via another EM-based Bayesian learning based on the generative model. We have conducted extensive experiments from more than 30 real-world Web sites in three different domains to demonstrate the effectiveness of our framework.", "venue": "IEEE Transactions on Knowledge and Data Engineering", "citationCount": 60, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ie", "ke", "ie"], "mention_counts": {"ke": 2, "ie": 3}, "nlp_mention_counts": {"ke": 2, "ie": 3}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "4d6c92b88fd6af968f66b462f9434370378857fa", "url": "https://www.semanticscholar.org/paper/4d6c92b88fd6af968f66b462f9434370378857fa", "title": "Inferencing in information extraction: Techniques and applications", "abstract": "Information extraction at Web scale has become one of the most important research topics in data management since major commercial search engines started incorporating knowledge in their search results a couple of years ago [1]. Users increasingly expect structured knowledge as answers to their search needs. Using Bing as an example, the result page for \u201cLionel Messi\u201d is full of structured knowledge facts, such as his birthday and awards. The research efforts towards improving the accuracy and coverage of such knowledge bases have led to significant advances in Information Extraction techniques [2], [3]. As the initial challenge of accurately extracting facts for popular entities are being addressed, more difficult challenges have emerged such as extending knowledge coverage to long tail entities and domains, understanding interestingness and usefulness of facts within a given context, and addressing information-seeking needs more directly and accurately. In this tutorial, we will survey the recent research efforts and provide an introduction to the techniques that address those challenges, and the applications that benefit from the adoption of those techniques. In particular, this tutorial will focus on a variety of techniques that can be broadly viewed as knowledge inferencing, i.e., combining multiple data sources and extraction techniques to verify existing knowledge and derive new knowledge. More specifically, we focus on four main categories of inferencing techniques: 1) deep natural language processing using machine learning techniques, 2) data cleaning using integrity constraints, 3) large-scale probabilistic reasoning, and 4) leveraging human expertise for domain knowledge extraction.", "venue": "IEEE International Conference on Data Engineering", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "kg", "nlp", "ie", "ie"], "mention_counts": {"nlp": 1, "ke": 1, "kg": 1, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 3}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "39cc4f2936b5c1dbab0c5d80ef5b383eb9fb0219", "url": "https://www.semanticscholar.org/paper/39cc4f2936b5c1dbab0c5d80ef5b383eb9fb0219", "title": "A study of semantic integration across archaeological data and reports in different languages", "abstract": "This study investigates the semantic integration of data extracted from archaeological datasets with information extracted via natural language processing (NLP) across different languages. The investigation follows a broad theme relating to wooden objects and their dating via dendrochronological techniques, including types of wooden material, samples taken and wooden objects including shipwrecks. The outcomes are an integrated RDF dataset coupled with an associated interactive research demonstrator query builder application. The semantic framework combines the CIDOC Conceptual Reference Model (CRM) with the Getty Art and Architecture Thesaurus (AAT). The NLP, data cleansing and integration methods are described in detail together with illustrative scenarios from the web application Demonstrator. Reflections and recommendations from the study are discussed. The Demonstrator is a novel SPARQL web application, with CRM/AAT-based data integration. Functionality includes the combination of free text and semantic search with browsing on semantic links, hierarchical and associative relationship thesaurus query expansion. Queries concern wooden objects (e.g. samples of beech wood keels), optionally from a given date range, with automatic expansion over AAT hierarchies of wood types and specialised associative relationships. Following a \u2018mapping pattern\u2019 approach (via the STELETO tool) ensured validity and consistency of all RDF output. The user is shielded from the complexity of the underlying semantic framework by a query builder user interface. The study demonstrates the feasibility of connecting information extracted from datasets and grey literature reports in different languages and semantic cross-searching of the integrated information. The semantic linking of textual reports and datasets opens new possibilities for integrative research across diverse resources.", "venue": "Journal of information science", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "ie", "nlp", "nlp", "rdf", "nlp", "ie"], "mention_counts": {"nlp": 3, "rdf": 2, "ie": 2}, "nlp_mention_counts": {"nlp": 3, "ie": 2}, "ld_mention_counts": {"rdf": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "16c4396928f7edce6f17f55701091aed9f36c8f5", "url": "https://www.semanticscholar.org/paper/16c4396928f7edce6f17f55701091aed9f36c8f5", "title": "Triple Extraction of Knowledge Graphs with DGWPN", "abstract": "Information extraction aims to provide required high-quality RDF triples for knowledge graph which is the basis of knowledge construction. The current two mainstream methods include a joint model concentrates on using deep learning to extract entities and relationships and a pipeline model based on sequential extraction. The joint method will be limited by the number of parameters and the types of parameters, this will lead to the loss of calculation results. In the pipeline model, the result of entity extraction needs to be substituted into the relationship classification, and this usually causes the interference of redundant entities to the relationship classification. For the joint training model, in the training process, entities and relationships are extracted at the same time. The results of extractions are continuously transmitted to each other during training and influence each other. If a certain parameter limit is deleted during the extraction of one type of result, the extraction result of the other side will also be deleted of the same type, which will eventually cause some results to be lost. For the pipeline model, if entities are first extracted, then relations are classified. In the process of entity extraction, invalid entity pairs with no matching relationship will be generated. This causes the convolution training speed to slow down. In this paper, in order to solve such problems in entity and relationship extraction, we propose an entity relationship extraction model includes Dilated Gate unit convolutional layers with Weight Pointer Networks, named this structure as DGWPN. This is helpful to obtain more receptive fields and multiple triples models. The advantages of our model are demonstrated through the comparison of multiple groups of experiments. The experimental results show that our method outperforms other current different baseline models on the SKE and CHIP data sets, about 5.3% higher than the current mainstream Bi-LSTM with CRF model on precision, and about 5.6% higher on F1 value, which indicates that our model can extract more high-quality triples for the construction of knowledge graphs.", "venue": "2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ke", "ie", "kg", "rdf"], "mention_counts": {"kg": 3, "ke": 1, "rdf": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 3, "ke": 1, "rdf": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "b114a7e586f1b7281efe4501dd90a60f127eb1f7", "url": "https://www.semanticscholar.org/paper/b114a7e586f1b7281efe4501dd90a60f127eb1f7", "title": "SOFIE: a self-organizing framework for information extraction", "abstract": "This paper presents SOFIE, a system for automated ontology extension. SOFIE can parse natural language documents, extract ontological facts from them and link the facts into an ontology. SOFIE uses logical reasoning on the existing knowledge and on the new knowledge in order to disambiguate words to their most probable meaning, to reason on the meaning of text patterns and to take into account world knowledge axioms. This allows SOFIE to check the plausibility of hypotheses and to avoid inconsistencies with the ontology. The framework of SOFIE unites the paradigms of pattern matching, word sense disambiguation and ontological reasoning in one unified model. Our experiments show that SOFIE delivers high-quality output, even from unstructured Internet documents.", "venue": "The Web Conference", "citationCount": 276, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "wsd", "onto", "ie", "onto", "onto", "onto"], "mention_counts": {"wsd": 1, "onto": 5, "ie": 1}, "nlp_mention_counts": {"wsd": 1, "ie": 1}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "332e9338f1b821958e3a15aa838185743d18cb64", "url": "https://www.semanticscholar.org/paper/332e9338f1b821958e3a15aa838185743d18cb64", "title": "Towards Cross-Media Feature Extraction", "abstract": "In this paper we describe past and present work dealing with the use of textual resources, out of which semantic information can be extracted in order to provide for semantic annotation and indexing of associated image or video material. Since the emergence of semantic web technologies and resources, entities, relations and events extracted from textual resources by means of Information Extraction (IE) can now be marked up with semantic classes derived from ontologies, and those classes can be used for the semantic annotation and indexing of related image and video material. More recently our work aims additionally at taking into account extracted Audio-Video (A/V) features (such as motion, audio-pitch, close-up, etc.) to be combined with the results of Ontology-Based Information Extraction for the annotation and indexing of specific event types. As extraction of A/V features is then supported by textual evidence, and possibly also the other way around, our work can be considered as going towards a \u201ccrossmedia feature extraction\u201d, which can be guided by shared ontologies (Multimedia, Linguistic and Domain ontologies).", "venue": "AAAI Fall Symposium: Multimedia Information Extraction", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "sw", "onto", "ie", "onto", "ie", "onto"], "mention_counts": {"sw": 1, "onto": 4, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.46718541496779636}, {"paperId": "a3ddad9a904fb69ce59ff6c7d0f0b6b4e68ddbba", "url": "https://www.semanticscholar.org/paper/a3ddad9a904fb69ce59ff6c7d0f0b6b4e68ddbba", "title": "Description Ontologies", "abstract": "Semantic annotation and information extraction are of paramount importance in the field of unstructured information management. Although several sophisticated and indeed complex approaches were proposed, they still have many limitations. In this paper we present a novel ontological paradigm called description ontology in which objects and classes (concepts) can be equipped by a set of rules named concept descriptors. These rules represent patterns that express how to automatically recognize ontology concepts within unstructured documents. A description ontology allows the semantic extraction of information from unstructured document which, in turn, can be stored in structured forms. Moreover, document contents can also be annotated with respect to the recognized concepts.", "venue": "2008 Third International Conference on Digital Information Management", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "onto", "ie", "onto"], "mention_counts": {"onto": 5, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "1499644704aa9e9321277d587fc015dff2931980", "url": "https://www.semanticscholar.org/paper/1499644704aa9e9321277d587fc015dff2931980", "title": "Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications", "abstract": "We aim to build and evaluate an open-source natural language processing system for information extraction from electronic medical record clinical free-text. We describe and evaluate our system, the clinical Text Analysis and Knowledge Extraction System (cTAKES), released open-source at http://www.ohnlp.org. The cTAKES builds on existing open-source technologies-the Unstructured Information Management Architecture framework and OpenNLP natural language processing toolkit. Its components, specifically trained for the clinical domain, create rich linguistic and semantic annotations. Performance of individual components: sentence boundary detector accuracy=0.949; tokenizer accuracy=0.949; part-of-speech tagger accuracy=0.936; shallow parser F-score=0.924; named entity recognizer and system-level evaluation F-score=0.715 for exact and 0.824 for overlapping spans, and accuracy for concept mapping, negation, and status attributes for exact and overlapping spans of 0.957, 0.943, 0.859, and 0.580, 0.939, and 0.839, respectively. Overall performance is discussed against five applications. The cTAKES annotations are the foundation for methods and modules for higher-level semantic processing of clinical free-text.", "venue": "J. Am. Medical Informatics Assoc.", "citationCount": 1679, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "ie", "nlp", "nlp"], "mention_counts": {"nlp": 2, "ke": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "25d71f262f6f1f81cdc7becc51f140bf148d0b45", "url": "https://www.semanticscholar.org/paper/25d71f262f6f1f81cdc7becc51f140bf148d0b45", "title": "Providing grades and feedback for student summaries by ontology-based information extraction", "abstract": "Automatic grading systems for summaries and essays have been studied for years. Most commercial and research implementations are based in statistical methods, such as Latent Semantic Analysis (LSA), which can provide high accuracy on similarity between the essay and the graded or standard essays, but they can offer very limited feedback. In the present work, we propose a novel method to provide both grades and meaningful feedback for student summaries by Ontology-based Information Extraction (OBIE). We use ontological concepts and relationships to create extraction rules to identify correct statements. Based on ontology constraints (e.g., disjointness between concepts), we define patterns that are logically inconsistent with the ontology to create rules to extract incorrect statements. Experiments show that the grades given to 18 student summaries on Ecosystems by OBIE are correlated to human gradings. OBIE also provide meaningful feedback on the errors those students made in their summaries.", "venue": "CIKM", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "onto", "onto"], "mention_counts": {"onto": 5, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "084f413e5405dc5af37677d9dd98599c5ac62934", "url": "https://www.semanticscholar.org/paper/084f413e5405dc5af37677d9dd98599c5ac62934", "title": "Towards web information extraction using extraction ontologies and (indirectly) domain ontologies", "abstract": "Extraction ontologies allow to swiftly proceed from initial domain modelling to running a functional prototype of a web information extraction application. We investigate the possibility of semi-automatically deriving extraction ontologies from third-party domain ontologies.", "venue": "K-CAP '07", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "onto", "onto", "ie"], "mention_counts": {"onto": 5, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "1ff8cc68c3715f89160d8b64394c321b70a83f96", "url": "https://www.semanticscholar.org/paper/1ff8cc68c3715f89160d8b64394c321b70a83f96", "title": "Information Extraction from Historical Texts: a Case Study", "abstract": "In this paper a set of information extraction experiments over historical texts are described. The experiments were done over the Spanish book Observaciones de Curvo written by Suarez de Ribera in 1735 based on the 1707 Curvo Semedo\u2019s work Observaciones medicas doutrinaes de cem casos gravissimos to evaluate which information can be extracted in a fully automatized way. Using publicly available NLP tools we extracted named entities (persons and places) and identified events. This information was used to populate a specialized ontology, allowing the application of powerful visualization and inference processes. A preliminary evaluation of the quality of the extracted information showed that, in spite of the use of generic NLP tools, this process is able to automatically identify relevant information and to help human experts in the creation of historical knowledge bases.", "venue": "DHandNLP@PROPOR", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ie", "ie", "onto", "kg", "nlp", "ie"], "mention_counts": {"nlp": 2, "onto": 1, "kg": 1, "ie": 3}, "nlp_mention_counts": {"nlp": 2, "ie": 3}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "44e8dfc94242f99c99968be8333c37e7b795d5a0", "url": "https://www.semanticscholar.org/paper/44e8dfc94242f99c99968be8333c37e7b795d5a0", "title": "A survey of temporal information extraction and language independent features", "abstract": "Since there has been an explosive growth of online documents, the knowledge extraction from natural language texts becomes more important to complement limited human ability. In this paper, we introduce existing methods for temporal information extraction from input texts in two viewpoints. One is the researches about language-independent feature generation. Even though linguistic features have been generally utilized to extract time expressions, we need different features which are independent from language characteristics in order to overcome the boundary of particular language. Another is the researches about temporal information extraction based on the common patterns or knowledge bases. By summarizing and discussing existing researches, we can see the current research direction on this field to help better understanding of the temporal information extraction methods.", "venue": "International Conference on Big Data and Smart Computing", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "ie", "ie", "ke", "kg"], "mention_counts": {"ke": 1, "kg": 1, "ie": 4}, "nlp_mention_counts": {"ke": 1, "ie": 4}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "5f915b210e586fd731fe8660bfa3cb89a4d87fa3", "url": "https://www.semanticscholar.org/paper/5f915b210e586fd731fe8660bfa3cb89a4d87fa3", "title": "Integrating Information Extraction, Ontology Learning and Semantic Browsing into Organizational Knowledge Processes", "abstract": "Ontology-based approaches to Knowledge Management promise better access to relevant knowledge by providing a domain-specific vocabulary that is used for describing the contents of knowledge as well as for retrieving that knowledge. Despite the potential benefits, ontology-based approaches require a considerable amount of commitment and expertise in tasks like creating and maintaining ontologies, annotating documents and integrating information. We are studying and implementing a methodology for automating such tasks, which makes use of Information Extraction and Integration, Ontology Learning and Semantic Browsing to effectively acquire, share and reuse knowledge in an organizational setting. Applications of the proposed methodology are under development for knowledge management in the legal domain and in the field of biotechnology.", "venue": "LSTKM@EKAW", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "ie", "onto", "onto"], "mention_counts": {"onto": 5, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "bcb0addd0a7d5ddd5f565dd98b664dd5dbedcb1b", "url": "https://www.semanticscholar.org/paper/bcb0addd0a7d5ddd5f565dd98b664dd5dbedcb1b", "title": "Evaluating Evaluation Metrics for Ontology-Based Applications: Infinite Reflection", "abstract": "In this paper, we discuss methods of measuring the performance of ontology-based information extraction systems. We focus particularly on the Balanced Distance Metric (BDM), a new metric we have proposed which aims to take into account the more flexible nature of ontologically-based applications. We first examine why traditional Precision and Recall metrics, as used for flat information extraction tasks, are inadequate when dealing with ontologies. We then describe the Balanced Distance Metric (BDM) which takes ontological similarity into account. Finally, we discuss a range of experiments designed to test the accuracy and usefulness of the BDM when compared with traditional metrics and with a standard distance-based metric.", "venue": "LREC", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "onto", "onto", "ie"], "mention_counts": {"onto": 5, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "a10b8c3d61a90145d7e0986a6056dc76175c90fb", "url": "https://www.semanticscholar.org/paper/a10b8c3d61a90145d7e0986a6056dc76175c90fb", "title": "A Resource-Poor Approach for Linking Ontology Classes to Wikipedia Articles", "abstract": "The applicability of ontologies for natural language processing depends on the ability to link ontological concepts and relations to their realisations in texts. We present a general, resource-poor account to create such a linking automatically by extracting Wikipedia articles corresponding to ontology classes. We evaluate our approach in an experiment with the Music Ontology. We consider linking as a promising starting point for subsequent steps of information extraction.", "venue": "Conference on Semantics in Text Processing", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "ie", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 5, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "f5bd7049f29c1a4c657c4e2e97fa87635b0d1a75", "url": "https://www.semanticscholar.org/paper/f5bd7049f29c1a4c657c4e2e97fa87635b0d1a75", "title": "Ontology-Based Information Extraction Technology", "abstract": "the performance of information extraction can be effectively improved through use of ontology. Based on the discussion of constructing ontology technology and standards related to, on the basis of three layers framework to constructing ontology to resume ontology, university teachers Racer reasoning machine to implement consistency and accuracy testing. Based on this improved the WordNet similarity calculation, through the use of WordNet semantic similarity calculation and manually collecting method of combining the concept of this body, obtain the specific examples, the result proves that the result can be extracted remarkably enhanced.", "venue": "Information Theory and Applications Workshop", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "ie", "ie", "onto"], "mention_counts": {"onto": 5, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "a2fa549f1e598b60b3f8e90418e9bb157817bc39", "url": "https://www.semanticscholar.org/paper/a2fa549f1e598b60b3f8e90418e9bb157817bc39", "title": "Word Vector Embeddings and Domain Specific Semantic based Semi-Supervised Ontology Instance Population", "abstract": "An ontology defines a set of representational primitives which model a domain of knowledge or discourse. With the arising fields such as information extraction and knowledge management, the role of ontology has become a driving factor of many modern day systems. Ontology population, on the other hand, is an inherently problematic process, as it needs manual intervention to prevent the conceptual drift. The semantic sensitive word embedding has become a popular topic in natural language processing with its capability to cope with the semantic challenges. Incorporating domain specific semantic similarity with the word embeddings could potentially improve the performance in terms of semantic similarity in specific domains. Thus, in this study, we propose a novel way of semi-supervised ontology population through word embeddings and domain specific semantic similarity as the basis. We built several models including traditional benchmark models and new types of models which are based on word embeddings. Finally, we ensemble them together to come up with a synergistic model which outperformed the candidate models by 33% in comparison to the best performed candidate model.", "venue": "International Journal on Advances in ICT for Emerging Regions (ICTer)", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "nlp", "onto", "ie"], "mention_counts": {"nlp": 1, "onto": 5, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "b81362cce2e1c7a057c705207a60eb27f3483847", "url": "https://www.semanticscholar.org/paper/b81362cce2e1c7a057c705207a60eb27f3483847", "title": "Integration and Querying of Genomic and Proteomic Semantic Annotations for Biomedical Knowledge Extraction", "abstract": "Understanding complex biological phenomena involves answering complex biomedical questions on multiple biomolecular information simultaneously, which are expressed through multiple genomic and proteomic semantic annotations scattered in many distributed and heterogeneous data sources; such heterogeneity and dispersion hamper the biologists' ability of asking global queries and performing global evaluations. To overcome this problem, we developed a software architecture to create and maintain a Genomic and Proteomic Knowledge Base (GPKB), which integrates several of the most relevant sources of such dispersed information (including Entrez Gene, UniProt, IntAct, Expasy Enzyme, GO, GOA, BioCyc, KEGG, Reactome, and OMIM). Our solution is general, as it uses a flexible, modular, and multilevel global data schema based on abstraction and generalization of integrated data features, and a set of automatic procedures for easing data integration and maintenance, also when the integrated data sources evolve in data content, structure, and number. These procedures also assure consistency, quality, and provenance tracking of all integrated data, and perform the semantic closure of the hierarchical relationships of the integrated biomedical ontologies. At http://www.bioinformatics.deib.polimi.it/GPKB/, a Web interface allows graphical easy composition of queries, although complex, on the knowledge base, supporting also semantic query expansion and comprehensive explorative search of the integrated data to better sustain biomedical knowledge extraction.", "venue": "IEEE/ACM Transactions on Computational Biology & Bioinformatics", "citationCount": 20, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["kg", "ke", "kg", "onto", "ke"], "mention_counts": {"kg": 2, "onto": 1, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 2, "onto": 1, "ke": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "9b083cfa0894121ce865bacfe0bbdddbf513e501", "url": "https://www.semanticscholar.org/paper/9b083cfa0894121ce865bacfe0bbdddbf513e501", "title": "Automating the Integration of Clinical Studies into Medical Ontologies", "abstract": "A popular approach to knowledge extraction from clinical databases is to first define an ontology of the concepts one wishes to model and subsequently, use these concepts to test various hypotheses and make predictions about a person's future health and well being. The challenge for medical experts is in the time taken to map between their concepts/hypotheses and information contained within clinical studies. Presently, most of this work is performed manually. We have developed a method to generate links between Risk Factors in a medical ontology and the questions and result data in longitudinal studies. This can then be exploited to express complex queries based on domain concepts, to extract knowledge from external studies.", "venue": "2014 47th Hawaii International Conference on System Sciences", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ke", "onto", "ke"], "mention_counts": {"ke": 2, "onto": 3}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 3}, "relevance_score": 0.46718541496779636}, {"paperId": "b3331afeb6d27035fa0cde80867d37e28d914076", "url": "https://www.semanticscholar.org/paper/b3331afeb6d27035fa0cde80867d37e28d914076", "title": "Artificial intelligence for ocean science data integration: current state, gaps, and way forward", "abstract": "Oceanographic research is a multidisciplinary endeavor that involves the acquisition of an increasing amount of in-situ and remotely sensed data. A large and growing number of studies and data repositories are now available on-line. However, manually integrating different datasets is a tedious and grueling process leading to a rising need for automated integration tools. A key challenge in oceanographic data integration is to map between data sources that have no common schema and that were collected, processed, and analyzed using different methodologies. Concurrently, artificial agents are becoming increasingly adept at extracting knowledge from text and using domain ontologies to integrate and align data. Here, we deconstruct the process of ocean science data integration, providing a detailed description of its three phases: discover, merge, and evaluate/correct. In addition, we identify the key missing tools and underutilized information sources currently limiting the automation of the integration process. The efforts to address these limitations should focus on (i) development of artificial intelligence-based tools for assisting ocean scientists in aligning their schema with existing ontologies when organizing their measurements in datasets; (ii) extension and refinement of conceptual coverage of \u2013 and conceptual alignment between \u2013 existing ontologies, to better fit the diverse and multidisciplinary nature of ocean science; (iii) creation of ocean-science-specific entity resolution benchmarks to accelerate the development of tools utilizing ocean science terminology and nomenclature; (iv) creation of ocean-science-specific schema matching and mapping benchmarks to accelerate the development of matching and mapping tools utilizing semantics encoded in existing vocabularies and ontologies; (v) annotation of datasets, and development of tools and benchmarks for the extraction and categorization of data quality and preprocessing descriptions from scientific text; and (vi) creation of large-scale word embeddings trained upon ocean science literature to accelerate the development of information extraction and matching tools based on artificial intelligence.", "venue": "Elementa: Science of the Anthropocene", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ke", "ie", "onto", "onto"], "mention_counts": {"ke": 1, "onto": 4, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 4}, "relevance_score": 0.46718541496779636}, {"paperId": "289fa0bab75d50646cf94f3930940d5c29493883", "url": "https://www.semanticscholar.org/paper/289fa0bab75d50646cf94f3930940d5c29493883", "title": "IDRA: An ontology driven Cognitive Computing System", "abstract": "The problem I intend to address in my PhD research is t e leak of quality of the information extracted from Big Data u sing only a statistical approach. Google Flu is an explanatory example of how the knowledge extraction of Big Data based only on statistical methods could produce low quality results and lead to a misinterpretation of reality. Could it be feasible and effective to design a system that is able to leverage ontologies to balance the limits of statistical methods? Here I present an architecture of an ontology driven Cognitive Computing System that leverages ontologies to filter s atistical data.", "venue": "DC@ISWC", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ke", "onto", "ie"], "mention_counts": {"ke": 1, "onto": 4, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 4}, "relevance_score": 0.46718541496779636}, {"paperId": "c2d143c018b161e6abe05010c6a489d2cc4642ee", "url": "https://www.semanticscholar.org/paper/c2d143c018b161e6abe05010c6a489d2cc4642ee", "title": "Multi-page list extraction: an agent-oriented approach to user-driven information extraction", "abstract": "A major problem with information extraction (IE) systems is that it is difficult to handle a large number of domains. This problem can be handled in various ways. One way is to try to create a system that is as general as possible and can extraction information from a large number of domains. Another way is to create a system that can be trained to extract information. To make such a solution useful, users should not be required to spend a lot of time training or required to have expertise knowledge. If it would be possible for average users to train IE systems to extract information, then systems can be created for a large number of domains and for a large number of users. This is called a user-driven approach. A system called ISSIE has been developed to evaluate this user-driven approach, and that take advantage of semistructured information to extract relevant pieces. The system is built using the JADE agent framework, together with other tools for working with ontologies and knowledge bases. The goal with that system is to be able to train-by-example to handle a new domain. That means that a user shall be able to train the system by simply surfing the Web using a traditional browser and only give small hints about what information that are to be extracted. The system monitors the traffic and behavior of the user and then tries to automate the extraction process. By monitoring the traffic from the Web browser, and trying to understand the communication, the system is able to handle extraction from advanced Web sites. The focus in this system is to be able to extract various types of lists, e.g. a list of products at a retailer's Web site. There are currently four types of lists: singleton, simple lists, complex lists, and multipage lists. The focus in this paper is management of multipage lists. A multipage list is divided into several pages and the system must be able to navigate to and extract from these pages. A set of experiments has been conducted to evaluate the approach and the management of multipage lists. The results showed that the multipage lists extraction works well but there are some problems. However, in general, the approach is promising and shows that a user-driven approach for multi-page list extraction is viable.", "venue": "International Conference on Integration of Knowledge Intensive Multi-Agent Systems, 2005.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "ie", "ie", "onto", "ie", "kg"], "mention_counts": {"kg": 1, "onto": 1, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"kg": 1, "onto": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "48dc3f8c080426d475715ecee163df2b177e4ebd", "url": "https://www.semanticscholar.org/paper/48dc3f8c080426d475715ecee163df2b177e4ebd", "title": "Understanding the Web through its Language", "abstract": "A tight integration between ontological and linguistic knowledge is critical within the information processes of the Semantic Web. In Information Extraction, ontologies should include knowledge components neglected in domain conceptualizations generally used for other tasks. In this paper, we analyze such critical information in the light of existing applications. Accordingly, a methodology for semi-automatic development of an IE ontology integrating pre-existing domain and lexical knowledge is presented. The proposed ontological framework supports the discovery of new relations among known concepts by means of text processing, but also induction of new conceptual information.", "venue": "IEEE/WIC/ACM International Conference on Web Intelligence (WI'04)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["tp", "onto", "sw", "onto", "ie", "onto", "onto"], "mention_counts": {"sw": 1, "onto": 4, "tp": 1, "ie": 1}, "nlp_mention_counts": {"tp": 1, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.46718541496779636}, {"paperId": "5348f6b7c9056312f8f4c04c844ccdbca34bd5d8", "url": "https://www.semanticscholar.org/paper/5348f6b7c9056312f8f4c04c844ccdbca34bd5d8", "title": "Semi-automatic dictionary curation for domain-specific ontologies", "abstract": "Within the broad area of information extraction, we study the problem of effective dictionary curation in an enterprise setting. Equipped with an ontology, representative of the domain of an enterprise, our approach populates the attributes of leaf nodes of the ontology with instances extracted from the enterprise corpus. For an attribute of interest, given a few seed examples or indicative features for the attribute, we first obtain a ranked list of 'list pages' potentially containing additional dictionary terms. Our ranking model ranks pages from the enterprise corpus based on their 'list' content using several visual and lexical features. We gather users' judgement of the result pages and the model continuously learns from this feedback. We compare different techniques of dictionary curation using rule based extractors and visual features of pages. Based on rule writing exercise, we show the benefit of dictionaries for leaf node attributes, in writing rule based extractors for higher level nodes in an ontology. We have implemented a dictionary curation system based on these ideas. Experimental analysis using academic domain ontology and universities corpora, reveal (in the context of enterprise analytics) (i) the merit of dictionary support in rule based information extraction (ii) the viability and effectiveness of an interactive approach for dictionary creation.", "venue": "2013 IEEE 25th International Conference on Tools with Artificial Intelligence", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "onto", "onto", "ie"], "mention_counts": {"onto": 5, "ie": 2}, "nlp_mention_counts": {"ie": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "bf3ee9a9021ccc332a7767a267f4ad00e329e5ee", "url": "https://www.semanticscholar.org/paper/bf3ee9a9021ccc332a7767a267f4ad00e329e5ee", "title": "SpiNet - A FrameNet-like Schema for Automatic Information Extraction about Spine from Scientific Papers", "abstract": "New medical research concerning the spine and its diseases are incrementally made available through biomedical literature repositories. Several Natural Language Processing (NLP) tasks, like Semantic Role Labelling (SRL) and Information Extraction (IE), can offer support for, automatically, extracting relevant information about spine, from scientific papers. This paper presents a domain-specific FrameNet, called SpiNet, for automatic information extraction about spine concepts and their semantic types. For this, we use the frame semantic and the MeSH ontology in order to extract the relevant information about a disease, a treatment, a medication, a sign or symptom, related to spine medical domain. The differential of this work is the enrichment of SpiNet's base with the MeSH ontology, whose terms, concepts, descriptors and semantic types enable automatic semantic annotation. We use the SpiNet framework in order to annotate one hundred of scientific papers and the F1-score metric, calculated between the classification of relevant sentences performed by the system and the human physiotherapists, achieved the result of 0.83.", "venue": "AMIA", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "onto", "ie", "onto", "ie", "nlp", "ie"], "mention_counts": {"nlp": 2, "onto": 2, "ie": 3}, "nlp_mention_counts": {"nlp": 2, "ie": 3}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "c173f41c0a6b6927cef610f4fb2fbbeeece4f513", "url": "https://www.semanticscholar.org/paper/c173f41c0a6b6927cef610f4fb2fbbeeece4f513", "title": "Knowledge extraction from online feedback system using ontology", "abstract": "To achieve success in any teaching program it is very important to have adequate communication between teachers and students. Feedback given by student helps teachers to improve on performance and behavior. Semantic web (SW) technology is a promising approach for data selection and retrieval. Different web mining techniques are used for extracting useful information from web data. In this paper the main focus is to extract knowledge from the feedback given by the students and this can be done by firing Sparql Query in Ontology. This knowledge can be represented in meaningful form.", "venue": "2015 International Conference on Technologies for Sustainable Development (ICTSD)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ke", "onto", "onto", "ke"], "mention_counts": {"sw": 1, "onto": 2, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"sw": 1, "onto": 2, "ke": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "f5cbdebd0d83f015b142a166ba7bef9455348a18", "url": "https://www.semanticscholar.org/paper/f5cbdebd0d83f015b142a166ba7bef9455348a18", "title": "Medical Adverse Events Classification for Domain Knowledge Extraction", "abstract": "In the viewpoint of sharing knowledge, users need to input keywords before the agent retrievals the related information from the Internet. At the same time, the traditional method ignored the true meaning of the terms. However, the semantic web has been created to improve the disadvantage. Ontology is the fundamental element of the semantic web that is a kind of knowledge presentation and can present the variety of property for concept.According to the regulation of Department of Health, Executive Yuan, R.O.C, the hospital has implemented the new Hospital Accreditation from 2005. Furthermore, the Hospital Accreditation evolves the variety of items of accreditation. Medical adverse event reporting is a kind of the Hospital Accreditation. The system which can deal with the medical adverse event reporting is operated by human at present. None the less, the system caused some questions, such as inconsistent knowledge and human negligence. The research proposed a new framework which can replace the human automatically. We get concepts through extraction of information and analyze sentences through classification of events. After that we analyze those classes which can construct variable and dynamic ontology in medical adverse event reporting.", "venue": "2009 Ninth International Conference on Hybrid Intelligent Systems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "onto", "ke", "sw", "onto", "ie"], "mention_counts": {"sw": 2, "onto": 2, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"sw": 2, "onto": 2, "ke": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "b8c6ff63ba23dcec60d964c802d3abdd12899e19", "url": "https://www.semanticscholar.org/paper/b8c6ff63ba23dcec60d964c802d3abdd12899e19", "title": "Knowledge Extraction of Long-Term Complications from Clinical Narratives of Blood Cancer Patients with HCT Treatments", "abstract": "Interactive information extraction (IE) systems supported by biomedical ontologies are intelligent natural language processing (NLP) tools to understand literature and clinical narratives and discover meaningful domain knowledge from unstructured text. This study developed integrated IE systems to detect treatment complications of blood cancer patients from Electrical Medical Records (EMR) in the Long-Term Follow-Up (LTFU) protocol following Hematopoietic Cell Transplantation (HCT). The performance of the proposed approach was very encouraging compared to the gold-standard datasets manually reviewed by domain experts. In addition, the NLP system identified significant amount of cases not caught by experts.", "venue": "BCB", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "nlp", "ie", "onto", "nlp", "ke"], "mention_counts": {"nlp": 3, "onto": 1, "ke": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "10ecfcfc6401fd203cc065607df5da845c0340e9", "url": "https://www.semanticscholar.org/paper/10ecfcfc6401fd203cc065607df5da845c0340e9", "title": "Knowledge mining of unstructured information: application to cyber-domain", "abstract": "Information on cyber-related crimes, incidents, and con\ufb02icts is abundantly available in numerous open online sources. However, processing the large volumes and streams of data is a challenging task for the analysts and experts, and entails the need for newer methods and techniques. In this article we present and implement a novel knowledge graph and knowledge mining framework for extracting the relevant information from free-form text about incidents in the cyberdomain. The framework includes a machine learning based pipeline for generating graphs of organizations, countries, industries, products and attackers with a non-technical cyber-ontology. The extracted knowledge graph is utilized to estimate the incidence of cyberattacks on a given graph con\ufb01guration. We use publicly available collections of real cyber-incident reports to test the ef\ufb01cacy of our methods. The knowledge extraction is found to be suf\ufb01ciently accurate, and the graph-based threat estimation demonstrates a level of correlation with the actual records of attacks. In practical use, an analyst utilizing the presented framework can infer additional information from the current cyber-landscape in terms of risk to various entities and propagation of the risk heuristic between industries and countries.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "kg", "onto", "ke"], "mention_counts": {"ke": 2, "onto": 1, "kg": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1, "kg": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "cf8e8b40dec3a184b90c4d217287f202a2b573c4", "url": "https://www.semanticscholar.org/paper/cf8e8b40dec3a184b90c4d217287f202a2b573c4", "title": "On-Demand Distributional Semantic Distance and Paraphrasing", "abstract": "Semantic distance measures aim to answer questions such as: How close in meaning are words A and B? Fore example: \"couch\" and \"sofa\"? (very); \"wave\" and \"ripple\"? (soso); \"wave\" and \"bank\"? (far). Distributional measures do that by modeling which words occur next to A and next to B in large corpora of text, and then comparing these models of A and B (based on the \"Distributional Hypothesis\"). Paraphrase generation is the task of finding B (or a set of B's) given A. Semantic distance measures can be used for both paraphrase detection and generation, in assessing this closeness between A and B. Both semantic measures and paraphrasing methods are extensible to other textual units such as phrases, sentences, or documents. \n \nParaphrase detection and generation have been gaining traction in various NLP subfields, including: \n \n\u2022 Statistical machine translation (e.g., phrase table expansion) \n \n\u2022 MT evaluation (e.g., TERp or Meteor) \n \n\u2022 Search, information retrieval and information extraction (e.g., query expansion) \n \n\u2022 Question answering and Watson-like applications (e.g., passage or document clustering) \n \n\u2022 Event extraction/event discovery/machine reading (e.g, fitting to existing frames) \n \n\u2022 Ontology expansion (e.g., WordNet) \n \n\u2022 Language modeling (e.g., semantic LM) \n \n\u2022 Textual entailment \n \n\u2022 (Multi-)document summarization and natural language generation \n \n\u2022 Sentiment analysis and opinion/social network mining (e.g., expansion of positive and negative classes) \n \n\u2022 Computational cognitive modeling \n \nThis tutorial concentrates on paraphrasing words and short word sequences, a. k. a. \"phrases\" -- and doing so overcoming previous working memory and representation limitations. We focus on distributional paraphrasing (Pasca and Dienes 2005; Marton et al., 2009; Marton, to appear 2012). We will also cover pivot paraphrasing (Bannard and Callison-Burch, 2005). \n \nWe will discuss several weaknesses of distributional paraphrasing, and where the state-of-the-art is. The most notable weakness of distributional paraphrasing is its tendency to rank high antonymous (e.g., big-small) and ontological sibling (e.g., cow-sheep) paraphrase candidates. What qualitative improvement can we hope to achieve with growing size of monolingual texts? What else can be done to ameliorate this problem? (Mohammad et al., EMNLP 2008; Hovy, 2010; Marton et al., WMT 2011). \n \nAnother potential weakness is the difficulty in detecting and generating longer-than-word (phrasal) paraphrases, because pre-calculating a collocation matrix for phrases becomes prohibitive in the matrix size with longer phrases, even with sparse representation. Unless all phrases are known in advance, this becomes a problem for real-world applications. \n \nWe will present an alternative to pre-calculation: on-demand paraphrasing, as described in Marton (to appear 2012). There, searching the monolingual text resource is done on-demand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007). This enables constructing large vector representation, since there is no longer a need to compute a whole matrix. Searching for paraphrase candidates can be done in a reasonable amount of time and memory, for phrases and paraphrases of an arbitrary maximal length. The resulting technique enables using richer -- and hence, potentially more accurate -- representations (including higher-dimension tensors). It opens up a great potential for further gains in research and product systems alike, from SMT to search and IR, event discovery, and many other NLP areas.", "venue": "NAACL", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlg", "mt", "ie", "nlp"], "mention_counts": {"onto": 2, "nlp": 2, "nlg": 1, "mt": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "nlg": 1, "mt": 1, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "be1cc4c865115c041d66639e2f7d3b2782219f63", "url": "https://www.semanticscholar.org/paper/be1cc4c865115c041d66639e2f7d3b2782219f63", "title": "From text to RDF", "abstract": "This demonstration will present automatic knowledge extraction from documents in different languages. The semantic extraction is done according to an ontology describing persons and organizations, their relations and the activities of persons. \n \nThe result of the extracted knowledge is stored in a triplestore. An interface is used to explore interactively the RDF extracted from the document. The text is presented with colored entities and actions, pronouns and their antecedent are linked, attributes of entities and description of events can be popped-up by clicking on words.", "venue": "OAIR", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "rdf", "ke", "ke", "rdf"], "mention_counts": {"ke": 2, "onto": 1, "rdf": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1, "rdf": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "f4f060b1fced6c4b13688db0975b1470d72e392f", "url": "https://www.semanticscholar.org/paper/f4f060b1fced6c4b13688db0975b1470d72e392f", "title": "Building Knowledge Graphs and Recommender Systems for Suggesting Reskilling and Upskilling Options from the Web", "abstract": "As advances in science and technology, crisis, and increased competition impact labor markets, reskilling and upskilling programs emerged to mitigate their effects. Since information on continuing education is highly distributed across websites, choosing career paths and suitable upskilling options is currently considered a challenging and cumbersome task. This article, therefore, introduces a method for building a comprehensive knowledge graph from the education providers\u2019 Web pages. We collect educational programs from 488 providers and leverage entity recognition and entity linking methods in conjunction with contextualization to extract knowledge on entities such as prerequisites, skills, learning objectives, and course content. Slot filling then integrates these entities into an extensive knowledge graph that contains close to 74,000 nodes and over 734,000 edges. A recommender system leverages the created graph, and background knowledge on occupations to provide a career path and upskilling suggestions. Finally, we evaluate the knowledge extraction approach on the CareerCoach 2022 gold standard and draw upon domain experts for judging the career paths and upskilling suggestions provided by the recommender system.", "venue": "Inf.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "kg", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 3}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 3}, "relevance_score": 0.46718541496779636}, {"paperId": "bfc7ebe67a88c346b30e9ed5e84054e2056ff191", "url": "https://www.semanticscholar.org/paper/bfc7ebe67a88c346b30e9ed5e84054e2056ff191", "title": "Beyond Tasks, Methods, and Metrics: Extracting Metrics-driven Mechanism from the Abstracts of AI Articles", "abstract": "Generally speaking, a scientific paper presents the result of a specific research area and provides a solution to the research question. With the exponential expansion in the number of scientific publications, a large amount of valuable information is submerged. Although existing information extraction methods can extract entities and relations, they are unable to directly provide readers with the mechanism that reveals the path to solve the problem. Inspired by the biomedical research of medical mechanism, in this paper, we propose a novel knowledge schema, i.e., metrics-driven mechanism knowledge schema ( Operation, Effect, Direction ), which depict the knowledge about \u201cHow to optimize the quantitative and qualitative metrics of a specific task?\u201d . Furthermore, we select the natural language processing domain for practice, which is a representative branch of Artificial Intelligence (AI). Specifically, we construct a mechanism sentence extraction dataset and a mechanism triple extraction dataset using abstract data from ACL papers based on the proposed schema. Then we propose a metrics-driven mechanism knowledge extraction pipeline based on the pre-trained model. Finally, a knowledge base of metrics-driven mechanisms in the natural language processing (NLP) domain, named NLPMKB, is constructed. The human evaluation results show that the extracted mechanism knowledge from NLPMKB is high-quality with 87.0% precision and 79.4% recall. Moreover, the experiments on the knowledge retrieval task demonstrate that the performance can be further improved with the support of the NLPMKB.", "venue": "EEKE@JCDL", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "nlp", "nlp", "kg", "ie"], "mention_counts": {"nlp": 3, "kg": 1, "ke": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "11212ee2fa4672733deac5dd18bc7603ffc82589", "url": "https://www.semanticscholar.org/paper/11212ee2fa4672733deac5dd18bc7603ffc82589", "title": "Extracting Knowledge With Constructivist Machine Learning: Conceptual and Procedural Models", "abstract": "As the most essential knowledge engineering technique, the process of knowledge extraction has been widely researched and applied in many domains of computer science. While other knowledge engineering methods have lately received increasing attention in combination with machine learning, the extraction of structured knowledge is rarely addressed with other than standard machine learning tasks like classification. To this end, we have introduced a novel approach for hybrid artificial intelligence based on constructivist learning theories. Termed Constructivist Machine Learning (conML), our frameworks provides improved interpretability by utilizing metadata for the creation and management of a data-driven knowledge base. Here, we explain and demonstrate how the conML framework may be employed to extract procedural or conceptual knowledge from data where a time stamp, sensor ID and specific purpose are available as metadata for each data sample. As an illustrative example, we extract both conceptual and procedural models from data modelled after spectroscopic measurements. For the resulting procedural and conceptual knowledge bases, we observe an automated generation and adaption of models with explicitly defined validity and ranging over up to three levels of abstraction. From this, we conclude that a constructivist knowledge base provides valuable insights into a given data set.", "venue": "AAAI Spring Symposium: MAKE", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "kg", "kg"], "mention_counts": {"kg": 3, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 3, "ke": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "a2099c5c6252216cb5b8c40594c8980e4a91e924", "url": "https://www.semanticscholar.org/paper/a2099c5c6252216cb5b8c40594c8980e4a91e924", "title": "Understanding Graph Structure of Wikipedia for Query Expansion", "abstract": "Knowledge bases are very good sources for knowledge extraction, the ability to create knowledge from structured and unstructured sources and use it to improve automatic processes as query expansion. However, extracting knowledge from unstructured sources is still an open challenge [9]. In this respect, understanding the structure of knowledge bases can provide significant benefits for the effectiveness of such purpose. In particular, Wikipedia has become a very popular knowledge base in the last years because it is a general encyclopedia that has a large amount of information and thus, covers a large amount of different topics. In this piece of work, we analyze how articles and categories of Wikipedia relate to each other and how these relationships can support a query expansion technique. In particular, we show that the structures in the form of dense cycles with a minimum amount of categories tend to identify the most relevant information.", "venue": "GRADES@SIGMOD/PODS", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ke", "ke", "kg"], "mention_counts": {"kg": 3, "ke": 2}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"kg": 3, "ke": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "2ad36e7d6e760d8078b181d80a7f1083bb44ef34", "url": "https://www.semanticscholar.org/paper/2ad36e7d6e760d8078b181d80a7f1083bb44ef34", "title": "ALBERT with Knowledge Graph Encoder Utilizing Semantic Similarity for Commonsense Question Answering", "abstract": "Recently, pre-trained language representation models such as bidirectional encoder representations from transformers (BERT) have been performing well in commonsense question answering (CSQA). However, there is a problem that the models do not directly use explicit information of knowledge sources existing outside. To augment this, additional methods such as knowledge-aware graph network (KagNet) and multi-hop graph relation network (MHGRN) have been proposed. In this study, we propose to use the latest pre-trained language model a lite bidirectional encoder representations from transformers (ALBERT) with knowledge graph information extraction technique. We also propose to applying the novel method, schema graph expansion to recent language models. Then, we analyze the effect of applying knowledge graph-based knowledge extraction techniques to recent pre-trained language models and confirm that schema graph expansion is effective in some extent. Furthermore, we show that our proposed model can achieve better performance than existing KagNet and MHGRN models in CommonsenseQA dataset.", "venue": "Intelligent Automation &amp; Soft Computing", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ie", "kg", "ke", "kg"], "mention_counts": {"kg": 4, "ke": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 4, "ke": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "c54f38857d25315ad1ca4024010cfd985d361e9b", "url": "https://www.semanticscholar.org/paper/c54f38857d25315ad1ca4024010cfd985d361e9b", "title": "Feature Generation for Text Categorization Using World Knowledge", "abstract": "We enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge. This knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the Open Directory; these ontologies are further enriched by several orders of magnitude through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words. Feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses the two main problems of natural language processing--synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone. Experimental results confirm improved performance, breaking through the plateau previously reached in the field.", "venue": "International Joint Conference on Artificial Intelligence", "citationCount": 306, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "kg", "onto", "onto", "nlp", "wsd"], "mention_counts": {"nlp": 1, "kg": 1, "wsd": 1, "onto": 4}, "nlp_mention_counts": {"nlp": 1, "wsd": 1}, "ld_mention_counts": {"kg": 1, "onto": 4}, "relevance_score": 0.46718541496779636}, {"paperId": "beb5c87a82e59b7a0c7dd55963721cfd2ee04b84", "url": "https://www.semanticscholar.org/paper/beb5c87a82e59b7a0c7dd55963721cfd2ee04b84", "title": "Design for the environment: An ontology\u2010based knowledge management model for green product development", "abstract": "Through appropriate operations and policies, such as green processes and product development (PDP), companies can respond to environmental sustainability. To remain competitive, one such approach, Design for X (DFX), involves considering the different environment and sustainable strategies through different factors Xs. With regards to the availability of different DFX techniques that consider environmental issues, the decision as to which approach needs to be adopted remains absent. This paper aims at presenting an overview from 1980 to 2020 of the developed research, applications, and DFX techniques for the assessment of green issues. Selected DFX techniques are linked with strategies used in organizations. Following a literature analysis, a collaborative knowledge-based framework that addresses the design concepts needed to assess environmental, safety, and health concerns in the development of the green product is proposed. Furthermore, as a pillar for considering the Semantic Web and an evolving approach linked with Natural language processing (NLP) and Artificial Intelligence (AI), an ontology-based knowledge management model for green assessment is developed for the representation, acquisition, organization and capitalization of knowledge in a computer interpretable manner. The findings are useful for both managers and practitioners as they provide a coherent domain ontology that can help them manage knowledge, improve teamwork, and make decisions in a collaborative green PDP. Besides, an understanding of the essential design considerations that are required to implement environmental, safety, and health issues, as well as competencies used in the PDP is presented. The key barriers, managerial and strategic implications and mitigation actions are also identified in this paper.", "venue": "Business Strategy and the Environment", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "kg", "sw", "nlp", "nlp", "kg", "onto"], "mention_counts": {"nlp": 2, "kg": 2, "sw": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 2, "sw": 1, "onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "790e6ecb74aa7bb6f8c9d93e656f9a193be20195", "url": "https://www.semanticscholar.org/paper/790e6ecb74aa7bb6f8c9d93e656f9a193be20195", "title": "The Building of a CBD-Based Domain Ontology in Chinese", "abstract": "This paper describes a method of building a medical ontology prototype in Chinese. During the procedure, we explored the following questions, which are crucial for the task of ontology engineering: (1) are there some more computer-understandable knowledge description models? We proposed a structural and fine-grained knowledge description model called concept-based description model (CBD) to describe the rich knowledge in the ontology. That is, to use other concept or the combination of the related concepts to represent the targeted concept, which is supposed to be more computer-understandable; (2) during large scale ontology engineering, how to use NLP technologies to reduce domain expertspsila work to the minimal? In our work, we used some NLP technologies to try to reduce domain expertspsila work to the minimal as possible as it can. The experiments show the significance of our method.", "venue": "2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Materials Science"], "mentions": ["onto", "onto", "onto", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 5}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "ab1399d256a00f0800862d803ac07bfa37db9078", "url": "https://www.semanticscholar.org/paper/ab1399d256a00f0800862d803ac07bfa37db9078", "title": "Enhanced semantic access to the protein engineering literature using ontologies populated by text mining", "abstract": "The biomedical literature is growing at an ever-increasing rate, which pronounces the need to support scientists with advanced, automated means of accessing knowledge. We investigate a novel approach employing description logics (DL)-based queries made to formal ontologies that have been created using the results of text mining full-text research papers. In this paradigm, an OWL-DL ontology becomes populated with instances detected through natural language processing (NLP). The generated ontology can be queried by biologists using DL reasoners or integrated into bioinformatics workflows for further automated analyses. We demonstrate the feasibility of this approach with a system targeting the protein mutation literature.", "venue": "International Journal of Bioinformatics Research and Applications", "citationCount": 17, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["onto", "onto", "nlp", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 5}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "ae2d369388ce1aded8eada5e17fad3317ca144e1", "url": "https://www.semanticscholar.org/paper/ae2d369388ce1aded8eada5e17fad3317ca144e1", "title": "Converting Mikrokosmos Frames into Description Logics", "abstract": "Mikrokosmos contains an ontology plus a number of lexicons in different languages that were originally developed for machine translation. The underlying representation formalism for these resources is an ad-hoc frame-based language which makes it difficult to inter-operate Mikrokosmos with state-of-the-art knowledge-based systems. \n \nIn this paper we propose a translation from the frame-based representation of Mikrokosmos into Description logics. This translation allows us to automatically transform Mikrokosmos sources into OWL and thus provide a powerful ontology in the formalism of the semantic web. Furthermore, the reasoning mechanisms of Description Logics may also support knowledge acquisition and maintenance as well as its application in natural language processing systems.", "venue": "NLPXML@ACL", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "nlp", "onto", "sw", "kg", "onto"], "mention_counts": {"onto": 3, "nlp": 1, "sw": 1, "mt": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "mt": 1}, "ld_mention_counts": {"sw": 1, "onto": 3, "kg": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "a10de250233411ebae768567e977b569ccc02817", "url": "https://www.semanticscholar.org/paper/a10de250233411ebae768567e977b569ccc02817", "title": "Application of sentence parsing for determining keywords in Ukrainian texts", "abstract": "The article presents the use of generative grammars in linguistic modeling. A description of sentence syntax modeling is used to automate the process of analysis and synthesis of natural language texts. The article reveals the features of synthesizing sentences of different languages with the use of generative grammars. The article examines influence of norms and rules of a language on the process of constructing grammars. The use of generative grammars has great potential in the development and creation of automated systems for text content processing, linguistic support for linguistic computer systems etc. In natural languages there are situations where notions, which are dependent on the context, are described as independent of context, i.e. in terms of context-free grammars. This description is complicated by the formation of new categories and rules. The article features the process of introducing new restrictions on these grammar classes through the introduction of new rules. Uncut grammars were received if the number of characters in the right part of the rules were not less than the number of characters in the left one. Then by replacing the only character a context-sensitive grammar was received. A grammar with only one character in the left part of the rule is called a context-free grammar. No further natural restrictions may be applied to the left part of a rule. Based on the importance of automatic processing of text content in modern information media (e.g., information retrieval systems, machine translation, semantic, statistical, optical and acoustic analysis and speech synthesis, automated editing, extracting knowledge from text content, abstracting and annotating text content, indexing text content, teaching and didactic, management of linguistic corpora, various tools for lexicography, etc.), specialists are actively looking for new models, ways of their description and methods of automatic processing of text content. One of such methods lies in developing general principles of syntactic lexicographical systems formation and developing mentioned systems of processing text content for specific languages based in these principles. Any parsing tools consist of two parts: a knowledge base of concrete natural language and parsing algorithm, i.e. a set of standard operators of text content processing based on this knowledge. The source of grammatical knowledge is data of morphological analysis and various tables filled with concepts and linguistic units. They are the result of an empirical study of the text content in natural language by experts aiming at highlighting the basic laws for parsing.", "venue": "International Conference on Computer Science and Information Technologies", "citationCount": 35, "fieldsOfStudy": ["Computer Science"], "mentions": ["tp", "tp", "tp", "mt", "ke", "kg"], "mention_counts": {"ke": 1, "kg": 1, "tp": 3, "mt": 1}, "nlp_mention_counts": {"ke": 1, "tp": 3, "mt": 1}, "ld_mention_counts": {"ke": 1, "kg": 1}, "relevance_score": 0.46718541496779636}, {"paperId": "31273da8b1cbb184f488e0a669154d96f80e10e9", "url": "https://www.semanticscholar.org/paper/31273da8b1cbb184f488e0a669154d96f80e10e9", "title": "Using natural language processing and the gene ontology to populate a structured pathway database", "abstract": "Reading literature is one of the most time consuming tasks a busy scientist has to contend with. As the volume of literature continues to grow there is a need to sort through this information in a more efficient manner. Mapping the pathways of genes and proteins of interest is one goal that requires frequent reference to the literature. Pathway databases can help here and scientists currently have a choice between buying access to externally curated pathway databases or building their own in house. However such databases are either expensive to license or slow to populate manually. Building upon easily available, open-source tools we have developed a pipeline to automate the collection, structuring and storage of gene and protein interaction data from the literature. As a team of both biologists and computer scientists we integrated our natural language processing (NLP) software with the gene ontology (GO) to collect and translate unstructured text data into structured interaction data. For NLP we used a machine learning approach with a rule induction program, RAPIER (http://www. cs. utexas. edu/users/mUrapier. html). RAPIER was modified to learn rules from tagged documents, and then it was trained on a corpus tagged by expert curators. The resulting rules were used to extract information from a test corpus automatically. Extracted genes and proteins were mapped onto Locuslink, and extracted interactions were mapped onto GO. Once information was structured in this way it was stored in a pathway database and this formal structure allowed us to perform advanced data mining and visualization.", "venue": "Computational Systems Bioinformatics. CSB2003. Proceedings of the 2003 IEEE Bioinformatics Conference. CSB2003", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ie", "nlp", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 4, "ie": 1}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "85563a66e03e3a99cd541efeea62af4d2f94ff07", "url": "https://www.semanticscholar.org/paper/85563a66e03e3a99cd541efeea62af4d2f94ff07", "title": "Ontology and Model Alignment as a Means for Requirements Validation", "abstract": "This paper reports on work that is investigating the application of ontology engineering and natural language processing to software engineering. Our focus is the transition from requirements to design which remains one of the main challenges in software engineering. A key reason for why this is so challenging is that the vast majority of requirements documents are informal, written in natural language, whereas the final goal (code) is formal. System models, as an intermediate step between the requirements and code, help understand requirements. Even a seemingly precise requirements document typically contains a lot of inconsistencies and omissions, which become visible when we model the system. Our hypothesis is that these inconsistencies become apparent when we compare the project-specific model with a generic model of the application domain. To test our hypothesis, we need to transform natural language representations of requirements information into a form that facilitates comparison with a domain model. Naturally, we also need a domain model against which to compare and this presupposes a means to construct such models. In the paper, we extract a conceptual model (an ontology) and a behavioural model from different sources. An ontology is generated from a generic domain description, and a projectspecific model is generated from requirements documents. For ontology generation, natural language processing techniques are used to aid the construction. By comparing the resulting models, we validate both of them. When inconsistencies are found, we generate feedback for the analyst. The generated feedback was validated on a case study and has proven useful to improve both requirements documents and models.", "venue": "2010 IEEE Fourth International Conference on Semantic Computing", "citationCount": 34, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 5}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "94ffd4efb57f1569a4b8363fdfaf8ae0c79deb4e", "url": "https://www.semanticscholar.org/paper/94ffd4efb57f1569a4b8363fdfaf8ae0c79deb4e", "title": "Toward enhanced Natural Language Processing to databases: Building a specific domain Ontology derived from database conceptual model", "abstract": "Natural Language Interface to database NLIDB applications achieve great success when dealing with simple user requests, however most of NLIDB applications fail dramatically when users issue indirect or sophisticated requests. One modern approach to enhance NLIDB is using Ontology. Ontologies are very helpful when used with Natural Language Processing applications for supporting extraction of relevant elements from databases. This paper proposes a framework and a semi automatic procedure for building domain specific Ontology by using data conceptual model and general purpose Ontology such as WordNet. The aim is to help NLIDB understanding and simplifying indirect users data requests.", "venue": "2010 The 7th International Conference on Informatics and Systems (INFOS)", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 5}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "65e6b04010ad51f113efdb554f3a8bf20336e7a8", "url": "https://www.semanticscholar.org/paper/65e6b04010ad51f113efdb554f3a8bf20336e7a8", "title": "Moving the mountain: analysis of the effort required to transform comparative anatomy into computable anatomy", "abstract": "The diverse phenotypes of living organisms have been described for centuries, and though they may be digitized, they are not readily available in a computable form. Using over 100 morphological studies, the Phenoscape project has demonstrated that by annotating characters with community ontology terms, links between novel species anatomy and the genes that may underlie them can be made. But given the enormity of the legacy literature, how can this largely unexploited wealth of descriptive data be rendered amenable to large-scale computation? To identify the bottlenecks, we quantified the time involved in the major aspects of phenotype curation as we annotated characters from the vertebrate phylogenetic systematics literature. This involves attaching fully computable logical expressions consisting of ontology terms to the descriptions in character-by-taxon matrices. The workflow consists of: (i) data preparation, (ii) phenotype annotation, (iii) ontology development and (iv) curation team discussions and software development feedback. Our results showed that the completion of this work required two person-years by a team of two post-docs, a lead data curator, and students. Manual data preparation required close to 13% of the effort. This part in particular could be reduced substantially with better community data practices, such as depositing fully populated matrices in public repositories. Phenotype annotation required \u223c40% of the effort. We are working to make this more efficient with Natural Language Processing tools. Ontology development (40%), however, remains a highly manual task requiring domain (anatomical) expertise and use of specialized software. The large overhead required for data preparation and ontology development contributed to a low annotation rate of approximately two characters per hour, compared with 14 characters per hour when activity was restricted to character annotation. Unlocking the potential of the vast stores of morphological descriptions requires better tools for efficiently processing natural language, and better community practices towards a born-digital morphology. Database URL: http://kb.phenoscape.org", "venue": "Database J. Biol. Databases Curation", "citationCount": 29, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "onto", "onto", "onto", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 5}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "fc9b98e573e0ace42616958a350ed36aa875b184", "url": "https://www.semanticscholar.org/paper/fc9b98e573e0ace42616958a350ed36aa875b184", "title": "The KOLON System: Tools for Ontological Natural Language Processing in Korean", "abstract": "This paper presents and analyzes the KOLON System, created to facilitate Ko- rean Natural Language Processing (KNLP) and to improve experimental results through the KOLON Ontology. It is currently under development at our Computational Linguistics Labo- ratory, and is based on previous works, namely the Mikrokosmos Ontology and 21 st Century Sejong Project. The KOLON System is also extended with software tools to simplify the han- dling and visualization of the data, as well as the creation of new programs. The mapping of words onto ontological concepts was performed automatically, with faulty information being corrected manually. In order to examine the effectiveness of using KOLON's data, we have rerun a previous sentiment analysis (SA) experiment, changing the approach to include data from the ontology. This new experiment obtained improved results, which is a strong indica- tion that the project will be of use after its completion.", "venue": "PACLIC", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 5}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "526506d67a1d4fb0dce26766a56894515e923494", "url": "https://www.semanticscholar.org/paper/526506d67a1d4fb0dce26766a56894515e923494", "title": "Interactive Cohort Identification of Sleep Disorder Patients Using Natural Language Processing and i2b2.", "abstract": "UNLABELLED\nNationwide Children's Hospital established an i2b2 (Informatics for Integrating Biology & the Bedside) application for sleep disorder cohort identification. Discrete data were gleaned from semistructured sleep study reports. The system showed to work more efficiently than the traditional manual chart review method, and it also enabled searching capabilities that were previously not possible.\n\n\nOBJECTIVE\nWe report on the development and implementation of the sleep disorder i2b2 cohort identification system using natural language processing of semi-structured documents.\n\n\nMETHODS\nWe developed a natural language processing approach to automatically parse concepts and their values from semi-structured sleep study documents. Two parsers were developed: a regular expression parser for extracting numeric concepts and a NLP based tree parser for extracting textual concepts. Concepts were further organized into i2b2 ontologies based on document structures and in-domain knowledge.\n\n\nRESULTS\n26,550 concepts were extracted with 99% being textual concepts. 1.01 million facts were extracted from sleep study documents such as demographic information, sleep study lab results, medications, procedures, diagnoses, among others. The average accuracy of terminology parsing was over 83% when comparing against those by experts. The system is capable of capturing both standard and non-standard terminologies. The time for cohort identification has been reduced significantly from a few weeks to a few seconds.\n\n\nCONCLUSION\nNatural language processing was shown to be powerful for quickly converting large amount of semi-structured or unstructured clinical data into discrete concepts, which in combination of intuitive domain specific ontologies, allows fast and effective interactive cohort identification through the i2b2 platform for research and clinical use.", "venue": "Applied Clinical Informatics", "citationCount": 9, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "nlp", "onto", "nlp", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 5, "onto": 2}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "b0a7bc75c8bea2e435ee029a95eb2e113f1e44b0", "url": "https://www.semanticscholar.org/paper/b0a7bc75c8bea2e435ee029a95eb2e113f1e44b0", "title": "Research and implementation of automatic question answering system based on ontology", "abstract": "Aiming at the problems in extant automatic question answering system, such as inadequate knowledge expression and weakness of indicating the inherent relations among knowledge, a solution with using Ontology to build curriculum domain knowledge base was put forward in this paper. In this paper, the course of \"Natural Language Processing\" was took as an example to construct an automatic question answering system based on Ontology. The system extracts keywords by analyzing users' question and transforms the intention of the question into the query of basic elements in Ontology. Finally, the system extracts the answer via Jena reasoning. Meanwhile, the system provides related knowledge to help students study systematically. This paper has achieved an automatic question answering system presented in the course of \u201cNatural Language Processing\u201d. This automatic question answering system supports asking questions in natural language and has some reference value in other areas.", "venue": "Chinese Control and Decision Conference", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "onto", "onto", "nlp", "kg"], "mention_counts": {"nlp": 2, "onto": 4, "kg": 1}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 1, "onto": 4}, "relevance_score": 0.46718541496779636}, {"paperId": "c9dbe44fbc4a9cba08757e6bd3704bcfd8635505", "url": "https://www.semanticscholar.org/paper/c9dbe44fbc4a9cba08757e6bd3704bcfd8635505", "title": "Post-Traumatic Stress Disorder (PTSD) Ontology and Use Case", "abstract": "* Abstract\u2014Ontologies play an increasingly important role in annotation, integration, and analysis of biomedical data. In this paper, we describe the design and development of a Post- Traumatic Stress Disorder (PTSD) Ontology and how we can use this ontology as a controlled vocabulary for supporting automatic annotation of clinical text. The automated annotation is performed using a natural language processing (NLP) tool called YTEX. In addition, we demonstrate how we can use the concepts and relationships defined in the PTSD Ontology to perform data summarization and categorization.", "venue": "ICBO", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 5}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "ad346b713be61ca2e531cfc14d7c1c65f4a469e2", "url": "https://www.semanticscholar.org/paper/ad346b713be61ca2e531cfc14d7c1c65f4a469e2", "title": "A computational model for speech disorders using problematic phonemes with ontological reasoning", "abstract": "This work presents a method for data gathering to construct a corpus related to speech disorders in children; such corpus will serve as the base to generate some semi-automatic ontologies, in order to become a computational model to support therapists for diagnosis and possible treatment. Speech disorders, phonemes and some additional information are classified using taxonomies obtained from speech disorders specialized literature. Based on the obtained taxonomies, the ontologies, which structure and formalize concepts defined by the main topic authors, are developed. The ontologies are constructed following some parts of classic methodologies and their subsequent validation is made through competency questions. The development of the model is based on Natural Language Processing (NLP) and Information Retrieval (IR) techniques. Integration of the ontologies is made to be able to make a classification based in problematic phonemes; this is suggested as a complement to the diagnostic tool in the model.", "venue": "J. Intell. Fuzzy Syst.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 5}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "9d81561d1a5c0ecf3fc26dfb2ac367e4ab32de43", "url": "https://www.semanticscholar.org/paper/9d81561d1a5c0ecf3fc26dfb2ac367e4ab32de43", "title": "Jump-starting a Body-of-Knowledge with a Semantic Wiki on a Discipline Ontology", "abstract": "Several communities have engaged recently in assembling a Body of Knowledge (BOK) to organize the discipline knowledge for learning and sharing. BOK ideally represents the domain, contextualizes assets (e.g. literature), and exploits the Social Web potential to maintain and improve it. Semantic wikis are excellent tools to handle domain (ontological) representations, to relate items, and to enable collaboration. Unfortunately, creating a whole BOK (structure, content and relations) from scratch may fall prey to the \u201cwhite page syndrome\u201d, given the size and complexity of the domain information. This article presents an approach to jump-start a BOK, by implementing it as a semantic wiki organized around a domain ontology. Domain representation (structure and content) are initialized by automatically creating wiki pages for each ontology concept and digital asset; the ontology itself is semi-automatically built using natural language processing (NLP) techniques. Contextualization is initialized by automatically linking conceptand asset-pages. The proposal\u2019s feasibility is shown with a prototype for a Software Architecture BOK, built from 1,000 articles indexed by a well-known scientific digital library and completed by volunteers. The proposed approach separates the issues of domain representation, resources contextualization, and social elaboration, allowing communities to try on alternate solutions for each issue.", "venue": "SemWiki@ESWC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 5}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "0e9b80c66063782eeba0710d12e4120ce3176ae2", "url": "https://www.semanticscholar.org/paper/0e9b80c66063782eeba0710d12e4120ce3176ae2", "title": "Adverse Childhood Experiences Identification from Clinical Notes with Ontologies and NLP", "abstract": "Adverse Childhood Experiences (ACEs) are defined as a collection of highly stressful, and potentially traumatic, events or circumstances that occur throughout childhood and/or adolescence. They have been shown to be associated with increased risks of mental health diseases or other abnormal behaviours in later lives. However, the identification of ACEs from free-text Electronic Health Records (EHRs) with Natural Language Processing (NLP) is challenging because (a) there is no NLP ready ACE ontologies; (b) there are limited cases available for machine learning, necessitating the data annotation from clinical experts. We are currently developing a tool that would use NLP techniques to assist us in surfacing ACEs from clinical notes. This will enable us further research in identifying evidence of the relationship between ACEs and the subsequent developments of mental illness (e.g., addictions) in large-scale and longitudinal free-text EHRs, which has previously not been possible.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 5, "onto": 2}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "fe07e582f1ba03a39c6b22fa591a33eaa80aec31", "url": "https://www.semanticscholar.org/paper/fe07e582f1ba03a39c6b22fa591a33eaa80aec31", "title": "Toward the Automated Construction of Probabilistic Knowledge Graphs for the Maritime Domain", "abstract": "International maritime crime is becoming increasingly sophisticated, often associated with wider criminal networks. Detecting maritime threats by means of fusing data purely related to physical movement (i.e., those generated by physical sensors, or hard data) is not sufficient. This has led to research and development efforts aimed at combining hard data with other types of data (especially human-generated or soft data). Existing work often assumes that input soft data is available in a structured format, or is focused on extracting certain relevant entities or concepts to accompany or annotate hard data. Much less attention has been given to extracting the rich knowledge about the situations of interest implicitly embedded in the large amount of soft data existing in unstructured formats (such as intelligence reports and news articles). In order to exploit the potentially useful and rich information from such sources, it is necessary to extract not only the relevant entities and concepts, but also their semantic relations, together with the uncertainty associated with the extracted knowledge (i.e., in the form of probabilistic knowledge graphs). This will increase the accuracy of, and confidence in, the extracted knowledge and facilitate subsequent reasoning and learning. To this end, we propose Maritime DeepDive, an initial prototype for the automated construction of probabilistic knowledge graphs from natural language data for the maritime domain. In this paper, we report on the current implementation of Maritime DeepDive, together with preliminary results on extracting probabilistic events from maritime piracy incidents. This pipeline was evaluated on a manually crafted gold standard, yielding promising results.", "venue": "2021 IEEE 24th International Conference on Information Fusion (FUSION)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "kg", "kg"], "mention_counts": {"ke": 2, "kg": 3}, "nlp_mention_counts": {"ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 3}, "relevance_score": 0.46718541496779636}, {"paperId": "88a477583931c06c79cf97991020dd13e682dfff", "url": "https://www.semanticscholar.org/paper/88a477583931c06c79cf97991020dd13e682dfff", "title": "Classifying Inconsistencies in DBpedia Language Specific Chapters", "abstract": "This paper proposes a methodology to identify and classify the semantic relations holding among the possible different answers obtained for a certain query on DBpedia language specific chapters. The goal is to reconcile information provided by language specific DBpedia chapters to obtain a consistent results set. Starting from the identified semantic relations between two pieces of information, we further classify them as positive or negative, and we exploit bipolar abstract argumentation to represent the result set as a unique graph, where using argumentation semantics we are able to detect the (possible multiple) consistent sets of elements of the query result. We experimented with the proposed methodology over a sample of triples extracted from 10 DBpedia ontology properties. We define the LingRel ontology to represent how the extracted information from different chapters is related to each other, and we map the properties of the LingRel ontology to the properties of the SIOC-Argumentation ontology to built argumentation graphs. The result is a pilot resource that can be profitably used both to train and to evaluate NLP applications querying linked data in detecting the semantic relations among the extracted values, in order to output consistent information sets.", "venue": "LREC", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "nlp", "ld", "onto"], "mention_counts": {"ld": 1, "nlp": 1, "onto": 4, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ie": 1}, "ld_mention_counts": {"ld": 1, "onto": 4}, "relevance_score": 0.46718541496779636}, {"paperId": "1010ad0a0393215c3bcdf83708cc31ec4f20e2e7", "url": "https://www.semanticscholar.org/paper/1010ad0a0393215c3bcdf83708cc31ec4f20e2e7", "title": "Towards Robust Multi-Tool Tagging. An OWL/DL-Based Approach", "abstract": "This paper describes a series of experiments to test the hypothesis that the parallel application of multiple NLP tools and the integration of their results improves the correctness and robustness of the resulting analysis. \n \nIt is shown how annotations created by seven NLP tools are mapped onto tool-independent descriptions that are defined with reference to an ontology of linguistic annotations, and how a majority vote and ontological consistency constraints can be used to integrate multiple alternative analyses of the same token in a consistent way. \n \nFor morphosyntactic (parts of speech) and morphological annotations of three German corpora, the resulting merged sets of ontological descriptions are evaluated in comparison to (ontological representation of) existing reference annotations.", "venue": "ACL", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 5}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "f3b1f6ea4954065e01819849ffe56845d791466e", "url": "https://www.semanticscholar.org/paper/f3b1f6ea4954065e01819849ffe56845d791466e", "title": "An Approach for Populating and Enriching Ontology-Based Repositories", "abstract": "Publically available text-based documents (e.g. news, meeting transcripts) are a very important source of knowledge, especially for organizations. These documents mention domain entities such as persons, places, professional positions, decisions and actions. Querying these documents (instead of browsing, searching and finding) is a very relevant task for any person in general, and particularly for professionals dealing with intensive knowledge tasks. Querying text-based documents' data, however, is not supported by common technology. For that, such documents' content has to be explicitly and formally captured as facts into a knowledge base. Making use of automatic NLP processes for capturing such facts is a common approach, but their relatively low precision and recall give rise to data quality problems. Furthermore, facts existing in the documents are often insufficient to answer complex queries, thus the need to enrich the captured facts with facts from third-party repositories (e.g. public LOD). This paper describes the adopted process to clean, populate and enrich a knowledge base repository that is further exploited to answer complex queries. This process is triggered by a previous NLP parsing process and conducted by the (rich) ontology describing such repository.", "venue": "2013 24th International Workshop on Database and Expert Systems Applications", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "lod", "nlp", "onto", "nlp", "onto", "kg"], "mention_counts": {"nlp": 2, "kg": 2, "lod": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"kg": 2, "lod": 1, "onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "0bfa6cb15ef38bd01db891e8c8ea6a14d7e9bfeb", "url": "https://www.semanticscholar.org/paper/0bfa6cb15ef38bd01db891e8c8ea6a14d7e9bfeb", "title": "Fuzzy Clustering and Genetic Algorithm for Clinical Pratice Guideline Execution Engines", "abstract": "The clinical practice guideline (CPG) is used to support the decision system using general practitioner. The patient symptoms are recommended and followed as per CPG. Medical knowledge is framed on the clinical decision making and clinical practice. The NLP is important data source for clinical decision. The NLP system is included in part of speech tagging. Fuzzy c means are used to group the symptoms. The fuzzy membership function is used to find the Fuzzy c means clustering approach to determine the centroid correctly. OWL Language is used to map (onto map) symptoms and disease in the clinical evaluation. The healthcare, semantic web and ontology's for the patients clinical practice is based on CPG. Ontology is potentially integrated with health care. Ontology's are accessed through the user interface and frame work. The gaps between the users requirement is suggested. Genetic Algorithms (GA) via clustering is used for posting the optimization issues. GA provides optimal results when used in complex issues. Genetic Algorithm is optimized for stomach disease prediction as well.", "venue": "2017 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "onto", "onto", "sw", "nlp"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.46718541496779636}, {"paperId": "34f81fb6d1d9bb13209239b7ade196f559d08498", "url": "https://www.semanticscholar.org/paper/34f81fb6d1d9bb13209239b7ade196f559d08498", "title": "Aligning Multilingual Thesauri", "abstract": "The aligning and merging of ontologies with overlapping information are actual one of the most active domain of investigation in the Semantic Web community. Multilingual lexical ontologies thesauri are fundamental knowledge sources for most NLP projects addressing multilinguality. The alignment of multilingual lexical knowledge sources has various applications ranging from knowledge acquisition to semantic validation of interlingual equivalence of presumably the same meaning express in different languages. In this paper, we present a general method for aligning ontologies, which was used to align a conceptual thesaurus, lexicalized in 20 languages with a partial version of it lexicalized in Romanian. The objective of our work was to align the existing terms in the Romanian Eurovoc to the terms in the English Eurovoc and to automatically update the Romanian Eurovoc. The general formulation of the ontology alignment problem was set up along the lines established by Heterogeneity group of the KnowledgeWeb consortium, but the actual case study was motivated by the needs of a specific NLP project.", "venue": "LREC", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "onto", "sw", "onto", "nlp"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 4}, "nlp_mention_counts": {"nlp": 2}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.46718541496779636}, {"paperId": "d770249832e9c97416ea1b8ebb4f255b22a1118d", "url": "https://www.semanticscholar.org/paper/d770249832e9c97416ea1b8ebb4f255b22a1118d", "title": "A Correspondence Repair Algorithm based on Word Sense Disambiguation and Upper Ontologies", "abstract": "In an ideal world, an ontology matching algorithm should return all the correct correspondences (it should be complete) and should return no wrong correspondences (it should be correct). In the real world, no imple- mented ontology matching algorithm is both correct and complete. For this reason, repairing wrong corre- spondences in an ontology alignment is a very pressing need to obtain more accurate alignments. This paper discusses an automatic correspondence repair method that exploits both upper ontologies to provide infor- mative context to concepts c2 o and c 0 2 o 0 belonging to an alignment a, and a context-based word sense disambiguation algorithm to assign c and c 0 their correct meaning. This meaning is used to decide whether c and c 0 are related, and to either keep or discard the correspondence 2 a, namely, to repair a. The experiments carried on are presented and the obtained results are provided. The advantages of the approach we propose are confirmed by a total average gain of 11,5% in precision for the alignments repaired against a 2% total average error.", "venue": "International Conference on Knowledge Engineering and Ontology Development", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "onto", "onto", "onto", "wsd", "onto", "onto"], "mention_counts": {"wsd": 2, "onto": 5}, "nlp_mention_counts": {"wsd": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.46718541496779636}, {"paperId": "2a86ffc829c8cbdeec19e2c489ea0d29b1b9431e", "url": "https://www.semanticscholar.org/paper/2a86ffc829c8cbdeec19e2c489ea0d29b1b9431e", "title": "CIDOC2VEC: Extracting Information from Atomized CIDOC-CRM Humanities Knowledge Graphs", "abstract": "The development of the field of digital humanities in recent years has led to the increased use of knowledge graphs within the community. Many digital humanities projects tend to model their data based on CIDOC-CRM ontology, which offers a wide array of classes appropriate for storing humanities and cultural heritage data. The CIDOC-CRM ontology model leads to a knowledge graph structure in which many entities are often linked to each other through chains of relations, which means that relevant information often lies many hops away from their entities. In this paper, we present a method based on graph walks and text processing to extract entity information and provide semantically relevant embeddings. In the process, we were able to generate similarity recommendations as well as explore their underlying data structure. This approach was then demonstrated on the Sphaera Dataset which was modeled according to the CIDOC-CRM data structure.", "venue": "Inf.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "kg", "onto", "ie", "tp", "kg"], "mention_counts": {"kg": 3, "onto": 2, "tp": 1, "ie": 1}, "nlp_mention_counts": {"tp": 1, "ie": 1}, "ld_mention_counts": {"kg": 3, "onto": 2}, "relevance_score": 0.46718541496779636}, {"paperId": "3d0c09611db9a716052fc2e4224aa5a479ce36fa", "url": "https://www.semanticscholar.org/paper/3d0c09611db9a716052fc2e4224aa5a479ce36fa", "title": "ONTOLOGY BASED MEANINGFUL SEARCH USING SEMANTIC WEB AND NATURAL LANGUAGE PROCESSING TECHNIQUES", "abstract": "The semantic web extends the current World Wide Web by adding facilities for the machine understood description of meaning. The ontology based search model is used to enhance efficiency and accuracy of information retrieval. Ontology is the core technology for the semantic web and this mechanism for representing formal and shared domain descriptions. In this paper, we proposed ontology based meaningful search using semantic web and Natural Language Processing (NLP) techniques in the educational domain. First we build the educational ontology then we present the semantic search system. The search model consisting three parts which are embedding spell-check, finding synonyms using WordNet API and querying ontology using SPARQL language. The results are both sensitive to spell check and synonymous context. This paper provides more accurate results and the complete details for the selected field in a", "venue": "Soft Computing Models in Industrial and Environmental Applications", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "sw", "sw", "onto", "onto", "sw", "onto", "nlp", "sw", "onto", "nlp", "onto"], "mention_counts": {"nlp": 3, "sw": 4, "onto": 6}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 4, "onto": 6}, "relevance_score": 0.4564675649397261}, {"paperId": "27ea4b7b4f0e38d363aa7d3e8dc3c7c282bf0603", "url": "https://www.semanticscholar.org/paper/27ea4b7b4f0e38d363aa7d3e8dc3c7c282bf0603", "title": "Ethical Dimensions for Data Quality", "abstract": "ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2019 Association for Computing Machinery. 1936-1955/2019/1-ART1 $15.00 https://doi.org/10.1145/3362121 ACM J. Data Inform. Quality, Vol. 1, No. 1, Article 1. Publication date: January 2019. 1:2 Donatella Firmani, Letizia Tanca, and Riccardo Torlone Transparency is the ability to interpret the information extraction process in order to verify which aspects of the data determine its results. In this context, transparency metrics can use the notions of (i) data provenance [19, 18], by measuring the amount of meta-data describing where the original data come from; (ii) explanation [15], by describing how a result has been obtained. Diversity is the degree to which different kinds of objects are represented in a dataset. Several metrics are proposed in [9]. Ensuring diversity at the beginning of the information extraction process may be useful for enforcing fairness at the end. The diversity dimension may conflict with established dimensions in the Trust cluster of [5], that prioritizes few high-reputation sources. Data Protection concerns the ways to secure data, algorithms and models against unauthorized access. Defining measures can be an elusive goal since, on the one hand, anonymized datasets that are secure in isolation can reveal sensible information when combined [1], and on the other hand, robust techniques such as \u03b5-differential privacy [10] can only describe the privacy impact of specific queries. Data protection is related to the well-established security dimension of [5]. 3 ETHICAL CHALLENGES IN THE INFORMATION EXTRACTION PROCESS We highlight some challenges of complying with the dimensions of the Ethics Cluster, throughout the three phases of the information extraction process mentioned in the Introduction. A. Source Selection. Data can typically come from multiple sources, and it is most desirable that each of these complies with the ethics dimensions described in the previous section. If sources do not comply with (some) dimension individually, we should consider that the really important requirement is that the data that are finally used for analysis or recommendations do. It is thus appropriate to consider ethics for multiple sources in combination, so that the bias towards a certain category in a single source can be eliminated by another source with opposite bias. While for the fairness, transparency and diversity dimensions this is clearly possible, for the privacy we can only act on the single data sources because adding more information can only lower the protection level, or, at most, leave it as it is. Ethics in source selection is tightly related to the transparency of the source, specifically for sources that are themselves aggregators. Information lineage is of paramount importance in this case and can be accomplished with the popular notion of provenance [18]; however, how to capture the most fine-grained type of provenance, namely data provenance, remains an open question [12]. A more general challenge is source meta-data extraction, especially for interpreting unstructured contents and thus their ethical implications. Finally, we note that also the data acquisition process plays a role, and developing inherently transparent and fair collection and extraction methods is an almost unstudied topic. B. Data Integration. Ensuring ethics in the selection step is not enough: even if the collected data satisfy the ethical requirements, not necessarily their integration does [1]. Data integration usually involves three main steps: (i) schema matching, i.e. the alignment of the schemata of the data sources (when present), (ii) identification of the items stored in different data sources that refer to the same entity (also called record linkage or entity resolution), and (iii) construction of an integrated database over the data sources, obtained by merging their contents (also called data fusion). Each step is prone to different ethical concerns, as discussed below. Schema Matching. Groups treated fairly in the sources can become overor under-represented as a consequence of the integration process, possibly causing, in the following steps, unfair decisions. Similar issues arise in connection with diversity. Entity Resolution. Integrating sources that, in isolation, protect identity (e.g. via anonymization) might generate a dataset that violates privacy: an instance of this is the so-called linkage attack [1]. We refer the reader to [21] for a survey of techniques and challenges of privacy-preserving entity resolution in the context of Big Data. ACM J. Data Inform. Quality, Vol. 1, No. 1, Article 1. Publication date: January 2019. Ethical Dimensions for DataQuality 1:3 Data Fusion. Data disclosure, i.e., violation of data protection, can happen also in the fusion step if privacy-preserving noise is accidentally removed by merging the data. Fusion can also affect fairness, when combining data coming from different sources leads to the exclusion of some groups. In all the above steps transparency is fundamental: we can check the fulfilment of the ethical dimensions only if we can (i) provide explanations of the intermediate results (ii) describe the provenance of the final data. Unfortunately, this can conflict with data protection since removing identity information can cause lack of transparency, which ultimately may lead to unfair outcomes. As source selection, also the integration process \u2013 especially the last two steps, where schema information is not present \u2013 can benefit from the existence of meta-data, allowing to infer contextual meanings for individual terms and phrases. Fair extraction of meta-data is an exciting topic, as stereotypes and prejudices can be often found into automatically derived word semantics. C. Knowledge Extraction. An information extraction process presents the user with data organized as to satisfy their information needs. Here we highlight some ethical challenges for a sample of the many possible information extractions operations. Search and Query. These are typical data selection tasks. Diversifying the results of information retrieval and recommendation systems has traditionally been used to minimize dissatisfaction of the average user [4]. However, since these search algorithms are employed also in critical tasks such as job candidate selection or for university admissions, diversity has also become a way to ensure the fairness of the selection process [9]. Interestingly, if integrated data are unfair and over-represent a certain category, diversity can lead to data exclusion of the same category. Aggregation. Many typical decision-support queries, such as GROUP BY queries, might yield biased result, e.g. trends appearing in different groups of data can disappear or even be reversedwhen these groups are combined, leading to incorrect insights. The work of [17] provides a framework for incorporating fairness in aggregated data based on independence tests, for specific aggregations. A future work is to detect bias in combined data with full-fledged query systems. Analytics. Data are typically analyzed by means of statistical, data mining and machine learning techniques, providing encouraging results in decision making, even in data management problems [14]. However, while we are able to understand statistics and data mining models, when using techniques such as deep learning we are still far from fully grasping how a model produces its output. Therefore, explaining systems has become an important new research area [16], related to the fairness and transparency of the training data as well as of the learning process. 4 RESEARCH DIRECTIONS In the spirit of the responsible data science initiatives towards a full-fledged data quality perspective on ethics (see, for instance, redasci.org and dataresponsibly.github.io), the key ingredient is shared responsibility. Like for any other engineering product, responsibility for data usage is shared by a contractor and a producer: only if the latter is able to provide a quality certification for the various ethical dimensions, the former can share the responsibility for improper usage. Similarly, producers should be aware of their responsibility when quality goes below the granted level. While such guarantees are available for many classical dimensions of quality, for instance timeliness, the same does not hold for most of the ethical dimensions. Privacy already has a well defined way for guaranteeing a privacy level by design: (i) in the knowledge extraction step, thanks to the notion of \u03b5-differential privacy [10], and (ii) in the integration step (see [21] for a survey). The so-called nutritional labels [13] mark a major step towards the idea of a quality certificate for fairness and diversity in the source selection and knowledge extraction steps, but how to preserve these properties throughout the process remains instead an open problem. Transparency is perhaps the hardest dimension to guarantee, and we believe that the well-known notion of provenance [12] ACM J. Data Inform. Quality, Vol. 1, No. 1, Article 1. Publication date: January 2019. 1:4 Donatella Firmani, Letizia Tanca, and Riccardo Torlone can provide a promising starting point. However, the rise of machine learning and deep learning techniques also for some data integration tasks [8] poses new and exciting challenges in tracking the way integration is achieved [22]. Summing up, recent literature provides a variety of methods for verifying/enforcing ethical dimensions. However, they typically apply to the very early (such as, collection) or very late steps (such as, analytics) of the information extraction process, but very few works study how to preserve ethics by design throughout the process. 5 RELATEDWORKS AND CONCLUDING REMARKS An early attempt to con", "venue": "ACM Journal of Data and Information Quality", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ie", "ie", "ie", "ke", "ie", "ke", "ie", "ie"], "mention_counts": {"ke": 3, "ie": 7}, "nlp_mention_counts": {"ke": 3, "ie": 7}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.4564675649397261}, {"paperId": "9f2b81392681b72bafc33b587ea5441be3a660bc", "url": "https://www.semanticscholar.org/paper/9f2b81392681b72bafc33b587ea5441be3a660bc", "title": "Ontology and NLP support for building disaster knowledge base", "abstract": "The data from pre, post and during disasters play a very important and vital role while immediate decision making in emergency management. Data availability, analysis, and exchange build up a critical knowledge base that helps in taking the right decision at the right time. Voluminous data is available through various sources in heterogeneous formats. It is a very big challenge for utilization of such massive heterogeneous data in disaster management situations. Disaster data is widely available in various documents and formats. Past disasters, anticipated disasters and during the disasters, a large amount of data is getting exchanged in various documents between different agencies. Effective techniques to extract and gather the data from heterogeneous documents helps in right data extraction and build up a knowledge base to formulate ease of analysis and modeling. Ontology based semantic technology is a powerful mechanism that supports information integration, exchange, share, reuse and build a knowledge base from heterogeneous information sources. The current research work focuses on three aspects of knowledge base system for effective disaster management. The current research work implemented a mechanism to extract relevant information from semi-structured and structured heterogeneous documents using NLP, representing extracted information in homogeneous and machine understandable format using RDF and map the RDF triples to appropriate concepts of the disaster management domain ontologies. This is very novel and effective means of data usage mechanism to help and guide policy makers, disaster mitigation agencies and society at large.", "venue": "International Conference on Communication and Electronics Systems", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "rdf", "onto", "kg", "onto", "rdf", "onto", "kg", "kg", "kg", "nlp", "nlp", "ie"], "mention_counts": {"onto": 3, "nlp": 2, "kg": 5, "rdf": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"kg": 5, "onto": 3, "rdf": 2}, "relevance_score": 0.4564675649397261}, {"paperId": "54df02663f228eeeefe739b45584bbe711b12f09", "url": "https://www.semanticscholar.org/paper/54df02663f228eeeefe739b45584bbe711b12f09", "title": "Information Retrieval from Software Bug Ontology Exploiting Formal Concept Analysis", "abstract": "Knowledge extraction and structuring is attaining importance in real world applications such as e-commerce, decision support, problem solving and semantic web. Extraction of knowledge from collection of text documents is based upon identification of semantic content. Ontology plays an important role in accessing and structuring information. Developing an ontology are at the core of new strategies which requires accurate domain knowledge. Identification of structural and logical concepts is a time-consuming process. This work presents an ontology-based retrieval approach, that visualizes and structure the data of software bug reports domain. It exploits formal concept analysis (FCA) to elicit conceptualizations from bug reports datasets and a hierarchical taxonomy is generated of extracted knowledge. A lattice diagram of concepts and relationships is constructed from concept-relationship matrix created by FCA. Ontology is constructed on fluent editor tool and knowledge is extracted with the help of small queries executed on a reasoner window. The proposed approach is evaluated on 21 bug reports of apache projects of jira repository. It can be concluded that information can be retrieved easily from ontology as compared to manual extraction of data.", "venue": "Computaci\u00f3n y Sistemas", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "onto", "onto", "sw", "ke", "onto", "ke", "onto", "onto"], "mention_counts": {"ke": 3, "sw": 1, "onto": 6}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "sw": 1, "onto": 6}, "relevance_score": 0.4564675649397261}, {"paperId": "a40f4f2f40fbbbf65601d00ea8fb5cad56d35384", "url": "https://www.semanticscholar.org/paper/a40f4f2f40fbbbf65601d00ea8fb5cad56d35384", "title": "Development and application of a high throughput natural language processing architecture to convert all clinical documents in a clinical data warehouse into standardized medical vocabularies", "abstract": "OBJECTIVE\nNatural language processing (NLP) engines such as the clinical Text Analysis and Knowledge Extraction System are a solution for processing notes for research, but optimizing their performance for a clinical data warehouse remains a challenge. We aim to develop a high throughput NLP architecture using the clinical Text Analysis and Knowledge Extraction System and present a predictive model use case.\n\n\nMATERIALS AND METHODS\nThe CDW was comprised of 1\u00a0103\u00a0038 patients across 10 years. The architecture was constructed using the Hadoop data repository for source data and 3 large-scale symmetric processing servers for NLP. Each named entity mention in a clinical document was mapped to the Unified Medical Language System concept unique identifier (CUI).\n\n\nRESULTS\nThe NLP architecture processed 83\u00a0867\u00a0802 clinical documents in 13.33 days and produced 37\u00a0721\u00a0886\u00a0606 CUIs across 8 standardized medical vocabularies. Performance of the architecture exceeded 500\u00a0000 documents per hour across 30 parallel instances of the clinical Text Analysis and Knowledge Extraction System including 10 instances dedicated to documents greater than 20\u00a0000 bytes. In a use-case example for predicting 30-day hospital readmission, a CUI-based model had similar discrimination to n-grams with an area under the curve receiver operating characteristic of 0.75 (95% CI, 0.74-0.76).\n\n\nDISCUSSION AND CONCLUSION\nOur health system's high throughput NLP architecture may serve as a benchmark for large-scale clinical research using a CUI-based approach.", "venue": "J. Am. Medical Informatics Assoc.", "citationCount": 16, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["nlp", "ke", "nlp", "nlp", "nlp", "nlp", "nlp", "nlp", "ke", "ke"], "mention_counts": {"nlp": 7, "ke": 3}, "nlp_mention_counts": {"nlp": 7, "ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.4564675649397261}]