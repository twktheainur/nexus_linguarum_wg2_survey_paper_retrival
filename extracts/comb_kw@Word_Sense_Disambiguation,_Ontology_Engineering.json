[
  {
    "paperId": "e5aa3018afc35474f5fae2497fe330f8f322ef71",
    "url": "https://www.semanticscholar.org/paper/e5aa3018afc35474f5fae2497fe330f8f322ef71",
    "title": "Polysemy and Synonymy Detection in Ontology Engineering",
    "abstract": "Polysemy, when a single term has multiple meanings, and synonymy, when multiple terms have the same meaning, are common phenomena in linguistics as well as in scientific knowledge. In ontology engineering, it is vital to detect the synonyms annotations and the multiple inheritances because of polysemy. The persistence of these issues in the semantic description of a knowledge domain causes problematic interoperability and data processing. The disambiguation of the entities, properties and relationships sense in a semantic web ontology significantly improves linked data generation and information retrieval. We explore the synonymy and polysemy in the setting of a cardiology terminology generated from textbooks on the basis of field coverage, professionals\u2019 associations\u2019 recommendations and bibliometrics, for the building of a cardiologic ontology. From 56,134 terms collected we found that 67.7% were unique. The indexed terms included single words, compound words and multi-word expressions. The frequency of their appearances in the combined master index was calculated and used as a marker of their significance. To cope with the linguistic polysemy and synonymy of terms, we examined them in WordNet, MeSH and BioPortal, as well as by latent semantic analysis (LSA) through singular value decomposition (SVD). Through these approaches we managed to identify and decipher semantic associations and relationships between the terms. We proposed a roadmap for ontology building from scratch by utilizing intrinsic and extrinsic knowledge resources and reuse of metadata. We anticipate that this approach is applicable in ontology engineering of different knowledge domains for relationships setting and linked data contextualization",
    "venue": "WSEAS Transactions on Information Science and Applications",
    "citationCount": 2,
    "fieldsOfStudy": null
  },
  {
    "paperId": "13b40b1e803003425b4f6a020002dc13f63d7028",
    "url": "https://www.semanticscholar.org/paper/13b40b1e803003425b4f6a020002dc13f63d7028",
    "title": "Word Level Translation (Tamil - English) with word sense disambiguation in Tamil using OntNet",
    "abstract": "This paper discusses a knowledge-engineering approach for word level translation of Tamil words to English. This approach also shows the need for word sense disambiguation to arrive at the contextual category or meaning of a word when different category/meaning can be assigned to a particular word in different contexts. An ambiguous word in a given Tamil sentence is subjected to contextual analysis. The VPs in the given text are located and then augmented with semantic information. These semantic features are captured using the ontology derived from the sub-categorization features. This semantic information will help in assigning the correct meaning for the given ambiguous word. A rule-based syntactic parser and word sense disambiguator have been developed. This has been tested on Tamil sentences from Tamil newspaper websites and the results are encouraging.",
    "venue": "2015 International Conference on Computing and Communications Technologies (ICCCT)",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "86beb79514fa66e30241a768907d9af52a84a0a2",
    "url": "https://www.semanticscholar.org/paper/86beb79514fa66e30241a768907d9af52a84a0a2",
    "title": "The Realization and Discussion of Word Sense Disambiguation Based on Ontology",
    "abstract": "In natural language, it is very common that one word has several different meanings. The well solution of Word Sense Disambiguation (WSD) problem is the basis of Natural Language Processing. In this paper, based on the certainty of word sense in certain language context, a new method is put forward based on Ontology to solve the problem of WSD. And also, a prototype is developed to solve the WSD of \"\u6253\" in Chinese. Ontology is used to describe the knowledge of word and sense, and Jena reasoning rules are used to reason the mean of one ambiguous word. This method can be of great help to solve WSD. Keywords-Natural Language Processing; Word Sense Disambiguation; Ontology; reasoning rules",
    "venue": "2009 International Conference on Computational Intelligence and Software Engineering",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "86e2955436ec032c68ef662e4bb0601f39110b2a",
    "url": "https://www.semanticscholar.org/paper/86e2955436ec032c68ef662e4bb0601f39110b2a",
    "title": "Linked open data-based framework for automatic biomedical ontology generation",
    "abstract": null,
    "venue": "BMC Bioinformatics",
    "citationCount": 20,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ]
  },
  {
    "paperId": "44b14bf658fe0b4d39dd6a6e5d01f80a2581a664",
    "url": "https://www.semanticscholar.org/paper/44b14bf658fe0b4d39dd6a6e5d01f80a2581a664",
    "title": "Linked open data-based framework for automatic biomedical ontology generation",
    "abstract": null,
    "venue": "BMC Bioinformatics",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "63180188e433bedda24fa2ef7a2eddda63dd224f",
    "url": "https://www.semanticscholar.org/paper/63180188e433bedda24fa2ef7a2eddda63dd224f",
    "title": "Knowledge Engineering and the Semantic Web",
    "abstract": null,
    "venue": "Communications in Computer and Information Science",
    "citationCount": 17,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "7abf32099f7599f5a05da7dbc1dde646536e5a25",
    "url": "https://www.semanticscholar.org/paper/7abf32099f7599f5a05da7dbc1dde646536e5a25",
    "title": "Integrating a Large Domain Ontology of Species into WordNet",
    "abstract": "With the proliferation of applications sharing information represented in multiple ontologies, the development of automatic methods for robust and accurate ontology matching will be crucial to their success. Connecting and merging already existing semantic networks is perhaps one of the most challenging task related to knowledge engineering. This paper presents a new approach for aligning automatically a very large domain ontology of Species to WordNet in the framework of the KYOTO project. The approach relies on the use of knowledge-based Word Sense Disambiguation algorithm which accurately assigns WordNet synsets to the concepts represented in \nSpecies 2000.",
    "venue": "LREC",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1f728b6a50d76298b9dd9a139873ce42a5016644",
    "url": "https://www.semanticscholar.org/paper/1f728b6a50d76298b9dd9a139873ce42a5016644",
    "title": "A Novel Knowledge - engineering based approach for anaphora resolution of Tamil pronouns",
    "abstract": "This paper discusses the problem of anaphora resolution that needs to be solved for efficient parsing of Tamil sentences and proposes a knowledge-engineering methodology to resolve it. Syntax is the scientific study of sentence structure. The sentences are hierarchically structured. The sentence is not just a linear string of symbols. The position of the words, relative to one another, makes a difference in the meaning of the sentence. Such a feature gives rise to certain problems. Anaphora resolution is one among them. This paper gives a guideline to extract the necessary grammatical and semantic information for imposing selectional restriction rules. This paper suggests an efficient way of combining Ontology and Wordnet to arrive at a new concept Ontnet which will facilitate word sense disambiguation as well as anaphora resolution.",
    "venue": "2015 International Conference on Smart Technologies and Management for Computing, Communication, Controls, Energy and Materials (ICSTM)",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e5154a647faa7e010d5344827f8baa6571dcdf6e",
    "url": "https://www.semanticscholar.org/paper/e5154a647faa7e010d5344827f8baa6571dcdf6e",
    "title": "Using Linguistic Knowledge for Fine-tuning Ontologies in the Context of Requirements Engineering",
    "abstract": "Nowadays ontology creation is on the one hand very of ten hand-knitted and thus arbitrary. On the other hand it is supported by statistically enhanced information extraction and concept filtering methods. Automatized generation i n th s sense very often evokes \u201cshallow ontologies\u201d including ga ps and missing links. In the requirements engineering doma in finegranulated domain ontologies are needed; therefore th suitability of both hand-knitted and automatically generated gap-afflicted ontologies for developing application s can not always be taken for granted. In this paper we focus o n finetuning ontologies through linguistically guided key concept optimization. In our approach we suggest an incremen tal process including rudimentary linguistic analysis awell as various mapping and disambiguation steps including concept optimization through word sense identification. We a rgue that the final step of word sense identification is esse ntial, since a main feature of ontologies is that their contents m ust be shareable and therefore also understandable and tra ceable for",
    "venue": "International Journal of Computational Linguistics and Applications",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "91f92d10b2d1ed0bd62cb152fba27841c2bce8de",
    "url": "https://www.semanticscholar.org/paper/91f92d10b2d1ed0bd62cb152fba27841c2bce8de",
    "title": "Towards Building Ontologies with the Wisdom of the Crowd",
    "abstract": "Crowdsourcing provides a valuable source of input that reflects the human diversity of domain knowledge. It has increasingly been used in ontology engineering and evaluation, however, few approaches consider different types of crowdsourcing for data acquisition. In this paper, we compare two crowdsourcing techniques a mechanized labor-based task and a game-based approach to acquire shared knowledge from which we semi-automatically build an ontology. This paper focuses on the first two steps of ontology engineering, the forming of concepts and their hierarchical relations. To this end, we adapt a distributional semantic and class-based word sense disambiguation approach and a knowledge-intensive tree traversal algorithm. Each step along the process and the final resources are evaluated manually and by a gold standard created from Wikipedia data. Our results show that the ontology resulting from data obtained with the mechanized labor-based approach provides a higher level of granularity than the game-based one. However, the latter is faster and seems more enticing to participants.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": null
  },
  {
    "paperId": "69aa5126098f39ada0ac4855d0f186b741bc7ed5",
    "url": "https://www.semanticscholar.org/paper/69aa5126098f39ada0ac4855d0f186b741bc7ed5",
    "title": "Towards the relationship between Semantic Web and NLP",
    "abstract": "With the development of Semantic the Web technology, the NLP technology has much broader prospects. This article analyses the fusion degrees between the two technologies based on the survey of relations of them. We explain the relationship between Semantic Web and NLP in two aspects. One is NLP how to support Semantic Web development in Ontology Learning, Ontology Query and Multilingual Ontology Mapping. The other is Semantic Web technologies how to improve NLP results in Information Extraction and Word Sense Disambiguation. We also propose some research challenges for the cooperation between Semantic Web and NLP.",
    "venue": "International Conference on Natural Language Processing and Knowledge Engineering",
    "citationCount": 13,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "68b163a22c1dc9abb7dde3aa9ade4a142a4b851a",
    "url": "https://www.semanticscholar.org/paper/68b163a22c1dc9abb7dde3aa9ade4a142a4b851a",
    "title": "Database and Expert Systems Applications, 19th International Conference, DEXA 2008, Turin, Italy, September 1-5, 2008. Proceedings",
    "abstract": null,
    "venue": "DEXA",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "2b2cfddd5f8c542fd074bd4ec2740478336c669e",
    "url": "https://www.semanticscholar.org/paper/2b2cfddd5f8c542fd074bd4ec2740478336c669e",
    "title": "Automated Word Sense Disambiguation Using WordNet Ontology",
    "abstract": "Automatic word sense disambiguation is a major challenge in natural language processing domain. In recent years, many of supervised and knowledge-based approaches were developed to solve this problem. The use of a sense inventory as a knowledge background to disambiguate words is a very useful technique rather than supervised approaches, with its need of a large pre-trained text corpus. This paper proposes a new approach to disambiguate words in text based on WordNet ontology, as sense inventory. The authors introduce a new technique called Gloss+ for word-sense disambiguation (WSD), which is based on using of the glosses of WordNet synonyms of the target word and the local context in which this word is used. This technique exploits a special behavior of the polysemous synsets. This behavior is detected during the disambiguation process and is used to improve the results obtained. In the experiment part, the authors compare the proposed approach to the methodologies which use synonyms or glosses only.",
    "venue": "International Journal of Organizational and Collective Intelligence",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "149d819171b298a9bd29f86cc7898e7f70e702bf",
    "url": "https://www.semanticscholar.org/paper/149d819171b298a9bd29f86cc7898e7f70e702bf",
    "title": "Hierarchical Ontology Based on Word Sense Disambiguation of English to Hindi Language",
    "abstract": null,
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a5ad9178b7dac5ab98920477940e0b764322fa63",
    "url": "https://www.semanticscholar.org/paper/a5ad9178b7dac5ab98920477940e0b764322fa63",
    "title": "Ontology Matching Using BabelNet Dictionary and Word Sense Disambiguation Algorithms",
    "abstract": "Ontology matching is a discipline that means two things: first, the process of discovering correspondences between two different ontologies, and second is the result of this process, that is to say the expression of correspondences. This discipline is a crucial task to solve problems merging and evolving of heterogeneous ontologies in applications of the Semantic Web. This domain imposes several challenges, among them, the selection of appropriate similarity measures to discover the correspondences. In this article, we are interested to study algorithms that calculate the semantic similarity by using Adapted Lesk algorithm, Wu & Palmer Algorithm, Resnik Algorithm, Leacock and Chodorow Algorithm, and similarity flooding between two ontologies and BabelNet as reference ontology, we implement them, and compared experimentally. Overall, the most effective methods are Wu & Palmer and Adapted Lesk, which is widely used for Word Sense Disambiguation (WSD) in the field of Automatic Natural Language Processing (NLP).",
    "venue": "",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9ebac0d205739c3d0cfe4ff273f7f54dc99e89b2",
    "url": "https://www.semanticscholar.org/paper/9ebac0d205739c3d0cfe4ff273f7f54dc99e89b2",
    "title": "A REVIEW OF WORD SENSE DISAMBIGUATION METHOD",
    "abstract": "Background: Word Sense Disambiguation (WSD) is known to have a detrimental effect on the precision of information retrieval systems, where WSD is the ability to identify the meanings of words in context. There is a challenge in inference-correct-sensing on ambiguous words. Through many years of research, there have been various solutions to WSD that have been proposed; they have been divided into supervised and knowledge-based unsupervised. Objective: The first objective of this study was to explore the state-of-art of the WSD method with a hybrid method using ontology concepts. Then, with the findings, we may understand which tools are available to build WSD components. The second objective was to determine which method would be the best in giving good performance results of WSD, by analysing how the methods were used to answer specific WSD questions, their production, and how their performance was analysed. Methods: A review of the literature was conducted relating to the performance of WSD research, which used a comparison method of information retrieval analysis. The study compared the types of methods used in case, and examined methods for tools production, tools training, and analysis of performance. Results: In total 12 papers were found that satisfied all 3 inclusion criteria, and there was an anchor paper assigned to be referred. We chose the knowledge-based unsupervised approach because it has fewer word sets constraints than the supervised approaches which require training data. Concept-based ontology will help WSD in finding the semantic words concept with respect to another concept around it. Conclusion: Many methods was explored and compared to determine the most suitable way to build a WSD model based on semantics between words in query texts that can be related to the knowledge concept by using ontological knowledge presentation.",
    "venue": "Journal of Information Systems and Technology Management",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "8f4be8a7e6ef17c35f732d41ca22d00982cf1ca2",
    "url": "https://www.semanticscholar.org/paper/8f4be8a7e6ef17c35f732d41ca22d00982cf1ca2",
    "title": "An Efficient Method for Biomedical Word Sense Disambiguation Based on Web-Kernel Similarity",
    "abstract": "Searching for the best sense for a polysemous word remains one of the greatest challenges in the representation of biomedical text. To this end, word sense disambiguation (WSD) algorithms mostly rely on an external source of knowledge, like a thesaurus or ontology, for automatically selecting the proper concept of an ambiguous term in a given window of context using semantic similarity and relatedness measures. In this paper, the authors propose a web-based kernel function for measuring the semantic relatedness between concepts to disambiguate an expression versus multiple possible concepts. This measure uses the large volume of documents returned by PubMed search engine to determine the greater context for a biomedical short text through a new term weighting scheme based on rough set theory (RST). To illustrate the efficiency of our proposed method, they evaluate a WSD algorithm based on this measure on a biomedical dataset (MSH-WSD) that contains 203 ambiguous terms and acronyms. The obtained results demonstrate promising improvements. KEyWoRDS Biomedical Word Sense Disambiguation, Conceptualization, Context Concept, MSH-WSD, Rough Set Theory, Short Text Similarity",
    "venue": "Int. J. Heal. Inf. Syst. Informatics",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "5af6d25908902515a5c6d010ede65c1e001fff3f",
    "url": "https://www.semanticscholar.org/paper/5af6d25908902515a5c6d010ede65c1e001fff3f",
    "title": "Ontology-based workflow extraction from texts using word sense disambiguation",
    "abstract": "This paper introduces a method for automatic workflow extraction from texts using Process-Oriented Case-Based Reasoning (POCBR). While the current workflow management systems implement mostly different complicated graphical tasks based on advanced distributed solutions (e.g. cloud computing and grid computation), workflow knowledge acquisition from texts using case-based reasoning represents more expressive and semantic cases representations. We propose in this context, an ontology-based workflow extraction framework to acquire processual knowledge from texts. Our methodology extends classic NLP techniques to extract and disambiguate tasks in texts. Using a graph-based representation of workflows and a domain ontology, our extraction process uses a context-based approach to recognize workflow components : data and control flows. We applied our framework in a technical domain in bioinformatics : i.e. phylogenetic analyses. An evaluation based on workflow semantic similarities on a gold standard proves that our approach provides promising results in the process extraction domain. Both data and implementation of our framework are available in : http://labo.bioinfo.uqam.ca/tgrowler.",
    "venue": "bioRxiv",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Biology",
      "Computer Science"
    ]
  },
  {
    "paperId": "9eea4b206b8be4c2b83c7a8fe1c32a4cead6b6c7",
    "url": "https://www.semanticscholar.org/paper/9eea4b206b8be4c2b83c7a8fe1c32a4cead6b6c7",
    "title": "Word-Sense Disambiguation for Ontology Mapping: Concept Disambiguation using Virtual Documents and Information Retrieval Techniques",
    "abstract": null,
    "venue": "Journal on Data Semantics",
    "citationCount": 17,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4ca52fbe5bca8e488725f380c9454414e7e16ae2",
    "url": "https://www.semanticscholar.org/paper/4ca52fbe5bca8e488725f380c9454414e7e16ae2",
    "title": "Toward an Arabic Ontology for Arabic Word Sense Disambiguation Based on Normalized Dictionaries",
    "abstract": null,
    "venue": "OTM Workshops",
    "citationCount": 15,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "05b4c9f6b7558f840bcec1e278b8d4794ac0545d",
    "url": "https://www.semanticscholar.org/paper/05b4c9f6b7558f840bcec1e278b8d4794ac0545d",
    "title": "Ontology-Based Word Sense Disambiguation for Scientific Literature",
    "abstract": null,
    "venue": "European Conference on Information Retrieval",
    "citationCount": 19,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "179957fd648e0af875bc8f3797904b7cf7955199",
    "url": "https://www.semanticscholar.org/paper/179957fd648e0af875bc8f3797904b7cf7955199",
    "title": "Word Sense Disambiguation for Ontology Learning",
    "abstract": "Ontology learning aims to automatically extract ontological concepts and relationships from related text repositories and is expected to be more efficient and scalable than manual ontology development. One of the challenging issues associated with ontology learning is word sense disambiguation (WSD). Most WSD research employs resources such as WordNet, text corpora, or a hybrid approach. Motivated by the large volume and richness of user-generated content in social media, this research explores the role of social media in ontology learning. Specifically, our approach exploits social media as a dynamic context rich data source for WSD. This paper presents a method and preliminary evidence for the efficacy of our proposed method for WSD. The research is in progress toward conducting a formal evaluation of the social media based method for WSD, and plans to incorporate the WSD routine into an ontology learning system in the future.",
    "venue": "Americas Conference on Information Systems",
    "citationCount": 9,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "2edfca27bd08a9614ce09e08b7d3b7850f7e9b3f",
    "url": "https://www.semanticscholar.org/paper/2edfca27bd08a9614ce09e08b7d3b7850f7e9b3f",
    "title": "Improving Semantic Parsing Using Statistical Word Sense Disambiguation (Student Abstract)",
    "abstract": "A Semantic Parser generates a logical form graph from an utterance where the edges are semantic roles and nodes are word senses in an ontology that supports reasoning. The generated representation attempts to capture the full meaning of the utterance. While the process of parsing works to resolve lexical ambiguity, a number of errors in the logical forms arise from incorrectly assigned word sense determinations. This is especially true in logical and rule-based semantic parsers. Although the performance of statistical word sense disambiguation methods is superior to the word sense output of semantic parser, these systems do not produce the rich role structure or a detailed semantic representation of the sentence content. In this work, we use decisions from a statistical WSD system to inform a logical semantic parser and greatly improve semantic type assignments in the resulting logical forms.",
    "venue": "AAAI",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "396c623b940df231168ea72369ad6ef342e81083",
    "url": "https://www.semanticscholar.org/paper/396c623b940df231168ea72369ad6ef342e81083",
    "title": "Word Sense Disambiguation for Ontology Learning Research-in-Progress",
    "abstract": "Ontology learning aims to automatically extract ontological concepts and relationships from related text repositories and is expected to be more efficient and scalable than manual ontology development. One of the challenging issues associated with ontology learning is word sense disambiguation (WSD). Most WSD research employs resources such as WordNet, text corpora, or a hybrid approach. Motivated by the large volume and richness of user-generated content in social media, this research explores the role of social media in ontology learning. Specifically, our approach exploits social media as a dynamic context rich data source for WSD. This paper presents a method and preliminary evidence for the efficacy of our proposed method for WSD. The research is in progress toward conducting a formal evaluation of the social media based method for WSD, and plans to incorporate the WSD routine into an ontology learning system in the future.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d77b5e06d85f786d3d0b59cfc0a017e78388c400",
    "url": "https://www.semanticscholar.org/paper/d77b5e06d85f786d3d0b59cfc0a017e78388c400",
    "title": "Two Sides of a Coin - Translate while Classify Multilanguage Annotations with Domain Ontology-driven Word Sense Disambiguation",
    "abstract": "In this paper we present an approach for the translation and classification of short texts in one step. Our work lays in the tradition of Domain-Driven Word Sense Disambiguation, though a major emphasis is given to domain ontologies as the right tool for sense-tagging and topic detection of short texts which, by their nature, are known to be reluctant to statistical treatment. We claim that in a scenario where users can annotate knowledge items using different languages, domain ontologies can prove very suitable for driving the word disambiguation and topic classification tasks. In this way, two tasks are gainfully collapsed in a single one. Although this study is still in its infancy, in what follows we are able to articulate motivations, design, workflow analysis, and concrete evolutions envisioned for our tool.",
    "venue": "International Conference on Agents and Artificial Intelligence",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0aa0e55fcc005040c29415fae4457cb521e388a0",
    "url": "https://www.semanticscholar.org/paper/0aa0e55fcc005040c29415fae4457cb521e388a0",
    "title": "Wiki sense bag creation using multilingual word sense disambiguation",
    "abstract": "Performance of word sense disambiguation (WSD) is one of the challenging tasks in the area of natural language processing (NLP). Generation of sense annotated corpus for multilingual word sense disambiguation is out of reach for most languages even if resources are available. In this paper we propose an unsupervised method using word and sense embedding or improving the performance of these systems using untagged. Corpora and create two bags namely ontological bag and wiki sense bag to generate the senses with highest similarity. Wiki sense bag provides external knowledge to the system required to boost the disambiguation accuracy. We explore Word2Vec model to generate the sense bag and observe significant performance gain for our dataset.",
    "venue": "IAES International Journal of Artificial Intelligence (IJ-AI)",
    "citationCount": 1,
    "fieldsOfStudy": null
  },
  {
    "paperId": "afe76ee01e297b6fd2940ea37a1710797f8f109b",
    "url": "https://www.semanticscholar.org/paper/afe76ee01e297b6fd2940ea37a1710797f8f109b",
    "title": "Public Sentiment Insights Analysis using Word Sense Disambiguation Application on Twitter Data during a Pandemic \u2013 Covid\u201919",
    "abstract": "The entire world is affected because of the global pandemic Covid-19 due to the virus belongs to the family of Coronavirus As the spread of infection and mortality rate is rapid people have started developing assorted emotions about the crisis It is more significant to administer the mental health and Psychological wellbeing of public during a crisis like this As many of the people broadly use the social media like twitter for sharing their opinions and thoughts, our work utilizes the Covid specific Tweets posted by the Tweeple and analyse them to understand the sentiments exhibited regarding the situation After the tweets are collected and the real sentiments behind them are discovered using the classifier model developed using the Machine Learning methods The experimental results may be used by the respective authorities to take necessary initiatives for addressing the concerns that affect the wellbeing of the society and the economic wellbeing as well As our word uses Lexical based sentiment analysis, it is important to remove the ambiguities of a word which is a main challenge of this technique on sentiment analysis To improve the performance of the Sentiment Analysis we have used the lexical dictionaries Wordnet and SentiWordNet along with Word Sense Disambiguation (WSD) to detect and remove the ambiguities understanding the context of the term used in the tweets \u00a9 2020, World Academy of Research in Science and Engineering All rights reserved",
    "venue": "International Journal of Advanced Trends in Computer Science and Engineering",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Psychology"
    ]
  },
  {
    "paperId": "ded9577c8225e34c6674a3ec0d7758435a964917",
    "url": "https://www.semanticscholar.org/paper/ded9577c8225e34c6674a3ec0d7758435a964917",
    "title": "A domain independent semantic measure for keyword sense disambiguation",
    "abstract": "Understanding the user's intention is crucial in human-machine interaction. When dealing with text input, Word Sense Disambiguation (WSD) techniques play an important role. WSD techniques typically require well-formed sentences as context to operate, and predefined catalogues of word senses. However, such conditions do not always apply, such as when there is a need to disambiguate keywords from a query, or sets of tags describing any Web resource. In this paper, we propose a keyword disambiguation method based on the semantic relatedness between words and ontological terms. Taking advantage of the semantic information captured by word embeddings, our approach maps a set of input keywords to their meanings within a given target ontology. We focus on situations where the available linguistic information is very scarce, hampering natural language based approaches. Experimental results show the feasibility of our approach without previous training for target domains.",
    "venue": "SAC",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1621654a200de15b6c1377478e0e535a7581f756",
    "url": "https://www.semanticscholar.org/paper/1621654a200de15b6c1377478e0e535a7581f756",
    "title": "A Supervised Word Sense Disambiguation Method Using Ontology and Context Knowledge",
    "abstract": "Word Sense Disambiguation is one of the basic tasks in Natural language processing. It is the method of selecting the correct sense of the word in the given context. It is applied whenever a semantic understanding of text is needed. In order to disambiguate a word, two resources are necessary: a context in which the word has been used, and some kind of knowledge related to the word. This paper presents a method for word sense disambiguation task using a tree-matching approach. The method requires a context knowledge base containing the corpus of sentences. This paper also gives some preliminary results when a corpus containing the ambiguous words is tested on this system. Keywords: Natural Language Understanding, Word Sense Disambiguation; Tree-matching; dependent-word matching",
    "venue": "",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d525ad9abfb7ee25e93fb1e45094bc0775721c57",
    "url": "https://www.semanticscholar.org/paper/d525ad9abfb7ee25e93fb1e45094bc0775721c57",
    "title": "A Weakly supervised word sense disambiguation for Polish using rich lexical resources",
    "abstract": "Abstract Automatic word sense disambiguation (WSD) has proven to be an important technique in many natural language processing tasks. For many years the problem of sense disambiguation has been approached with a wide range of methods, however, it is still a challenging problem, especially in the unsupervised setting. One of the well-known and successful approaches to WSD are knowledge-based methods leveraging lexical knowledge resources such as wordnets. As the knowledge-based approaches mostly do not use any labelled training data their performance strongly relies on the structure and the quality of used knowledge sources. However, a pure knowledge-base such as a wordnet cannot reflect all the semantic knowledge necessary to correctly disambiguate word senses in text. In this paper we explore various expansions to plWordNet as knowledge-bases for WSD. Semantic links extracted from a large valency lexicon (Walenty), glosses and usage examples, Wikipedia articles and SUMO ontology are combined with plWordNet and tested in a PageRank-based WSD algorithm. In addition, we analyse also the influence of lexical semantics vector models extracted with the help of the distributional semantics methods. Several new Polish test data sets for WSD are also introduced. All the resources, methods and tools are available on open licences.",
    "venue": "Poznan Studies in Contemporary Linguistics",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Psychology"
    ]
  },
  {
    "paperId": "03b7fe82dd7b08a9d86e39ed9a4d4000d90308b4",
    "url": "https://www.semanticscholar.org/paper/03b7fe82dd7b08a9d86e39ed9a4d4000d90308b4",
    "title": "Abstractive Text Summarization: Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization",
    "abstract": "Abstract Nowadays, most research conducted in the field of abstractive text summarization focuses on neural-based models alone, without considering their combination with knowledge-based approaches that could further enhance their efficiency. In this direction, this work presents a novel framework that combines sequence-to-sequence neural-based text summarization along with structure and semantic-based methodologies. The proposed framework is capable of dealing with the problem of out-of-vocabulary or rare words, improving the performance of the deep learning models. The overall methodology is based on a well-defined theoretical model of knowledge-based content generalization and deep learning predictions for generating abstractive summaries. The framework is composed of three key elements: (i) a pre-processing task, (ii) a machine learning methodology, and (iii) a post-processing task. The pre-processing task is a knowledge-based approach, based on ontological knowledge resources, word sense disambiguation, and named entity recognition, along with content generalization, that transforms ordinary text into a generalized form. A deep learning model of attentive encoder-decoder architecture, which is expanded to enable a coping and coverage mechanism, as well as reinforcement learning and transformer-based architectures, is trained on a generalized version of text-summary pairs, learning to predict summaries in a generalized form. The post-processing task utilizes knowledge resources, word embeddings, word sense disambiguation, and heuristic algorithms based on text similarity methods in order to transform the generalized version of a predicted summary to a final, human-readable form. An extensive experimental procedure on three popular data sets evaluates key aspects of the proposed framework, while the obtained results exhibit promising performance, validating the robustness of the proposed approach.",
    "venue": "International Conference on Computational Logic",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "209662012ed4fe77b6194772ea859c9fadfd0e3c",
    "url": "https://www.semanticscholar.org/paper/209662012ed4fe77b6194772ea859c9fadfd0e3c",
    "title": "Matching Domain and Top-level ontologies exploring Word Sense Disambiguation and Word Embedding",
    "abstract": "Top-level ontologies play an important role in the construction and integration of domain ontologies, providing a well-founded reference model that can be shared across domains. While most efforts in ontology matching have been particularly dedicated to domain ontologies, the problem of matching domain and top-level ontologies has been addressed to a lesser extent. This is a challenging task in the field, specially due to the different levels of abstraction of these ontologies. This paper addresses this problem by proposing an approach that relies on existing alignments between WordNet and top-level ontologies. Our approach explores word sense disambiguation and word embedding models. We evaluate our approach in the task of matching DOLCE and SUMO top-level ontologies to ontologies from three different domains.",
    "venue": "International Workshop on the Semantic Web",
    "citationCount": 9,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "ece99a7c2ee571664eba8ed6182104f93a32931f",
    "url": "https://www.semanticscholar.org/paper/ece99a7c2ee571664eba8ed6182104f93a32931f",
    "title": "An Analytical Review on Word Sense Disambiguation",
    "abstract": "Due to high Semantic Ambiguity that is associated with language, Word Sense Disambiguation is an open problem of Natural Language Processing and Ontology in Computer Linguistics. For instance, the word Head can mean the part of body and a person in charge of organization. The task of automatically assigning the most suitable meaning to a polysemous word can be defined as Word Sense Disambiguation. Several approaches have been done, from Dictionary-and Knowledge-based method that uses Lexical resource like WordNet, to Supervised-Machine learning method that use trained classifier on manually sense-annotated corpus, to cluster based Un-Supervised methods. These approaches have been applied for several languages like German, English, French, Chinese, and some Indian languages like Assamese, Hindi, Marathi and Malayalam. This survey aims to present about some aspect of word sense disambiguation and its approaches, focusing more on Unsupervised Graph based approach.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "13a0f60b8e1808b7aa4e236e69ca80d9773b3b08",
    "url": "https://www.semanticscholar.org/paper/13a0f60b8e1808b7aa4e236e69ca80d9773b3b08",
    "title": "Bioinformatic Workflow Extraction from Scientific Texts based on Word Sense Disambiguation",
    "abstract": "This paper introduces a method for automatic workflow extraction from texts using Process-Oriented Case-Based Reasoning (POCBR). While the current workflow management systems implement mostly different complicated graphical tasks based on advanced distributed solutions (e.g., cloud computing and grid computation), workflow knowledge acquisition from texts using case-based reasoning represents more expressive and semantic case representations. We propose in this context, an ontology-based workflow extraction framework to acquire processual knowledge from texts. Our methodology extends the classic NLP techniques to extract and disambiguate complex tasks and relations in texts. Using a graph-based representation of workflows and a domain ontology, our extraction process uses a context-aware approach to recognize workflow components in texts: data and control flows. We applied our framework in a technical domain in bioinformatics: i.e., phylogenetic analyses. An evaluation based on workflow semantic similarities in a gold standard proves that our approach provides promising results in the process extraction domain. Both data and implementation of our framework are available in:\u00a0http://labo.bioinfo.uqam.ca/tgowler.",
    "venue": "IEEE/ACM Transactions on Computational Biology & Bioinformatics",
    "citationCount": 10,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ]
  },
  {
    "paperId": "5ddeb7cbf6d879a31091eed56b80c2dc47838474",
    "url": "https://www.semanticscholar.org/paper/5ddeb7cbf6d879a31091eed56b80c2dc47838474",
    "title": "An ontology clarification tool for word sense disambiguation",
    "abstract": "This paper presents a method for word sense disambiguation based on Lesk algorithm which uses lexical database WordNet as knowledge base. The word sense disambiguation is the process of automatically clarifying a meaning of a word in its context. In general, ontology means the meaning or analogous term. It can be interpreted by relating the word with other words in the sentence. This tool accepts English statement as input and gives best possible meaning of given word. Method is experimented with senseval-2 test data for lexical sample task. The results show the betterment over the original Lesk algorithm.",
    "venue": "2011 3rd International Conference on Electronics Computer Technology",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "036773ee10266851a471fbcb89c9e161b0ee3590",
    "url": "https://www.semanticscholar.org/paper/036773ee10266851a471fbcb89c9e161b0ee3590",
    "title": "Urdu word sense disambiguation using machine learning approach",
    "abstract": null,
    "venue": "Cluster Computing",
    "citationCount": 16,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9fba7523c25feb27ea8e65ed07160fc0bc150fe5",
    "url": "https://www.semanticscholar.org/paper/9fba7523c25feb27ea8e65ed07160fc0bc150fe5",
    "title": "Ontology Based Query Expansion Using Word Sense Disambiguation",
    "abstract": "Abstract - The existing information retrieval techniques do not consider the context of the keywords present in the user\u2019s queries. Therefore, the search engines sometimes do not provide sufficient information to the users. New methods based on the semantics of user keywords must be developed to search in the vast web space without incurring loss of information. The semantic based information retrieval techniques need to understand the meaning of the concepts in the user queries. This will improve the precision-recall of the search results. Therefore, this approach focuses on the concept based semantic information retrieval. This work is based on Word sense disambiguation, thesaurus WordNet and ontology of any domain for retrieving information in order to capture the context of particular concept(s) and discover semantic relationships between them. reaction. Index terms \u2013 Word Sense Disambiguation, Semantic Information Retrieval, Clustering, Ontology. I. INTRODUCTION",
    "venue": "ArXiv",
    "citationCount": 19,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9df1ac67c0982d3f89ad26a751be998e2ed085b1",
    "url": "https://www.semanticscholar.org/paper/9df1ac67c0982d3f89ad26a751be998e2ed085b1",
    "title": "Natural Language Engineering: The Study of Word Sense Disambiguation in Punjabi",
    "abstract": "Word Sense Disambiguation (WSD) is an important part of Machine Readable Dictionary (MRD) which is extensively used in Expert System/Intelligent Systems. All languages have multiple meanings of words or phrases depending on the context of their usage. WSD draws the correct (intended) meaning using a database called Machine Readable Dictionary (MRD). Some rudimentary designs of MRD have been made for some European Languages. In this paper a preliminary attempt has been made towards the formulation and design of MRD in Punjabi Language using modified Lesk Algorithm which uses a simple method for relating the appropriate word sense relative to set of dictionary meanings of the word or phrase.",
    "venue": "",
    "citationCount": 17,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "61c8d5ea1b6be923f71f2f568b07a42fc0dd1336",
    "url": "https://www.semanticscholar.org/paper/61c8d5ea1b6be923f71f2f568b07a42fc0dd1336",
    "title": "WORD SENSE DISAMBIGUATION USING LESK",
    "abstract": "Word Sense Disambiguation (WSD) is an open problem of natural language processing and ontology. WSD is identifying which sense of a word (i.e. meaning) is used in a sentence, when the word has multiple meanings. Disambiguating a word needs two things: Dictionary having a list of senses of ambiguous word i.e. semantic relations of a polysemous word and Corpus (Real World Text) consisting real world knowledge. It is difficult for system or even to human being to identify the correct sense without a sense repository i.e. knowledge sources. There are two types of knowledge sources, one is corpus and another one is WordNet. This paper Lesk algorithm is used to resolving ambiguity in a sentence which is based on integrating through WordNet. KeywordsWord Sense Disambiguation, Information extraction and retrieval, WordNet. Word Sense Disambiguation Using Lesk 64 Knowledge based approach, often refer as dictionary based approach uses lexical knowledge bases such as dictionaries like WordNet [3], thesauri, ontology etc and acquire information related to word from word definition and relations present the respective knowledge base[2,4]. Agirre, Eneko & German Rigau (1996) [5] proposed Word Sense Disambiguation with Conceptual Density method which uses lexical knowledge base. This method\u2019s basic idea is to select a sense based on the conceptual distance i.e. how the ambiguous word and its context words are related. First find the noun in context then its senses and relations majorly the hypernym. This result is later extended by the same researcher i.e Agirre, Eneko & David Martinez (2001) suggested to finding the correct sense use Selectional preferences method [6]. This method look for the probable associations between word categories, the simplest measure for this word to word relation is frequency count. Overlap based approaches like Lesk, Extended Lesk are purely based on the matching of word and context words. This approach is suggested by Satanjeev Banerjee, Ted Pedersen, 2002 [7]. Basic problems with this approach is it is heavily depends on dictionaries, which is also having some restrictions over acquiring the common sense knowledge. Methods treat a dictionary as both the source of the sense inventory as well as a repository of information about words that can be exploited to distinguish their meanings in text. WordNet as lexical database or sense inventory to access meanings and other information related to words. Supervised approach: Supervised approach makes use of sense annotated corpora to train from. These approaches use machine learning techniques to learn a classifier from labeled training sets. Some of the common techniques used are decision lists, decision trees, naive bayes, neural networks, support vector machines (SVM). Unsupervised approach: Unsupervised approach makes use of only raw annotated corpora and do not exploit any sense-tagged corpus to provide a sense choice for a word in context. These methods are context clustering, word clustering and cooccurrence graphs. Semi supervised approach: Semi supervised approach makes use of secondary source of knowledge such as small annotated corpus as seed data in bootstrapping process. It actually overcomes the main problems associated with building a classifier: the lack of annotated data and the data sparsity problem.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "ff6d6ac5951e0c8781060200d2c20f1d45b0b4fc",
    "url": "https://www.semanticscholar.org/paper/ff6d6ac5951e0c8781060200d2c20f1d45b0b4fc",
    "title": "The Various Approaches for Word Sense Disambiguation: A Survey",
    "abstract": "- Word sense disambiguation is an important problem of natural language processing and ontology in the field of computational linguistics. It is defined as identifying the exact sense of a word which is used in the sentence. It is used in various applications of natural language processing like machine translation, information extraction and text mining, information retrieval etc. This survey paper describes the various approaches adopted for word sense disambiguation such as dictionary and knowledge based approaches, supervised approaches, semi-supervised approaches and unsupervised approaches.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "df7f54e3f702a8a33eb54c20be19c6fd31654e76",
    "url": "https://www.semanticscholar.org/paper/df7f54e3f702a8a33eb54c20be19c6fd31654e76",
    "title": "Biomedical Word Sense Disambiguation context-based: Improvement of SenseRelate method",
    "abstract": "Context information is very important to identify the sense of a polysemous word and may offer clues to the Word Sense Disambiguation (WSD). WSD algorithms, therefore, mostly rely on an External Source of Knowledge like a Thesaurus or Ontology to automatically select the proper concept of an ambiguous term in a given Window of Context. SenseRelate is a well-known WSD algorithm, which use a fixed window size and taking into consideration the distance weight on how far the terms in the context are from the target word. This may impact negatively on the yielded concepts or senses. To overcome this problem, and therefore to enhance the process of Biomedical WSD, in this paper we propose a simple modified versions of SenseRelate Algorithm named NoDistanceSenseRelate which simply ignore the distance, that is the terms in the context will have the same distance weight. In this study, we realize a comparative study between SenseRelate and NoDistanceSenseRelate methods, and evaluate the effect of context window size to WSD Context-Based, by exploiting Semantic Similarity and Relatedness measures extracted from Biomedical Resources. We evaluate our system on a biomedical dataset (MSH-WSD) that contains 203 ambiguous terms and acronyms. The obtained results show that NoDistanceSenseRealte method consistently obtain higher disambiguation accuracy on window size of 3, However on window size of 2, SenseRelate method obtain higher disambiguation accuracy than NoDistance-SenseRealte.",
    "venue": "2016 International Conference on Information Technology for Organizations Development (IT4OD)",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6ec3bc9c651f08165b3572e65a20fdd139234621",
    "url": "https://www.semanticscholar.org/paper/6ec3bc9c651f08165b3572e65a20fdd139234621",
    "title": "User ontology and word sense disambiguation for query expansion",
    "abstract": "Inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval. In this paper we propose to have twice query expansion, the first query expansion is progressed by determining the relatedness between two word senses via structural and domain relatedness computation based on WordNet and WordNet Domain ontology. Further more, the second query expansion is carried through an ontology based user model, called user ontology.",
    "venue": "2010 International Conference on Computer Application and System Modeling (ICCASM 2010)",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "25c6242a3fd98a0fdea5e985905f30032e23d522",
    "url": "https://www.semanticscholar.org/paper/25c6242a3fd98a0fdea5e985905f30032e23d522",
    "title": "DIFFERENT TECHNIQUES IMPLEMENTED IN GURUMUKHI WORD SENSE DISAMBIGUATION",
    "abstract": "One of the most important issues in the field of Natural Language Engineering is Word Sense Disambiguation (WSD).Gurumukhi or more commonly known as Punjabi, is world\u2019s 12th most widely spoken language and this language is morphologically rich. But surprisingly, there are relatively less efforts in the field of computerization and development of lexical resources of this language. It is therefore motivating to develop a corpus of Punjabi Language that will help in tagging the sense of the words.The availability of sense tagged corpora contribute a lot in advances in WSD. Most accurate WSD systems use supervised learning algorithm to learn contextual rules or classification models automatically from sense-annotated examples, like Na\u00efve Bayes, k-NN and Support Vector Machine (SVM) classifiers have shown high accuracy in WSD. The majority of work on WSD is focused on English and other European languages and standard test corpora are available for these languages. The lack of such standards put a major hindrance on WSD research for Punjabi and other Regional Indian languages. Thus, this defines the objective of this survey.",
    "venue": "",
    "citationCount": 6,
    "fieldsOfStudy": null
  },
  {
    "paperId": "662d6869632018f14d5e6cf6de57a64f2f18a521",
    "url": "https://www.semanticscholar.org/paper/662d6869632018f14d5e6cf6de57a64f2f18a521",
    "title": "Disjoint Semi-supervised Spanish Verb Sense Disambiguation Using Word Embeddings",
    "abstract": "This work explores the use of word embeddings, also known as word vectors, trained on Spanish corpora, to use as features for Spanish verb sense disambiguation (VSD). This type of learning technique is named disjoint semisupervised learning [1]: an unsupervised algorithm is trained on unlabeled data separately as a first step, and then its results (i.e. the word embeddings) are fed to a supervised classifier. Throughout this paper we try to assert two hypothesis: (i) representations of training instances based on word embeddings improve the performance of supervised models for VSD, in contrast to more standard feature engineering techniques based on information taken from the training data; (ii) using word embeddings trained on a specific domain, in this case the same domain the labeled data is gathered from, has a positive impact on the model\u2019s performance, when compared to general domain\u2019s word embeddings. The performance of a model over the data is not only measured using standard metric techniques (e.g. accuracy or precision/recall) but also measuring the model tendency to overfit the available data by analyzing the learning curve. Measuring this overfitting tendency is important as there is a small amount of available data, thus we need to find models to generalize better the VSD problem. For the task we use SenSem [2], a corpus and lexicon of Spanish and Catalan disambiguated verbs, as our base resource for experimentation.",
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "08342c5a989dafd430905c383f205e85f05d6435",
    "url": "https://www.semanticscholar.org/paper/08342c5a989dafd430905c383f205e85f05d6435",
    "title": "Textual Analysis by using Knowledge-based Word Sense Disambiguation Approach",
    "abstract": "Textual analysis had been widely used in the software engineering area. Even though some approaches had been suggested over the time, these approaches encounter number of challenges, especially dealing with information extracted from the text requirement. Most studies had chosen to analyse the text manually in order to overcome this challenge. However, the long and complex text would consume more time. This paper will discuss a framework based on the knowledgebased word sense disambiguation approach, an attempt to improve the knowledge representation. In this approach, WordNet 2.1 would be used as the knowledge source used to identify concepts represented by each word in a text.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6917fa7409916e7098d6c6a961d5d387e4a68f7a",
    "url": "https://www.semanticscholar.org/paper/6917fa7409916e7098d6c6a961d5d387e4a68f7a",
    "title": "An LMF-based Normalization approach of Arabic Islamic dictionaries for Arabic Word Sense Disambiguation: application on hadith",
    "abstract": "In this paper, we propose an approach for normalizing Arabic Dictionaries. This approach is used to transform non structured Arabic dictionaries into LMF (Lexical Markup Framework) based-normalized ones. We are basically exploiting Arabic Islamic dictionaries of hadith. An ontology will be then constructed from these normalized dictionaries. This ontology will contain explicit and formal knowledge about information in hadith. It will be used later by an information retrieval system for Word Sense Disambiguation of Arabic terms of hadith either in the formulated user query or in the texts of hadith.",
    "venue": "",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "b624d2d8dbac62892cbf49222cf1e2dd0378391a",
    "url": "https://www.semanticscholar.org/paper/b624d2d8dbac62892cbf49222cf1e2dd0378391a",
    "title": "Urdu word sense disambiguation using machine learning approach",
    "abstract": null,
    "venue": "Cluster Computing",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "e7299d9e4d36add533ffc4925c6d075d408693ab",
    "url": "https://www.semanticscholar.org/paper/e7299d9e4d36add533ffc4925c6d075d408693ab",
    "title": "Word sense disambiguation using word ontology and concept distribution",
    "abstract": "Abstract This paper presents a method of word sense disambiguation that assigns a target word the sense that is most related to the senses of its neighbor words. We explore the use of measures of relatedness between word senses based on a novel hybrid approach. First, we investigate how to \u201cliterally\u201d and \u201cregularly\u201d express a \u201cconcept\u201d. We apply set algebra to Wordnet's synsets cooperating with Wordnet's word ontology. In this way we establish regular rules for constructing various representations (lexical notations) of a concept using Boolean operators and word forms in various synset(s) defined in Wordnet. Then we establish a formal mechanism for quantifying and estimating the semantic relatedness between concepts \u2013 we facilitate \u201cconcept distribution statistics\u201d to determine the degree of semantic relatedness between two lexically expressed concepts. Human languages have words that can mean different things in different contexts, such words with multiple meanings are potentially \u201cambiguous\u201d. The process of \u201cdeciding which of several meanings of a term is intended in a given context\u201d is known as \u201cWord Sense Disambiguation (WSD)\u201d. The proposed method is not supervised, and does not require any manually created sense\u2010tagged training examples. The experimental results showed good performance on Semcor, a subset of the Brown corpus. We observe that measures of semantic relatedness are useful sources of information for word sense disambiguation.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6100eaa12cb1fd9c59821e1341785d3db9f70da2",
    "url": "https://www.semanticscholar.org/paper/6100eaa12cb1fd9c59821e1341785d3db9f70da2",
    "title": "Approaches To Word Sense Disambiguation",
    "abstract": "Removing ambiguous meaning of a word has been an extensive area of research in the field of computational linguistics. This paper presents a comprehensive study of the different dictionary approaches to Word Sense Disambiguation. Three approaches have been explored; graph based, ontology based and knowledge based. All the approaches use a corpus to evaluate words unlike in a machine learning approach. A machine learning approach trains a dataset of words and calculates the probability that a sense is correct. Knowledge based approaches are better but require world knowledge to be most efficient. Hence it is suggested to use hybrid methods which combine machine learning algorithms with corpus analysing algorithms. Keywords\u2014Word Sense disambiguation; graph based approach; ontology; Lesk Algorithm; Conceptual Density; Random Walks.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "8102a64a0f4b353f9a9e15ef671469270fdfd1f8",
    "url": "https://www.semanticscholar.org/paper/8102a64a0f4b353f9a9e15ef671469270fdfd1f8",
    "title": "On the Application of the Cyc Ontology to Word Sense Disambiguation",
    "abstract": "This paper describes a novel, unsupervised method of word sense disambiguation that is wholly semantic, drawing upon a complex, rich ontology and inference engine (the Cyc system). This method goes beyond more familiar semantic closeness approaches to disambiguation that rely on string cooccurrence or relative location in a taxonomy or concept map by 1) exploiting a rich array of properties, including higher-order properties, not available in merely taxonomic (or other first-order) systems, and 2) appealing to the semantic contribution a word sense makes to the content of the target text. Experiments show that this method produces results markedly better than chance when disambiguating word senses in a corpus of topically unrelated documents.",
    "venue": "The Florida AI Research Society",
    "citationCount": 58,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9a17207e9528fa4c795350db899c04aabae4a1d3",
    "url": "https://www.semanticscholar.org/paper/9a17207e9528fa4c795350db899c04aabae4a1d3",
    "title": "Word Sense Disambiguation in Software Requirement Specifications Using WordNet and Association Mining Rule",
    "abstract": "The most significant phase in the development of a quality software project is Requirement engineering. The objective of the software requirement engineering is the elicitation of the requirements of the clients and their analysis. In general the requirements are expressed in natural languages which are ambiguous in nature. Ambiguity means the same word or sentence can be interpreted differently by different persons. The Word Sense Disambiguation (WSD) system assigns the correct meaning to the words having multiple interpretations, depending on the context of use. In this paper, we propose a framework, for removing ambiguities in an SRS (Software Requirement Specifications) document in an efficient way. This framework uses the WordNet and the concept of Association rule mining for assigning the correct interpretation of a word in given context.",
    "venue": "Italian Conference on Theoretical Computer Science",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a2857e1ad4361f05e2f774665954e7753b85087f",
    "url": "https://www.semanticscholar.org/paper/a2857e1ad4361f05e2f774665954e7753b85087f",
    "title": "Word Sense Disambiguation as the Primary Step of Ontology Integration",
    "abstract": null,
    "venue": "DEXA",
    "citationCount": 12,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "cd349397fe11eae15f5b9fecb8f4ededb99ab112",
    "url": "https://www.semanticscholar.org/paper/cd349397fe11eae15f5b9fecb8f4ededb99ab112",
    "title": "A Knowledge-Based Sense Disambiguation Method to Semantically Enhanced NL Question for Restricted Domain",
    "abstract": "Within the space of question answering (QA) systems, the most critical module to improve overall performance is question analysis processing. Extracting the lexical semantic of a Natural Language (NL) question presents challenges at syntactic and semantic levels for most QA systems. This is due to the difference between the words posed by a user and the terms presently stored in the knowledge bases. Many studies have achieved encouraging results in lexical semantic resolution on the topic of word sense disambiguation (WSD), and several other works consider these challenges in the context of QA applications. Additionally, few scholars have examined the role of WSD in returning potential answers corresponding to particular questions. However, natural language processing (NLP) is still facing several challenges to determine the precise meaning of various ambiguities. Therefore, the motivation of this work is to propose a novel knowledge-based sense disambiguation (KSD) method for resolving the problem of lexical ambiguity associated with questions posed in QA systems. The major contribution is the proposed innovative method, which incorporates multiple knowledge sources. This includes the question\u2019s metadata (date/GPS), context knowledge, and domain ontology into a shallow NLP. The proposed KSD method is developed into a unique tool for a mobile QA application that aims to determine the intended meaning of questions expressed by pilgrims. The experimental results reveal that our method obtained comparable and better accuracy performance than the baselines in the context of the pilgrimage domain.",
    "venue": "Inf.",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "3abc90b134460209f32486d6c0c187762cd16db0",
    "url": "https://www.semanticscholar.org/paper/3abc90b134460209f32486d6c0c187762cd16db0",
    "title": "Word Sense Disambiguation Based on Large Scale Polish CLARIN Heterogeneous Lexical Resources",
    "abstract": "Word Sense Disambiguation Based on Large Scale Polish CLARIN Heterogeneous Lexical Resources Lexical resources can be applied in many different Natural Language Engineering tasks, but the most fundamental task is the recognition of word senses used in text contexts. The problem is difficult, not yet fully solved and different lexical resources provided varied support for it. Polish CLARIN lexical semantic resources are based on the plWordNet \u2014 a very large wordnet for Polish \u2014 as a central structure which is a basis for linking together several resources of different types. In this paper, several Word Sense Disambiguation (henceforth WSD) methods developed for Polish that utilise plWordNet are discussed. Textual sense descriptions in the traditional lexicon can be compared with text contexts using Lesk\u2019s algorithm in order to find best matching senses. In the case of a wordnet, lexico-semantic relations provide the main description of word senses. Thus, first, we adapted and applied to Polish a WSD method based on the Page Rank. According to it, text words are mapped on their senses in the plWordNet graph and Page Rank algorithm is run to find senses with the highest scores. The method presents results lower but comparable to those reported for English. The error analysis showed that the main problems are: fine grained sense distinctions in plWordNet and limited number of connections between words of different parts of speech. In the second approach plWordNet expanded with the mapping onto the SUMO ontology concepts was used. Two scenarios for WSD were investigated: two step disambiguation and disambiguation based on combined networks of plWordNet and SUMO. In the former scenario, words are first assigned SUMO concepts and next plWordNet senses are disambiguated. In latter, plWordNet and SUMO are combined in one large network used next for the disambiguation of senses. The additional knowledge sources used in WSD improved the performance. The obtained results and potential further lines of developments were discussed.",
    "venue": "",
    "citationCount": 23,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c1e3f57a789cc0631e7ffa0503ffe4e7fa01e949",
    "url": "https://www.semanticscholar.org/paper/c1e3f57a789cc0631e7ffa0503ffe4e7fa01e949",
    "title": "oWSD: A Tool for Word Sense Disambiguation in Its Ontology Context",
    "abstract": "Word sense disambiguation (abbr. WSD) is very important to the semantic web/web 2.0. However, there is still no easy-to-use tool available. As a remedy, here a simple and very efficient tool called oWSD is demonstrated. It disambiguates the senses of words in their ontological contexts, and obtains the right word senses from WordNet. It is very helpful to applications involving ontologies and natural language processing as well.",
    "venue": "SEMWEB",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "562368b6ced7ff4af4eae7a728dc0a8e4a1f3a3b",
    "url": "https://www.semanticscholar.org/paper/562368b6ced7ff4af4eae7a728dc0a8e4a1f3a3b",
    "title": "Big Data-Assisted Word Sense Disambiguation for Sign Language",
    "abstract": null,
    "venue": "Research & Innovation Forum",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "872b9fa074e4050550b4acb597ab0a4d317e95ed",
    "url": "https://www.semanticscholar.org/paper/872b9fa074e4050550b4acb597ab0a4d317e95ed",
    "title": "Ontology-Supported Text Classification Based on Cross-Lingual Word Sense Disambiguation",
    "abstract": null,
    "venue": "International Workshop on Fuzzy Logic and Applications",
    "citationCount": 9,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "3e1d7192aa69d4a5a363e092a05648f49725e3a9",
    "url": "https://www.semanticscholar.org/paper/3e1d7192aa69d4a5a363e092a05648f49725e3a9",
    "title": "Chinese Word Sense Disambiguation Based on Lexical Semantic Ontology",
    "abstract": "This paper describes preliminary works of word sense disambiguation on Chinese verbs using the information derived from lexical semantic ontology (LSO). In spite of sophisticated methods, simple algorithm is employed to underline the characters of the features chosen from LSO data. Several groups of tests are designed to find different effects of the features and other aspects. Some promising results are gotten from the prime tests on nine Chinese ambiguous verbs. The results show what informative features the LSO provides and the potential improving ways.",
    "venue": "J. Chin. Lang. Comput.",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "196a6ac3065c6a01db89395ffea501cd8042bfd6",
    "url": "https://www.semanticscholar.org/paper/196a6ac3065c6a01db89395ffea501cd8042bfd6",
    "title": "Ontology-based word sense disambiguation using semi-automatically constructed ontology",
    "abstract": "This paper describes a method for disambiguating word senses by using semi-automatically constructed ontology. The ontology stores rich semantic constraints among 1,110 concepts, and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology. In order to acquire a reasonably practical ontology in limited time and with less manpower, we extend the existing Kadokawa thesaurus by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. The former can be obtained by converting valency information and case frames from previously-built electronic dictionaries used in machine translation. The latter can be acquired from concept co-occurrence information, which is extracted automatically from large corpora. In our practical machine translation system, our word sense disambiguation method achieved a 9.2% improvement over methods which do not use an ontology for Korean translation.",
    "venue": "MTSUMMIT",
    "citationCount": 10,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "cd78b7341b8f48f2e418d137d7b89b0e383be8aa",
    "url": "https://www.semanticscholar.org/paper/cd78b7341b8f48f2e418d137d7b89b0e383be8aa",
    "title": "Building an Ontology-Based Multilingual Lexicon for Word Sense Disambiguation in Machine Translation",
    "abstract": "Word sense disambiguation (WSD) requires the establishment of a list of the different meanings of words. WSD efforts in machine translation require, in addition, the equivalent translation words in target languages. To facilitate WSD in machine translation systems, we propose the construction of an ontology-based multilingual lexicon, from various existing language resources, as an alternative to existing hierarchical lexicons such as WordNet and Roget\u2019s Thesaurus. Apart from providing equivalent words from different languages, the lexicon will be used to extend a WSD algorithm that calculates lexical conceptual distance data. The information in the lexicon to be constructed can also be used for other natural language processing tasks.",
    "venue": "",
    "citationCount": 7,
    "fieldsOfStudy": null
  },
  {
    "paperId": "d0ba4f846182278ebaef5493817ecad74a6af436",
    "url": "https://www.semanticscholar.org/paper/d0ba4f846182278ebaef5493817ecad74a6af436",
    "title": "Word Sense Disambiguation Using Aggregated Similarity Based on WordNet Graph Representation",
    "abstract": "The term of word sense disambiguation, WSD, is introduced in the context of text document processing. A knowledge based approach is conducted using WordNet lexical ontology, describing its structure and components used for the process of identification of context related senses of each polysemy words. The principal distance measures using the graph associated to WordNet are presented, analyzing their advantages and disadvantages. A general model for aggregation of distances and probabilities is proposed and implemented in an application in order to detect the context senses of each word. For the non-existing words from WordNet, a similarity measure is used based on probabilities of co-occurrences. The module of WSD is proposed for integration in the step of processing documents such as supervised and unsupervised classification in order to maximize the correctness of the classification. Future work is related to the implementation of different domain oriented ontologies.",
    "venue": "",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "ba1bc10e15ed312ec30e3ff576e227961ee385ac",
    "url": "https://www.semanticscholar.org/paper/ba1bc10e15ed312ec30e3ff576e227961ee385ac",
    "title": "Corpus-based Ontology Learning for Word Sense Disambiguation",
    "abstract": "This paper proposes to disambiguate word senses by corpus-based ontology learning. Our approach is a hybrid method. First, we apply the previously-secured dictionary information to select the correct senses of some ambiguous words with high precision, and then use the ontology to disambiguate the remaining ambiguous words. The mutual information between concepts in the ontology was calculated before using the ontology as knowledge for disambiguating word senses. If mutual information is regarded as a weight between ontology concepts, the ontology can be treated as a graph with weighted edges, and then we locate the least weighted path from one concept to the other concept. In our practical machine translation system, our word sense disambiguation method achieved a 9% improvement over methods which do not use ontology for Korean translation.",
    "venue": "PACLIC",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e5473a31796dd1db73bef5c1ce6e93ceaac36b01",
    "url": "https://www.semanticscholar.org/paper/e5473a31796dd1db73bef5c1ce6e93ceaac36b01",
    "title": "A Framework for WordNet-based Word Sense Disambiguation",
    "abstract": "This paper a framework and method for resolving word sense disambiguation and present the results. In this work, WordNet is used for two different purposes: one as a dictionary and the other as an ontology, containing the hierarchical structure, representing hypernym-hyponym relations. The advantage of this approach is twofold. First, it provides a very simple method that is easily implemented. Second, we do not suffer from the lack of large corpus data which would have been necessary in a statistical method. In the future this can be extended to incorporate other relations, such as synonyms, meronyms, and",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "325600648190fc945fb0587545fda7451ec0ef6c",
    "url": "https://www.semanticscholar.org/paper/325600648190fc945fb0587545fda7451ec0ef6c",
    "title": "Ontology-Based Word Sense Disambiguation in Parallel Corpora",
    "abstract": "Lately, there seems to be a growing acceptance of the idea that multilingual lexical ontologies might be the key towards aligning different views on the semantic atomic units to be used in characterizing the general meaning of various and multilingual documents. Comparing performances of word sense disambiguation systems is a difficult evaluation task when different sense inventories are used and, even more difficult when the sense distinctions are not of the same granularity. The paper substantiates this statement by presenting a statistics based system for word alignment and word sense disambiguation in parallel corpora. The system is supported by a lexical ontology made of aligned wordnets for the languages in the corpora. The wordnets are aligned via the Princeton Wordnet, used as an interlingual index. The evaluation of the WSD system was performed on the same data, using three different sense inventories.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "faf43bd83fbfde282fcb5f228a5aebc4fae0680c",
    "url": "https://www.semanticscholar.org/paper/faf43bd83fbfde282fcb5f228a5aebc4fae0680c",
    "title": "A Word Sense Disambiguation Method Based on Extended Conceptual Ontology Graph",
    "abstract": "This paper presents a word sense disambiguation algorithm based on extended conceptual ontology graph. The algorithm tries to find the branches of the paths converging to the same common ancestor nodes from two senses of the word to be disambiguated, by searching the crotch of the ancestor nodes of the two senses. The branches of the paths here are called the decisive paths for one sense against another. Then, with the help of the context words in the running text, it calculates each decisive path's support, compares each two senses and finally chooses the sense with top support of the decisive paths as the best suited sense. For evaluation and comparison, we test our algorithm on SemCor according with WordNet release 1.6. The result shows that our algorithm can disambiguate word sense with quite efficient speed and promising precision.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e373d77ef7ae6adb3595c7a9c841c17b914071a6",
    "url": "https://www.semanticscholar.org/paper/e373d77ef7ae6adb3595c7a9c841c17b914071a6",
    "title": "Word sense disambiguation features for taxonomy extraction",
    "abstract": "Many NLP tasks, such as fact extraction, coreference reso- lution and alike, rely on existing lexical taxonomies or ontologies. One of possible ways to create a lexical taxonomy is to extract taxonomic re- lations from monolingual dictionary or encyclopedia: a semi-formalized resource designed to contain many such relations. Word-sense disam- biguation (WSD) is a mandatory tool in such approaches. Quality of extracted taxonomy greatly depends on WSD results. Most WSD approaches can be formulated as machine learning task. For this sake feature representation ranges from collocation vectors as in Lesk algorithm or neural network features in word2vec to highly specialized vector sense representation models such as AdaGram. In this paper we apply several WSD algorithms to dictionary dentions. Our main focus is on inuence of dierent approaches to extract WSD features from dictionary denitions on WSD performance.",
    "venue": "Journal of Computacion y Sistemas",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "814292af7727dcffcbbb00f16d5b03b39363a755",
    "url": "https://www.semanticscholar.org/paper/814292af7727dcffcbbb00f16d5b03b39363a755",
    "title": "Word Sense Disambiguation using Clue Words",
    "abstract": "This paper presents a new model to disambiguate the correct sense of polysemy word based on the related context words for each different sense of the polysemy word. The related context words for each sense are referred to as clue words for the sense. The WordNet organises nouns, verbs, adjectives and adverbs together into sets of synonyms called synsets each expressing a different concept. In contrast to the structure of WordNet, we developed a model that organizes the different senses of polysemy words based on the clue words. These clue words for each sense of a polysemy word are used to disambiguate the correct meaning of the polysemy word in the given context using any WSD algorithm. The clue word for a sense of a polysemy word may be a noun, verb, adjective or adverb. DOI: http://dx.doi.org/10.3126/jie.v10i1.10900 Journal of the Institute of Engineering, Vol. 10, No. 1, 2014, pp. 192\u2013198",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "edf76104f93166512322bdceba07cbb9f4c427cd",
    "url": "https://www.semanticscholar.org/paper/edf76104f93166512322bdceba07cbb9f4c427cd",
    "title": "Toward full-text ontology-based word sense disambiguation",
    "abstract": null,
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d9a1efa3c8130a7f26cb2e8b9ceff7ab88a5a8ec",
    "url": "https://www.semanticscholar.org/paper/d9a1efa3c8130a7f26cb2e8b9ceff7ab88a5a8ec",
    "title": "REPORT ON THE STATE OF THE ART OF NAMED ENTITY AND WORD SENSE DISAMBIGUATION",
    "abstract": "QTLeapPROJECT FP7 #610516 P2 QTLeap Machine translation is a computational procedure that seeks to provide the translation of utterances from one language into another language. Research and development around this grand challenge is bringing this technology to a level of maturity that already supports useful practical solutions. It permits to get at least the gist of the utterances being translated, and even to get pretty good results for some language pairs in some focused discourse domains, helping to reduce costs and to improve productivity in international businesses. There is nevertheless still a way to go for this technology to attain a level of maturity that permits the delivery of quality translation across the board. The goal of the QTLeap project is to research on and deliver an articulated methodology for machine translation that explores deep language engineering approaches in view of breaking the way to translations of higher quality. The deeper the processing of utterances the less language-specific differences remain between the representation of the meaning of a given utterance and the meaning representation of its translation. Further chances of success can thus be explored by machine translation systems that are based on deeper semantic engineering approaches. Deep language processing has its stepping-stone in linguistically principled methods and generalizations. It has been evolving towards supporting realistic applications, namely by embedding more data based solutions, and by exploring new types of datasets recently developed, such as parallel DeepBanks. This progress is further supported by recent advances in terms of lexical processing. These advances have been made possible by enhanced techniques for referential and conceptual ambiguity resolution, and supported also by new types of datasets recently developed as linked open data. The project QTLeap explores novel ways for attaining machine translation of higher quality that are opened by a new generation of increasingly sophisticated semantic datasets and by recent advances in deep language processing.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": null
  },
  {
    "paperId": "bb1705de8b420a1e826df8ad3c0c43745a556bb5",
    "url": "https://www.semanticscholar.org/paper/bb1705de8b420a1e826df8ad3c0c43745a556bb5",
    "title": "Deliverable D 3 . 2 . 1 Early ontological word-sense-disambiguation prototype",
    "abstract": "Semantic similarity and relatedness between concepts have been extensively studied in different areas ranging from psychology to computational linguistics. In this paper we address the problem of determining the similarity between concepts defined in a knowledge source such as an ontology. We propose a concept similarity algorithm based on geometric models for representing concepts and relationships, which can be applied to different types of ontologies. The key idea is the concept weighting scheme which allows for quantifying the degree of abstractness of concepts. The evaluation settings involving two ontologies validate and highlight the advantages of the proposed approach. Using our measure, which closely resembles the human judgment of similarity, we can reliably recreate predefined concept clusters, and generate more informative concept paths.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "c066b8368aebd7c5189fc4ab703ab6e8b47e2270",
    "url": "https://www.semanticscholar.org/paper/c066b8368aebd7c5189fc4ab703ab6e8b47e2270",
    "title": "Using Context Information for Knowledge-Based Word Sense Disambiguation",
    "abstract": null,
    "venue": "Artificial Intelligence: Methodology, Systems, Applications",
    "citationCount": 16,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "365683133db8b52890f7cff5ef0edfbdd23aebd9",
    "url": "https://www.semanticscholar.org/paper/365683133db8b52890f7cff5ef0edfbdd23aebd9",
    "title": "Exploiting Lexical Sensitivity in Performing Word Sense Disambiguation",
    "abstract": null,
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "dcc1f04548fea981c93132468e91a817c83c85ab",
    "url": "https://www.semanticscholar.org/paper/dcc1f04548fea981c93132468e91a817c83c85ab",
    "title": "Interactive Medical Word Sense Disambiguation with Instance and Feature Labeling",
    "abstract": "Medical documents contain many ambiguous words. Word sense disambiguation (WSD), i.e. assigning the appropriate meaning to an ambiguous word in context, is a critical step for many medical natural language processing applications, such as named entity extraction and computer-assisted review. Previous works have proposed many approaches for medical WSD, including unsupervised learning, supervised learning, semi-supervised learning, active learning, and interactive search and classification. However, these approaches still lack the ability to learn from domain knowledge, and take relatively long time to reach a reasonable performance, which is known as the \u201ccold-start\u201d problem. In this study, we design a novel interactive learning algorithm that directly incorporates expert knowledge into the WSD model training process. We consider two types of expert knowledge in the WSD task: prior knowledge and reasoning process. The expanded form of many ambiguous abbreviations are documented in medical ontologies, and the expert may know contextual words of a particular sense before looking at any instance, both of which are prior knowledge. Upon seeing an instance containing an ambiguous word, the expert can pinpoint the words and phrases that support his decision on the word sense, which is the reasoning process hidden behind the label. In the new interactive learning process, an expert can express knowledge as the association between senses and textual patterns, and the machine learning algorithm can directly learn from such association. Experiments on one biomedical literature corpus and two clinical notes corpora show that the proposed algorithm makes better use of human efforts in training WSD models than all previous approaches, achieving the state-of-the-art performance with the least effort from domain experts.",
    "venue": "AMIA",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "31fec5a1e239cb81fa4491c9935e8d319530582a",
    "url": "https://www.semanticscholar.org/paper/31fec5a1e239cb81fa4491c9935e8d319530582a",
    "title": "Learning taxonomical relations from domain texts using WordNet and word sense disambiguation",
    "abstract": "Learning taxonomical relations from domain texts is an important task for ontology learning from texts. We observe that rich information on taxonomical relations is available in the lexical knowledge base WordNet. However, in order to exploit the taxonomical relations in WordNet we need to tackle the difficult problem of word sense disambiguation. In this paper, we present a weighted word sense disambiguation method and show its application for learning taxonomical relations from domain texts. The experimental results indicate that using Word Net and our word sense disambiguation method achieves good accuracy and coverage for the learning task.",
    "venue": "IEEE International Conference on Granular Computing",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "901255b7ca65375c18e310a6229d9084acd00a24",
    "url": "https://www.semanticscholar.org/paper/901255b7ca65375c18e310a6229d9084acd00a24",
    "title": "Word Sense Disambiguation Using Semi-Supervised Naive Bayes with Ontological Constraints Jakob",
    "abstract": "Background. Word sense disambiguation (WSD) is the task of mapping an ambiguous word to its correct sense given its context. As high-quality sensetagged data is scarce and expensive to obtain, attention has shifted from fullysupervised to semi-supervised and knowledge-based approaches to WSD that rely on a lexical knowledge base such as WordNet instead of large amounts of hand-labeled data. What is currently missing is a method to effectively combine elements of semi-supervised and knowledge-based approaches into a single system. Aim. Our goal is to improve the performance of semi-supervised and knowledgebased lexical-sample WSD on a benchmark dataset by designing a system that uses both automatically acquired examples and explicitly models ontological constraints between senses in the classification stage. Data. Our training data consists of automatically acquired examples unlabeled from the ukWaC corpus, and glosses and example usages from WordNet. The system is evaluated on twelve different target lemmas from the Koeling et al. (2005) benchmark dataset. Methods. We use a semi-supervised Naive Bayes classifier that is trained on automatically acquired examples and that explicitly takes into account ontological constraints between senses at the classification stage. Results. We find that our method does not uniformly outperform state-of-theart baselines such as gloss vectors and personalized PageRank. Possible reasons are semantic drift, deficiencies in how the ontological constraints are modeled, and bad sense priors. Conclusions. Although our current system is not able to outperform stateof-the-art baselines, we believe that the error analysis provided in this paper can help guide future research in effectively combining semi-supervised and knowledge-based approaches to WSD.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "2b04a612ae187902ff6914b264df51f0a23851a7",
    "url": "https://www.semanticscholar.org/paper/2b04a612ae187902ff6914b264df51f0a23851a7",
    "title": "Word Sense Disambiguation in Clinical Text",
    "abstract": "Thesis (M. Eng.)--Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2013.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6cabf414a02f15b6f5202b883002d0881e717e6e",
    "url": "https://www.semanticscholar.org/paper/6cabf414a02f15b6f5202b883002d0881e717e6e",
    "title": "Cross-Lingual Word Sense Disambiguation for Languages with Scarce Resources",
    "abstract": null,
    "venue": "Canadian Conference on AI",
    "citationCount": 15,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1cab9d9e0fedb7a6874b2e1cb3cc5706abf71353",
    "url": "https://www.semanticscholar.org/paper/1cab9d9e0fedb7a6874b2e1cb3cc5706abf71353",
    "title": "Evaluating a Semantic Network Automatically Constructed from Lexical Co-occurrence on a Word Sense Disambiguation Task",
    "abstract": "We describe the extension and objective evaluation of a network1 of semantically related noun senses (or concepts) that has been automatically acquired by analyzing lexical cooccurrence in Wikipedia. The acquisition process makes no use of the metadata or links that have been manually built into the encyclopedia, and nouns in the network are automatically disambiguated to their corresponding noun senses without supervision. For this task, we use the noun sense inventory of WordNet 3.0. Thus, this work can be conceived of as augmenting the WordNet noun ontology with unweighted, undirected related-to edges between synsets. Our network contains 208,832 such edges. \n \nWe evaluate our network's performance on a word sense disambiguation (WSD) task and show: a) the network is competitive with WordNet when used as a stand-alone knowledge source for two WSD algorithms; b) combining our network with WordNet achieves disambiguation results that exceed the performance of either resource individually; and c) our network outperforms a similar resource that has been automatically derived from semantic annotations in the Wikipedia corpus.",
    "venue": "Conference on Computational Natural Language Learning",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "fdf0e551985e1fa2a755b7c2b258dea121b0c52d",
    "url": "https://www.semanticscholar.org/paper/fdf0e551985e1fa2a755b7c2b258dea121b0c52d",
    "title": "Categorizing Search Result Records Using Word Sense Disambiguation",
    "abstract": "\u2014 Web search engines are designed to retrieve and extract the information in the web databases and to return dynamic web pages. The Semantic Web is an extension of the current web in which it includes semantic content in web pages. The main goal of semantic web is to promote the quality of the current web by changing its contents into machine understandable form. Therefore, the milestone of semantic web is to have semantic level information in the web. Nowadays, people use different keyword- based search engines to find the relevant information they need from the web. But many of the words are polysemous. When these words are used to query a search engine, it displays the Search Result Records (SRRs) with different meanings. The SRRs with similar meanings are grouped together based on Word Sense Disambiguation (WSD). In addition to that semantic annotation is also performed to improve the efficiency of search result records. Semantic Annotation is the process of adding the semantic metadata to web resources. Thus the grouped SRRs are annotated and generate a summary which describes the information in SRRs. But the automatic semantic annotation is a significant challenge in the semantic web. Here ontology and knowledge based representation are used to annotate the web pages.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4b302a398fd2f3cc62ef1cf5832eddf0b686c74c",
    "url": "https://www.semanticscholar.org/paper/4b302a398fd2f3cc62ef1cf5832eddf0b686c74c",
    "title": "Metadata and Linked Data in Word Sense Disambiguation",
    "abstract": "IntroductionWord Sense Disambiguation (WSD) is referred to as an \"AI-complete\" problem (Mallery, 1998), i.e., a task that is relatively easy for people, but considerably more difficult for machines. If someone makes a query for a polysemous word (e.g., \"plant,\" \"bass,\" \"mercury,\" etc...), how is an information retrieval system to understand which sense of the word is intended? There exist tried-and-tested methods, such as just using the most predominant sense of the word (McCarthy, Koeling, Weeds, & Carroll, 2004); or looking at the words next to the query term to determine the statistically most likely meaning (Jurafsky & Martin, 2009; Manning & Schutze, 1999); but these methods often produce less-than-satisfactory results [often around 70%] (Navigli, 2009). Furthermore, these methods have been heavily dependent on the manual creation of knowledge sources (Edmonds, 2000), which are expensive to create and subject to change, thus creating what is termed a knowledge acquisition bottleneck (Gale, Church, & Yarowsky, 1992). Linked Data technologies (Berners-Lee, 2006), however, allow us to utilize existing ontologies and lexica, which can then be exploited to improve the automatic semantic understanding of the word. This paper will examine several systems that purport to disambiguate words by using Linked Data, and some of the models these systems use to ensure interoperability.Literature ReviewThe most complete treatment of the subject of WSD is arguably Agirre & Edmonds [ed.] (2007), which presents a detailed definition of the problem, along with a history thereof, and numerous algorithms which are used in practice. Kwong (2013) offers slightly more recent coverage, along with predictions as to how WSD methods will evolve in the near future. Generalists might find sufficient the survey from Navigli (2009), or the chapters covering WSD in either Jurafsky & Martin (2009) or Manning & Schutze (1999). SemEval [which was originally named Senseval (Kilgarriff, 1998)] is an ongoing evaluation project which is used as a baseline to assess various WSD methods, including many which will be examined in this paper.Linked Linguistic Open Data (LLOD) is heavily dependent on metadata, and any consideration thereof would require an examination of its standards. A brief history of the topic of linguistic annotation can be found in Palmer & Xue (2013). Bird & Simons (2003a) and Ide, Romary, & de la Clergerie (2004) proposed sets of best practices for linguistic annotations, while Simons, Bird, & Spanne (2008) offered a more recent set of recommendations that specifically suggested language codes from ISO 639-31 be used in metadata. Ide & Pustejovsky (2010) suggested a list of best practices for language technology metadata, focusing heavily on the work of the OLAC and European Languages Resource Association (ELRA). Gracia, Montiel-Ponsoda, Cimiano, Gomez-Perez. Buitelaar, & McCrae (2012) considered the issue of Linked Data being stored in different languages, and suggested that techniques such as ontology localization, ontology mapping, and cross-lingual ontology-based information access and presentation would help prevent information from being locked up in linguistic data silos. Gayo, Kontokostas, & Auer (2013) presented a set of best practices for multilingual linked open data, and point out that SPARQL queries can be improved if tags are identified by language. Reviews of specific linguistic annotation schemes include: the Open Languages Archives Community [OLAC] metadata set (Bird & Simons,2003b); the General Ontology for Linguistic Description [GOLD] (Farrar & Langendoen; 2003); ISOcat, a Data Category Registry (DCR) for the ISO TC 37 (terminology and other language and content resources) registry (Kemps-Snijders, Windhouwer, Wittenburg, & Wright (2009); the ISO/TC 37/SC 4 standard (Lee & Romary, 2010); the lemon (LExicon Model for ONtologies) model (McCrae, Aguado-de-Cea, Buitelaar, Cimiano, Declerck, Gomez-Perez, . \u2026",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "22b92cee2b865f8ea7da90bf95678be39f17bb5e",
    "url": "https://www.semanticscholar.org/paper/22b92cee2b865f8ea7da90bf95678be39f17bb5e",
    "title": "Word Sense Disambiguation Using Semantic Web for Tamil to English Statistical Machine Translation",
    "abstract": "Machine Translation has been an area of linguistic research for almost more than two decades now. But it still remains a very challenging task for devising an automated system which will deliver accurate translations of the natural languages. However, great strides have been made in this field with more success owing to the development of technologies of the web and off late there is a renewed interest in this area of research.\u00a0 Technological advancements in the preceding two decades have influenced Machine Translation in a considerable way. Several MT approaches including Statistical Machine Translation greatly benefitted from these advancements, basically making use of the availability of extensive corpora. Web technology web3.0 uses the semantic web technology which represents any object or resource in the web both syntactically and semantically.\u00a0 This type of representation is very much useful for the computing systems to search any content on the internet similar to lexical search and improve the internet based translations making it more effective and efficient. In this paper we propose a technique to improve existing statistical Machine Translation methods by making use of semantic web technology. Our focus will be on Tamil and Tamil to English MT. The proposed method could successfully integrate a semantic web technique in the process of WSD which forms part of the MT system. The integration is accomplished by using the capabilities of RDFS and OWL into the WSD component of the MT model. The contribution of this work lies in showing that integrating a Semantic web technique in the WSD system significantly improves the performance of a statistical MT system for a translation from Tamil to English. In this paper we assume the availability of large corpora in Tamil language and specific domain based ontologies with Tamil semantic web technology using web3.0. We are positive on the expansion and development of Tamil semantic web and subsequently infer that Tamil to English MT will greatly improve the disambiguation concept apart from other related benefits. This method could enable the enhancement of translation quality by improving on word sense disambiguation process while text is translated from Tamil to English language. This method can also be extended to other languages such as Hindi and Indian Languages.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1a8a19b9e3cb52bc634f7ec67733d039bb7d3db4",
    "url": "https://www.semanticscholar.org/paper/1a8a19b9e3cb52bc634f7ec67733d039bb7d3db4",
    "title": "Semantic Annotation of SOAP Web Services based on Word Sense Disambiguation Techniques",
    "abstract": "A key requirement for materializing the Semantic Web involves the annotation of resources and Web services with semantic metadata. This procedure is traditionally addressed as a manual task, which involves a high consumption of time and resources as well as the expertise on description formats and formal representations of knowledge, such as ontologies. Some research has promoted the development of mechanisms that partially automate the semantic annotation procedure, however, for the services particular case, those works lack of an analysis of the linguistic context of descriptor documents or interfaces, which provide adequate assignment of semantic annotations on the functional attributes of the services. In this context, this paper introduces a mechanism to automate the semantic annotation of SOAP services, supported by Word Sense Disambiguation techniques (WSD), from which it is possible to link the context of descriptor documents to the procedure of identification and association of ontological entities related to service attributes. This document discusses the mechanism described above, by developing an example, as well as the results of the experimental evaluation performed on a prototype that implements the proposal.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "029a26dcff8a9a5f47c5015b70e5b95b442da26b",
    "url": "https://www.semanticscholar.org/paper/029a26dcff8a9a5f47c5015b70e5b95b442da26b",
    "title": "A Reexamination of MRD-Based Word Sense Disambiguation",
    "abstract": "This article reconsiders the task of MRD-based word sense disambiguation, in extending the basic Lesk algorithm to investigate the impact on WSD performance of different tokenization schemes and methods of definition extension. In experimentation over the Hinoki Sensebank and the Japanese Senseval-2 dictionary task, we demonstrate that sense-sensitive definition extension over hyponyms, hypernyms, and synonyms, combined with definition extension and word tokenization leads to WSD accuracy above both unsupervised and supervised baselines. In doing so, we demonstrate the utility of ontology induction and establish new opportunities for the development of baseline unsupervised WSD methods.",
    "venue": "TALIP",
    "citationCount": 19,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "49c079c47b2fdcdd4e0d9e694c87c59ca3d849c6",
    "url": "https://www.semanticscholar.org/paper/49c079c47b2fdcdd4e0d9e694c87c59ca3d849c6",
    "title": "Learning Well-Founded Ontologies through Word Sense Disambiguation",
    "abstract": "Foundational Ontologies help maintaining and expanding ontologies expressivity power, thus enabling them to be more precise and free of ambiguities. The use of modeling languages based on these ontologies, such as OntoUML, requires not only the modeler's experience regarding such languages, but also a good understanding about the domain being modeled. Aiming to facilitate, or even enable the modeling of complex domains, several techniques have been proposed in order to automatically generate ontologies from texts. However, none is able to generate well-founded ontologies (which are constructed based on Foundational Ontologies). Moreover, an important issue on learning from text is how to distinguish among different meanings of a word, which impacts on concepts expressed by the ontologies. Therefore, techniques for word sense disambiguation must be considered. This paper proposes a technique for automatically learn well-founded ontologies described in OntoUML through word sense disambiguation.",
    "venue": "2013 Brazilian Conference on Intelligent Systems",
    "citationCount": 12,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "216f5b1df8624bf1446ff6d870fdd2e644b735e1",
    "url": "https://www.semanticscholar.org/paper/216f5b1df8624bf1446ff6d870fdd2e644b735e1",
    "title": "Word Sense Disambiguation for English Quranic IR System",
    "abstract": "Quranic text Information Retrieval (IR) is quite demanding yet very trivial due to that user will not always use the exact keywords to retrieve the relevant Quranic text (verse). Many have tried to overcome this problem by expanding or reformulating the query entered by users using semantic approaches with resources such as ontologies and thesauri. Word Sense Disambiguation (WSD) has been less interest to the IR research community due to the insignificant or very little significant impact on the IR performance. Recently, researchers pay interest on applying WSD to the IR problem due to the intuition that deep semantic analysis on the query process will give good impact on the IR performance. However, we have not seen any articles mentioning the use of WSD for Quranic IR, which we are assuming less or none research on WSD for Quranic IR have been carried out. Thus, this motivates us to explore WSD impact on Quranic IR performance. This paper will describe our on-going project on building an English Quranic WSD at the early stage, which is still at the proposal stage, where we layout what could be the best approach, resources and disambiguation algorithm for Quranic WSD for IR.",
    "venue": "2013 Taibah University International Conference on Advances in Information Technology for the Holy Quran and Its Sciences",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a25d8318d8454ad513000d20b165c5e3a4bee086",
    "url": "https://www.semanticscholar.org/paper/a25d8318d8454ad513000d20b165c5e3a4bee086",
    "title": "Unsupervised word sense disambiguation based on WordNet",
    "abstract": "Word sense disambiguation (WSD) based on supervised machine learning is hard to deal with large-scale WSD because of its big labor cost.To solve this problem,an unsupervised WSD method was provided,which describes the word senses of an ambiguous word via synthesizing multiple knowledge sources in WordNet ontology,including definition glosses,samples,structured semantic relations,domain attributes,etc.From the description,a representative glossary and a domain representative glossary are deduced.The two structures together with the word sense frequency distribution and the context are used for WSD.The average disambiguation accuracy was 49.93% by this method in open test for six representative unsupervised WSD methods with Senseval-3 English lexical sample data set.",
    "venue": "",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "11033a90655816be1195edccd2c67ee5698ad86b",
    "url": "https://www.semanticscholar.org/paper/11033a90655816be1195edccd2c67ee5698ad86b",
    "title": "Word-sense disambiguation in biomedical ontologies",
    "abstract": "With the ever increase in biomedical literature, text-mining has emerged as an important technology to support bio-curation and search. Word sense disambiguation (WSD), the correct identification of terms in text in the light of ambiguity, is an important problem in text-mining. Since the late 1940s many approaches based on supervised (decision trees, naive Bayes, neural networks, support vector machines) and unsupervised machine learning (context-clustering, word-clustering, co-occurrence graphs) have been developed. Knowledge-based methods that make use of the WordNet computational lexicon have also been developed. But only few make use of ontologies, i.e. hierarchical controlled vocabularies, to solve the problem and none exploit inference over ontologies and the use of metadata from publications. This thesis addresses the WSD problem in biomedical ontologies by suggesting different approaches for word sense disambiguation that use ontologies and metadata. The \u201cClosest Sense\u201d method assumes that the ontology defines multiple senses of the term; it computes the shortest path of co-occurring terms in the document to one of these senses. The \u201cTerm Cooc\u201d method defines a log-odds ratio for co-occurring terms including inferred co-occurrences. The \u201cMetaData\u201d approach trains a classifier on metadata; it does not require any ontology, but requires training data, which the other methods do not. These approaches are compared to each other when applied to a manually curated training corpus of 2600 documents for seven ambiguous terms from the Gene Ontology and MeSH. All approaches over all conditions achieve 80% success rate on average. The MetaData approach performs best with 96%, when trained on high-quality data. Its performance deteriorates as quality of the training data decreases. The Term Cooc approach performs better on Gene Ontology (92% success) than on MeSH (73% success) as MeSH is not a strict is-a/part-of, but rather a loose is-related-to hierarchy. The Closest Sense approach achieves on average 80% success rate. Furthermore, the thesis showcases applications ranging from ontology design to semantic search where WSD is important.",
    "venue": "",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "452466b9cc918f8502806925a4fafd4fae0bb7bd",
    "url": "https://www.semanticscholar.org/paper/452466b9cc918f8502806925a4fafd4fae0bb7bd",
    "title": "Improvement of Word Sense Disambiguation Using MINION",
    "abstract": "Word Sense Disambiguation problem is comes from NLP (Natural Language Processing); it is basically to select an appropriate sense of a word in the given context. This paper shows the word sense disambiguation improvement using MINION i.e. a constraint solver. In this paper the Word Sense Disambiguation problem is solved by collecting the aligned meaning of a Word with the help of MINION (A Tool) and then the RULES are formed using CLIPS language to get correct sense of a word. Keyword: Word Sense Disambiguation (WSD), CHAMPOLLION, MINION, C Language Integrated Production System (CLIPS), Natural Language Processing. 1.Introduction Word sense disambiguation (WSD) is the ability to identify the correct sense of a words based on the context in a computational manner. Some words have multiple meanings; these types of words are called Polysemy. For example: word \u201cBank\u201d can be a financial institute or to depend/trust. Sometimes two completely different words are spelled the same; these types of words are called Homonymy. For example: word \u201cCan\u201d, can be used as model verb: You can do it, or as container: She brought a can of soda. By forming the WSD rules using CLIPS language with the help of MINION. So we can get the correct sense of the same kinds of words i.e. used in different meaning in different context. A word having more than one sense depending on their context, which is Word Sense Disambiguation task to determine correct sense of a word in a given context[3]. Through Machine Translation a word having different senses in source language gives different translations in target language it means a word have multiple senses in the various contexts [6]. Word sense disambiguation will give the correct sense of a word by the Machine Translation in the particular context. Word Sense Disambiguation used in the various applications such as Machine Translation and Information retrieval [7]. The usefulness of Word Sense Disambiguation in statistical based machine translation, which is more popular challenge now a day\u2019s [8]. In Word Sense Disambiguation there are two approaches i.e. Deep approach and Shallow approach. Deep approaches always presume access to a comprehensive body of world knowledge but these approaches are not successful in practice because the body of world knowledge is not in a computer readable format. While, the Shallow approach do not understand the complete text, but only considering the surrounding text. Aim of Word sense Disambiguation is to find the correct sense of a word in a given context in which the word exists. The repository sense can come from WordNet (Computation lexicon), Dictionary (Machine readable) and a thesaurus [9]. When an ambiguous word is pronounced the sense of that word is correctly understood by the humans according to the situation and the context of that sentence, but it is difficult for a machine to decide the correct sense of the word in a given context. So where the machine processes the natural language application the problem of an ambiguity will arise. Consider an example take a word \u201cbat\u201d one sense is used: Bat hit the ball and another one is: Bat is a flying mammal [11]. Word Sense Disambiguation is described as an \u2018AI-Complete\u2019 problem it means first it solve the Artificial intelligence problems like encyclopedic knowledge and then sense word [14]. To identify the particular sense is a very difficult task for linguistics. The method used in word sense disambiguation is applied as to solve the problem of identifying the correct sense [15]. Word sense disambiguation facing the problem of a word having different senses and problem is, to find what the correct sense is. There are different types of dictionaries present, provide different sense on a given International Journal of Engineering Research & Technology (IJERT) Vol. 2 Issue 3, March 2013 ISSN: 2278-0181 1 www.ijert.org IJ E R T",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "5bc30a36f8b8c4125baa17f241ba5ae1c19ad6c6",
    "url": "https://www.semanticscholar.org/paper/5bc30a36f8b8c4125baa17f241ba5ae1c19ad6c6",
    "title": "Biomedical word sense disambiguation with ontologies and metadata: automation meets accuracy",
    "abstract": null,
    "venue": "BMC Bioinformatics",
    "citationCount": 40,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ]
  },
  {
    "paperId": "6697f2c8beb1c74bccb2ce83e489615729f8be4b",
    "url": "https://www.semanticscholar.org/paper/6697f2c8beb1c74bccb2ce83e489615729f8be4b",
    "title": "Word Sense Disambiguation for XML Structure Feature Generation",
    "abstract": null,
    "venue": "Extended Semantic Web Conference",
    "citationCount": 18,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e5c4aa82e5a35a8ecafb242fad400237dca0f686",
    "url": "https://www.semanticscholar.org/paper/e5c4aa82e5a35a8ecafb242fad400237dca0f686",
    "title": "Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art",
    "abstract": "ANIMATE, HUMAN, etc. and encode type restrictions on nouns and adjectives and on the arguments of verbs. Subject codes use another set of primitives to classify senses of words by subject (ECONOMICS, ENGINEERING, etc.). Guthrie et al. (1991) demonstrate a typical use of this information: in addition to using the Lesk-based method of counting overlaps between definitions and contexts, they impose a correspondence of subject codes in an iterative process. No quantitative evaluation of this method is available, but Cowie et al. (1992) improve the method using simulated annealing and report results of 47% for sense distinctions and 72% for homographs. The use of LDOCE box codes, however, is problematic: the codes are not systematic (see, for example, Fontenelle, 1990); in later work, Braden-Harder (1993) showed that simply matching box or subject codes is not sufficient for disambiguation. For example, in I tipped the driver, the codes for several senses of the words in the sentence satisfy the necessary constraints (e.g. tip-money + human object or tip-tilt + movable solid object). In many ways, the supplemen7 Note that the assumptions underlying this method are very similar to Quillian\u2019s: Thus one may think of a full concept analogically as consisting of all the information one would have if he looked up what will be called the \u201cpatriarch\u201d word in a dictionary, then looked up every word in each of its definitions, then looked up every word found in each of these, and so on, continually branching outward[...] (Quillian, 1968, p. 238). However, Quillian\u2019s network also keeps track of semantic relationships among the words encountered along the path between two words, which are encoded in his semantic network; the neural network avoids the overhead of creating the semantic network but loses this relational information. 13 tary information in the LDOCE, and in particular the subject codes, are similar to those in a thesaurus, which, however, are more systematically structured. Inconsistencies in dictionaries, noted earlier, are not the only and perhaps not the major source of their limitations for WSD. While dictionaries provide detailed information at the lexical level, they lack pragmatic information that enters into sense determination (see, e.g., Hobbs, 1987). For example, the link between ash and tobacco, cigarette or tray in a network such as Quillian\u2019s is very indirect, whereas in the Brown Corpus, the word ash co-occurs frequently with one of these words. It is therefore not surprising that corpora have become a primary source of information for WSD; this development is outlined below in section 2.3. 2.3.2 Thesauri. Thesauri provide information about relationships among words, most notably synonymy. Roget's International Thesaurus, which was put into machine-tractable form in the 1950's8 and has been used in a variety of applications including machine translation (Masterman, 1957), information retrieval (Sparck Jones, 1964, 1986), and content analysis (Sedelow and Sedelow, 1969; see also Sedelow and Sedelow, 1986, 1992), also supplies an explicit concept hierarchy consisting of up to eight increasingly refined levels. Typically, each occurrence of the same word under different categories of the thesaurus represent different senses of that word; i.e., the categories correspond roughly to word senses (Yarowsky, 1992). A set of words in the same category are semantically related. The earliest known use of Roget\u2019s for WSD is the work of Masterman (1957), described above in section 2.1. Several years later, Patrick (1985) used Roget\u2019s to discriminate among verb senses, by examining semantic clusters formed by \u201ce-chains\u201d derived from the thesaurus (Bryan, 1973, 1974; see also Sedelow and Sedelow, 1986). He uses \u201cword-strong neighborhoods,\u201d comprising word groups in low-level semicolon groups, which are the most closely related semantically in the thesaurus, and words connected to the group via chains. He is able to discriminate the correct sense of verbs such as inspire (to raise the spirits vs. to inhale, breathe in, sniff, etc.), question (to doubt vs. to ask a question) with \u201chigh reliability.\u201d Bryan's earlier work had already demonstrated that homographs can be distinguished by applying a metric based on relationships defined by his chains (Bryan, 1973, 1974). Similar work is described in Sedelow and Mooney (1988). Yarowsky (1992) derives classes of words by starting with words in common categories in Roget's (4th ed.). A 100-word context of each word in the category is extracted from a corpus (the 1991 electronic text of Grolier's Encyclopedia), and a mutual-information-like statistic is used to identify words most likely to co-occur with the category 8 The work of Masterman (1957) and Sparck Jones (1964) relied on a version of Roget\u2019s that was hand-punched onto cards in the 1950\u2019s; the Sedelow\u2019s (1969) work relied on a machine readable version of the 3rd Edition. Roget\u2019s is now widely available via anonymous ftp from various sites.",
    "venue": "Computational Linguistics",
    "citationCount": 1121,
    "fieldsOfStudy": [
      "Philosophy",
      "Computer Science"
    ]
  },
  {
    "paperId": "d770249832e9c97416ea1b8ebb4f255b22a1118d",
    "url": "https://www.semanticscholar.org/paper/d770249832e9c97416ea1b8ebb4f255b22a1118d",
    "title": "A Correspondence Repair Algorithm based on Word Sense Disambiguation and Upper Ontologies",
    "abstract": "In an ideal world, an ontology matching algorithm should return all the correct correspondences (it should be complete) and should return no wrong correspondences (it should be correct). In the real world, no imple- mented ontology matching algorithm is both correct and complete. For this reason, repairing wrong corre- spondences in an ontology alignment is a very pressing need to obtain more accurate alignments. This paper discusses an automatic correspondence repair method that exploits both upper ontologies to provide infor- mative context to concepts c2 o and c 0 2 o 0 belonging to an alignment a, and a context-based word sense disambiguation algorithm to assign c and c 0 their correct meaning. This meaning is used to decide whether c and c 0 are related, and to either keep or discard the correspondence 2 a, namely, to repair a. The experiments carried on are presented and the obtained results are provided. The advantages of the approach we propose are confirmed by a total average gain of 11,5% in precision for the alignments repaired against a 2% total average error.",
    "venue": "International Conference on Knowledge Engineering and Ontology Development",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d82865add53ac9edd08f8bdd6b5298dabe92e8fb",
    "url": "https://www.semanticscholar.org/paper/d82865add53ac9edd08f8bdd6b5298dabe92e8fb",
    "title": "A Learning-Based Approach for Biomedical Word Sense Disambiguation",
    "abstract": "In the biomedical domain, word sense ambiguity is a widely spread problem with bioinformatics research effort devoted to it being not commensurate and allowing for more development. This paper presents and evaluates a learning-based approach for sense disambiguation within the biomedical domain. The main limitation with supervised methods is the need for a corpus of manually disambiguated instances of the ambiguous words. However, the advances in automatic text annotation and tagging techniques with the help of the plethora of knowledge sources like ontologies and text literature in the biomedical domain will help lessen this limitation. The proposed method utilizes the interaction model (mutual information) between the context words and the senses of the target word to induce reliable learning models for sense disambiguation. The method has been evaluated with the benchmark dataset NLM-WSD with various settings and in biomedical entity species disambiguation. The evaluation results showed that the approach is very competitive and outperforms recently reported results of other published techniques.",
    "venue": "TheScientificWorldJournal",
    "citationCount": 10,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ]
  },
  {
    "paperId": "4bacf5398d6a0ec3f19d3e7a654fc9ad78ba407c",
    "url": "https://www.semanticscholar.org/paper/4bacf5398d6a0ec3f19d3e7a654fc9ad78ba407c",
    "title": "OntoNotes: Corpus Cleanup of Mistaken Agreement Using Word Sense Disambiguation",
    "abstract": "Annotated corpora are only useful if their annotations are consistent. Most large-scale annotation efforts take special measures to reconcile inter-annotator disagreement. To date, however, no-one has investigated how to automatically determine exemplars in which the annotators agree but are wrong. In this paper, we use OntoNotes, a large-scale corpus of semantic annotations, including word senses, predicate-argument structure, ontology linking, and coreference. To determine the mistaken agreements in word sense annotation, we employ word sense disambiguation (WSD) to select a set of suspicious candidates for human evaluation. Experiments are conducted from three aspects (precision, cost-effectiveness ratio, and entropy) to examine the performance of WSD. The experimental results show that WSD is most effective on identifying erroneous annotations for highly-ambiguous words, while a baseline is better for other cases. The two methods can be combined to improve the cleanup process. This procedure allows us to find approximately 2% remaining erroneous agreements in the OntoNotes corpus. A similar procedure can be easily defined to check other annotated corpora.",
    "venue": "International Conference on Computational Linguistics",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "2fcd85fe4241bc6a2d82a3966d98bd5852fa1207",
    "url": "https://www.semanticscholar.org/paper/2fcd85fe4241bc6a2d82a3966d98bd5852fa1207",
    "title": "Using the Intension of Classes and Properties Definition in Ontologies for Word Sense Disambiguation",
    "abstract": null,
    "venue": "EKAW",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "3222c6c2c1c87dfbb63293d62563779f5faea2a3",
    "url": "https://www.semanticscholar.org/paper/3222c6c2c1c87dfbb63293d62563779f5faea2a3",
    "title": "Automating Reuse of Online Semantic Resources by Concept Extraction Using Word Sense Disambiguation",
    "abstract": "Several dimensions have been added since the idea of Semantic web was conceived by Tim B. Lee, to the previously linear representation of the World Wide Web that consisted of web resources like web pages and hyperlinks. These dimensions allow for software agents/machines to make knowledgeable decisions and retrieve relevant resources which are related in meaning and not only by named links. The new infrastructure consists of semantic repositories like network of ontologies, also called semantic networks. Some interesting research issues have emerged in the context of these semantic repositories and these are their creation, bootstrapping and interoperability to make semantic web a reality. A novel approach is proposed to deal with the issue of creation of ontologies from existing online semantic repositories by using Swoogle API. The aim is to discover concepts from online resources indexed by the Swoogle Search Engine by using advanced techniques of Word Sense Disambiguation in Computational Linguistics Techniques.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "fbbad217bf66ad625b366ee9a40032d0de990d3d",
    "url": "https://www.semanticscholar.org/paper/fbbad217bf66ad625b366ee9a40032d0de990d3d",
    "title": "Word Sense Disambiguation Incorporating Lexical and Structural Semantic Information",
    "abstract": "We present results that show that incorporating lexical and structural semantic information is effective for word sense disambiguation. We evaluated the method by using precise information from a large treebank and an ontology automatically created from dictionary sentences. Exploiting rich semantic and structural information improves precision 2\u20103%. The most gains are seen with verbs, with an improvement of 5.7% over a model using only bag of words and n-gram features.",
    "venue": "EMNLP",
    "citationCount": 15,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "8a3591b7af3bca0658fe5d9404b1a69d5dfa2918",
    "url": "https://www.semanticscholar.org/paper/8a3591b7af3bca0658fe5d9404b1a69d5dfa2918",
    "title": "Word sense disambiguation: why statistics when we have these numbers?",
    "abstract": "Word sense disambiguation continues to be a difficult problem in machine translation (MT). Current methods either demand large amounts of corpus data and training or rely on knowledge of hard selectional constraints. In either case, the methods have been demonstrated only on a small scale and mostly in isolation, where disambiguation is a task by itself. It is not clear that the methods can be scaled up and integrated with other components of analysis and generation that constitute an end-to-end MT system. In this paper, we illustrate how the Mikrokosmos Knowledge-Based MT system disambiguates word senses in real-world texts with a very high degree of correctness. Disambiguation in Mikrokosmos is achieved by a combination of (i) a broad-coverage ontology with many selectional constraints per concept, (ii) a large computational-semantic lexicon grounded in the ontology, (iii) an optimized search algorithm for checking selectional constraints in the ontology, and (iv) an efficient control mechanism with near-linear processing complexity. Moreover, Mikrokosmos constructs complete meaning representations of an input text using the chosen word senses. 1 Word Sense Ambiguity Word sense disambiguation continues to be a difficult problem for machine translation (MT) systems.The most common current methods for resolving word sense ambiguities are based on statistical collocations or static selectional preferences between pairs of word senses. The real power of word sense selection seems to lie in the ability to constrain the possible senses of a word based on selections made for other words in the local context. Although methods using selectional constraints and semantic networks have been delineated at least since Katz and Fodor (1963), computational models have not demonstrated the effectiveness of knowledgebased methods in resolving word senses in real-world texts on a large scale. This has resulted in a predominant shift of attention from knowledge-based to corpus-based, statistical methods for word sense resolution, despite the far greater potential of knowledge-based methods for advancing the development of large, practical, domain independent NLP/MT systems. In this article, we illustrate how the semantic analyzer of the Mikrokosmos machine translation system resolves word sense ambiguities in real-world Spanish texts (news articles on company mergers and acquisitions from the EFE newswire) with a high degree of correctness. We begin by presenting the results from Mikrokosmos and then illustrate how they were obtained. 1 See Guthrie et al (1996) and Wilks et al (1995) for recent surveys of related work.",
    "venue": "TMI",
    "citationCount": 25,
    "fieldsOfStudy": null
  },
  {
    "paperId": "2a4077abed967af84a72be3c6fb436cc5cad961c",
    "url": "https://www.semanticscholar.org/paper/2a4077abed967af84a72be3c6fb436cc5cad961c",
    "title": "Text Analysis Integration into a Medical Information Retrieval System: Challenges Related to Word Sense Disambiguation",
    "abstract": "We describe a high throughput, real-time modularized text analysis system integrated into a production medical information retrieval system at the Mayo Clinic. The system identifies clinically relevant entities and concepts from unstructured medical reports, e.g. drugs, diagnosis and concepts, as defined in RxNorm, SNOMED CT and UMLS. To achieve the desired performance, the text analysis components were either developed for or adapted to the clinical domain. In this paper, we discuss one of several required innovations, disambiguation of mapped concepts with respect to an ontology. The methodology for the identification of a set of problem ambiguities and the creation of a manually sense-tagged data set for each of them is presented. Based on this data set, methods for word sense disambiguation for clinical notes were developed. In this paper, we report one set of F-scores (range is 0.47-0.98) which are substantially above the majority sense baseline.",
    "venue": "",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "fd6547956b8e7ac5ea8c8bce7f495d1eb463e04e",
    "url": "https://www.semanticscholar.org/paper/fd6547956b8e7ac5ea8c8bce7f495d1eb463e04e",
    "title": "Word sense disambiguation using semantic relatedness measurement",
    "abstract": "All human languages have words that can mean different things in different contexts, such words with multiple meanings are potentially \u201cambiguous\u201d. The process of \u201cdeciding which of several meanings of a term is intended in a given context\u201d is known as \u201cword sense disambiguation (WSD)\u201d. This paper presents a method of WSD that assigns a target word the sense that is most related to the senses of its neighbor words. We explore the use of measures of relatedness between word senses based on a novel hybrid approach. First, we investigate how to \u201cliterally\u201d and \u201cregularly\u201d express a \u201cconcept\u201d. We apply set algebra to WordNet\u2019s synsets cooperating with WordNet\u2019s word ontology. In this way we establish regular rules for constructing various representations (lexical notations) of a concept using Boolean operators and word forms in various synset(s) defined in WordNet. Then we establish a formal mechanism for quantifying and estimating the semantic relatedness between concepts\u2014we facilitate \u201cconcept distribution statistics\u201d to determine the degree of semantic relatedness between two lexically expressed concepts. The experimental results showed good performance on Semcor, a subset of Brown corpus. We observe that measures of semantic relatedness are useful sources of information for WSD.",
    "venue": "",
    "citationCount": 10,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "5a83ded81da19fe9c992c791ae1a455e4eb200e0",
    "url": "https://www.semanticscholar.org/paper/5a83ded81da19fe9c992c791ae1a455e4eb200e0",
    "title": "Corpus Cleanup of Mistaken Agreement Using Word Sense Disambiguation",
    "abstract": "Word sense annotated corpora are useful resources for many text mining applications. Such corpora are only useful if their annotations are consistent. Most large-scale annotation efforts take special measures to reconcile inter-annotator disagreement. To date, however, nobody has investigated how to automatically determine exemplars in which the annotators agree but are wrong. In this paper, we use OntoNotes, a large-scale corpus of semantic annotations, including word senses, predicate-argument structure, ontology linking, and coreference. To determine the mistaken agreements in word sense annotation, we employ word sense disambiguation (WSD) to select a set of suspicious candidates for human evaluation. Experiments are conducted from three aspects (precision, cost-effectiveness ratio, and entropy) to examine the performance of WSD. The experimental results show that WSD is most effective in identifying erroneous annotations for highly-ambiguous words, while a baseline is better for other cases. The two methods can be combined to improve the cleanup process. This procedure allows us to find approximately 2% of the remaining erroneous agreements in the OntoNotes corpus. A similar procedure can be easily defined to check other annotated corpora.",
    "venue": "Int. J. Comput. Linguistics Chin. Lang. Process.",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "75740925cfa673db7477a9787945ec5c0437b369",
    "url": "https://www.semanticscholar.org/paper/75740925cfa673db7477a9787945ec5c0437b369",
    "title": "Evaluating the Word Sense Disambiguation Accuracy with Three Different Sense Inventories",
    "abstract": "Comparing performances of word sense disambiguation systems is a very difficult evaluation task when different sense inventories are used and, even more difficult when the sense distinctions are not of the same granularity. The paper substantiates this statement by briefly presenting a system for word sense disambiguation (WSD) based on parallel corpora. The method relies on word alignment, word clustering and is supported by a lexical ontology made of aligned wordnets for the languages in the corpora. The wordnets are aligned to the Princeton Wordnet, according to the principles established by EuroWordNet. The evaluation of the WSD system was performed on the same data, using three different granularity sense inventories.",
    "venue": "NLUCS",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "3543d7db405647f6b0dd92fa8cb9f3d552259c5f",
    "url": "https://www.semanticscholar.org/paper/3543d7db405647f6b0dd92fa8cb9f3d552259c5f",
    "title": "Multi-component Word Sense Disambiguation",
    "abstract": "This paper describes the system MC-WSD presented for the English Lexical Sample task. The system is based on a multicomponent architecture. It consists of one classifier with two components. One is trained on the data provided for the task. The second is trained on this data and, additionally, on an external training set extracted from the Wordnet glosses. The goal of the additional component is to lessen sparse data problems by exploiting the information encoded in the ontology.",
    "venue": "SENSEVAL@ACL",
    "citationCount": 9,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "2eec76a17565052ee49701a3e2009ed35745f12f",
    "url": "https://www.semanticscholar.org/paper/2eec76a17565052ee49701a3e2009ed35745f12f",
    "title": "Word sense disambiguation: a case study on the granularity of sense distinctions",
    "abstract": "The paper presents a method for word sense disambiguation (WSD) based on parallel corpora. The method exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and is supported by a lexical ontology made of aligned wordnets for the languages in the corpora. The wordnets are aligned to the Princeton Wordnet, according to the principles established by EuroWordNet. The evaluation of the WSD system was performed using three different granularity sense inventories.",
    "venue": "",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f94c95e84b63b6c74643af7f349bae309ae310b9",
    "url": "https://www.semanticscholar.org/paper/f94c95e84b63b6c74643af7f349bae309ae310b9",
    "title": "An Hybrid Approach for Improving Word Sense Disambiguation and Text Clustering",
    "abstract": "In this paper we suggest a new approach to represent text document collections, integrating background knowledge to improve clustering effectiveness. Background knowledge is inferred from previous classification tasks on the same collection and it\u2019s used as context to assign semantic values to words. The WordNet ontology acts as a source repository of semantic meanings. More specifically we propose a new approach to Word Semantic Disambiguation, based on the evaluation of functions related with syntactical and statistical properties of the WordNet synsets or with contextual and synonymical informations.",
    "venue": "IRCDL",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d2607c2e896b1c5428958cda078ceea1b47593fa",
    "url": "https://www.semanticscholar.org/paper/d2607c2e896b1c5428958cda078ceea1b47593fa",
    "title": "Hindi Word Sense Disambiguation",
    "abstract": "Department of Computer Science and Engineering Indian Institute of Technology Bombay, Mumbai India {manish, mahesh, pb,pandey,yupu}@cse.iitb.ac.in Abstract Word Sense Disambiguation (WSD) is defined as the task of finding the correct sense of a word in a specific context. This is crucial for applications like Machine Translation and Information Extraction. While the work on automatic WSD for English is voluminous, to our knowledge, this is the first attempt for an Indian language at automatic WSD. We make use of the Wordnet for Hindi developed at IIT Bombay, which is a highly important lexical knowledge base for Hindi. The main idea is to compare the context of the word in a sentence with the contexts constructed from the Wordnet and chooses the winner. The output of the system is a particular synset number designating the sense of the word. The mentioned Wordnet contexts are built from the semantic relations and glosses, using the Application Programming Interface created around the lexical data. The evaluation has been done on the Hindi corpora provided by the Central Institute of Indian Languages and the results are encouraging. Currently the system disambiguates nouns. Work is on for other parts of speech too.",
    "venue": "",
    "citationCount": 57,
    "fieldsOfStudy": null
  },
  {
    "paperId": "100d860cfe7daecc84a9344a514d1fc79457ae2a",
    "url": "https://www.semanticscholar.org/paper/100d860cfe7daecc84a9344a514d1fc79457ae2a",
    "title": "Memory-Based Word Sense Disambiguation",
    "abstract": null,
    "venue": "Computers and the Humanities",
    "citationCount": 64,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "57dd5669aa2f35094d0168721920c1e5e54af595",
    "url": "https://www.semanticscholar.org/paper/57dd5669aa2f35094d0168721920c1e5e54af595",
    "title": "Contextual Word Sense Tuning and Disambiguation",
    "abstract": "The discrimination of word senses, word sense disambiguation (WSD), is a major problem in natural language processing (NLP) applications, e.g., text classification and understanding. The problem of determining the correct sense of lexical items in raw texts is relevant to the activities of categorization, machine translation, information retrieval, and any language engineering task. Problems are related to the pervasive ambiguity of words and their use in texts. Moreover, the specificity of senses in the knowledge domains, where words are used, tends to augment the complexity of the disambiguation task, affecting the completeness of most on-line sources, like dictionaries and general purpose lexical resources. In this article an integrated method based on a well-known lexical knowledge base (i.e., WordNet) and on corpus statistics is used to tune the sense classification to a specific sublanguage and to drive contextual disambiguation of word senses. The method results in a system (General Purpose Ontolog...",
    "venue": "Appl. Artif. Intell.",
    "citationCount": 25,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a79d84c59c4d294c54fe57f7578a1223f679ec3b",
    "url": "https://www.semanticscholar.org/paper/a79d84c59c4d294c54fe57f7578a1223f679ec3b",
    "title": "Lexical Semantic Analysis to support Ontology Maintenance Modeling of FMEA",
    "abstract": "Business produces procedures written in natural language that are meant to store technical and engineering information, management decision and operation experience during production system life cycle. A maintenance procedure composes of consecutive logical arguments to determine step-by-step cause-effect events resulting in mitigative tasks. Therefore, the context meaning representation to mimic the purpose of a maintenance procedure is highly dependent upon word sense, syntax-semantic interface, and sematic features of argument. This paper proposes an event-based ontology approach to support failure-Mode-Effect Analysis (FMEA) using lexical semantic analysis and context meaning. At first, it maps argument structure into causal event structure in which event is a group of highly frequent contextual features or words logically linked together to shape structured arguments. Then, Dowty and Van Valin's decomposition model are employed into the format of [Event-State-Activity- Accomplishment -Result] to determine syntax-sematic interface and linking rules in causal chain. Also, we employ Van Valin's model to differentiate between active and causative accomplishments for punctual/non-punctual change of states in causal chain. Finally, metadata or hypernym of causal event is represented to accommodate ontology modeling for semantic extraction and cause-effect interpretation.",
    "venue": "2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "e4db8ebedcb735fc7172e8e39a18424dabe2ed88",
    "url": "https://www.semanticscholar.org/paper/e4db8ebedcb735fc7172e8e39a18424dabe2ed88",
    "title": "An End-to-End Supervised Target-Word Sense Disambiguation System",
    "abstract": "We present an extensible supervised Target-Word Sense Disambiguation system that leverages upon GATE (General Architecture for Text Engineering), NSP (Ngram Statistics Package) and WEKA (Waikato Environment for Knowledge Analysis) to present an end-to-end solution that integrates feature identification, feature extraction, preprocessing and classification.",
    "venue": "AAAI",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "531aa51965d53329666803e6b40440b248b29c1a",
    "url": "https://www.semanticscholar.org/paper/531aa51965d53329666803e6b40440b248b29c1a",
    "title": "A Learning Approach for Word Sense Disambiguation in the Biomedical Domain",
    "abstract": "Word sense disambiguation, WSD, task has been investigated extensively within the natural language processing domain. In the biomedical domain, word sense ambiguity is more widely spread with bioinformatics research effort devoted to it is not commensurate and is allowing for more development. In this paper, we present and evaluate a machine learning based approach for WSD. The main limitation with supervised methods is the requirement for manually disambiguated instances of the ambiguous word to be used for training. However, the advances in automatic text annotation and tagging techniques with the help of the plethora of knowledge sources like ontologies and text literature in the biomedical domain will help lessen this limitation. Our approach has been evaluated with the benchmark dataset NLM-WSD with three settings. The accuracy results showed that our method performs better than recently reported results of other published techniques. * corresponding author: Hisham@uhcl.edu",
    "venue": "Bioinformatics and Computational Biology",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "99b319ded8ee87c0690efcd9a429a36fbe8d64b9",
    "url": "https://www.semanticscholar.org/paper/99b319ded8ee87c0690efcd9a429a36fbe8d64b9",
    "title": "WordNet Powered Faceted Semantic Search with Automatic Sense Disambiguation for Bioenergy Domain",
    "abstract": "WordNet is a lexicon widely known and used as an ontological resource hosting comparatively large collection of semantically interconnected words. Use of such resources produces meaningful results and improves users' search experience through the increased precision and recall. This paper presents our facet-enabled WordNet powered semantic search work done in the context of the bioenergy domain. The main hurdle to achieving the expected result was sense disambiguation further complicated by the occasional fine-grained distinction of meanings of the terms in WordNet. To overcome this issue, a novel sense disambiguation approach based on automatically built domain specific ontologies, WordNet synset hierarchy and term (or word) sense ranks is proposed.",
    "venue": "2016 IEEE Tenth International Conference on Semantic Computing (ICSC)",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "afae7621598a6438fe16d707cfce8a672916e658",
    "url": "https://www.semanticscholar.org/paper/afae7621598a6438fe16d707cfce8a672916e658",
    "title": "CiLin Expansion and Its Application to Word Sense Disambiguation",
    "abstract": "40 000 new entries are added to CiLin in knowledge engineering. Statistics are presented about the sense ambiguity of Chinese words in CiLin. An automatic solution is proposed for the problem of Chinese sense disambiguation, which utilizes the heuristics of part-of-speech tagging and Naive Bayes classification based on a partially disambiguated corpus.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Mathematics"
    ]
  },
  {
    "paperId": "d984ab26eb27c1879b95b97e69dffbbb874ea07c",
    "url": "https://www.semanticscholar.org/paper/d984ab26eb27c1879b95b97e69dffbbb874ea07c",
    "title": "Microsoft Word-329214202121719834.docx",
    "abstract": "Business produces procedures written in natural language that are meant to store technical and engineering information, management decision and operation experience during production system life cycle. A maintenance procedure composes of consecutive logical arguments to determine step-bystep cause-effect events resulting in mitigative tasks. Therefore, the context meaning representation to mimic the purpose of a maintenance procedure is highly dependent upon word sense, syntax-semantic interface, and sematic features of argument. This paper proposes an event-based ontology approach to support failure-Mode-Effect Analysis (FMEA) using lexical semantic analysis and context meaning. At first, it maps argument structure into causal event structure in which event is a group of highly frequent contextual features or words logically linked together to shape structured arguments. Then, Dowty and Van Valin\u2019s decomposition model are employed into the format of [EventState-ActivityAccomplishment -Result] to determine syntaxsematic interface and linking rules in causal chain. Also, we employ Van Valin\u2019s model to differentiate between active and causative accomplishments for punctual/non-punctual change of states in causal chain. Finally, metadata or hypernym of causal event is represented to accommodate ontology modeling for semantic extraction and cause-effect interpretation. Keywords\u2014 Maintenance Data Modeling, Lexical Semantic Analysis, Meta Data Modeling, Contextual Meaning Extraction.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "f1dc3c6fe251b273722793255855b2049ebbf9e9",
    "url": "https://www.semanticscholar.org/paper/f1dc3c6fe251b273722793255855b2049ebbf9e9",
    "title": "Verb Sense Disambiguation based on Thesaurus of Predicate-argument Structure - An Evaluation of Thesaurus of Predicate-argument Structure for Japanese Verbs",
    "abstract": "This paper presents a system for word sense disambiguation based on a manually constructed thesaurus of predicate-argument structure, which is an ontology on the linguistic side providing essential information for mapping form texts to verb concepts. This system can be effective for word sense disambiguation even though the target word sense system is different from the thesaurus. We applied the proposed word sense disambiguation system to the test corpus of SemEval-2010 Japanese tasks. Experimental results showed that the thesaurus-based disambiguation system outperformed a CRFs-based system in recall rates of verb sense disambiguation. From the results of verb sense disambiguation, we clarified that the abstracted verb classes (709 types) in our proposed system were effective sets for verb sense disambiguation.",
    "venue": "KEOD",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "93fe7447d966d54320cb2774188e09b98e7d6793",
    "url": "https://www.semanticscholar.org/paper/93fe7447d966d54320cb2774188e09b98e7d6793",
    "title": "Searching Semantic Resources for Complex Selectional Restrictions to Support Verb Sense Disambiguation",
    "abstract": "Abstract : Natural language processing systems are increasingly integrating lexicons with ontologies for word sense disambiguation (WSD). Manually acquiring a lexicon that is integrated with a large ontology and other semantic resources can be difficult and inefficient in part due to the complexity of ontologies and inconsistency of entity extractors supporting WSD applications. A major contributing factor to the difficulty is the creation of selectional restrictions with respect to particular semantic resources. This paper presents a process for acquiring complex expressions for selectional restrictions via search through an ontology.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e19dcb63c5aadfd5fef2aaefa9d59a2e3db97d58",
    "url": "https://www.semanticscholar.org/paper/e19dcb63c5aadfd5fef2aaefa9d59a2e3db97d58",
    "title": "Semantic Relatedness for Keyword Disambiguation: Exploiting Different Embeddings",
    "abstract": "Understanding the meaning of words is crucial for many tasks that involve human-machine interaction. This has been tackled by research in Word Sense Disambiguation (WSD) in the Natural Language Processing (NLP) field. Recently, WSD and many other NLP tasks have taken advantage of embeddings-based representation of words, sentences, and documents. However, when it comes to WSD, most embeddings models suffer from ambiguity as they do not capture the different possible meanings of the words. Even when they do, the list of possible meanings for a word (sense inventory) has to be known in advance at training time to be included in the embeddings space. Unfortunately, there are situations in which such a sense inventory is not known in advance (e.g., an ontology selected at run-time), or it evolves with time and its status diverges from the one at training time. This hampers the use of embeddings models for WSD. Furthermore, traditional WSD techniques do not perform well in situations in which the available linguistic information is very scarce, such as the case of keyword-based queries. In this paper, we propose an approach to keyword disambiguation which grounds on a semantic relatedness between words and senses provided by an external inventory (ontology) that is not known at training time. Building on previous works, we present a semantic relatedness measure that uses word embeddings, and explore different disambiguation algorithms to also exploit both word and sentence representations. Experimental results show that this approach achieves results comparable with the state of the art when applied for WSD, without training for a particular domain.",
    "venue": "ArXiv",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "2b320263ba2d9a3a3bf61aa4ca359614b576a07c",
    "url": "https://www.semanticscholar.org/paper/2b320263ba2d9a3a3bf61aa4ca359614b576a07c",
    "title": "Knowledge Graph Extension for Word Sense Annotation",
    "abstract": null,
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "fec04ff73c63fa05b1f886f114bb4db779a944f4",
    "url": "https://www.semanticscholar.org/paper/fec04ff73c63fa05b1f886f114bb4db779a944f4",
    "title": "Semantic Relatedness Metric for Wikipedia Concepts Based on Link Analysis and its Application to Word Sense Disambiguation",
    "abstract": "Wikipedia has grown into a high quality up-todate knowledge base and can enable many knowledge-based applications, which rely on semantic information. One of the most general and quite powerful semantic tools is a measure of semantic relatedness between concepts. Moreover, the ability to efficiently produce a list of ranked similar concepts for a given concept is very important for a wide range of applications. We propose to use a simple measure of similarity between Wikipedia concepts, based on Dice\u2019s measure, and provide very efficient heuristic methods to compute top k ranking results. Furthermore, since our heuristics are based on statistical properties of scale-free networks, we show that these heuristics are applicable to other complex ontologies. Finally, in order to evaluate the measure, we have used it to solve the problem of word-sense disambiguation. Our approach to word sense disambiguation is based solely on the similarity measure and produces results with high accuracy.",
    "venue": "Spring Young Researchers Colloquium on Databases and Information Systems",
    "citationCount": 59,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "44f0da00610b157fed91d578ccf3e2eae2247456",
    "url": "https://www.semanticscholar.org/paper/44f0da00610b157fed91d578ccf3e2eae2247456",
    "title": "Word Sense Disambiguation in biomedical ontologies with term co-occurrence analysis and document clustering",
    "abstract": "With more and more genomes being sequenced, a lot of effort is devoted to their annotation with terms from controlled vocabularies such as the GeneOntology. Manual annotation based on relevant literature is tedious, but automation of this process is difficult. One particularly challenging problem is word sense disambiguation. Terms such as 'development' can refer to developmental biology or to the more general sense. Here, we present two approaches to address this problem by using term co-occurrences and document clustering. To evaluate our method we defined a corpus of 331 documents on development and developmental biology. Term co-occurrence analysis achieves an F-measure of 77%. Additionally, applying document clustering improves precision to 82%. We applied the same approach to disambiguate 'nucleus', 'transport', and 'spindle', and we achieved consistent results. Thus, our method is a viable approach towards the automation of literature-based genome annotation.",
    "venue": "International Journal of Data Mining and Bioinformatics",
    "citationCount": 27,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ]
  },
  {
    "paperId": "d18076820d6e6a4e04d17a9af987ab6bf1204104",
    "url": "https://www.semanticscholar.org/paper/d18076820d6e6a4e04d17a9af987ab6bf1204104",
    "title": "Word sense alignment of lexical resources",
    "abstract": "Lexical-semantic resources (LSRs) are a cornerstone for many areas of Natural Language Processing (NLP) such as word sense disambiguation or information extraction. LSRs exist in many varieties, focusing on different information types and languages, or being constructed according to different paradigms. However, the large number of different LSRs is still not able to meet the growing demand for large-scale resources for different languages and application purposes. Thus, the orchestrated usage of different LSRs is necessary in order to cover more words and senses, and also to have access to a richer knowledge representation when word senses are covered in more than one resource. In this thesis, we address the task of finding equivalent senses in these resources, which is known as \\emph{Word Sense Alignment} (WSA), and report various contributions to this area. \n \nFirst, we give a formal definition of WSA and describe suitable evaluation metrics and baselines for this task. Then, we position WSA in the broad area of semantic processing by comparing it to related tasks from NLP and other fields, establishing that WSA indeed displays a unique set of properties and challenges which need to be addressed. \n \n \nAfter that, we discuss the resources we employ for WSA, distinguishing between expert-built and collaboratively constructed resources. We give a brief description and refer to related work for each resource, and we discuss the collaboratively constructed, multilingual resource OmegaWiki in greater detail, as it has not been exhaustively covered in previous work and also presents a unique, concept-centered and language-agnostic structure, which makes it interesting for NLP applications. At the same time, we shed light on disadvantages of this approach and gaps in OmegaWiki's content. After the presentation of the resources, we perform a comparative analysis of them which focuses on their suitability for different approaches to WSA. In particular, we analyze their glosses as well as their structure and point out flaws and differences between them. Based on this, we motivate the selection of resource pairs we investigate and describe the WSA gold standard datasets they participate in. On top of the ones presented in previous work, we discuss four new datasets we created, filling gaps in the body of WSA research. \n \nWe then go on to present an alignment between Wiktionary and OmegaWiki, using a similarity-based framework. For the first time, it is applied to two collaboratively constructed resources. We improve this framework by adding a machine translation component, which we use to align WordNet and the German part of OmegaWiki. A cross-validation experiment with the English OmegaWiki (i.e. for the monolingual case) shows that both configurations perform comparably as only few errors are introduced by the translation component. This confirms the general validity of the idea. \n \nBuilding on the observation that similarity-based approaches suffer from the insufficient lexical overlap between different glosses, we also present the novel alignment algorithm Dijkstra-WSA. It works on graph representations of LSRs induced, for instance, by semantic relations or links, and exploits the intuition that related senses are concentrated in adjacent regions of the resources. This algorithm performs competitively on six out of eight evaluation datasets, and we also present a combination with the similarity-based approach mentioned above in a backoff configuration. This approach achieves a significant improvement over previous work on all considered datasets. \n \nTo further exploit the insight that text similarity-based and graph-based approaches complement each other, we also combine these notions in a machine learning framework. This way, we achieve a further overall improvement in terms of F-measure for four out of eight considered datasets, while for three others we could achieve a significant improvement in alignment precision and accuracy. We investigate different machine learning classifiers and conclude that Bayesian Networks show the most robust results across datasets. While we also discuss additional machine learning features, none of these lead to further improvements, which we consider proof that structure and glosses of the LSRs are sufficiently informative for finding equivalent senses in LSRs. Moreover, we discuss different approaches to aligning more than two resources at once (N-way alignment), which however do not yield satisfactory results. We also analyze the reasons for that and identify a great demand for future research. \n \nThe unified LSR UBY provides the greater context for this thesis. Its representation format UBY-LMF (based on the \\emph{Lexical Markup Framework} standard) reflects the structure and content of many different LSRs with the greatest possible level of accuracy, making them interoperable and accessible. We demonstrate how the standardization is operationalized, where OmegaWiki serves as a showcase for presenting the properties of UBY-LMF, including the representation of the sense alignments. We also discuss the final, instantiated resource UBY, as well as the Java-based API, which allows easy programmatic access to it, a web interface for conveniently browsing UBY's contents, and the alignment framework we used for our experiments, whose implementation was enabled by the standardization efforts and the API. \n \nTo demonstrate that sense alignments are indeed beneficial for NLP, we discuss different applications which make use of them. The clustering of fine-grained GermaNet and WordNet senses by exploiting 1:n alignments to OmegaWiki, Wiktionary and Wikipedia significantly improves word sense disambiguation accuracy on standard evaluation datasets for German and English, while this approach is language-independent and does not require external knowledge or resource-specific feature engineering. The second scenario is computer-aided translation. We argue that the multilingual resources OmegaWiki and Wiktionary can be a useful source of knowledge, and especially translations, for this kind of applications. In this context, we also further discuss the results of the alignment we produce between them, and we give examples of the additional knowledge that becomes available through their combined usage. \n \nFinally, we point out many directions for future work, not only for WSA, but also for the design of aligned resources such as UBY and the applications that benefit from them.",
    "venue": "",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a5ae09cfe0115c310d982d17288008d2f64cbb42",
    "url": "https://www.semanticscholar.org/paper/a5ae09cfe0115c310d982d17288008d2f64cbb42",
    "title": "A New Method for Calculating Word Sense Similarity in WordNet 1",
    "abstract": "Semantic similarity between word senses is hot topic in many applications of computational linguistics and artificial intelligence, such as word sense disambiguation, information extraction, semantic annotation and ontology learning. Many methods for calculating word sense similarity have been proposed. In recent years the methods based on WordNet have shown its talents and attracted great concern. In the paper, we present a new method in WordNet for calculating word sense similarity, which is noun and is-a relation based. We evaluate our method on the data set of Rubenstein and Goodenough, which is traditional and widely used. The correlation with human judgment is o.8804 in proposed measure, which is more close to human judgments than related works. Experiments show that our new measure significantly outperformed than other existing computational methods.",
    "venue": "",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "312e6b42a75b2dc510aa30924da4ea7018697deb",
    "url": "https://www.semanticscholar.org/paper/312e6b42a75b2dc510aa30924da4ea7018697deb",
    "title": "Structural semantic interconnections: a knowledge-based approach to word sense disambiguation",
    "abstract": "Word sense disambiguation (WSD) is traditionally considered an AI-hard problem. A break-through in this field would have a significant impact on many relevant Web-based applications, such as Web information retrieval, improved access to Web services, information extraction, etc. Early approaches to WSD, based on knowledge representation techniques, have been replaced in the past few years by more robust machine learning and statistical techniques. The results of recent comparative evaluations of WSD systems, however, show that these methods have inherent limitations. On the other hand, the increasing availability of large-scale, rich lexical knowledge resources seems to provide new challenges to knowledge-based approaches. In this paper, we present a method, called structural semantic interconnections (SSI), which creates structural specifications of the possible senses for each word in a context and selects the best hypothesis according to a grammar G, describing relations between sense specifications. Sense specifications are created from several available lexical resources that we integrated in part manually, in part with the help of automatic procedures. The SSI algorithm has been applied to different semantic disambiguation problems, like automatic ontology population, disambiguation of sentences in generic texts, disambiguation of words in glossary definitions. Evaluation experiments have been performed on specific knowledge domains (e.g., tourism, computer networks, enterprise interoperability), as well as on standard disambiguation test sets.",
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "citationCount": 382,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ]
  },
  {
    "paperId": "72409eda31b88dbb03648565e9b61ebfe8f86b49",
    "url": "https://www.semanticscholar.org/paper/72409eda31b88dbb03648565e9b61ebfe8f86b49",
    "title": "Word Sense Disambiguation for Acquisition of Selectional Preferences",
    "abstract": "The selectional preferences of verbal predicates are an important component of lexical information useful for a number of NLP tasks including disambigliation of word senses. Approaches to selectional preference acquisition without word sense disambiguation are reported to be prone to errors arising from erroneous word senses. Large scale automatic semantic tagging of texts in sufficient quantity for preference acquisition has received little attention as most research in word sense disambiguation has concentrated on quality word sense disambiguation of a handful of target words. The work described here concentrates on adapting semantic tagging methods that do not require a massive overhead of manual semantic tagging and that strike a reasonable compromise between accuracy and cost so that large amounts of text can be tagged relatively quickly. The results of some of these adaptations are described here along with a comparison of the selectional preferences acquired with and without one of these methods. Results of a bootstrapping approach are also outlined in which the preferences obtained are used for coarse grained sense disambiguation and then the partially disambiguated data is fed back into the preference acquisition system. 1 1This work was supported by CEC Telematics Applications Programme project LE1-2111 \"SPARKLE: Shallow PARsing and Knowledge extraction for Language Engineering\".",
    "venue": "",
    "citationCount": 31,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "68db3484b84496a5af46bd0d7eaa074deba5df90",
    "url": "https://www.semanticscholar.org/paper/68db3484b84496a5af46bd0d7eaa074deba5df90",
    "title": "A Hybrid Approach to Word Sense Disambiguation : Neural Clustering with Class Labeling",
    "abstract": "By combining a neural algorithm with the WordNet lexical database we were able to semi-automatically label the groups of items clustered in a multi-branched hierarchy, paving way for the use of neural algorithms together with ontological knowledge in word sense disambiguation tasks.",
    "venue": "",
    "citationCount": 17,
    "fieldsOfStudy": null
  },
  {
    "paperId": "edc9518c04e76db20cf5cdefa50e0d620cfa2e1f",
    "url": "https://www.semanticscholar.org/paper/edc9518c04e76db20cf5cdefa50e0d620cfa2e1f",
    "title": "A New Method for Calculating Word Sense Similarity in WordNet1",
    "abstract": "Semantic similarity between word senses is hot topic in many applications of computational linguistics and artificial intelligence, such as word sense disambiguation, information extraction, semantic annotation and ontology learning. Many methods for calculating word sense similarity have been proposed. In recent years the methods based on WordNet have shown its talents and attracted great concern. In the paper, we present a new method in WordNet for calculating word sense similarity, which is noun and is-a relation based. We evaluate our method on the data set of Rubenstein and Goodenough, which is traditional and widely used. The correlation with human judgment is o.8804 in proposed measure, which is more close to human judgments than related works. Experiments show that our new measure significantly outperformed than other existing computational methods.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": null
  },
  {
    "paperId": "62723f39ff24330270d5753f49605d72416d7245",
    "url": "https://www.semanticscholar.org/paper/62723f39ff24330270d5753f49605d72416d7245",
    "title": "Kernel Methods for Word Sense Disambiguation and Acronym Expansion",
    "abstract": "The scarcity of manually labeled data for supervised machine learning methods presents a significant limitation on their ability to acquire knowledge. The use of kernels in Support Vector Machines (SVMs) provides an excellent mechanism to introduce prior knowledge into the SVM learners, such as by using unlabeled text or existing ontologies as additional knowledge sources. Our aim is to develop three kernels - one that makes use of knowledge derived from unlabeled text, the second using semantic knowledge from ontologies, and finally a third, additive kernel consisting of the first two kernels - and study their effect on the tasks of word sense disambiguation and automatic expansion of ambiguous acronyms.",
    "venue": "AAAI",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "fabbd75708174c961e1e00ae203b1b73053a4c3b",
    "url": "https://www.semanticscholar.org/paper/fabbd75708174c961e1e00ae203b1b73053a4c3b",
    "title": "If you have it, flaunt it: using full ontological knowledge for word sense disambiguation",
    "abstract": "Word sense disambiguation continues to be a difficult problem in natural language processing. Current methods, such as marker passing and spreading activation, for applying world knowledge in the form of selectional preferences to solve this problem do not make effective use of available knowledge. Moreover, their effectiveness decreases as the knowledge is made richer by acquiring more and more conceptual relationships. Effective resolution of word sense ambiguities requires inferring the dynamic context in processing a sentence in order to find the right selectional preferences to be applied. In this article, we propose such an inference operator and show how it finds the most specific context to resolve word sense ambiguities in the Mikrokosmos semantic analyzer. Our method retains its effectiveness even in a rich, large-scale knowledge base with a high degree of connectivity among its concepts. 1. Disambiguation in Context Word sense disambiguation continues to be a difficult problem for programs that process natural language. The goals of word sense resolution methods are: (a) to select as small a subset of possible senses of a word as possible, ideally just one sense, and (b) to select the best sense(s) given all the knowledge available to the system, including the dynamic context in processing the text. The most common methods for resolving word sense ambiguities are based on statistical collocations or selectional preferences (for a recent survey, see Guthrie et al, 1996) between pairs of word senses. Often, individual selectional preferences applicable to a word are not strong enough to exclude all but one sense of the word. The real power of word sense selection seems to lie in the ability to constrain the possible senses of a word based on selections made for other words in the dynamic context. Although it is a truism that context plays a significant role in sense disambiguation, computational models have not demonstrated the effectiveness of modeling context for resolving word senses in a large-scale NLP system. This work presupposes a semantic analysis environment, such as the Mikrokosmos system (Beale et al, 1995; Mahesh et al, submitted; Onyshkevych and Nirenburg, 1995), in which the results are expressions in a text meaning representation language whose syntax is based on propositions and their arguments, as well 1. Research reported in this paper was supported in part by Contract MDA904-92-C-5189 from the U.S. Department of Defense.",
    "venue": "TMI",
    "citationCount": 15,
    "fieldsOfStudy": null
  },
  {
    "paperId": "23da743867ff33038f45651e5eb32bbe44d98883",
    "url": "https://www.semanticscholar.org/paper/23da743867ff33038f45651e5eb32bbe44d98883",
    "title": "Efficient Ranking and Computation of Semantic Relatedness and its Application to Word Sense Disambiguation",
    "abstract": "Wikipedia has grown into a high quality up-to-date knowledge base and can enable many intelligent systems that rely on semantic information. One of the most general and quite powerful semantic tools is a measure of semantic relatedness between concepts. Moreover, the ability to efficiently produce a list of ranked similar concepts for a given concept is very important for a wide range of applications. We propose to use a simple measure of similarity between Wikipedia concepts, based on Dice\u2019s measure, and provide very efficient heuristic methods to compute top k ranking results. We also present a randomized algorithm that speeds up the evaluation of the measure for a pair of articles. Furthermore, since our heuristics are based on statistical properties of scale-free networks, we show that these heuristics are applicable to other complex ontologies. Finally, in order to evaluate the measure, we have used it to solve the problem of word-sense disambiguation. Our approach to word sense disambiguation is based solely on the similarity measure and produces results with high accuracy.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": null
  },
  {
    "paperId": "e7c19bfd1bc75c976fbddb15321613369cea5e04",
    "url": "https://www.semanticscholar.org/paper/e7c19bfd1bc75c976fbddb15321613369cea5e04",
    "title": "Knowledge Sources for Word Sense Disambiguation",
    "abstract": null,
    "venue": "International Conference on Text, Speech and Dialogue",
    "citationCount": 62,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "167bd5020530230e889ad2d9ffa509b131e2a0db",
    "url": "https://www.semanticscholar.org/paper/167bd5020530230e889ad2d9ffa509b131e2a0db",
    "title": "Automatic Noun Sense Disambiguation",
    "abstract": null,
    "venue": "Conference on Intelligent Text Processing and Computational Linguistics",
    "citationCount": 26,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "95c46bf04289576dcef1fbb008d271cd9709f5c7",
    "url": "https://www.semanticscholar.org/paper/95c46bf04289576dcef1fbb008d271cd9709f5c7",
    "title": "Integrating Conceptual Density with WordNet Domains and CALD Glosses for Noun Sense Disambiguation",
    "abstract": null,
    "venue": "EsTAL",
    "citationCount": 9,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "80661b553798dc182be2273c781c1e356614d7e9",
    "url": "https://www.semanticscholar.org/paper/80661b553798dc182be2273c781c1e356614d7e9",
    "title": "Kernel Methods for Word Sense Disambiguation and Abbreviation Expansion in the Medical Domain",
    "abstract": "Word Sense Disambiguation (WSD) is the problem of automatically deciding the correct meaning of an ambiguous word based on the surrounding context in which it appears. The automatic expansion of abbreviations having multiple possible expansions can be viewed as a special form of WSD with the multiple expansions acting as \u201dsenses\u201d of the ambiguous abbreviation. Both of these are significant problems, especially in domains such as medical text with abstracts of articles in scholarly medical journals and clinical notes taken by physicians. Most popular approaches to WSD involve supervised machine learning methods which require a set of manually annotated examples of disambiguated words or abbreviations to learn patterns that help in disambiguating future unseen instances. However, manual annotation imposes a limit on the amount of labeled data that can be made available to the supervised machine learning algorithms such as Support Vector Machines (SVMs), because annotation requires significant human work. Kernel methods for SVMs provide an elegant framework to incorporate knowledge from unlabeled data into the SVM learners. This thesis explores the application of kernel methods to two datasets from the medical domain, one containing ambiguous words and the other containing ambiguous abbreviations. We have developed two classes of semantic kernels Latent Semantic Analysis (LSA) Kernels and Word Association Kernels (ASSOC) for SVMs, that are learned from unlabeled text containing the ambiguous words or abbreviations using unsupervised methods. We have found that our semantic kernels improve the accuracy of SVMs on the task of WSD in the medical domain. In particular, we find that our LSA kernels with unigram features and ASSOC kernels with bigram features perform better than off-the-shelf SVM learners and they are significantly better for five out of 11 ambiguous words, which have a balanced sense distribution and for nine out of ten abbreviations. We also focus on the feature engineering aspect of the abbreviation expansion problem in the domain of clinical notes text. We propose a flexible window approach for capturing features and show that it significantly improves performance. We make use of features specific to clinical notes such as the gender code of the patient and show that this improves performance significantly in combination with Part-of-Speech features.",
    "venue": "",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "aa6024dd447993da065336276044596d1e4ec3d9",
    "url": "https://www.semanticscholar.org/paper/aa6024dd447993da065336276044596d1e4ec3d9",
    "title": "CoKE : Extending Contextualized Knowledge Embeddings for Word Sense Induction",
    "abstract": "Word Embeddings are able to capture lexico-semantic information but remain 1 flawed in their inability to assign unique representations to different senses of 2 a polysemous words. They also fail to include information from well curated 3 semantic lexicons and dictionaries. Previous approaches that integrate polysemy 4 and knowledge bases fall distinctly under two categories a)retrofitting vectors to 5 ontologies or b)learning from sense tagged corpora. While embeddings learned 6 from these methods are superior in understanding contextual similarity, they are 7 outperformed by single prototype word vectors on several relatedness tasks. In this 8 work, we introduce a new approach that can induce polysemy to any pre-trained 9 embedding space by jointly grounding contextualized sense representations and 10 word embeddings to a knowledge base. Along with word sense induction, the 11 resulting representations reduces the effect of vocabulary bias that arises in natural 12 language corpora and in turn embedding spaces. By grounding them to knowledge 13 bases they are able to learn multi-word representations and are also interpretable. 14 We evaluate our vectors across 12 datasets on several similarity and relatedness 15 tasks along with two extrinsic tasks ,we also evaluate against other transfer learning 16 methods and find that our approach consistently outperforms current state of the 17 art. 18 1 Related Work and Introduction 19 Distributed representations of words (Mikolov et al., 2013b) have proven to be successful in addressing 20 multiple drawbacks of symbolic representations which treats words as atomic units of meaning. By 21 grouping similar words and capturing analogical and lexical relationships, they are a popular choice 22 in several downstream NLP applications. 23 While these embeddings capture meaningful relationships, they come with their own set of drawbacks. 24 For instance, complete reliance on natural language corpora amplifies existing societal and vocabulary 25 bias that are inherent in datasets. A study by Bolukbasi et al., 2016 discussed societal biases in the 26 form of gender stereotypes present in these VSMs. Vocabulary bias is caused by words not seen in 27 the training corpora and also extends to bias in word usage where some words, often morphologically 28 complex words, are used less frequently than other words or phrases with the same meaning. This 29 also becomes evident in the relatively lower performance of word embeddings on the rare word 30 similarity task (Luong et al., 2013b). A recent approach by (Bojanowski et al., 2016a) proposes 31 using character n-gram representations to address the problem of out-of-vocabulary and rare words. 32 (Faruqui et al., 2014) also proposed retrofitting vectors to an ontology. However, these methods don\u2019t 33 account for polysemy. 34 Polysemy is an important feature of language which causes words to have different meanings based 35 on the context in which they occur. For instance, the word \u2018bank\u2019 can mean \u2018financial institution\u2019 or 36 Submitted to 32nd Conference on Neural Information Processing Systems (NIPS 2018). Do not distribute. \u2018land on either side of a river\u2019.A well known drawback with word embeddings is the assignment of a 37 single vector representation to a word type, irrespective of polysemy. A large body of work has gone 38 into developing word sense disambiguation systems to identify the correct sense of a word based on 39 it\u2019s context. The availability of such disambiguation systems coupled with the growing reliance of 40 NLP systems on distributional semantics has led to an increasing interest in obtaining powerful sense 41 representations. 42 Some of the previous work that has gone into learning sense representations includes unsupervised 43 learning techniques to cluster contexts and learn multi prototype vectors(Reisinger and Mooney 44 (2010) , Huang et al. (2012) and Wu and Giles (2015)). However, a common drawback with the 45 cluster based models is the difficulty in deciding the number of clusters apriori. ( Neelakantan et al. 46 (2015) , Tian et al. (2014) ,Cheng and Kartsaklis (2015) also learn multiple word embeddings by 47 modifying the Skip-Gram model. These approaches yield to sense representations that are limited in 48 terms of interpretability which makes it challenging to include in downstream tasks. 49 As a remedy to limitation in sense tagged corpora, Jauhar et al. (2015) and Rothe and Sch\u00fctze (2015) 50 explored grounding word embeddings to ontologies to obtain sense representations. As a result, these 51 techniques drastically improved performance on several similarity tasks but an observed pattern is 52 that this leads to compromised performance on word relatedness tasks(Faruqui et al. (2014), Jauhar 53 et al. (2015)). We suspect this is a result of directly modifying word embedding spaces based on 54 ontology structure. 55 In this work, we present a novel approach that uses knowledge bases and sense representations to 56 directly induce polysemy to any predefined word embedding space. We show how our approach 57 allows the integration of ontological information and leads to improvements in both word similarity 58 and relatedness tasks. The advantages of this are plenty, it allows the integration of knowledge into 59 embedding spaces and can readily induce polysemy in them. We thus rely on a) Sense tagged corpora 60 to obtain contextualized sense representations. The objective of which is to capture sense relations 61 and interactions in naturally occurring corpora. The sense representations are interpretable and have 62 lexical mappings to a knowledge base. We use them to induce polysemy in word embedding spaces. 63 b) Pretrained word embeddings to capture many useful lexical relationships that are inherent in them 64 on account of being trained on large amounts of data. These relationships are not effectively captured 65 by sense representations due to the limited size of sense tagged corpora they are trained on. c) Lastly, 66 in order to account for the vocabulary bias which causes similar meaning words to be farther apart in 67 embedding spaces as a result of bias in co-occurrence statistics found in corpora, we use a knowledge 68 base to jointly ground word and sense representations. 69 We thus obtain unique multiple word sense representations that show superior performance in 70 similarity, relatedness and extrinsic tasks. They also show performance benefits when used with 71 transfer learning methods like CoVE (McCann et al. (2017)) and ELMo (Peters et al. (2018)) 72",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "1c0f0494d31c754711d3afcad374ada986839f1e",
    "url": "https://www.semanticscholar.org/paper/1c0f0494d31c754711d3afcad374ada986839f1e",
    "title": "Noun Sense Disambiguation with WordNet for Software Design Retrieval",
    "abstract": null,
    "venue": "Canadian Conference on AI",
    "citationCount": 13,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "5087514af0ec5ec1d47dcfbde50be966fe77f809",
    "url": "https://www.semanticscholar.org/paper/5087514af0ec5ec1d47dcfbde50be966fe77f809",
    "title": "Indonesian Journal of Electrical Engineering and Computer Science",
    "abstract": "Received Mar 25, 2022 Revised Sep 3, 2022 Accepted Sep 14, 2022 Neural machine translation (NMT) is a fast-evolving MT paradigm and showed good results, particularly in large training data circumstances, for several language pairs. In this paper, we have utilized Sanskrit to Malayalam language pair neural machines translation. The attention-based mechanism for the development of the machine translation system was particularly exploited. Word sense disambiguation (WSD) is a phenomenon for disambiguating the text to let the machine infer the proper definition of the particular word. Sequential deep learning approaches such as a recurrent neural network (RNN), a gated recurrent unit (GRU), a long short term memory (LSTM), and a bi-directional LSTM (BLSTM) were used to analyze the tagged data. By adding morphological elements and evolutionary word sense disambiguation, the suggested common characterword embedding-based NMT model gives a BLEU score of 38.58 which was higher than the others.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "9c6048f927762837820ee0c1d1e1b2ebb6c16be8",
    "url": "https://www.semanticscholar.org/paper/9c6048f927762837820ee0c1d1e1b2ebb6c16be8",
    "title": "Calculating Word Sense Probability Distributions for Semantic Web Applications",
    "abstract": "Researchers have found that Word Sense Disambiguation (WSD) is useful for tasks such as ontology alignment. Many other Semantic Web applications could also be enhanced with WSD results of Semantic Web documents. A system that can provide reusable intermediate WSD results is desirable. Compared to the top sense or a rank of senses, an output of meaningful scores of each possible sense informs subsequent processes of the certainty in results, and facilitates the application of other knowledge in choosing the correct sense. We propose that probabilistic models, which have proved successful in many other fields, can also be applied to WSD. Based on such observations, we focus on the problem of calculating probability distributions of senses for terms. In this paper we propose our novel WSD approach with our probability model, derive the problem formula into small computable pieces, and propose ways to estimate the values of these pieces.",
    "venue": "2010 IEEE Fourth International Conference on Semantic Computing",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9c17e40e537e466b624fedb536b0bfe40ab3bfbb",
    "url": "https://www.semanticscholar.org/paper/9c17e40e537e466b624fedb536b0bfe40ab3bfbb",
    "title": "What Is an Ontology?",
    "abstract": null,
    "venue": "Handbook on Ontologies",
    "citationCount": 1278,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0832c626c1bd917f794c7b6e3e65e6ce2ae14812",
    "url": "https://www.semanticscholar.org/paper/0832c626c1bd917f794c7b6e3e65e6ce2ae14812",
    "title": "Ontology-Driven News Classification with Aethalides",
    "abstract": "The ever-increasing amount of Web information offered to news readers (e.g., news analysts) stimulates the need for news selection, so that informed decisions can be made with up-to-date knowledge. Hermes is an ontology-based framework for building news personalization services. It uses an ontology crafted from available news sources, allowing users to select and filter interesting concepts from a domain ontology. The Aethalides framework enhances the Hermes framework by enabling news classification through lexicographic and semantic properties. For this, Aethalides applies word sense disambiguation and ontology learning methods to news items. When tested on a set of news items on finance and politics, the Aethalides implementation yields a precision and recall of 74.4% and 49.4%, respectively, yielding an F0.5-measure of 67.6% when valuing precision more than recall.",
    "venue": "Journal of Web Engineering",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "8e5a58c138eb9195676ef87e4fb5a2f43ab595a0",
    "url": "https://www.semanticscholar.org/paper/8e5a58c138eb9195676ef87e4fb5a2f43ab595a0",
    "title": "Corpora based Approach for Arabic/English Word Translation Disambiguation",
    "abstract": "We are presenting a word sense disambiguation method applied in automatic translation of a query from Arabic into English. The developed machine learning approach is based on statistical models, that can learn from parallel corpora by analysing the relations between the items included in this corpora in order to use them in the word sense disambiguation task. The relations between items in this corpora are obtained by using and developing a purely statistical method from corpora in order to avoid the use of structured linguistic resources like ontology which are not yet available for Arabic in an appropriate quality. The results of this analysis should provide us with some useful semantic information that can help to find the best translation equivalents of the polysemous items.",
    "venue": "",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d1b5e537e92b5452e0c3b0bbc3f00af64828de70",
    "url": "https://www.semanticscholar.org/paper/d1b5e537e92b5452e0c3b0bbc3f00af64828de70",
    "title": "Word Sense Determination using WordNet and Sense Co-occurrence",
    "abstract": "This paper presents a method of word sense disambiguation that assigns a target word the sense that is most related to the senses of its neighbor words. We explore the use of measures of relatedness between word senses based on a novel hybrid approach. First, we investigate how to \"literally\" and \"regularly\" express a ''concept\". We apply set algebra to Wordnet's synsets cooperating with Wordnet's word ontology. In this way we establish regular rules for constructing various representations (lexical notations) of a concept using Boolean operators and various word forms in synset(s). Then we construct a formal mechanism for quantifying and estimating the semantic relatedness between concepts-we facilitate \"concept distribution statistics\" to determine the degree of semantic relatedness between two lexically expressed concepts. Then we applied the measure of semantic relatedness to the WSD task. The experimental results showed good performance on Semcor, a subset of Brown corpus. We observe that measures of semantic relatedness are useful sources of information for word sense disambiguation",
    "venue": "20th International Conference on Advanced Information Networking and Applications - Volume 1 (AINA'06)",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c5cf3c5c21ce6826cfea8c3e2cef4c5d3d0fcbed",
    "url": "https://www.semanticscholar.org/paper/c5cf3c5c21ce6826cfea8c3e2cef4c5d3d0fcbed",
    "title": "Ontology-based Tamil\u2013English cross-lingual information retrieval system",
    "abstract": null,
    "venue": "S\u0101dhan\u0101",
    "citationCount": 10,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "bec08c0e1d8a26a15493651017e34c4510c62ca1",
    "url": "https://www.semanticscholar.org/paper/bec08c0e1d8a26a15493651017e34c4510c62ca1",
    "title": "Ontology clarification by using semantic disambiguation",
    "abstract": "Semantic Web technology highly depends on the quality of ontology. In order to enhance quality of ontology, a vast amount of research has focused on concept modeling task, but there is one major problem with lexical representation of ontology. Current lexical representation is term which may have different meanings, so as to result in frustrating misunderstanding and ambiguity during the application of ontology. To solve this problem, sense is used to replace term as the lexical representation of concepts and properties for its unique meaning. Ontology clarification is the process of disambiguating terms in ontology by using its surrounding ontology elements and its nearby terms in annotated documents using this ontology. The right sense is assigned to a target term by maximizing the relatedness between the target and its neighbors for semantic relatedness between them. Experiments show our ontology clarification method is valid. Comparing with the best word sense disambiguation method, the concept precision is almost 2 times than the precision of noun, and the property precision is almost 3 times than the precision of verb. Another experiment proves that our method is also effective in a semi-automatic process.",
    "venue": "2008 12th International Conference on Computer Supported Cooperative Work in Design",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "61967fd07555a4aa3af10b6cc622291584221f11",
    "url": "https://www.semanticscholar.org/paper/61967fd07555a4aa3af10b6cc622291584221f11",
    "title": "RETRACTION: A New WSD approach using word ontology and concept distribution",
    "abstract": "This paper presents a method of word sense disambiguation that assigns a target word the sense that is most related to the senses of its neighbor words. Human languages have words that can mean different things in different contexts, and such words with multiple meanings are potentially `ambiguous'. `Word sense disambiguation' means the process of `deciding which of several meanings of a term is intended in a given context'. We explore the use of measures of relatedness between word senses based on a novel hybrid approach. First, we investigate how to express a `concept' literally and regularly. We apply set algebra to Wordnet's synsets, cooperating with Wordnet's word ontology. We establish regular rules for constructing various representations (lexical notations) of a concept using Boolean operators and word forms in various synset(s) defined in Wordnet. Thus, we establish a formal mechanism for quantifying and estimating the semantic relatedness between concepts\u2014we facilitate `concept distribution statistics' to determine the semantic relatedness between two lexically expressed concepts. Our method does not require any training in advanced. The experimental results showed good performance on Semcor, a subset of the Brown corpus. We observe that measures of semantic relatedness are useful sources of information for word sense disambiguation.",
    "venue": "J. Inf. Sci.",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "776d7adca818651e475b91876978e52be74d250c",
    "url": "https://www.semanticscholar.org/paper/776d7adca818651e475b91876978e52be74d250c",
    "title": "An Ontology-Based Approach to Disambiguation of Semantic Relations",
    "abstract": "This paper describes experiments in using machine learning for relation disambiguation. There have been succesfuld experiments in combining machine learning and ontologies, or light-weight ontologies such as WordNet, for word sense disambiguation. However, what we are trying to do, is to disambiguate complex concepts consisting of two simpler concepts and the relation that holds between them. The motivation behind the approach is to expand existing methods for content based information retrieval. The experiments have been performed using an annotated extract of a corpus, consisting of prepositions surrounded by noun phrases, where the prepositions denote the relation we are trying disambiguate. The results show an unexploited opportunity of including prepositions and the relations they denote, e.g. in content based information retrieval.",
    "venue": "Learning Structured Information@EACL",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "3e9ba2c260e0c64180b796ed7aa53ae050521f87",
    "url": "https://www.semanticscholar.org/paper/3e9ba2c260e0c64180b796ed7aa53ae050521f87",
    "title": "Joint semantic similarity assessment with raw corpus and structured ontology for semantic-oriented service discovery",
    "abstract": null,
    "venue": "Personal and Ubiquitous Computing",
    "citationCount": 20,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c7f496711c9473cde4a8cf19ead6526f55d71ef3",
    "url": "https://www.semanticscholar.org/paper/c7f496711c9473cde4a8cf19ead6526f55d71ef3",
    "title": "An Overview on XML Semantic Disambiguation from Unstructured Text to Semi-Structured Data: Background, Applications, and Ongoing Challenges",
    "abstract": "Since the last two decades, XML has gained momentum as the standard for web information management and complex data representation. Also, collaboratively built semi-structured information resources, such as Wikipedia, have become prevalent on the Web and can be inherently encoded in XML. Yet most methods for processing XML and semi-structured information handle mainly the syntactic properties of the data, while ignoring the semantics involved. To devise more intelligent applications, one needs to augment syntactic features with machine-readable semantic meaning. This can be achieved through the computational identification of the meaning of data in context, also known as (a.k.a.) automated semantic analysis and disambiguation, which is nowadays one of the main challenges at the core of the Semantic Web. This survey paper provides a concise and comprehensive review of the methods related to XML-based semi-structured semantic analysis and disambiguation. It is made of four logical parts. First, we briefly cover traditional word sense disambiguation methods for processing flat textual data. Second, we describe and categorize disambiguation techniques developed and extended to handle semi-structured and XML data. Third, we describe current and potential application scenarios that can benefit from XML semantic analysis, including: data clustering and semantic-aware indexing, data integration and selective dissemination, semantic-aware and temporal querying, web and mobile services matching and composition, blog and social semantic network analysis, and ontology learning. Fourth, we describe and discuss ongoing challenges and future directions, including: the quantification of semantic ambiguity, expanding XML disambiguation context, combining structure and content, using collaborative/social information sources, integrating explicit and implicit semantic analysis, emphasizing user involvement, and reducing computational complexity.",
    "venue": "IEEE Transactions on Knowledge and Data Engineering",
    "citationCount": 41,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "b17cc18e4130505b939f7d527082eb6be2a7fd5b",
    "url": "https://www.semanticscholar.org/paper/b17cc18e4130505b939f7d527082eb6be2a7fd5b",
    "title": "Rationale-Augmented Ensembles in Language Models",
    "abstract": "Recent research has shown that rationales , or step-by-step chains of thought, can be used to improve performance in multi-step reasoning tasks. We reconsider rationale-augmented prompting for few-shot in-context learning, where (input \u2192 output) prompts are expanded to (input, rationale \u2192 output) prompts. For rationale-augmented prompting we demonstrate how existing approaches, which rely on manual prompt engineering, are subject to sub-optimal rationales that may harm performance. To mitigate this brittleness, we propose a uni\ufb01ed framework of rationale-augmented ensembles , where we identify rationale sampling in the output space as the key component to robustly improve performance. This framework is general and can easily be extended to common natural language processing tasks, even those that do not traditionally leverage intermediate steps, such as question answering, word sense disambiguation, and sentiment analysis. We demonstrate that rationale-augmented ensembles achieve more accurate and interpretable results than existing prompting approaches\u2014including standard prompting without rationales and rationale-based chain-of-thought prompting\u2014while simultaneously improving interpretability of model predictions through the associated rationales.",
    "venue": "ArXiv",
    "citationCount": 20,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "ed973511798f34841df288421e8dc3db73c01e12",
    "url": "https://www.semanticscholar.org/paper/ed973511798f34841df288421e8dc3db73c01e12",
    "title": "Bridging the Word Disambiguation Gap with the Help of OWL and Semantic Web Ontologies",
    "abstract": "Due to the complexity of natural language, sufficiently reliable Word Sense Disambiguation (WSD) systems are yet to see the daylight in spite of years of work directed towards that goal in Artificial Intelligence, Computational Linguistics and other related disciplines. We describe how the goal could be approached by applying hybrid methods to information sources and knowledge types. The overall aim is to chart the shorfalls of the present WSD systems related to the use of knowledge types and information sources in them. Real world ontologies and other ontologies in the Semantic Web will make a useful contribution towards the WSD knowledge base envisaged here. The inference capabilities inherent in Ontology Web Language (OWL) especially will have an important role to play in natural language disambiguation and knowledge acquisition. The emphasis is on ontologies as one of the important information sources for hybrid WSD.",
    "venue": "",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f234b5a71548dbf44fb9656b5305b9a75b8a1e87",
    "url": "https://www.semanticscholar.org/paper/f234b5a71548dbf44fb9656b5305b9a75b8a1e87",
    "title": "Ontology-based Domain-specific Semantic Similarity Analysis and Applications",
    "abstract": "Millions of text data are penetrating into our daily life. These unstructured text data serve as a huge source of information. Efficient organization and analysis of the overwhelming text can filter out irrelevant and redundant information, uncover invaluable knowledge, thus significantly reduce human effort, facilitate knowledge discovery and enhance cognitive abilities. Semantic similarity analysis among text objects is one of the fundamental problems in text mining including document classification/clustering, recommendation, query expansion, information retrieval, relevance feedback, word sense disambiguation, etc. While a combination of common sense and domain knowledge could let a person quickly determine if two objects are similar, the computers understand very little of human thinking. Knowledge resources such as ontologies can greatly capture the semantics of text objects, which enables the numeric representation of both domain knowledge and context information. In this dissertation, we develop a series of techniques to measure the semantic similarity of objects in multiple domains. By utilizing the structured knowledge that has already been established, we explore the domain knowledge from the existing lexical resources and incorporate it into specific applications within different domains. Specifically, we investigate the semantic similarities between gene products using Gene Ontology in biology domain. In text domain, we propose a hybrid representation of text objects (words and documents) based on WordNet which exploits both context and ontology",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9c01a3e29f62b7e8967b03af0016e2a555ddd2c2",
    "url": "https://www.semanticscholar.org/paper/9c01a3e29f62b7e8967b03af0016e2a555ddd2c2",
    "title": "FNLP\u2010ONT: A feasible ontology for improving NLP tasks in Persian",
    "abstract": "Natural language processing is a composition of several error\u2010prone and challenging tasks, including part of speech tagging, word sense disambiguation, named entity recognition, and compound verb detection. Studying intrasentence relations and roles is essential to improve the mentioned subtasks. Semi\u2010automatic schemes such as ontologies can be applied to clarify word's dependencies. This paper presents an ontology that is targeting to improve POS tagging, WSD, NER, and compound verb detection in Persian with extra properties that may ameliorate machine translation. The ontology is tested in combinations with several state\u2010of\u2010art algorithms on Dadegan corpus. The results show that coping semantic analysis with machine learning methods enhance relation detection and consequently precision of the mentioned subtasks, which is not widely addressed in Persian. Furthermore, the experimental results declare that the accuracy rate increases between 4.5 and 23% for different tasks.",
    "venue": "Expert Syst. J. Knowl. Eng.",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d39ad6c9511843ac451d7577edd7bec2ac8eb26b",
    "url": "https://www.semanticscholar.org/paper/d39ad6c9511843ac451d7577edd7bec2ac8eb26b",
    "title": "Ontology Construction and Its Application to Disambiguate Word Senses",
    "abstract": "This paper presents an ontology construction method using various computational language resources, and an ontology-based word sense disambiguation method. In order to acquire a reasonably practical ontology the Kadokawa thesaurus is extended by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. To apply the ontology to disambiguate word senses, we apply the previously-secured dictionary information to select the correct senses of some ambiguous words with high precision, and then use the ontology to disambiguate the remaining ambiguous words. The mutual information between concepts in the ontology was calculated before using the ontology as knowledge for disambiguating word senses. If mutual information is regarded as a weight between ontology concepts, the ontology can be treated as a graph with weighted edges, and then we locate the weighted path from one concept to the other concept. In our practical machine translation system, our word sense disambiguation method achieved a 9% improvement over methods which do not use ontology for Korean translation.",
    "venue": "",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "412a3aaae34e368d1cc654f8643a65bb4bc9a39c",
    "url": "https://www.semanticscholar.org/paper/412a3aaae34e368d1cc654f8643a65bb4bc9a39c",
    "title": "Applying Semantic Similarity Measures Based on Information Content in the Evaluation of a Domain Ontology",
    "abstract": "Semantic similarity is a metric used to know the similarity degree of two concepts in an ontology or a taxonomy. Semantic similarity has a wide variety applications on artificial intelligence, natural language processing, biomedical informatics, geoinformatics and semantic web and is usually applied on machine translation and word-sense disambiguation. In this research, semantic similarity measures are used to evaluate taxonomic relationships in a domain ontology. This evaluation was carried out by using a proposed algorithm and through the accuracy measure. The semantic similarity measures implemented are based on information content and were proposed by the following authors: Resnik, Lin, Jiang & Conrath and Mazandu & Mulder. Mainly, this research contributes to the automatic evaluation of ontologies in the task of evaluating the ontology taxonomy. The experimental results show that the measures have at least 88% accuracy. In addition, the system has an accuracy of 94% compared to validation responses from an expert.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4df3ccc07790062a96e610d525bfd14fab0186ff",
    "url": "https://www.semanticscholar.org/paper/4df3ccc07790062a96e610d525bfd14fab0186ff",
    "title": "Ontology-based Tamil\u2013English cross-lingual information retrieval system",
    "abstract": null,
    "venue": "S\u0101dhan\u0101",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "459474126c67ee0c5517ac3eac9f2e1c0fcafda2",
    "url": "https://www.semanticscholar.org/paper/459474126c67ee0c5517ac3eac9f2e1c0fcafda2",
    "title": "Named Entity Disambiguation using Freebase and Syntactic Parsing",
    "abstract": "Named Entity Disambiguation (NED) is a fundamental task of semantic annotation for the Semantic Web. The task of Word Sense Disambiguation (WSD) in Ontology-Based Information Extraction (OBIE) aims to establish a link between the textual entity mention and the corresponding class in the ontology. In this paper, we propose a NED process integrated in a rule-based OBIE system for French. We show that our SVM approach can improve disambiguation efficiency using syntactic features provided by the Fips parser and popularity score features extracted from the Freebase knowledge base.",
    "venue": "LD4IE@ISWC",
    "citationCount": 14,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "7b9d24e2634a87f4fbb0f3fa8575533b2a4275f7",
    "url": "https://www.semanticscholar.org/paper/7b9d24e2634a87f4fbb0f3fa8575533b2a4275f7",
    "title": "Ontology-Based Textual Emotion Detection",
    "abstract": "Emotion Detection from text is a very important area of natural language processing. This paper shows a new method for emotion detection from text which depends on ontology. This method is depending on ontology extraction from the input sentence by using a triplet extraction algorithm by the OpenNLP parser, then make an ontology matching with the ontology base that we created by similarity and word sense disambiguation. This ontology base consists of ontologies and the emotion label related to each one. We choose the emotion label of the sentence with the highest score of matching. If the extracted ontology doesn\u2019t match any ontology from the ontology base we use the keyword-based approach. This method doesn\u2019t depend only on keywords like previous approaches; it depends on the meaning of sentence words and the syntax and semantic analysis of the context.",
    "venue": "",
    "citationCount": 12,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e873ee6469cf78c631b055ad412e92e027de0c82",
    "url": "https://www.semanticscholar.org/paper/e873ee6469cf78c631b055ad412e92e027de0c82",
    "title": "Hungarian Word-Sense Disambiguated Corpus",
    "abstract": "To create the first Hungarian WSD corpus, 39 suitable word form samples were selected for the purpose of word sense disambiguation. Among others, selection criteria required the given word form to be frequent in Hungarian language usage, and to have more than one sense considered frequent in usage. HNC and its Heti Vil\u00e1ggazdas\u00e1g subcorpus provided the basis for corpus text selection. This way, each sample has a relevant context (whole article), and information on the lemma, POS-tagging and automatic tokenization is also available. When planning the corpus, 300-500 samples of each word form were to be annotated. This size makes it possible that the subcorpora prepared for the individual word forms can be compared to data available for other languages. However, the finalized database also contains unannotated samples and samples with single annotation, which were annotated only by one of the linguists. The corpus follows the ACL\u0092s SensEval/SemEval WSD tasks format. The first version of the corpus was developed within the scope of the project titled The construction Hungarian WordNet Ontology and its application in Information Extraction Systems (Hatvani et al., 2007). The corpus \u0093 for research and educational purposes\u0094 is available and can be downloaded free of charge.",
    "venue": "International Conference on Language Resources and Evaluation",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "bca66bc301216f04bba7f1647fbb31956a48ac88",
    "url": "https://www.semanticscholar.org/paper/bca66bc301216f04bba7f1647fbb31956a48ac88",
    "title": "Automatically generating taxonomy for grouping app reviews \u2014 a study of three apps",
    "abstract": null,
    "venue": "Softw. Qual. J.",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "fee664e4007f87c218828d7953f5cff07f7e4c1a",
    "url": "https://www.semanticscholar.org/paper/fee664e4007f87c218828d7953f5cff07f7e4c1a",
    "title": "The Paradox of the Fuzzy Disambiguation in the Information Retrieval",
    "abstract": "Current methods of data mining, word sense disambiguation\nin the information retrieval, semantic relation, fuzzy\nsets theory, fuzzy description logic, fuzzy ontology and their\nimplementation, omit the existence of paradox called here the\nparadox of the fuzzy disambiguation. The paradox lies in the\nfact that due to fuzzy data and the experts knowledge it can\nbe obtained precise knowledge. In this paper to describe this\nparadox, is introduced a conceptual apparatus. Moreover, there\nis formulated an information retrieval logic. There are suggested\ncertain applications of this logic to search information on the\nWeb.",
    "venue": "",
    "citationCount": 9,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c40475fe5aca03a11dff58a3d2e38de52a73539f",
    "url": "https://www.semanticscholar.org/paper/c40475fe5aca03a11dff58a3d2e38de52a73539f",
    "title": "OBTAINING FEATURE- AND SENTIMENT-BASED LINKED INSTANCE RDF DATA FROM UNSTRUCTURED REVIEWS USING ONTOLOGY-BASED MACHINE LEARNING",
    "abstract": "Online reviews have a profound impact on the customer or \u201cnewbie\u201d who wishes to purchase or consume a product via Web 2.0 e-commerce. Online reviews contain features that form half of the analysis in opinion mining. Most of today\u2019s systems work on the basis of summarization, looking at the average obtained features and their sentiments, leading to structured review information being generated. Often, the context surrounding a feature, which helps the sentiment of the review to be classified clearly, is overlooked. The Web 3.0-based machine interpretable Resource Description Framework (RDF) can be used to structure these unstructured reviews into features and sentiments, which are obtained via traditional preprocessing and extraction techniques. Here, data about the context is also provided for future ontology-based analysis, with support from the WordNet lexical database for word sense disambiguation and SentiWordNet scores for sentiment word extraction. Many popular RDF vocabularies are helpful for obtaining such machine-processable data. This work forms the basis for creating/upgrading the (available) OWL Ontology that can be used as a structured data model with rich semantics for supervised machine learning. With this method, the classified sentiment categories are validated in relation to precise sentiments and are sent back to the interface in corresponding \u201cfeature/sentiment\u201d pairs so that reviews are filtered clearly, which helps to satisfy the feature set of the customer.",
    "venue": "",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c9c55ba79698c878d265c807e6eff3df1abb3b22",
    "url": "https://www.semanticscholar.org/paper/c9c55ba79698c878d265c807e6eff3df1abb3b22",
    "title": "Robust Word Sense Translation by EM Learning of Frame Semantics",
    "abstract": "We propose a robust method of automatically constructing a bilingual word sense dictionary from readily available monolingual ontologies by using estimation-maximization, without any annotated training data or manual tuning. We demonstrate our method on the English FrameNet and Chinese HowNet structures. Owing to the robustness of EM iterations in improving translation likelihoods, our word sense translation accuracies are very high, at 82% on average, for the 11 most ambiguous words in the English FrameNet with 5 senses or more. We also carried out a pilot study on using this automatically generated bilingual word sense dictionary to choose the best translation candidates and show the first significant evidence that frame semantics are useful for translation disambiguation. Translation disambiguation accuracy using frame semantics is 75%, compared to 15% by using dictionary glossing only. These results demonstrate the great potential for future application of bilingual frame semantics to machine translation tasks.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f37f9c6162ab65a23c004ad65be39b75106a62cf",
    "url": "https://www.semanticscholar.org/paper/f37f9c6162ab65a23c004ad65be39b75106a62cf",
    "title": "Enriching WordNet Ontology using Coarse-Grained Word Senses",
    "abstract": "All technologies have been emerged during the vision of Semantic Web are helpful for knowledge applications in various research areas. Semantic Web will consist of a distributed environment of shared and interoperable ontologies. Since building ontologies from scratch is not an easy task and is a time-consuming process, there is another perspective, which studies the approaches for developing and enriching ontologies either from reusing existing ontologies or from reusing different data sources. This paper presents a new approach that enriches WordNet based on Coarse-Grained word senses. These senses caused by applying WordNet Fine-Grained word senses to a Merging Sense algorithm. This algorithm merges only semantically similar word senses instead of applying traditional clustering techniques. Two different data sources are used in the enrichment process: Web documents and Corpus documents. The results obtained from using Coarse-Grained word senses yields better precision than Fine-Grained word senses in Word Sense Disambiguation task.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "a3db41caffaccd07d582935d3e9cdb9224eaa683",
    "url": "https://www.semanticscholar.org/paper/a3db41caffaccd07d582935d3e9cdb9224eaa683",
    "title": "EXTENDED INFERENCING IN CONTEXTUAL ONTOLOGY-SUPPORTED RULE-BASED SYSTEMS FOR READING MATERIAL CLASSIFICATION",
    "abstract": "Reading Material Classification (RMC) determines Determining the particular readability graded reading material from an unclassified text based on its text readability. RMC have used Natural Language Processing (NLP) methods, i.e., machine-learning-based RMC, to overcome disadvantages of using syntactic features, i.e., insufficiency for modelling the levels of text reading difficulty. Concepts can be varied somewhat between different contexts, therefore \u00abcontextual concept-variants\u00bb wanted in our ontologies which will result in Contextual Ontology (CO) by using basic NLP techniques such as stemming and word sense disambiguation. Rule-Based Systems (RBSs) are Knowledge-Based Systems whose knowledge is structured by rules. Given RMC as the test-bed, we propose to combine CO with RBSs for synergising their advantages, to improve RBS performance by adding contextual ontological processing into RBSs. This addition will also provide an extended inferencing to RBSs, so that (a) in preprocessing of inferencing, a powerful CO can be extracted, (b) in in-processing of inferencing CO can be utilised for making inferences along with some features (rule strength incremental modification, the state of memory affection and system\u2019s weight), and (c) in post-processing in inferencing, CO can be exploited. Based on the evaluation experiments, we do not claim that our proposed method is better than machine-learning-based RMC. Instead, our system performance is just on a par with them. Rather than beating those methods, we aimed to use RMC to show that adding contextual ontological processing into RBSs (RMC-RBS + CO) presents a considerable benefit for RBSs than not adding it (RMC-RBS + O). 31.25% and 25.89% improvements can be obtained for validation and testing data, respectively.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "02ff592a3e944cbec762a2ab90618ff291c2885c",
    "url": "https://www.semanticscholar.org/paper/02ff592a3e944cbec762a2ab90618ff291c2885c",
    "title": "EXTENDING THE ONTOLOGY\u2019S COVERAGE AND ACCURACY FOR READING MATERIAL CLASSIFICATION BY INTEGRATING CONTEXTUAL INFORMATION INTO ONTOLOGIES",
    "abstract": "Reading Material Classification (RMC) classifies an unclassified text into the readability graded reading material based on its text readability. Recent approaches for RMC have used Natural Language Processing (NLP) methods such as machine-learning-based methods (e.g., Support Vector Machine, Multinomial Na\u00efve Bayes and Latent Semantic Indexing) to overcome disadvantages of using the syntactic features, i.e., insufficiency for modelling the levels of text reading difficulty. Ontologies have been used for sharing and reusing knowledge, and perhaps supporting the inference. It will be used for RMC. Concepts and contexts are treated separately in ontologies. By only using basic NLP techniques such as stemming and word sense disambiguation, integrating contextual information into ontologies, i.e., Contextual Ontology (CO), is proposed which aimed to improve the ontology\u2019s performance and possibly other types of quality for RMC. Once a CO has been built, the coverage and accuracy of the ontology can be extended. From the evaluation experiments, we do not claim that our proposed method is better than machine-learning based RMC, our system performance is just on a par with them. Rather than beating them, we aimed to use RMC to show that integrating contextual information into ontologies (RMC-CO) provides a considerable benefit for ontologies than not integrate it (RMC-O). 1.56% and 2.11% improvements can be obtained for validation and testing data, respectively.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "41299ea124d9f70cc8e0ac738201749fd4a261db",
    "url": "https://www.semanticscholar.org/paper/41299ea124d9f70cc8e0ac738201749fd4a261db",
    "title": "ALTERNATIVE VIEW IN CONTEXTUAL ONTOLOGY-BASED READING MATERIAL CLASSIFICATION",
    "abstract": "Reading Material Classification (RMC) classifies an unclassified text into the readability graded reading material based on its text readability. Recent approaches for RMC have used Natural Language Processing (NLP) methods such as machine-learning-based methods (e.g., Support Vector Machine, Multinomial Na\u00efve Bayes and Latent Semantic Indexing) to overcome disadvantages of using the syntactic features, i.e., insufficiency for modelling the levels of text reading difficulty. Ontologies have been used for sharing and reusing knowledge, and perhaps supporting the inference. It will be used for RMC. Concepts and contexts are treated separately in ontologies. By only using basic NLP techniques such as stemming and word sense disambiguation, integrating contextual information into ontologies, i.e., Contextual Ontology (CO), is proposed which aimed to improve the ontology\u2019s performance and possibly other types of quality for RMC. Since CO is the gradation of concepts, i.e., concept variants, then an alternative view in determining the results of RMC can be obtained and utilised. From the evaluation experiments, we do not claim that our proposed method is better than machinelearning based RMC, our system performance is just on a par with them. Rather than beating them, we aimed to use RMC to show that integrating contextual information into ontologies (RMC-CO) provides a considerable benefit for ontologies than not integrate it (RMC-O). 1.56% and 2.11% improvements can be obtained for validation and testing data, respectively.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "18fe89bbff40916ecf39513fca423197ae29024a",
    "url": "https://www.semanticscholar.org/paper/18fe89bbff40916ecf39513fca423197ae29024a",
    "title": "Feature based Sentiment Analysis using a Domain Ontology",
    "abstract": "With the increase of unstructured social media data, sentiment analysis can be applied to infer useful information to assist organizations and their customers. We propose a model for feature-based sentiment analysis using ontology to address queries like: \u201cwhich car is more comfortable?, which car has better performance and interior?\u201d. Feature based sentiment analysis is done using SentiWordNet with word sense disambiguation and an ontology that is developed by us. Data Tables are prepared from the RDF triples of parsed ontology and the sentiment ranks of car attributes. To relate the RDBM data to the built ontology of car, mapping axioms are proposed to connect them using OBDA model. Using SPARQL query, the results of the proposed model are compared with a dictionary-based method with respect to different car attributes. The performance of our model is better than dictionary based method.",
    "venue": "ICON",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "aefc7ddb64c76b46c49151cdf4414e0da9ce703f",
    "url": "https://www.semanticscholar.org/paper/aefc7ddb64c76b46c49151cdf4414e0da9ce703f",
    "title": "An Ontology-Based Semantic Similarity Measure Considering Multi-Inheritance in Biomedicine",
    "abstract": "Computation of semantic similarity between words for text understanding is a vital issue in many applications such as word sense disambiguation, document categorization, and information retrieval. In recent years, different paradigms have been proposed to compute semantic similarity based on different ontologies and knowledge resources. In this paper, we propose a new similarity measure combining both superconcepts of the evaluated concepts and their common specificity feature. The common specificity feature considers the depth of the Least Common Subsumer (LCS) of two concepts and the depth of the ontology to obtain more semantic evidence. The multiple inheritance phenomenon in a large and complex taxonomy is taken into account by all superconcepts of the evaluated concepts. We evaluate and compare the correlation obtained by our measure with human scores against other existing measures exploiting SNOMED CT as the input ontology. The experimental evaluations show the applicability of the measure on different datasets and confirm the efficiency and simplicity of our proposed measure.",
    "venue": "",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "91e1dfd280fbcfa0e808596fc46481fa19e8a511",
    "url": "https://www.semanticscholar.org/paper/91e1dfd280fbcfa0e808596fc46481fa19e8a511",
    "title": "An Enhanced Ontology Based Measure of Similarity between Words and Semantic Similarity Search",
    "abstract": null,
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "3e74be96a48ff56b64bf6074058766cdf2eeca78",
    "url": "https://www.semanticscholar.org/paper/3e74be96a48ff56b64bf6074058766cdf2eeca78",
    "title": "Feature and Sentiment based Linked Instance RDF Data towards Ontology based Review Categorization",
    "abstract": "Online reviews have a potential impact on the green customer who wants to purchase or consume the product through e-commerce. Online reviews contain features which are useful for the analysis in opinion mining. Most of the today\u2019s systems work on the summarization of the features taking the average features and their sentiments leading to structured review information. Often the context of surrounding feature is undermined which helps while classifying the sentiment of the review. In web 3.0 machine interpretable Resource Description Framework (RDF) were introduced which helps in structuring these unstructured reviews in the form of features and sentiments obtained from traditional preprocessing and extraction techniques. The context data also supports for future ontology based analysis taking support of Wordnet lexical database for word sense disambiguation and Sentiwordnet scores used for sentiment word extraction. Many popular RDF vocabularies are helpful in the creation of such machine processable data. In the future work, such instance RDF data will be used in the OWL Ontology to reason the data to clearly identify the features and sentiments against the applied data set. These results are sent back to the interface as corresponding {feature, sentiment} pair so that reviews are filtered clearly and helps in satisfying the feature set of the customer. Keywords-Opinion mining, Feature, Sentiment, Resource Description Framework, Ontology",
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": null
  },
  {
    "paperId": "83d33b7055bc884ed3b068cc937b82ec30a15d7e",
    "url": "https://www.semanticscholar.org/paper/83d33b7055bc884ed3b068cc937b82ec30a15d7e",
    "title": "Text Categorization for Generation of a Historical Shipbuilding Ontology",
    "abstract": null,
    "venue": "KESW",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ]
  },
  {
    "paperId": "0a496554a7e1629cb3001d6b30888f85bf19269f",
    "url": "https://www.semanticscholar.org/paper/0a496554a7e1629cb3001d6b30888f85bf19269f",
    "title": "CRCTOL: A semantic-based domain ontology learning system",
    "abstract": "Domain ontologies play an important role in supporting knowledge-based applications in the Semantic Web. To facilitate the building of ontologies, text mining techniques have been used to perform ontology learning from texts. However, traditional systems employ shallow natural language processing techniques and focus only on concept and taxonomic relation extraction. In this paper we present a system, known as Concept-Relation-Concept Tuple-based Ontology Learning (CRCTOL), for mining ontologies automatically from domain-specific documents. Specifically, CRCTOL adopts a full text parsing technique and employs a combination of statistical and lexico-syntactic methods, including a statistical algorithm that extracts key concepts from a document collection, a word sense disambiguation algorithm that disambiguates words in the key concepts, a rule-based algorithm that extracts relations between the key concepts, and a modified generalized association rule mining algorithm that prunes unimportant relations for ontology learning. As a result, the ontologies learned by CRCTOL are more concise and contain a richer semantics in terms of the range and number of semantic relations compared with alternative systems. We present two case studies where CRCTOL is used to build a terrorism domain ontology and a sport event domain ontology. At the component level, quantitative evaluation by comparing with Text-To-Onto and its successor Text2Onto has shown that CRCTOL is able to extract concepts and semantic relations with a significantly higher level of accuracy. At the ontology level, the quality of the learned ontologies is evaluated by either employing a set of quantitative and qualitative methods including analyzing the graph structural property, comparison to WordNet, and expert rating, or directly comparing with a human-edited benchmark ontology, demonstrating the high quality of the ontologies learned. \u00a9 2010 Wiley Periodicals, Inc.",
    "venue": "J. Assoc. Inf. Sci. Technol.",
    "citationCount": 133,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "31b27befbac1d3456b819ab8af5545f416be676f",
    "url": "https://www.semanticscholar.org/paper/31b27befbac1d3456b819ab8af5545f416be676f",
    "title": "Question Driven Semantics Interpretation for Collaborative Knowledge Engineering and Ontology Reuse",
    "abstract": "Ontology integration, alignment, and reuse are at the heart of semantic Web vision. The alignment between two ontologies can be achieved easily provided both share the same axiomatic space. This in turn can be achieved by reusing constructs from foundational ontologies and standard domain thesauri. Although the current breed of ontology development methodologies and tools have made it a lot easier to build a new ontology, reusing concepts from existing ontologies remains difficult. The main reasons for this are the ambiguities in semantics interpretations of concepts from foundational ontologies, which are biased by philosophical aspirations, domain nuance, and design rationale introduced at the time of their creation. In this paper we have introduced a novel methodology for sense disambiguation to help the human expert in semantics interpretation. It is a collaborative and interaction intensive question driven approach based on a DOLCE aligned form of WordNet -OntoWordNet. The approach was used successfully for ontology reuse during the collaborative ontology building process; results are provided here.",
    "venue": "2007 IEEE International Conference on Information Reuse and Integration",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "ccac5f7c1e3d3a835ad114fcc22d4652e969a45a",
    "url": "https://www.semanticscholar.org/paper/ccac5f7c1e3d3a835ad114fcc22d4652e969a45a",
    "title": "A Study on Ontology Life Cycle & Co-Relation Matrix",
    "abstract": "In modern virtual world, knowing each other and understanding is very difficult. In this paper we present a study on ontology life cycle which would help us in understanding the human emotions. Ontology is an effective computing paradigm which made it possible to include findings from social and psychology. The new method for emotion detection from text which depends on ontology extraction from the input sentence by using a triplet extraction algorithm. Then make an ontology matching with the ontology base that we created by similarity and word sense disambiguation. This ontology base consists of ontologies and the emotion label related to each one. We choose the emotion label of the sentence with the highest score of matching. If the extracted ontology doesn\u2019t match any ontology from the ontology base we use the keyword-based approach. This method doesn\u2019t depend only on keywords like the previous approaches it depends on the meaning of sentence words and the syntax and semantic analysis of the context. Ontology-based applications play an increasingly important role in the public and corporate Semantic Web. Methodologies used for the development of knowledge-based applications focus purely on knowledge engineering. Architectures for semantic web services involve ontologies, but naturally focus on services. For instance, WSMO or ODE-SWS provide ontology-based mechanisms to formally describe services and arriving at co-relation matrix between emotion classes.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "90446907a0a33ff535e79d32e2c90ac30e26f72a",
    "url": "https://www.semanticscholar.org/paper/90446907a0a33ff535e79d32e2c90ac30e26f72a",
    "title": "Joint semantic similarity assessment with raw corpus and structured ontology for semantic-oriented service discovery",
    "abstract": null,
    "venue": "Personal and Ubiquitous Computing",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "48be4054a210b877aa4d949557fe028668b31c45",
    "url": "https://www.semanticscholar.org/paper/48be4054a210b877aa4d949557fe028668b31c45",
    "title": "Using Concept Definitions and Ontology Structure to Measure Semantic Similarity in Biomedicine",
    "abstract": "Semantic similarity is useful in different areas of Natural Language Processing (NLP), such as word-sense disambiguation and nameentity recognition, as well as in information retrieval. On the other hand, specialised NLP tools are required in the biomedical context due to the huge amount of information currently available in digital publications that can be explored. This paper presents a method for calculating the semantic similarity between pairs of biomedical concepts defined in an ontology derived from the SNOMED-CT vocabulary. A final semantic similarity is obtained by calculating the similarity between the components of the two concept definitions based on their shared and unshared ancestors in the ontology hierarchy. The results are compared with other methods as well as with human expert ranks as baseline.",
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "22db4baa4e6a378f96f1c54b9e88ecea01b67a19",
    "url": "https://www.semanticscholar.org/paper/22db4baa4e6a378f96f1c54b9e88ecea01b67a19",
    "title": "Cognitive Metaphor and Discourse: Research Methods and Paradigms",
    "abstract": "The article dwells on a modern cognitive and discourse study of metaphors. Taking the advantage of the analysis and fusion of information in foreign and domestic papers, the researcher delves into their classification from the ontological, axiological and epistemological points of view. The ontological level breaks down into two basic approaches, namely metaphorical nature of discourse and discursive nature of metaphors. The former analyses metaphors to fathom characteristics of discourse, while the other provides for the study of metaphorical features in the context of discursive communication. The axiological aspect covers critical and descriptive studies and the epistemological angle comprises quantitive and qualitative methods in metaphorical studies. Other issues covered in the paper incorporate a thorough review of methods for identification of metaphors to include computer-assisted solutions (Word Sense Disambiguation, Categorisation, Metaphor Clusters) and numerical analysis of the metaphorical nature of discourse \u2013 descriptor analysis, metaphor power index, cluster analysis, and complex metaphor power analysis. On the one hand, the conceptualization of research papers boils down to major features of the discursive approach to metaphors and on the other, multiple studies of metaphors in the context of discourse pave the way for a discursive trend in cognitive metaphorology.",
    "venue": "Vestnik Volgogradskogo gosudarstvennogo universiteta. Serija 2. Jazykoznanije",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "ebe3322c92751e65420ff7ea310db3e213f18852",
    "url": "https://www.semanticscholar.org/paper/ebe3322c92751e65420ff7ea310db3e213f18852",
    "title": "Some Strategies to Capture Karaka-Yogyata with Special Reference to apadana",
    "abstract": "In today\u2019s digital world language technology has gained importance. Several software, have been developed and are available in the field of computational linguistics. Such tools play a crucial role in making classical language texts easily accessible. Some Indian philosophical schools have contributed towards various techniques of verbal cognition to analyze sentence correctly. These theories can be used to build computational tools for word sense disambiguation (WSD). In the absence of WSD, one cannot have proper verbal cognition. These theories considered the concept of \u2018Yogyat\u0101\u2019 (congruity or compatibility) as the indispensable cause of verbal cognition. In this work, we come up with some insights on the basis of these theories to create a tool that will capture Yogyat\u0101 of words. We describe the problem of ambiguity in a text and present a method to resolve it computationally with the help of Yogyat\u0101. Here, only two major schools i.e. Ny\u0101ya and Vy\u0101kara\u1e47a are considered. Our paper attempts to show the implication of the creation of our tool in this area. Also, our tool involves the creation of an \u2018ontological tag-set\u2019 as well as strategies to mark up the lexicon. The introductory description of ablation is also covered in this paper. Such strategies and some case studies shall form the core of our paper.",
    "venue": "ArXiv",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "8812d59f065524cf7bb68c5ba1030c93e211d977",
    "url": "https://www.semanticscholar.org/paper/8812d59f065524cf7bb68c5ba1030c93e211d977",
    "title": "A method for ontology-based semantic relatedness measurement",
    "abstract": "There are many methods having different approaches for assessing similarity and relatedness and they are used in many application areas, including web service discovery, invocation and composition, word sense disambiguation, information retrieval, ontology alignment and merging, document clustering, and short answer grading. These methods can be categorized as path-based, information content-based, feature-based, geometric model-based, and hybrid approaches. These approaches use resources such as concept hierarchy, conceptual graph, and corpus for computing similarity and relatedness. With the rise of the semantic web, ontologies have attracted the attention of several researchers. Ontologies represented in the Web Ontology Language (OWL) are also valuable resources for similarity and relatedness measurement. The method proposed in this paper interprets some OWL constructs to assess semantic relatedness. The motivation behind this is to benefit from the rich expressive power of OWL to obtain better semantic relatedness measurement results. The success of the method has been validated against human judgments. The correlation between human judgments and automatically computed semantic relatedness values was calculated as 0.685 and was significant at the 0.01 level.",
    "venue": "",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1893169f7b5e30792d374dbdbfc9973119ee78e2",
    "url": "https://www.semanticscholar.org/paper/1893169f7b5e30792d374dbdbfc9973119ee78e2",
    "title": "Ontology enrichment with causation relations",
    "abstract": "Ontology learning is considered a potential approach that can help to reduce the bottleneck of knowledge acquisition. However it suffers from a lack of standards to define concepts, besides the lack of fully automatic knowledge acquisition methods. In performing this learning process, the discovery of non-taxonomic relationships has been identified as being the most difficult. This study is then an attempt to create an enhanced framework for discovering and classifying ontological relationships by using a machine learning strategy. We take into consideration the context of the input text in performing the classification of the semantic relations, in particular, causation relations. The proposed framework extracts initial semantic patterns for causation relation from the input samples, then filters these patterns using two novel algorithms, namely, the \u201cPurpose Based Word Sense Disambiguation\u201d which helps in determining the causation senses for input pair of words and the \u201cGraph Based Semantics\u201d which determines the existence of the causation relations in the sentence and to extract their cause-effect parts. The results show a good performance and the implemented framework cut off many steps of the usual process to produce the final results.",
    "venue": "IEEE Conference on Systems, Process and Control",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0e68f2cd1294d23f8ecb896a4b47b06d63f3f512",
    "url": "https://www.semanticscholar.org/paper/0e68f2cd1294d23f8ecb896a4b47b06d63f3f512",
    "title": "Ontology based Semantic Indexing Approach for Information Retrieval System",
    "abstract": "This paper shows how the gap between the texts based web pages and the Resource Descriptive Framework based pages of the semantic web can be bridged by ontologies. Most traditional search engines use indexes that are engineered at the syntactical level and come back hits based mostly on straightforward string comparisons or use the static keyword based indexing. However, the indexes don't contain synonyms, cannot differentiate between homonyms (\u201emouse\u201f as a Pointing device vs. \u201eMouse\u201f as a living animal) and users receive completely different search results after they use different conjugation varieties of identical word. During this work, we have a tendency to gift a system that uses ontologies and Natural Language Processing techniques to construct index, and therefore supports word sense disambiguation. Therefore the retrieval of document that contains equivalent term as the context demands is achieved to provide efficient search engines through ontological indexing.",
    "venue": "",
    "citationCount": 18,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9936c943024d36e38ee475173b0d5c063b93c90f",
    "url": "https://www.semanticscholar.org/paper/9936c943024d36e38ee475173b0d5c063b93c90f",
    "title": "A meaning-based algorithm for ontology matching",
    "abstract": "Ontology Matching Algorithms establish a relationship, namely alignment, between two elements of different ontologies. The efficacy of the process has implications in other semantic processes such as retrieving information, storing data, inference mechanisms, ontology versioning, and so on. Ontologies are domain representations in a context. We claim that this context should be taken into account to provide accurate alignments. We present an Ontology Matching algorithm that integrates a Word Sense Disambiguation process of OWL classes and provides semantic alignments using an extension of OWL constructors. The disambiguation process builds a network of words and links, using external thesaurus as WordNet and Roget. We use the network like a representation of our context for the selection of the meaning. We use multiple techniques based on the application of specific rules and a combination of weighted terms, frequencies and type of links using the network of data. In the evaluation phase, we compare our model with two benchmarks: one regarding with the disambiguation of classes of ontologies and the other one using Ontology Alignment Evaluation Initiative datasets. The quality and number of alignments show a slight improvement.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "7c26344529a60550220cec832bf3a198552231a9",
    "url": "https://www.semanticscholar.org/paper/7c26344529a60550220cec832bf3a198552231a9",
    "title": "ArabGlossBERT: Fine-Tuning BERT on Context-Gloss Pairs for WSD",
    "abstract": "Using pre-trained transformer models such as BERT has proven to be effective in many NLP tasks. This paper presents our work to fine-tune BERT models for Arabic Word Sense Disambiguation (WSD). We treated the WSD task as a sentence-pair binary classification task. First, we constructed a dataset of labeled Arabic context-gloss pairs (~167k pairs) we extracted from the Arabic Ontology and the large lexicographic database available at Birzeit University. Each pair was labeled as True or False and target words in each context were identified and annotated. Second, we used this dataset for fine-tuning three pre-trained Arabic BERT models. Third, we experimented the use of different supervised signals used to emphasize target words in context. Our experiments achieved promising results (accuracy of 84%) although we used a large set of senses in the experiment.",
    "venue": "RANLP",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6678498e2503b6a1bac0203042de038f696d49ec",
    "url": "https://www.semanticscholar.org/paper/6678498e2503b6a1bac0203042de038f696d49ec",
    "title": "Identification of keywords and phrases in text Document and sensing a word for document retrieval and ranking: First Review",
    "abstract": "Keywords, phrases, sentences are atomic subatomic molecular levels of a document. A keyword justifies a phrase. A phrase justifies a sentence. Sentences justify paragraph. Single value to group value identif5es a document i.e Relevance of information. Relevance counters the precision of information retrieved by search engines .A semantic concept based framework which identifies the core keywords and phrases incorporating meaning i.e sense of word to phrase ,sense of phrase to sentence ,to document which eliminate the disambiguation of Natural language search in search engines or information retrieval systems is research area. A prototype design of engine is proposed that performs concept mapping of user queries .we propose semantic framework that would be useful to enhance the ability of reasoning in Answering machines (e.g. Google semantically mapped concept search) .GUI interface performs mapping of user query in concept class(ontology i.e. domain ontology, spatial ontology, task ontology) .concept based search is performed at with ranked results are hierarchically presented on precision and relevance with adha ranking and machine intelligence for reducing search time In this paper a Analysis search on concept extraction, sensing word and phrases is been done, with investigation in Document retrieval engines functioning, text mining, ranking algorithm and machine learning methods for acumen information classification. The paper gives a concept overview of 20 latest research papers with best methodologies to incorporate in our proposed system. This is investigation report which would find directions of research in GUI development which incorporates NLP understanding to search engine.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "15bda20402778469e3830bfff5e12e7eb3d518ad",
    "url": "https://www.semanticscholar.org/paper/15bda20402778469e3830bfff5e12e7eb3d518ad",
    "title": "Word Semantic Similarity Measurement Based on Evidence Theory",
    "abstract": "Measuring semantic similarity between words is a classical and hot problem in nature language processing, the achievement of which has great impact on many applications such as word sense disambiguation, machine translation, ontology mapping, computational linguistics, etc. This paper proposes a novel approach to measure words semantic similarity by combining evidence theory with knowledge base. Firstly, we extract evidences based on WordNet; secondly, we analyze the reasonableness of the extracted evidence using scatter plot; thirdly, we generate basic probability assignment by statistics and piecewise linear interpolation technique; fourthly, we obtain global basic probability assignment by integrating evidence conflict resolution, importance distribution, and D-S combination rules; finally, we quantify word semantic similarity. On data set R&G(65), we conducted experiment through 5-fold cross validation, and the correlation of our experimental results with human judgment was 0.912, with 0.4% improvements over existing best practice P&S, 7%\u223c 13% improvements over classical methods (reLHS\u3001distJC\u3001simLC\u3001simL, simR); the experimental results based on M&C(30) and WordSim353 were also good with correlations being 0.915 and 0.941. The operational efficiency of our method is as good as classical methods\u2032, showing that using evidence theory to measure word semantic similarity is reasonable and effective.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": null
  },
  {
    "paperId": "e251156752c571a3f13a7dd1d862d962dd99ac7c",
    "url": "https://www.semanticscholar.org/paper/e251156752c571a3f13a7dd1d862d962dd99ac7c",
    "title": "UMCC-DLSI: Integrative Resource for Disambiguation Task",
    "abstract": "This paper describes the UMCC-DLSI system in SemEval-2010 task number 17 (All-words Word Sense Disambiguation on Specific Domain). The main purpose of this work is to evaluate and compare our computational resource of WordNet's mappings using 3 different methods: Relevant Semantic Tree, Relevant Semantic Tree 2 and an Adaptation of k-clique's Technique. Our proposal is a non-supervised and knowledge-based system that uses Domains Ontology and SUMO.",
    "venue": "SemEval@ACL",
    "citationCount": 22,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c5401d0f91c8a61d36d8cce8fe7839ac786ecbd0",
    "url": "https://www.semanticscholar.org/paper/c5401d0f91c8a61d36d8cce8fe7839ac786ecbd0",
    "title": "WSD Tool for Ontology-based Text Document Classification",
    "abstract": "The classification of document is required to extract relevant information from the huge set of documents. There are various traditional approaches which are being used satisfactorily, but even such approaches or techniques are not enough. These traditional approaches require training sets of pre-classified documents in order to train the classifier. These approaches mainly depend only on \u201ebag of words\u201f, this representation used is unsatisfactory as it ignores possible relations between terms. When training set is not available, ontology provides us with knowledge that can be efficiently used for classification without using training sets. Ontology expresses information in the document form of hierarchical structure. For classifying the documents using ontology we need to define the class or the concepts to categorize the document. Here we use WordNet to capture the relations between the words. Also it is seen that WordNet alone is not sufficient to remove Word Sense Disambiguation (WSD). So in our approach we use Lesk algorithm to deal with the WSD. In this paper, we implement the tool which disambiguates a keyword in the text file. This tool is actually a utility where the input will be a text file and the utility will process the input file to give the best sense for the most occurring keyword in the file. There are various modules for achieving this. This keyword is further used for mapping with concepts to create ontology. The ontology will have classes/concepts defined for all the files in the corpus. Our approach is leveraging the strengths of ontology, WordNet and Lesk Algorithm for improving text document classification. General Terms Ontology-based Text document classification, concepts, and keywords.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "80994ab5429f39faa3eed9e3033894377d55bb38",
    "url": "https://www.semanticscholar.org/paper/80994ab5429f39faa3eed9e3033894377d55bb38",
    "title": "Web Context Analysis Based on Generic Ontology",
    "abstract": null,
    "venue": "ICICA",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6cfdc29e1107d7c3e979f97ef66e9da895ce8f5e",
    "url": "https://www.semanticscholar.org/paper/6cfdc29e1107d7c3e979f97ef66e9da895ce8f5e",
    "title": "Conceptual Clustering of Documents for Automatic Ontology Generation",
    "abstract": null,
    "venue": "BICS",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c129ee744165be49aaf2a6914a623870c474b7bb",
    "url": "https://www.semanticscholar.org/paper/c129ee744165be49aaf2a6914a623870c474b7bb",
    "title": "An Overview on XML Semantic Disambiguation from Unstructured Text to Semi-Structured Data : Background , Applications , and Ongoing Challenges",
    "abstract": "Since the last two decades, XML has gained momentum as the standard for Web information management and complex data representation. Also, collaboratively built semi-structured information resources, such as Wikipedia, have become prevalent on the Web and can be inherently encoded in XML. Yet most methods for processing XML and semi-structured information handle mainly the syntactic properties of the data, while ignoring the semantics involved. To devise more intelligent applications, one needs to augment syntactic features with machine-readable semantic meaning. This can be achieved through the computational identification of the meaning of data in context, also known as (a.k.a.) automated semantic analysis and disambiguation, which is nowadays one of the main challenges at the core of the Semantic Web. This survey paper provides a concise and comprehensive review of the methods related to XML-based semi-structured semantic analysis and disambiguation. It is made of four logical parts. First, we briefly cover traditional word sense disambiguation methods for processing flat textual data. Second, we describe and categorize disambiguation techniques developed and extended to handle semi-structured and XML data. Third, we describe current and potential application scenarios that can benefit from XML semantic analysis, including: data clustering and semantic-aware indexing, data integration and selective dissemination, semantic-aware and temporal querying, Web and Mobile Services matching and composition, blog and social semantic network analysis, and ontology learning. Fourth, we describe and discuss ongoing challenges and future directions, including: the quantification of semantic ambiguity, expanding XML disambiguation context, combining structure and content, using collaborative/social information sources, integrating explicit and implicit semantic analysis, emphasizing user involvement, and reducing computational complexity.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "289a92d2770865d78a9ebfc4ddfaec92ce9b6ea7",
    "url": "https://www.semanticscholar.org/paper/289a92d2770865d78a9ebfc4ddfaec92ce9b6ea7",
    "title": "Automatically generating taxonomy for grouping app reviews \u2014 a study of three apps",
    "abstract": null,
    "venue": "Software quality journal",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "f5a81e8c5e56549d36721352a994ae0e7fd3c1dc",
    "url": "https://www.semanticscholar.org/paper/f5a81e8c5e56549d36721352a994ae0e7fd3c1dc",
    "title": "An Approach for Finding Semantic Relatedness Score Between Two Sentences Based on their Senses",
    "abstract": "Corresponding Author: Nazreena Rahman Department of Computer Science and Engineering, Kaziranga University, India Email: nazreena.rehman@gmail.com Abstract: Finding semantic relatedness score between two sentences is useful in many research areas. Existing relatedness methods do not consider its sense while computing semantic relatedness score between two sentences. In this study, a Word Sense Disambiguation (WSD) method is proposed which is used in finding the sense-oriented sentence semantic relatedness measure. The WSD method is used to find the correct sense of a word present in a sentence. The proposed method uses both the WordNet lexical dictionary and the Wikipedia corpus. The sense-oriented sentence semantic relatedness measure combines edge-based score between words depending the context of the sentence; sense based score which finds sentences having similar senses; as well as word order score. We have evaluated the proposed WSD method on publicly available English WSD corpora. We have compared our proposed sense-oriented sentence semantic relatedness measure on standard datasets. Experimental analysis illustrates the significance of proposed method over many baseline and current systems like Lesk, UKB, IMS, Babelfy.",
    "venue": "Journal of Computer Science",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c212eab7a430dd931ac17e7cd9779838296dd578",
    "url": "https://www.semanticscholar.org/paper/c212eab7a430dd931ac17e7cd9779838296dd578",
    "title": "\"I Don\u2019t Believe in Word Senses\"",
    "abstract": null,
    "venue": "Computers and the Humanities",
    "citationCount": 473,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f26613fa000a2e6879e15de57534c59d00604fa4",
    "url": "https://www.semanticscholar.org/paper/f26613fa000a2e6879e15de57534c59d00604fa4",
    "title": "Subjective Bayes Method for Word Semantic Similarity Measurement",
    "abstract": "Measuring semantic similarity between words is a classical problem in nature language processing, the result of which can promote many applications such as machine translation, word sense disambiguation, ontology mapping, computational linguistics, etc. This paper combines knowledge-based methods with statistical methods in measuring words similarity, the novel aspect of which is that subjective Bayes method is employed. Firstly, extract evidences based on Word Net, secondly, analyze reasonableness of candidate evidence using scatter plot, thirdly, generate sufficiency measure by statistics and piecewise linear interpolation technique, fourthly, obtain comprehensive posteriori by integrating uncertainty reasoning with conclusion uncertainty synthetic strategy, finally, we quantify word semantic similarity. On data set R&G (65), we conducted experiment through 5-fold cross validation, and the correlation of our experimental results with human judgment is 0.912, with 0.4% improvements over existing best practice, which show that using subjective Bayes method to measure word semantic similarity is reasonable and effective.",
    "venue": "2013 IEEE 13th International Conference on Data Mining Workshops",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1b964b5e0e33e474de5fadf68576c74d4f5d3cee",
    "url": "https://www.semanticscholar.org/paper/1b964b5e0e33e474de5fadf68576c74d4f5d3cee",
    "title": "Enriching Ontology Concepts Based on Texts from WWW and Corpus",
    "abstract": "In spite of the growing of ontological engineering tools, ontology knowledge acquisition remains a highly manual, time-consuming and complex task. Automatic ontology learning is a well-established research field whose goal is to support the semi-automatic construction of ontologies starting from available digital resources (e.g., A corpus, web pages, dictionaries, semi-structured and structured sources) in order to reduce the time and effort in the ontology development process. This paper proposes an enhanced methodology for enriching Lexical Ontologies such as the popular open-domain vocabulary -WordNet. Ontologies like WordNet can be semantically enriched to obtain extensions and enhancements to its lexical database. The proliferation of senses in WordNet is considered as one of its main shortcomings for practical applications. Therefore, the presented methodology depends on the Coarse-Grained word senses. These senses are generated from applying WordNet Fine-Grained word senses to a Merging Sense algorithm. This algorithm merges only semantically similar word senses instead of applying traditional clustering techniques. A performance comparison is illustrated between two different data sources (Web, Corpus) used in the Enrichment process. The results obtained from using Coarse-Grained word senses in both cases yields better precision than Fine-Grained word senses in the Word Sense Disambiguation task.",
    "venue": "J. Univers. Comput. Sci.",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a4ff275ec7d5589dc70cecc59e1a7aa39cb7a178",
    "url": "https://www.semanticscholar.org/paper/a4ff275ec7d5589dc70cecc59e1a7aa39cb7a178",
    "title": "International Journal of Recent Technology and Engineering (IJRTE)",
    "abstract": "Word Sense Disambiguation (WSD) is a complex problem as it entirely depends on the language convolutions. Gujarati language is a multifaceted language which has so many variations. In this paper, the debate has advanced two methodologies for WSD: knowledge-based and deep learning approach. Accordingly, the Deep learning approach is found to perform even better one of its shortcoming is the essential of colossal data sources without which getting ready is near incomprehensible. On the other hand, uses data sources to pick the implications of words in a particular setting. Provided with that, deep learning approaches appear to be more suitable to manage word sense disambiguation; however, the process will always be challenging given the ambiguity of natural languages.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "cb8ce87ac203212a8eba8fd433604e391559be46",
    "url": "https://www.semanticscholar.org/paper/cb8ce87ac203212a8eba8fd433604e391559be46",
    "title": "An Ontological Framework for Information Extraction From Diverse Scientific Sources",
    "abstract": "Automatic information extraction from online published scientific documents is useful in various applications such as tagging, web indexing and search engine optimization. As a result, automatic information extraction has become among the hottest areas of research in text mining. Although various information extraction techniques have been proposed in the literature, their efficiency demands domain specific documents with static and well-defined format. Furthermore, their accuracy is challenged with a slight modification in the format. To overcome these issues, a novel ontological framework for information extraction (OFIE) using fuzzy rule-base (FRB) and word sense disambiguation (WSD) is proposed. The proposed approach is validated with a significantly wider document domains sourced from well-known publishing services such as IEEE, ACM, Elsevier, and Springer. We have also compared the proposed information extraction approach against state-of-the-art techniques. The results of the experiment show that the proposed approach is less sensitive to changes in the document format and has a significantly better average accuracy of 89.14% and F-score as 89%.",
    "venue": "IEEE Access",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "5fdb1de0cda6f7c2d9494e4e606fe05e9aa9c7b0",
    "url": "https://www.semanticscholar.org/paper/5fdb1de0cda6f7c2d9494e4e606fe05e9aa9c7b0",
    "title": "Semantic Annotation for the Web of Data: An Ontology and RDF based Automated Approach",
    "abstract": "The construction of metadata which annotates the documents is one of the major tasks for making data understandable to the machine on the Semantic Web. Most of the exiting Semantic Web authoring tools allow user with limited knowledge Semantic Web technologies such as RDF, OWL to markup documents with semantics. These tools help to create the semantic annotations to the documents either during or after composing the documents. The common is these approaches all need human annotator to manually determine the semantic information of the content and then create the metadata which describes this information via tools. This research proposes an approach to bridge the gap between today\u2019s Web and the Semantic Web. The word sense disambiguation technique is utilized to automatically discover the underlying semantic information of the text document, based on the popular open-domain vocabulary ontology the Wordnet. This semantic information is then annotated to the documents in RDF that conforms to the Semantic Web standards for future reuse. We believe this is one of the feasible ways to bring a large-scale of documents to the Semantic Web.",
    "venue": "",
    "citationCount": 10,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "88e98c6fcafffa3c5599190603f1eb4ebce6baa1",
    "url": "https://www.semanticscholar.org/paper/88e98c6fcafffa3c5599190603f1eb4ebce6baa1",
    "title": "Integrating Lexical Units, Synsets and Ontology in the Cornetto Database",
    "abstract": "Cornetto is a two-year Stevin project (project number STE05039) in which a lexical semantic database is built that combines Wordnet with Framenet-like information for Dutch. The combination of the two lexical resources (the Dutch Wordnet and the Referentie Bestand Nederlands) will result in a much richer relational database that may improve natural language processing (NLP) technologies, such as word sense-disambiguation, and language-generation systems. In addition to merging the Dutch lexicons, the database is also mapped to a formal ontology to provide a more solid semantic backbone. Since the database represents different traditions and perspectives of semantic organization, a key issue in the project is the alignment of concepts across the resources. This paper discusses our methodology to first automatically align the word meanings and secondly to manually revise the most critical cases.",
    "venue": "International Conference on Language Resources and Evaluation",
    "citationCount": 63,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "3189bb15bb482d188c6a1d9a2b67b4de2f2c717e",
    "url": "https://www.semanticscholar.org/paper/3189bb15bb482d188c6a1d9a2b67b4de2f2c717e",
    "title": "Ontology-based semantic relatedness for detecting the relevance of learning resources",
    "abstract": "Semantic relatedness measures have proven useful for a number of applications, such as querying personalized web resources, word sense disambiguation, or real-word spelling error correction. Most semantic relatedness measures between concepts are based on the concept hierarchy of a domain ontology. In this article, we propose a semantic relevance (SR) measure that expresses the semantic relatedness between a learning resource and the learning context of a learner. In our case, both the learning resource and the learning context are described by graphs using the learning concepts of the domain model. Our SR measure aims to detect the relevance of a learning resource for a particular learning context of a learner. In our work, the SR measure is based on the assignment of relative weights to the learning concepts describing the learning resource according to their relationships with the current concept of interest to the learner. The proposed measure achieves better results than the relatedness measures from the literature and yet is much simpler than most of them. It is shown to achieve a correlation of from 0.627 to 0.945 with expert ratings. This measure is implemented and used in a learning organizer, a system which generates adaptive hypermedia courses and reuses learning resources from distant web repositories, called Organisateur de Parcours Adaptatifs de Formation (OrPAF). In OrPAF, learning resources are annotated in order to be queried for a particular learning context, which is represented by a map of annotated learning concepts, called an adaptive conceptual map. The proposed SR measure is used in order to automatically detect the learning resource relevance.",
    "venue": "Interactive Learning Environments",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e4844a44440dd39af1633fea2a70f94570da20f9",
    "url": "https://www.semanticscholar.org/paper/e4844a44440dd39af1633fea2a70f94570da20f9",
    "title": "Improving the state-of-the-art in Thai semantic similarity using distributional semantics and ontological information.",
    "abstract": "Research into semantic similarity has a long history in lexical semantics, and it has applications in many natural language processing (NLP) tasks like word sense disambiguation or machine translation. The task of calculating semantic similarity is usually presented in the form of datasets which contain word pairs and a human-assigned similarity score. Algorithms are then evaluated by their ability to approximate the gold standard similarity scores. Many such datasets, with different characteristics, have been created for English language. Recently, four of those were transformed to Thai language versions, namely WordSim-353, SimLex-999, SemEval-2017-500, and R&G-65. Given those four datasets, in this work we aim to improve the previous baseline evaluations for Thai semantic similarity and solve challenges of unsegmented Asian languages (particularly the high fraction of out-of-vocabulary (OOV) dataset terms). To this end we apply and integrate different strategies to compute similarity, including traditional word-level embeddings, subword-unit embeddings, and ontological or hybrid sources like WordNet and ConceptNet. With our best model, which combines self-trained fastText subword embeddings with ConceptNet Numberbatch, we managed to raise the state-of-the-art, measured with the harmonic mean of Pearson on Spearman \u03c1, by a large margin from 0.356 to 0.688 for TH-WordSim-353, from 0.286 to 0.769 for TH-SemEval-500, from 0.397 to 0.717 for TH-SimLex-999, and from 0.505 to 0.901 for TWS-65.",
    "venue": "PLoS ONE",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Medicine"
    ]
  },
  {
    "paperId": "e12ccb35cfcfdd826f6df32fea9d33c0805b1855",
    "url": "https://www.semanticscholar.org/paper/e12ccb35cfcfdd826f6df32fea9d33c0805b1855",
    "title": "Solving Semantic Ambiguity to Improve Semantic Web based Ontology Matching",
    "abstract": "A new paradigm in Semantic Web research focuses on the development of a new generation of knowledge-based problem solvers, which can exploit the massive amounts of formally specified information available on the Web, to produce novel intelligent functionalities. An important example of this paradigm can be found in the area of Ontology Matching, where new algorithms, which derive mappings from an exploration of multiple and heterogeneous online ontologies, have been proposed. While these algorithms exhibit very good performance, they rely on merely syntactical techniques to anchor the terms to be matched to those found on the Semantic Web. As a result, their precision can be affected by ambiguous words. In this paper, we aim to solve these problems by introducing techniques from Word Sense Disambiguation, which validate the mappings by exploring the semantics of the ontological terms involved in the matching process. Specifically we discuss how two techniques, which exploit the ontological context of the matched and anchor terms, and the information provided by WordNet, can be used to filter out mappings resulting from the incorrect anchoring of ambiguous terms. Our experiments show that each of the proposed disambiguation techniques, and even more their combination, can lead to an important increase in precision, without having too negative an impact on recall.",
    "venue": "Organizational Memories",
    "citationCount": 80,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0bee1e221f6355e31d40e6c88afa6976ee8b1c46",
    "url": "https://www.semanticscholar.org/paper/0bee1e221f6355e31d40e6c88afa6976ee8b1c46",
    "title": "Ontology and the Lexicon: Merging global and specialized linguistic ontologies",
    "abstract": "There is an increasing interest in linguistic ontologies (e.g. WordNet) for a variety of content-based tasks, including conceptual indexing, word sense disambiguation and cross-language information retrieval. A relevant contribution in this direction is represented by linguistic ontologies with domain specific coverage, which are a crucial topic for the development of concrete application systems. This paper tries to go a step further in the direction of the interoperability of specialized linguistic ontologies, by addressing the problem of their integration with global ontologies. This scenario poses some simplifications with respect to the general problem of merging ontologies, since it enables to define a strong precedence criterion so that terminological information overshadows generic information whenever conflicts arise. We assume the EuroWordNet model and propose a methodology to \u201cplug\u201d specialized linguistic ontologies into global ontologies. Experimental data related to an implemented algorithm, which has been tested on a global and a specialized linguistic ontology for the Italian language, are provided.",
    "venue": "",
    "citationCount": 18,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f1d4b707cccbaf2fbba7ef960e0dc32e5edaad16",
    "url": "https://www.semanticscholar.org/paper/f1d4b707cccbaf2fbba7ef960e0dc32e5edaad16",
    "title": "An analysis of ontology-based query expansion strategies",
    "abstract": "Sense based query expansion never proved its effectiveness except for the so-called \" open domain question answering \" task. The present work is still inconclusive at this regard, due to some experimental limitations, but we provide interesting evidence suggesting new guidelines for future research. Word sense disambiguation is in fact only one of the problems involved with sense based query expansion. The second is how to use sense information (and ontologies in general) to expand the query. We show that expanding with synonyms or hyperonyms has a limited effect on web information retrieval performance, while other types of semantic information derivable from an ontology are much more effective at improving search results.",
    "venue": "",
    "citationCount": 243,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9b1e8c86c8ecace7d0dffb77d49dc41c1ba7425a",
    "url": "https://www.semanticscholar.org/paper/9b1e8c86c8ecace7d0dffb77d49dc41c1ba7425a",
    "title": "A Large DataBase of Hypernymy Relations Extracted from the Web.",
    "abstract": "Hypernymy relations (those where an hyponym term shares a \u201cisa\u201d relationship with his hypernym) play a key role for many Natural Language Processing (NLP) tasks, e.g. ontology learning, automatically building or extending knowledge bases, or word sense disambiguation and induction. In fact, such relations may provide the basis for the construction of more complex structures such as taxonomies, or be used as effective background knowledge for many word understanding applications. We present a publicly available database containing more than 400 million hypernymy relations we extracted from the CommonCrawl web corpus. We describe the infrastructure we developed to iterate over the web corpus for extracting the hypernymy relations and store them effectively into a large database. This collection of relations represents a rich source of knowledge and may be useful for many researchers. We offer the tuple dataset for public download and an Application Programming Interface (API) to help other researchers programmatically query the database.",
    "venue": "International Conference on Language Resources and Evaluation",
    "citationCount": 100,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "39efcdf5ad5e0fe85e64094ca9f90b4a1eb0d9e0",
    "url": "https://www.semanticscholar.org/paper/39efcdf5ad5e0fe85e64094ca9f90b4a1eb0d9e0",
    "title": "\u57fa\u65bc\u300a\u77e5\u7db2\u300b\u7684\u8fad\u5f59\u8a9e\u7fa9\u76f8\u4f3c\u5ea6\u8a08\u7b97 (Word Similarity Computing Based on How-net) [In Chinese]",
    "abstract": "Word similarity is broadly used in many applications, such as information retrieval, information extraction, text classification, word sense disambiguation, example -based machine translation, etc. There are two different methods used to compute similarity: one is based on ontology or a semantic taxonomy; the other is based on collocations of words in a corpus. As a lexical knowledgebase with rich semantic information, How-net has been employed in various researches. Unlike other thesauri, such as WordNet and Tongyici Cilin, in which word similarity is defined based on the distance between words in a semantic taxonomy tree, How-net defines a word in a complicated multi-dimensional knowledge description language. As a result, a series of problems arise in the process of word similarity computation using How-net. The difficulties are outlined below: 1. The description of each word consists of a group of sememes. For example, the Chinese word \u201c\u6697\u7bb1(camera obscura)\u201d is described as: \u201cpart|\u90e8\u4ef6, #TakePicture|\u62cd\u651d, %tool|\u7528\u5177 , body|\u8eab\u201d, and the Chinese word \u201c\u5beb\u4fe1 (write a letter)\u201d is described as: \u201cwrite|\u5beb, ContentProduct=letter|\u4fe1\u4ef6\u201d; 2. The meaning of a word is not a simple combination of these sememes. Sememes are organized using a specific knowledge description language. To meet these challenges, our work includes: 1. A study on the How-net knowledge description language. We rewrite the How-net definition of a word in a more structural format, using the abstract data structure of set and feature structure. 2. A study on the algorithm used to compute word similarity based on How-net. The similarity between sememes, that between sets , and that between feature structures are given. To compute the similarity between two sememes, we",
    "venue": "ROCLING/IJCLCLP",
    "citationCount": 249,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "bb62f44dcfc6f4a8d6e844b117f661b094a6a75d",
    "url": "https://www.semanticscholar.org/paper/bb62f44dcfc6f4a8d6e844b117f661b094a6a75d",
    "title": "Low-cost ontology development",
    "abstract": "In this paper, we present the project building new lexical\nresource -- shallow ontology derived from the corpora. The\nontology should be used primarily for machine translation,\nsyntactic parsing and word sense disambiguation. Currently, the\nontology for Czech language is developed, but the methodology\nand tools are suitable for other languages with similar\nstructure. Ontology is based on BushBank corpus, which improves\nhandling of ambiguity in natural language. BushBank data and\ntools are application-driven, thus reducing the time and costs\nneeded to annotate the corpora and develop new lexical\nresources.",
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "455bfbb47af2fdd8db8f7db9db2d4e4d8db8a44f",
    "url": "https://www.semanticscholar.org/paper/455bfbb47af2fdd8db8f7db9db2d4e4d8db8a44f",
    "title": "Ontology-enriched semantic space for video search",
    "abstract": "Multimedia-based ontology construction and reasoning have recently been recognized as two important issues in video search, particularly for bridging semantic gap. The lack of coincidence between low-level features and user expectation makes concept-based ontology reasoning an attractive mid-level framework for interpreting high-level semantics. In this paper, we propose a novel model, namely ontology-enriched semantic space (OSS), to provide a computable platform for modeling and reasoning concepts in a linear space. OSS enlightens the possibility of answering conceptual questions such as a high coverage of semantic space with minimal set of concepts, and the set of concepts to be developed for video search. More importantly, the query-to-concept mapping can be more reasonably conducted by guaranteeing the uniform and consistent comparison of concept scores for video search. We explore OSS for several tasks including concept-based video search, word sense disambiguation and multi-modality fusion. Our empirical findings show that OSS is a feasible solution to timely issues such as the measurement of concept combination and query-concept dependent fusion.",
    "venue": "ACM Multimedia",
    "citationCount": 44,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6048a723f49652801c76fdf80d74da81f0a40a27",
    "url": "https://www.semanticscholar.org/paper/6048a723f49652801c76fdf80d74da81f0a40a27",
    "title": "An automated semantic annotation based-on Wordnet ontology",
    "abstract": "The construction of metadata which annotates the documents is one of the major tasks for making data understandable to the machine on the Semantic Web. Most of the exiting Semantic Web authoring tools allow user with limited knowledge Semantic Web technologies such as RDF, OWL to markup documents with semantics. These tools help to create the semantic annotations to the documents either during or after composing the documents. The common is these approaches all need human annotator to manually determine the semantic information of the content and then create the metadata which describes this information via tools. This research proposes an approach to bridge the gap between today's Web and the Semantic Web. The word sense disambiguation technique is utilized to automatically discover the underlying semantic information of the text document, based on the popular open-domain vocabulary ontology \u2014 the Wordnet. This semantic information is then annotated to the documents in RDF that conforms to the Semantic Web standards for future reuse. We believe this is one of the feasible ways to bring a large-scale of documents to the Semantic Web.",
    "venue": "International Conference on Networked Computing and Advanced Information Management",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "b001a3f33ff6f95b38c7ef7c3f82267756584889",
    "url": "https://www.semanticscholar.org/paper/b001a3f33ff6f95b38c7ef7c3f82267756584889",
    "title": "Semantic Approach to Image Retrieval Using Statistical Models Based on a Lexical Ontology",
    "abstract": null,
    "venue": "KES",
    "citationCount": 9,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "bc40ca2bc6fe067890aedf735c291fa74f8b5c54",
    "url": "https://www.semanticscholar.org/paper/bc40ca2bc6fe067890aedf735c291fa74f8b5c54",
    "title": "Towards Building a Lexical Ontology Resource Based on Intrinsic Senses of Words",
    "abstract": "Relation between reality and language is an age old problem. The term \u2018meaning\u2019 embodies hordes of issues which are found factored into the problem. One contemporary aspect of the problem is how to computationally specify \u2018meaning\u2019 of a word. To conceive a web resource for the \u2018meaning\u2019 of a word involves ontological as well as linguistic considerations. Usually \u2018meaning\u2019 of a word is specified using other words like in dictionaries, thesauruses etc. Ontologically \u2018meaning\u2019 of a word is thought to be specified in terms of it participating in classes, relations and events. Both these accounts seem extensional as they refer to objects outside a word to determine its \u2018meaning\u2019. For both of these accounts words \u2018mean\u2019 only in the company of other words or in association with the outlying reality which words represent. We take a view that \u2018meaning\u2019 of a single word has a necessary aspect of intension. This intension is in situ formations embodied in the \u2018meaning\u2019 of a word. The in situ forms are present intrinsically in the \u2018meaning\u2019 of words and are the causes for the association of words as well as assertion of knowledge. The work presents a new approach to formal ontology of natural language based on in situ formations in the \u2018meanings\u2019 of words and pairings of words. The basic motivation is to computationally manipulate language at the level of intrinsic lexical meanings. Lexical meanings, in the transaction of language, have discrete intrinsic forms of types and classes. Intrinsic sense-types and sense-classes have been identified for 3867 verbs,1980 adverbs and 300 adjectives. Identification of sense-types and sense-classes for adjectives and nouns is in progress. These types and classes are unambiguously locatable in parts of speech through collective introspective inquiry first and then enriched with the help of computational methods of corpus study. This work reports on the construction of a web resource, in which \u2018meanings\u2019 of English words are given in terms of formal ontology of language inspired by Leibniz, Patanjali and Bhartrihari. The experimental results on testing data of SemEval 2010 (word sense induction and disambiguation task), the measure of synonimity of verbs and nouns and the inferences drawn from the sense-class and sense-type distributions show the potentiality of the resource. Integration of resource of sense-types and sense-classes with lexical grammar of cases, interpreted as verb-noun pairs, leads to paraphrasing of English sentences into graphs of formal senses. This gives confidence that the resource can be enriched through these studies and resource exploration framework can be made such that as a resultant resource itself is continuously enriched. The proposed ontology of verbs, adverbs, adjectives and nouns, if can be implemented for large numbers of words, would make the resource adventitious in knowledge computing. We aim at extensive lexical coverage of language to show that our approach",
    "venue": "",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "591019feee9f7306b9ddc3da27be01b0ed48f68f",
    "url": "https://www.semanticscholar.org/paper/591019feee9f7306b9ddc3da27be01b0ed48f68f",
    "title": "Towards Automated Taxonomy Generation for Grouping App Reviews: A Preliminary Empirical Study",
    "abstract": null,
    "venue": "Quality of Information and Communications Technology",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d7be0d0d30c8d2bdf08e755ab89dd9bc7c3d7b41",
    "url": "https://www.semanticscholar.org/paper/d7be0d0d30c8d2bdf08e755ab89dd9bc7c3d7b41",
    "title": "A semantic framework for extracting taxonomic relations from text corpus",
    "abstract": "Nowadays, ontologies have been exploited in many current applications due to the abilities in representing knowledge and inferring new knowledge. However, the manual construction of ontologies is tedious and time-consuming. Therefore, the automated ontology construction from text has been investigated. The extraction of taxonomic relations between concepts is a crucial step in constructing domain ontologies. To obtain taxonomic relations from a text corpus, especially when the data is deficient, the approach of using the web as a source of collective knowledge (a.k.a web-based approach) is usually applied. The important challenge of this approach is how to collect relevant knowledge from a large amount of web pages. To overcome this issue, we propose a framework that combines Word Sense Disambiguation (WSD) and web approach to extract taxonomic relations from a domain-text corpus. This framework consists of two main stages: concept extraction and taxonomic-relation extraction. Concepts acquired from the concept-extraction stage are disambiguated through WSD module and passed to stage of extraction taxonomic relations afterward. To evaluate the efficiency of the proposed framework, we conduct experiments on datasets about two domains of tourism and sport. The obtained results show that the proposed method is efficient in corpora which are insufficient or have no training data. Besides, the proposed method outperforms the state of the art method in corpora having high WSD results.",
    "venue": "\u02dcThe \u0153international Arab journal of information technology",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a608c60ccd44692a9a17677dbe8b751f27b0f3e2",
    "url": "https://www.semanticscholar.org/paper/a608c60ccd44692a9a17677dbe8b751f27b0f3e2",
    "title": "SCALABLE INFORMATION RETRIEVAL SYSTEM IN SEMANTIC WEB BY QUERY EXPANSION AND ONTOLOGICAL BASED LSA RANKING SIMILARITY MEASUREMENT",
    "abstract": "In recent days, semantic web presents a key role in intelligent retrieval of information system that resolves vocabulary mismatch problem by query expansion process. However, achieving the scalable information retrieval (IR) in semantic web is a challenging issue in a large dataset. The semantic IR problem is addressed by an ontological-based semantic similarity measurement using natural language processing. The two novel algorithms namely syntactic correlation coefficient (SCC) and mapping-based K-nearest neighbour (M-KNN) for semantic similarity measurement is proposed which improves the accuracy of relevant result. The ontological constructs with word sense disambiguation (WSD) algorithm for document repository improves the conceptual relationships, reduces the ambiguities in ontology and improves scalability by intensely analysing the semantic relationship as well as dynamically reconstructing the ontology when numbers of documents are updated. Ranking is done with latent semantic analysis (LSA) after semantic similarity analysis, which improves the retrieved result and reduces the complexity in relevancy. The performance of the system is analysed with respect to different metrics such as processing time, F-measure (0.97), time complexity, precision (0.95), recall (0.98) and space complexity.",
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9137e08a7e5e53b0d8e34b3b54d661421a234ca4",
    "url": "https://www.semanticscholar.org/paper/9137e08a7e5e53b0d8e34b3b54d661421a234ca4",
    "title": "Improving the Relevance of Search Results: Search-term Disambiguation and Ontological Filtering",
    "abstract": "Synonymy & polysemy of natural languages together with information overload are two main factors that affect the relevance of Web hits. When users submit a query, search engines usually return a long list of hits with syntactic similarity. Users are confronted with choosing a needle from a haystack ? relevant items from long lists of hits. This book proposes an improved strategy for increasing the relevance of Web search results via search term disambiguation and ontological filtering. Results are classified into an ontology, such as Open Directory Project. Semantic characteristics of ontology categories are represented by a category-document and similarities of this and search results are evaluated using a Vector Space Model. Users choose a category to obtain only the search results classified under the selected category. Experimental data show the approach boosts the Web hits precision by more than 20%. The book should help shed some light on Web searching and word sense disambiguation, and should be useful to students and researchers in the fields of information retrieval, text classification, and data mining; or anyone else interested in Web searching.",
    "venue": "",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "669e43e6f142a8be393b1003b219a3ff26af109f",
    "url": "https://www.semanticscholar.org/paper/669e43e6f142a8be393b1003b219a3ff26af109f",
    "title": "DBpedia: A Multilingual Cross-domain Knowledge Base",
    "abstract": "The DBpedia project extracts structured information from Wikipedia editions in 97 different languages and combines this information into a large multi-lingual knowledge base covering many specific domains and general world knowledge. The knowledge base contains textual descriptions (titles and abstracts) of concepts in up to 97 languages. It also contains structured knowledge that has been extracted from the infobox systems of Wikipedias in 15 different languages and is mapped onto a single consistent ontology by a community effort. The knowledge base can be queried using the SPARQL query language and all its data sets are freely available for download. In this paper, we describe the general DBpedia knowledge base and as well as the DBpedia data sets that specifically aim at supporting computational linguistics tasks. These task include Entity Linking, Word Sense Disambiguation, Question Answering, Slot Filling and Relationship Extraction. These use cases are outlined, pointing at added value that the structured data of DBpedia provides.",
    "venue": "International Conference on Language Resources and Evaluation",
    "citationCount": 167,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4a88f6bb7d64eb22ad321226e6c430f806b969d0",
    "url": "https://www.semanticscholar.org/paper/4a88f6bb7d64eb22ad321226e6c430f806b969d0",
    "title": "A Fault Model for Ontology Mapping, Alignment, and Linking Systems",
    "abstract": "There has been much work devoted to the mapping, alignment, and linking of ontologies (MALO), but little has been published about how to evaluate systems that do this. A fault model for conducting fine-grained evaluations of MALO systems is proposed, and its application to the system described in Johnson et al. [15] is illustrated. Two judges categorized errors according to the model, and inter-judge agreement was calculated by error category. Overall inter-judge agreement was 98% after dispute resolution, suggesting that the model is consistently applicable. The results of applying the model to the system described in [15] reveal the reason for a puzzling set of results in that paper, and also suggest a number of avenues and techniques for improving the state of the art in MALO, including the development of biomedical domain specific language processing tools, filtering of high frequency matching results, and word sense disambiguation.",
    "venue": "Pacific Symposium on Biocomputing",
    "citationCount": 24,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ]
  },
  {
    "paperId": "f97364e3f1756f0cd13682169c5dc8607e767679",
    "url": "https://www.semanticscholar.org/paper/f97364e3f1756f0cd13682169c5dc8607e767679",
    "title": "Text Semantic Annotation: A Distributed Methodology Based on Community Coherence",
    "abstract": "Text annotation is the process of identifying the sense of a textual segment within a given context to a corresponding entity on a concept ontology. As the bag of words paradigm\u2019s limitations become increasingly discernible in modern applications, several information retrieval and artificial intelligence tasks are shifting to semantic representations for addressing the inherent natural language polysemy and homonymy challenges. With extensive application in a broad range of scientific fields, such as digital marketing, bioinformatics, chemical engineering, neuroscience, and social sciences, community detection has attracted great scientific interest. Focusing on linguistics, by aiming to identify groups of densely interconnected subgroups of semantic ontologies, community detection application has proven beneficial in terms of disambiguation improvement and ontology enhancement. In this paper we introduce a novel distributed supervised knowledge-based methodology employing community detection algorithms for text annotation with Wikipedia Entities, establishing the unprecedented concept of community Coherence as a metric for local contextual coherence compatibility. Our experimental evaluation revealed that deeper inference of relatedness and local entity community coherence in the Wikipedia graph bears substantial improvements overall via a focus on accuracy amelioration of less common annotations. The proposed methodology is propitious for wider adoption, attaining robust disambiguation performance.",
    "venue": "Algorithms",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0cbb5f29b4d45f8849b1851b406f43449794917a",
    "url": "https://www.semanticscholar.org/paper/0cbb5f29b4d45f8849b1851b406f43449794917a",
    "title": "Learning Domain Ontologies from Document Warehouses and Dedicated Web Sites",
    "abstract": "We present a method and a tool, OntoLearn, aimed at the extraction of domain ontologies from Web sites, and more generally from documents shared among the members of virtual organizations. OntoLearn first extracts a domain terminology from available documents. Then, complex domain terms are semantically interpreted and arranged in a hierarchical fashion. Finally, a general-purpose ontology, WordNet, is trimmed and enriched with the detected domain concepts. The major novel aspect of this approach is semantic interpretation, that is, the association of a complex concept with a complex term. This involves finding the appropriate WordNet concept for each word of a terminological string and the appropriate conceptual relations that hold among the concept components. Semantic interpretation is based on a new word sense disambiguation algorithm, called structural semantic interconnections.",
    "venue": "International Conference on Computational Logic",
    "citationCount": 468,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e447b0901232ffa9f13f5f674ad73184d2112a4d",
    "url": "https://www.semanticscholar.org/paper/e447b0901232ffa9f13f5f674ad73184d2112a4d",
    "title": "Local Semantic Context Analysis for Automatic Ontology Matching",
    "abstract": "This paper proposes an algorithm for concept matching, applied in the ontology mapping domain. Basic idea is to seek the effective semantics embedded in the concept name by analyzing the context in which it appears. Through simple interactions with the known lexicon WordNet, the right meaning associated to a concept is unequivocally elicited by exploring their local semantic context, viz. the surrounding concepts. This approach reveals interesting results for the word sense disambiguation, when polysemy problems requires a semantic interpretation.",
    "venue": "IFSA/EUSFLAT Conf.",
    "citationCount": 16,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "938540c957f3613fa2fc84741957a32dbafbd6af",
    "url": "https://www.semanticscholar.org/paper/938540c957f3613fa2fc84741957a32dbafbd6af",
    "title": "Geographic Named Entity Disambiguation with Automatic Profile Generation",
    "abstract": "Knowledge rich approach of processing documents has been viewed as a method to improve over simple bag-of-word representation. Extracting location information from documents and link them to some ontology such as world gazetteer through a disambiguation process becomes an interesting and important topic. Lacking of training data is a problem in disambiguation method. In this paper we described a method to automatically extract training data from large collection of documents based on local context disambiguation, and then sense profiles are generated automatically for disambiguation use. Another topic of this paper is to describe a linear combination method to combine different types of evidences of disambiguation. We explored three different evidences including location sense context in training documents, local neighbor context, and the popularity of individual location sense. Our results show that combining the three evidences generates reasonable results",
    "venue": "2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06)",
    "citationCount": 14,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "5a70400b902e73d65cc6c8f46026343fec77e7ed",
    "url": "https://www.semanticscholar.org/paper/5a70400b902e73d65cc6c8f46026343fec77e7ed",
    "title": "Use of Ontology for Reusing Web Repositories for eLearning",
    "abstract": null,
    "venue": "",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "b114a7e586f1b7281efe4501dd90a60f127eb1f7",
    "url": "https://www.semanticscholar.org/paper/b114a7e586f1b7281efe4501dd90a60f127eb1f7",
    "title": "SOFIE: a self-organizing framework for information extraction",
    "abstract": "This paper presents SOFIE, a system for automated ontology extension. SOFIE can parse natural language documents, extract ontological facts from them and link the facts into an ontology. SOFIE uses logical reasoning on the existing knowledge and on the new knowledge in order to disambiguate words to their most probable meaning, to reason on the meaning of text patterns and to take into account world knowledge axioms. This allows SOFIE to check the plausibility of hypotheses and to avoid inconsistencies with the ontology. The framework of SOFIE unites the paradigms of pattern matching, word sense disambiguation and ontological reasoning in one unified model. Our experiments show that SOFIE delivers high-quality output, even from unstructured Internet documents.",
    "venue": "The Web Conference",
    "citationCount": 276,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6dbd35fbabe0d7be61190869efe4a770e724001a",
    "url": "https://www.semanticscholar.org/paper/6dbd35fbabe0d7be61190869efe4a770e724001a",
    "title": "Ontology-based indexing of annotated images using semantic DNA and vector space model",
    "abstract": "The study presented in this paper focuses on the preprocessing stage of image retrieval by proposing an ontology-based indexing approach which captures the meaning of image annotations by extracting the semantic importance of the words in them. The indexing algorithm is based on the classic vector-space model that is adapted by employing index weighting and a word sense disambiguation. It uses sets of Semantic DNA, extracted from a lexical ontology, to represent the images in a vector space. As discussed in the paper, the use of Semantic DNA in text-based image retrieval aims to overcome some of the major drawbacks of well known traditional approaches such as \u2018bags of words\u2019 and term frequency-(TF) based indexing. The proposed approach is evaluated by comparing the indexing achieved using the proposed semantic algorithm with results obtained using a traditional TF-based indexing in vector space model (VSM) with singular value decomposition (SVD) technique. The experimental results show that the proposed ontology-based approach generates a better-quality index which captures the conceptual meaning of the image annotations.",
    "venue": "2011 International Conference on Semantic Technology and Information Retrieval",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "73d4c4b6b1ec0c9d4aaa231f931f6b6702ed7172",
    "url": "https://www.semanticscholar.org/paper/73d4c4b6b1ec0c9d4aaa231f931f6b6702ed7172",
    "title": "Enriching concept descriptions in an amphibian ontology with vocabulary extracted from wordnet",
    "abstract": "An important task of ontology learning is to enrich the vocabulary for domain ontologies using different sources of information. WordNet, an online lexical database covering many domains, has been widely used as a source from which to mine new vocabulary for ontology enrichment. However, since each word submitted to WordNet may have several different meanings (senses), existing approaches still face the problem of semantic disambiguation in order to select the correct sense for the new vocabulary to be added. In this paper, we present a similarity computation method that allows us to efficiently select the correct WordNet sense for a concept-word in a given ontology. Once the correct sense is identified, we can then enrich the concept's vocabularly using nearby words in WordNet. Experimental results using an amphibian ontology show that the similarity computation method reach a good average accuracy and our approach is able to enrich the vocabulary of each concept with words mined from WordNet synonyms and hypernyms.",
    "venue": "IEEE International Symposium on Computer-Based Medical Systems",
    "citationCount": 12,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d8d1bead6e91233a633b049d76af2549f84bc03c",
    "url": "https://www.semanticscholar.org/paper/d8d1bead6e91233a633b049d76af2549f84bc03c",
    "title": "Semantification of Identifiers in Mathematics for Better Math Information Retrieval",
    "abstract": "Mathematical formulae are essential in science, but face challenges of ambiguity, due to the use of a small number of identifiers to represent an immense number of concepts. Corresponding to word sense disambiguation in Natural Language Processing, we disambiguate mathematical identifiers. By regarding formulae and natural text as one monolithic information source, we are able to extract the semantics of identifiers in a process we term Mathematical Language Processing (MLP). As scientific communities tend to establish standard (identifier) notations, we use the document domain to infer the actual meaning of an identifier. Therefore, we adapt the software development concept of namespaces to mathematical notation. Thus, we learn namespace definitions by clustering the MLP results and mapping those clusters to subject classification schemata. In addition, this gives fundamental insights into the usage of mathematical notations in science, technology, engineering and mathematics. Our gold standard based evaluation shows that MLP extracts relevant identifier-definitions. Moreover, we discover that identifier namespaces improve the performance of automated identifier-definition extraction, and elevate it to a level that cannot be achieved within the document context alone.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "citationCount": 56,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "dd7d5e67ef095edbf8750368b64bfb72e4bac9d1",
    "url": "https://www.semanticscholar.org/paper/dd7d5e67ef095edbf8750368b64bfb72e4bac9d1",
    "title": "Extracting Concepts' Relations and Users' Preferences for Personalizing Query Disambiguation",
    "abstract": "For most Web searching applications, queries are commonly ambiguous because words usually contain several meanings. Traditional Word Sense Disambiguation (WSD) methods use statistic models or ontology-based knowledge models to find the most appropriate sense for the ambiguous word. Since queries are usually short, the contexts of the queries may not always provide enough information for disambiguating queries. Thus, more than one interpretation may be found for one ambiguous query. In this paper, we propose a cluster-based WSD method, which finds out all appropriate interpretations for the query. Because some senses of one ambiguous word usually have very close semantic relations, we group those similar senses together for explaining the ambiguous word in one interpretation. If the cluster-based WSD method generates several contradictory interpretations for one ambiguous query, we extract users\u00e2\u20ac\u2122 preferences from clickthrough data, and determine suitable concepts or concepts\u00e2\u20ac\u2122 clusters that meet users\u00e2\u20ac\u2122 interests for explaining the ambiguous query.",
    "venue": "Int. J. Semantic Web Inf. Syst.",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f3837bfc18794c4a7584a996c60af3609600eb24",
    "url": "https://www.semanticscholar.org/paper/f3837bfc18794c4a7584a996c60af3609600eb24",
    "title": "Lexical Enrichment of a Human Anatomy Ontology using WordNet",
    "abstract": "This paper is concerned with lexical enrichment of ontologies, i.e. how to enrich a given ontology with lexical entries derived from a semantic lexicon. We present an approach towards the integration of both types of resources, in particular for the human anatomy domain as represented by the Foundational Model of Anatomy (FMA). The paper describes our approach on combining the FMA with WordNet by use of a simple algorithm for domain-specific word sense disambiguation, which selects the most likely sense for an FMA term by computing statistical significance of synsets on a corpus of Wikipedia pages on human anatomy. The approach is evaluated on a benchmark of 50 ambiguous FMA terms with manually assigned WordNet synsets (i.e. senses).",
    "venue": "",
    "citationCount": 13,
    "fieldsOfStudy": null
  },
  {
    "paperId": "634585946a2fb5def5f31010d38064895b8747c8",
    "url": "https://www.semanticscholar.org/paper/634585946a2fb5def5f31010d38064895b8747c8",
    "title": "LODifier: Generating Linked Data from Unstructured Text",
    "abstract": null,
    "venue": "Extended Semantic Web Conference",
    "citationCount": 105,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "8c300bf6090c427631d772b875fdabf57af4257a",
    "url": "https://www.semanticscholar.org/paper/8c300bf6090c427631d772b875fdabf57af4257a",
    "title": "Semantic Similarity from Natural Language and Ontology Analysis",
    "abstract": "Automatic text understanding requires knowledge and, so far, machines know only what we give or teach them. As a consequence, most natural language processing (NLP) tasks crucially rely on the existence of linguistic resources that encode information about language, be it of morphological, syntactic, or semantic nature. Such resources are typically acquired via two main approaches: the knowledge-based approach, or top\u2013down, where information is manually curated by humans, and the corpus-based approach, or bottom\u2013up, where information is automatically learned from corpora. Although the latter has gained ground during the last decade\u2014benefiting from the availability of large amounts of text and from increased computing capacities\u2014the former remains fundamental for it allows us to collect reliable, fine-grained, and explicit information. Lexical knowledge bases (LKBs), also known as lexico-semantic resources, provide information about words and potentially entities, and are at the core of knowledgebased approaches. They are widely used in a variety of NLP tasks (e.g., word sense disambiguation, information retrieval, and question answering), all the more so since their traditional limitations (i.e., lack of language and domain-specific coverage) have recently started to fall. Indeed, beside the long-established process of expert-based resource creation (e.g., WordNet), Web technologies have enabled the collaborative, crowd-based construction of resources (e.g., Wikipedia and Wiktionary). This contributed to significantly widen the scope of the available machine-readable knowledge and, in the context of an already diverse landscape of LKBs, it encouraged and motivated even more the need to integrate different resources so as to make the best of them all. This book introduces linked lexical knowledge bases by giving an account of their foundations and presenting their main applications. Its target audience includes NLP practitioners or students who wish to better understand linked lexical knowledge bases, how they are built, and their typical usages and added value. The book is organized into eight chapters plus a preface, and is additionally put into perspective by a foreword (by Ido Dagan) that considers the history, recent evolution, and probable future directions of knowledge acquisition.",
    "venue": "Computational Linguistics",
    "citationCount": 39,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c660e6e4f3f02345663550f4e6a4e7febbf4a80c",
    "url": "https://www.semanticscholar.org/paper/c660e6e4f3f02345663550f4e6a4e7febbf4a80c",
    "title": "Exploring Concepts' Semantic Relations for Clustering-Based Query Senses Disambiguation",
    "abstract": null,
    "venue": "RSKT",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "479617b32867251c270ad88cd2881dbd46bd34ad",
    "url": "https://www.semanticscholar.org/paper/479617b32867251c270ad88cd2881dbd46bd34ad",
    "title": "The Lexical Ontology for Romanian",
    "abstract": null,
    "venue": "",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "5a763e235f8ce677aa10f527cd0180db833c06d7",
    "url": "https://www.semanticscholar.org/paper/5a763e235f8ce677aa10f527cd0180db833c06d7",
    "title": "Spanish All-Words Semantic Class Disambiguation Using Cast3LB Corpus",
    "abstract": null,
    "venue": "MICAI",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d2152059477ca5ac6424e6890879fce18469c8b1",
    "url": "https://www.semanticscholar.org/paper/d2152059477ca5ac6424e6890879fce18469c8b1",
    "title": "Semi-Automatic Practical Ontology Construction by Using a Thesaurus, Computational Dictionaries, and Large Corpora",
    "abstract": "This paper presents the semi-automatic construction method of a practical ontology by using various resources. In order to acquire a reasonably practical ontology in a limited time and with less manpower, we extend the Kadokawa thesaurus by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. The former can be obtained by converting valency information and case frames from previously-built computational dictionaries used in machine translation. The latter can be acquired from concept co-occurrence information, which is extracted automatically from large corpora. The ontology stores rich semantic constraints among 1,110 concepts, and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology. In our practical machine translation system, our ontology-based word sense disambiguation method achieved an 8.7% improvement over methods which do not use an ontology for Korean translation.",
    "venue": "HTLKM@ACL",
    "citationCount": 23,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "795f6ffce1327d718235ed068f1e7bb25e82604c",
    "url": "https://www.semanticscholar.org/paper/795f6ffce1327d718235ed068f1e7bb25e82604c",
    "title": "Current Trends in Web Engineering",
    "abstract": null,
    "venue": "Lecture Notes in Computer Science",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a2813ed90ed4038a5a6076275378282aca954daa",
    "url": "https://www.semanticscholar.org/paper/a2813ed90ed4038a5a6076275378282aca954daa",
    "title": "Ontology mapping with auxiliary resources",
    "abstract": "Upper Ontology Resources belonging to this group have the singular focus of creating an abstract ontology using an upper-level list of concept descriptions. Such an ontology can then serve as a base for domain specific resources. An example of such a resource is the SUMO ontology, containing approximately 2.000 abstract concept descriptions (Niles and Pease, 2001). These concepts can then be used to model more specific domains. MILO for instance is an extension of SUMO which includes many mid-level concepts (Niles and Terry, 2004). Cyc is another example of a multi-layered ontology based on an abstract upper level-ontology (Matuszek et al., 2006), of which a subset is freely available under the name OpenCyc (Sicilia et al., 2004). Multi-lingual When mapping ontologies, it can occur that some concept descriptions are formulated in a different language. In these situations mono-lingual resources are insufficiently applicable, necessitating the usage of multi-lingual resources, e.g. UWN (De Melo and Weikum, 2009) or BabelNet (Navigli and Ponzetto, 2010). LSMs are a powerful metric and are commonly used in contemporary stateof-the-art ontology mapping systems (Shvaiko and Euzenat, 2005; Kalfoglou and Schorlemmer, 2003; Saruladha, Aghila, and Sathiya, 2011), with WordNet being the most widely used resource as basis. However, a common occurrence in concepts formulated using natural language is word-sense ambiguity. This entails that a word can have multiple and possibly vastly different meanings, such that one must eliminate all meanings which do not adequately represent the intended meaning of the word. This task, while at a glance quite intuitive for a human, can be deceptively difficult for a computer program. Given the word house for instance, the intended meaning might be obvious to a human reader, however this word has 14 different meanings listed in WordNet, such that an accurate identification of the correct sense is necessary in order to obtain accurate results. The histogram in Figure 4.1 indicates the extent of such situations occurring within WordNet (Miller, 1995). Here, all unique words that occur in WordNet have been gathered and binned according to how many different meanings each word describes. One can see from Figure 4.1 that while there is a large number of words with only one meaning, there is a significant proportion of words which do have more than one meaning and hence can ambiguous. The general working hypothesis is that a word in a given context has only a single correct sense. The rejection of this hypothesis, the acknowledgement of polysemous words, is an emerging field of research for which new approaches are emerging (Cruz et al., 2013). Ultimately a LSM has to calculate the similarity between two sets of senses, where the assumption whether these sets can contain multiple correct senses may influence the choice of specific employed techniques, including disambiguation methods. LSMs can incorporate polysemous concepts by for instance calculating an aggregate similarity between these sets of",
    "venue": "",
    "citationCount": 17,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0e3c97e0dea495e39501b6adfcd2e04166725a72",
    "url": "https://www.semanticscholar.org/paper/0e3c97e0dea495e39501b6adfcd2e04166725a72",
    "title": "From Web directories to lightweight ontology:natural language processing and formalization",
    "abstract": "In order to solve the problem that classifications were very hard to be reasoned about by automated software agents and represent annotations of little use for semantic Web applications since their labels or nodes were written in natural language,this paper introduced an approach to transform a hierarchical directory into lightweight ontology by a series of steps,including part-of-speech tagging,word sense disambiguation,coordination disambiguation,and new controlled natural language definition and conversion,which then helped formalize the natural language labels into simple description logic formulae and provided the significant basis for further ontology reasoning and document retrieval.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1e6a913784d4a6aedeb6234f4455e6aefef03a2a",
    "url": "https://www.semanticscholar.org/paper/1e6a913784d4a6aedeb6234f4455e6aefef03a2a",
    "title": "Generalization of Ontology Instances Based on WordNet and Google",
    "abstract": "In order to populate ontology, this paper presents a generalization method of ontology instances, extracted from texts and web pages, by using unsupervised learning techniques for word sense disambiguation, which uses open APIs and lexical resources such as Google and WordNet. According to the experimental results, our method achieved a 15.8% improvement over the previous research.",
    "venue": "",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "648bb562a8f731207fdf66acfb56fd48a6a8657a",
    "url": "https://www.semanticscholar.org/paper/648bb562a8f731207fdf66acfb56fd48a6a8657a",
    "title": "An ontology-based and domain specific clustering methodology for financial documents",
    "abstract": "Financial documents play an important role in modern financial analysis and information retrieval tasks. In order to accomplish various investigational needs, financial organizations continuously search for accurate and meaningful unsupervised document classification techniques. Nevertheless, unsupervised document categorization or document clustering is a challenging problem studied by many scientists. Incorporating semantic knowledge from an ontology into document clustering has been extensively studied and it has provided enhanced clustering performances. The incorporated semantic knowledge is generally used for identifying the correct meanings of the ambiguous words in the documents. Most of the proposed methodologies were experimented on general document datasets and most of the few available domain specific clustering studies were constrained to specific domains where complete domain ontologies are available. Although financial domain has several domain ontologies, none of them are complete and suitable for semantic document clustering. In this context, our study proposes a document clustering methodology for financial documents which adapts WordNet ontology to the financial domain to serve as an external knowledge source. This study empirically shows that nouns are relatively prevalent and more important for document clustering rather than other terms in a document. Afterwards, a subset of nouns is identified as most important for the clustering, based on their frequency distribution within the main noun list. We developed a word sense disambiguation technique which uses ontological knowledge for noun disambiguation. Finally, nouns in each document are disambiguated with the proposed word sense disambiguation technique, associated with tf-idf weights and clustered. On the basis of the empirical results of this research, it can be concluded that the proposed methodology can significantly enhance the clustering performance compared to no disambiguation and pure WordNet based disambiguation approaches.",
    "venue": "2017 Seventeenth International Conference on Advances in ICT for Emerging Regions (ICTer)",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c54f38857d25315ad1ca4024010cfd985d361e9b",
    "url": "https://www.semanticscholar.org/paper/c54f38857d25315ad1ca4024010cfd985d361e9b",
    "title": "Feature Generation for Text Categorization Using World Knowledge",
    "abstract": "We enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge. This knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the Open Directory; these ontologies are further enriched by several orders of magnitude through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words. Feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses the two main problems of natural language processing--synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone. Experimental results confirm improved performance, breaking through the plateau previously reached in the field.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "citationCount": 306,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d0800ab91a4bb07d4f63fc91cde2bea59b09567b",
    "url": "https://www.semanticscholar.org/paper/d0800ab91a4bb07d4f63fc91cde2bea59b09567b",
    "title": "Enriching very large ontologies using the WWW",
    "abstract": "This paper explores the possibility to exploit text on the world wide web in order to enrich the concepts in existing ontologies. First, a method to retrieve documents from the WWW related to a concept is described. These document collections are used 1) to construct topic signatures (lists of topically related words) for each concept in WordNet, and 2) to build hierarchical clusters of the concepts (the word senses) that lexicalize a given word. The overall goal is to overcome two shortcomings of WordNet: the lack of topical links among concepts, and the proliferation of senses. Topic signatures are validated on a word sense disambiguation task with good results, which are improved when the hierarchical clusters are used.",
    "venue": "ECAI Workshop on Ontology Learning",
    "citationCount": 378,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "511b7a8e6fb1a69f71c42430ae64b55df7cc97a1",
    "url": "https://www.semanticscholar.org/paper/511b7a8e6fb1a69f71c42430ae64b55df7cc97a1",
    "title": "Novel Query Expansion Method based on User Interest Context and Ontology",
    "abstract": "We proposed a novel query expansion method by combining user interest and ontology. Firstly, users\u2019 interests are described by contextual words which are generated based on ontology, and the user interest degree with respect to each contextual word is calculated. Secondly, the contextual words are organized according to ontology relevance and divided into different subsets, and each subset can be seen as a candidate suggestion set. By calculating the weight of each contextual word, we obtain the meaningful expansions for a query. Comparative experiments show that, the proposed method is superior to other methods when precision and recall measurement are used and gives personalized query suggestions to users efficiently. Introduction The effectiveness of information retrieval from the web largely depends on whether users can issue queries to search engines, which properly describe their information needs [1]. Writing queries is not very easy, because the queries are usually short and the words may be ambiguous [2, 3]. Most existing works on query expansion utilize query logs to suggest queries [4]. Generally, the web search engines have millions of users. When a user has some information needs, there always exist many users who have searched the same query before. Therefore, the search engine can use these large amounts of past usage data to offer possible query expansions [5]. Because the query submitted by the user is closely related to his interests and intents, different users who submit a same query may want to express different requirements. Effective query expansion requires inferring user\u2019s query intent and then expanded queries that help retrieving webpages which contain the relevant information [6]. Inspired by this, we propose a method of query expansion based on user interest context and ontology. It does not depend on query logs of the whole web and utilizes only the terms occurring in the user browsed logs. The proposed method In this paper, the proposed query expansion method is executed in two steps: the user interest context mining and the query expansion. The details are given as follows. (1) User interest context mining Firstly, we execute webpage parsing to extract the main body of the webpage. Stop words are filtered out and the root of each word is extracted by using the Porter Stemming algorithm [7]. The webpage pi is represented by the vector Wi=(wi1,wi2,...wim), where wim is the term of pi; Secondly, we use the natural language processing technology to implement word sense disambiguation[8]. Further, we obtain the hypernyms of the terms which are called contextual words and denoted as Ci=(ci1,ci2,...cim) in pi through generic ontology, where, cim is the contextual word of wim. To calculate the user interest degree of contextual word, the browsed webpages are organized by the day, and each day is seen as one session. The webpages user u browsed in j-th session is denoted as Dayj. The interest degree of contextual word c is formulated as follows, denoted as I(c): ( ) log 2 ( ) 1 ( ) ( , ) ( , ) init n d d l j j j I c f c Day t c Day e a \u03b2 \u2212 \u2212",
    "venue": "International Conference on Interaction Sciences",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c823521300431fc55649c2f290259d91b303afa1",
    "url": "https://www.semanticscholar.org/paper/c823521300431fc55649c2f290259d91b303afa1",
    "title": "Arabic-English Automatic Ontology Mapping Based on Machine Readable Dictionary",
    "abstract": "Ontologies are the backbone of the semantic web and allow software agents to interoperate effectively. An ontology is able to represent and to clarify concepts and inter-concept relationships and can be used as a framework to represent underlying domain concepts expressed in many different languages. One way to do this is by mapping Ontologies in different languages using an inter-lingual index. In this paper we present a new methodology for ontology mapping in different script human languages (Arabic/English). We identify the steps of extracting concepts on both ontologies and automatically mapping them based on Machine Readable Dictionary (MRD) and Word Sense Disambiguation (WSD) tools. The paper also discusses a unique tool that automatically extracts unmapped concepts and uses MRD and WSD to match them and create semantic bridges between the ontologies.",
    "venue": "2009 Mexican International Conference on Computer Science",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a5db8f4b1a190cc2db38728dad1192e6f74a5fd9",
    "url": "https://www.semanticscholar.org/paper/a5db8f4b1a190cc2db38728dad1192e6f74a5fd9",
    "title": "A Review of Recent Trends: Text Mining of Taxonomy Using WordNet 3.1 for the Solution and Problems of Ambiguity in Social Media",
    "abstract": null,
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c8b397a2789e080499b7bd2ba95970e0ea94741e",
    "url": "https://www.semanticscholar.org/paper/c8b397a2789e080499b7bd2ba95970e0ea94741e",
    "title": "AN ONTOLOGY-BASED INFORMATION EXTRACTOR FOR DATA-RICH DOCUMENTS IN THE INFORMATION TECHNOLOGY DOMAIN",
    "abstract": "This paper presents an information extraction method, suitable for data-rich documents, based on the knowledge represented in a domain ontology. The extractor combines a fuzzy string matcher and a word sense disambiguation (WSD) algorithm. \nThe fuzzy string matcher finds mentions of terms combining character-level and token-level similarity measures dealing with non-standardized acronyms and inconsistent abbreviation styles. \nWe propose a new character-level edit distance sensitive to prefixes called root distance and a token-level similarity algorithm for fuzzy acronym detection. Additionally, a WSD strategy using an ontology-based semantic relatedness measure is used to solve the inherent ambiguity of some entities. The WSD module finds a sense combination over all the document length optimizing the document semantic coherence. Our approach seems to be suitable to extract information from data-rich documents describing Orly one main object (i.e. product) by document. The results showed a precision of 78.9% with 99.5% recall using documents and an ontology related to laptop computers domain.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "98db91a2deae2c241814ff008d95322f30d5f55b",
    "url": "https://www.semanticscholar.org/paper/98db91a2deae2c241814ff008d95322f30d5f55b",
    "title": "Shallow Ontology Based on VerbaLex",
    "abstract": "Ontologies have proven to be a useful resource in natural\nlanguage processing. In this paper, we introduce basic ideas of\na shallow ontology named Sholva. This ontology is based on\nVerbaLex, a database of verb valencies, where each valency\npointer also contains a pointer into EuroWordnet. We focused\nour effort on building ontology which would help us in solving\nreal problems in syntactic analysis, word sense disambiguation\nand machine translation.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "105e44a9263ba4f05ee0e9b08c73922eb58c0171",
    "url": "https://www.semanticscholar.org/paper/105e44a9263ba4f05ee0e9b08c73922eb58c0171",
    "title": "Powering Semantic Analysis with Bio-ontologies",
    "abstract": "In the past three decades, bio-ontologies have moved from esoteric artifacts to key resources for the semantic analysis of biomedical text. In this presentation, we will follow the evolution of the creation and integration of bio-ontologies, as well as their role in biomedical applications, including literature analysis. Bio-ontologies include a variety of resources that provide a source of names for biomedical entities and specify relations among these entities. These resources are generally developed independently by individuals, collectives, institutions, and standard development organizations. Examples of bio-ontologies include the Medical Subject Headings (MeSH), developed by the National Library of Medicine to support the indexing and retrieval of the biomedical literature, the Gene Ontology (GO), developed by the GO Consortium to support consistent annotation and analysis of gene products across organisms, and SNOMED CT, developed by SNOMED International to support clinical documentation and analytics worldwide. While these three examples illustrate resources with a large scope, many other bio-ontologies focus on a specialized subdomain of medicine. Bio-ontologies use different formalisms and various degrees of formality for their representation. Bio-ontologies play an important role in the semantic analysis of biomedical datasets, including the biomedical literature. Bio-ontologies provide a source of vocabulary for biomedical entities, used for named entity recognition (i.e., finding mentions of biomedical entities in text) and entity resolution (i.e., mapping mentions a specific reference). Bio-ontologies also provide semantic categorization for biomedical entities, which is leveraged for word-sense disambiguation and co-reference resolution (especially when a specific entity is referred to with a broader category). Finally, bio-ontologies provide a source of relations among entities, which can form the basis for relation extraction, hypothesis generation and, more generally, literature-based discovery. Since most bio-ontologies are developed independently, but often need to be used together, ontology alignment techniques have been developed to identify correspondences among entities across ontologies. Repositories of bio-ontologies, such as the Unified Medical Language System (UMLS) Metathesaurus, the National Center for Biomedical Ontology (NCBO) BioPortal and the Open Biological and Biomedical Ontology (OBO) Foundry are useful sources of ontologies and contribute to the development of tools to support semantic analysis of biomedical text.",
    "venue": "The Web Conference",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1e7395bc91b8fad1cb140597da1330e2ad4952ee",
    "url": "https://www.semanticscholar.org/paper/1e7395bc91b8fad1cb140597da1330e2ad4952ee",
    "title": "A knowledge\u2010based approach to Information Extraction for semantic interoperability in the archaeology domain",
    "abstract": "The article presents a method for automatic semantic indexing of archaeological grey\u2010literature reports using empirical (rule\u2010based) Information Extraction techniques in combination with domain\u2010specific knowledge organization systems. The semantic annotation system (OPTIMA) performs the tasks of Named Entity Recognition, Relation Extraction, Negation Detection, and Word\u2010Sense Disambiguation using hand\u2010crafted rules and terminological resources for associating contextual abstractions with classes of the standard ontology CIDOC Conceptual Reference Model (CRM) for cultural heritage and its archaeological extension, CRM\u2010EH.",
    "venue": "J. Assoc. Inf. Sci. Technol.",
    "citationCount": 25,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4b3d47a085386f410e882acecb3788c3f44e3419",
    "url": "https://www.semanticscholar.org/paper/4b3d47a085386f410e882acecb3788c3f44e3419",
    "title": "The semantics of humor",
    "abstract": "This chapter opens the second part of the book on humor competence (chapters 6-9). This chapter introduces Raskin\u2019s semantic theory of humor competence based on scripts (Semantic-Script Theory of Humor, SSTH). The concept of script (or frame) is introduced. Dynamic scripts (i.e., scripts that are updated with new informatiom) are described as well as the mechanics of combinations of word senses and disambiguation. The two conditions for a text to be a joke are described: 1) two scripts must be opposite, and 2) they must overlap fully or in part, i.e., be compatible with (parts of) the text. The pragmatic aspects of the SSTH are also discussed, with the idea of non-bona-fide communication, i.e., communication not entirely focused on cooperative (in the Gricean sense) communication. Finally, the recent development of Ontological semantics and its application to humor is discussed.",
    "venue": "The Linguistics of Humor",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "81d049ca6821eaaa878fb9667bdbb8a003f9ca65",
    "url": "https://www.semanticscholar.org/paper/81d049ca6821eaaa878fb9667bdbb8a003f9ca65",
    "title": "Ontology-enriched Semantic Space for Video Retrieval Submitted to School of Graduate Studies",
    "abstract": "Multimedia-based ontology construction and reasoning have recently been recognized as two important issues in video search, particularly for bridging semantic gap. The lack of coincidence between low-level features and user expectation makes conceptbased ontology reasoning an attractive mid-level framework for interpreting highlevel semantics. In this report, we propose a novel model, namely ontology-enriched semantic space (OSS), to provide a computable platform for modeling and reasoning concepts in a linear space. OSS enlightens the possibility of answering conceptual questions such as a high coverage of semantic space with minimal set of concepts, and the set of concepts to be developed for video search. More importantly, the queryto-concept mapping can be more reasonably conducted by guaranteeing the uniform and consistent comparison of concept scores for video search. We explore OSS for several tasks including concept-based video search, word sense disambiguation and detecor fusion. Our empirical findings show that OSS is a feasible solution to timely issues such as the measurement of concept combination and query-concept dependent fusion.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "31b0f9f8f7cf59ad2937a807df30fb600e7e77ee",
    "url": "https://www.semanticscholar.org/paper/31b0f9f8f7cf59ad2937a807df30fb600e7e77ee",
    "title": "Who stole what from whom?",
    "abstract": "\nDrawing on the Lexical Grammar Model, Frame Semantics and Corpus Pattern Analysis, we analyze and contrast verbs of stealing in English and Spanish from a lexico-semantic perspective. This involves looking at the lexical collocates and their corresponding semantic categories that fill the argument slots of verbs of stealing. Our corpus search is performed with the Word Sketch tool on Sketch Engine. To the best of our knowledge, no study has yet taken advantage of the Word Sketch tool in the study of the selection preferences of verbs of stealing, let alone a semantic, cross-linguistic study of those verbs. Our findings reveal that English and Spanish verbs of stealing map out the same underlying semantic space. This shared conceptual layer can thus be incorporated into an ontology based on deep semantics, which could in turn enhance NLP tasks such as word sense disambiguation, machine translation, semantic tagging, and semantic parsing.",
    "venue": "Languages in Contrast",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "af0c7c45dfb08b56ed2185a2ca1729f066aa5c23",
    "url": "https://www.semanticscholar.org/paper/af0c7c45dfb08b56ed2185a2ca1729f066aa5c23",
    "title": "Rebuilding Lexical Resources for Information Retrieval using Sense Folder Detection and Merging Methods",
    "abstract": "In this paper we discuss the problem of sense disambiguation using lexical resources like ontologies or thesauri with a focus on the application of sense detection and merging methods in information retrieval systems. For an information retrieval task it is important to detect the meaning of a query word for retrieving the related relevant documents. In order to recognize the meaning of a search word, lexical resources, like WordNet, can be used for word sense disambiguation. But, analyzing the WordNet structure, we see that this ontology is fraught with different problems. The too fine grained distinction between word senses, for example, is unfavorable for a usage in information retrieval. We describe related problems and present four implemented online methods to merge SynSets based on relations like hypernyms and hyponyms, and further context information like glosses and domain. Afterwards we show a first evaluation of our approach, compare the different merging methods and discuss briefly future work.",
    "venue": "LREC",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1a98b675d4c059e72d3772a57b5763b8881dbbee",
    "url": "https://www.semanticscholar.org/paper/1a98b675d4c059e72d3772a57b5763b8881dbbee",
    "title": "The Cornetto Datbase. Architecture and Alignment Issues of Combining Lexical Units, Synsets and an Ontology.",
    "abstract": "Cornetto is a two-year Stevin project (project number STE05039) in which a lexical semantic database is built that combines Wordnet with Framenet-like information for Dutch. The combination of the two lexical resources (the Dutch wordnet and the Referentie Bestand Nederlands) will result in a much richer relational database that may improve natural language processing (NLP) technologies, such as word sense-disambiguation, and language-generation systems. In addition to merging the Dutch lexicons, the database is also mapped to a formal ontology to provide a more solid semantic backbone. Since the database represents different traditions and perspectives of semantic organization, a key issue in the project is the alignment of concepts across the resources. This paper discusses our methodology to first automatically align the word meanings and secondly to manually revise the most critical cases.",
    "venue": "",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "699ae646f57616ffd0d6f94508a47cf5cd119076",
    "url": "https://www.semanticscholar.org/paper/699ae646f57616ffd0d6f94508a47cf5cd119076",
    "title": "Neat and Scruffy: On Computational Generation and Interpretation of Spatial Descriptions (Abstract)",
    "abstract": "Physical sciences have developed ways in which space can be described with a high degree of accuracy, for example by measuring distances and angles in coordinate systems. Such measures can be represented on a continuous scale of real numbers and their mathematical modelling and computation is well understood. However, humans refer to space quite differently. Descriptions such as \u201cthe chair is to the left of the table\u201d, \u2018the flowers are in a vase\u201d or \u201c\u2018turn right at the next crossroad\u201d refer to discrete units such as points, regions and volumes. They require common sense knowledge how objects related by a preposition interact with each other. Their semantics take into account aspects of linguistic interaction such as communicative intents of speakers and their conversational partners, for example in negotiation of spatial perspective or frame of reference. Mechanisms of attention are used to select information from different contexts and evaluate potential distractors which makes their interpretation notoriously vague. Spatial descriptions connect both human conceptual and perceptual domains and therefore, I argue, can only be modelled with computational architectures that combine aspects of neat and scruffy models. The majority of current models consider only the geometric perceptual context as a meaning component of spatial descriptions. We argue that common-sense functional knowledge about object interactions (semantic information about their affordances) and reference to objects in different linguistic interac-tive contexts can be captured by distributional semantic models commonly used in natural language processing, that is from word co-occurrences in contexts. Contextual distributional semantic models or word embeddings can be trained with deep neural networks alongside with other modalities such image and geometric features in the form of grounded language models. Different families of neural networks can be stacked together as modules and the neural architecture naturally supports information fusion. Bottom-up learning of semantics can be combined with top-down engineering in terms of model design, features and injection of conceptual information from ontologies. Although grounded language models have proven to be very successful in generation and interpretation of spatial language in tasks such as image captioning and visual questing answering, the problem of spatial cognition and inference is by no means solved. I will discuss findings of our studies what such models learn. I will argue that the majority of the model shortcomings come from the fact that we train them in the scenarios where they have to match patterns rather than model inference, from the neural architecture designs which fail to cover all aspects of spatial semantics and from the biases in training datasets which frequently lead to hallucinations, cases where the perceptual modality is ignored. As spatial cognition is not yet fully understood, questions such as what features are to be modelled, what kind of representations should be used and at what granularity present interesting future challenges for collaborative work between theoretical, experimental and computational research of spatial cognition.",
    "venue": "STRL@IJCAI",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "cef848e6d64745a4086bbe88fc89670ced029aa4",
    "url": "https://www.semanticscholar.org/paper/cef848e6d64745a4086bbe88fc89670ced029aa4",
    "title": "Language Technologies Meet Ontology Acquisition",
    "abstract": null,
    "venue": "International Conference on Conceptual Structures",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "2cd61e316059a05a732bacd084a01974eb6db1eb",
    "url": "https://www.semanticscholar.org/paper/2cd61e316059a05a732bacd084a01974eb6db1eb",
    "title": "Design of Syntax Analyzer for Kannada Sentences Using Rule-Based Approach",
    "abstract": null,
    "venue": "Lecture Notes in Electrical Engineering",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "87cdcf11b88b69ae835167c0431caf90adfabf72",
    "url": "https://www.semanticscholar.org/paper/87cdcf11b88b69ae835167c0431caf90adfabf72",
    "title": "Discovery of Ontologies for Learning Resources using Word-based Clustering",
    "abstract": "Educational intermediaries are information systems that support the exchange of learning resources among dispersed users. The selection of the appropriate learning resources that cover specific educational needs requires a concise interaction between the user and system. This paper describes a data mining process for the discovery of ontologies from learning resources repositories. Ontologies express the associations between the metadata of learning resources and provide a controlled vocabulary of concepts. This paper illustrates how ontologies and the derived vocabularies can be used for the development of taxonomies of learning resources. Hereby they contribute to the sense disambiguation in seeking interesting and appropriate knowledge.",
    "venue": "",
    "citationCount": 43,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9237329117d75a9cd1f7429d46b8b15200bd848d",
    "url": "https://www.semanticscholar.org/paper/9237329117d75a9cd1f7429d46b8b15200bd848d",
    "title": "MULTIPLE SENSE INVENTORIES AND TEST-BED CORPORA",
    "abstract": "Comparing performances of word sense disambiguation systems is a very difficult evaluation task when different sense inventories are used and, even more difficult when the sense distinctions are not of the same granularity. The paper substantiates this statement by briefly presenting a system for word sense disambiguation (WSD) based on parallel corpora. The method relies on word alignment, word clustering and is supported by a lexical ontology made of aligned wordnets for the languages in the corpora. The wordnets are aligned to the Princeton Wordnet, according to the principles established by EuroWordNet. The evaluation of the WSD system was performed on the same data, using three different granularity sense inventories.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": null
  },
  {
    "paperId": "b6de9d0ca42a03967287aa7abfd59479e086a35a",
    "url": "https://www.semanticscholar.org/paper/b6de9d0ca42a03967287aa7abfd59479e086a35a",
    "title": "Automatic Gloss Finding for a Knowledge Base using Ontological Constraints",
    "abstract": "While there has been much research on automatically constructing structured Knowledge Bases (KBs), most of it has focused on generating facts to populate a KB. However, a useful KB must go beyond facts. For example, glosses (short natural language definitions) have been found to be very useful in tasks such as Word Sense Disambiguation. However, the important problem of Automatic Gloss Finding, i.e., assigning glosses to entities in an initially gloss-free KB, is relatively unexplored. We address that gap in this paper. In particular, we propose GLOFIN, a hierarchical semi-supervised learning algorithm for this problem which makes effective use of limited amounts of supervision and available ontological constraints. To the best of our knowledge, GLOFIN is the first system for this task. Through extensive experiments on real-world datasets, we demonstrate GLOFIN's effectiveness. It is encouraging to see that GLOFIN outperforms other state-of-the-art SSL algorithms, especially in low supervision settings. We also demonstrate GLOFIN's robustness to noise through experiments on a wide variety of KBs, ranging from user contributed (e.g., Freebase) to automatically constructed (e.g., NELL). To facilitate further research in this area, we have made the datasets and code used in this paper publicly available.",
    "venue": "Web Search and Data Mining",
    "citationCount": 34,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9eaeabd38c997f69ee1e0fe9953d7df3cb5e6fea",
    "url": "https://www.semanticscholar.org/paper/9eaeabd38c997f69ee1e0fe9953d7df3cb5e6fea",
    "title": "Exploring Diseases Relationships",
    "abstract": "Developing an efficient algorithm for traversing large ontologies is a key challenge for many semantic-based applications. This chapter introduces an approach, spreading activation over ontology (SAOO), to explore the relationship between two human diseases using an ontology-based spreading activation approach. SAOO comprises two phases: semantic matching and diseases relatedness detection. In the semantic matching phase, user-submitted diseases are semantically identified in the ontology graph using the proposed matching algorithm. Semantic matching conducts more analysis in the matching process, which comprises term normalization; phrase analysis, and word sense disambiguation. In the diseases relatedness detection phase, the URIs of these diseases are passed to the relatedness detector to detect the relationship connecting them. SAOO improves healthcare systems by considering semantic domain knowledge and a set of SWRL rules to infer diseases relatedness. We present a use case that outlines how SAOO can be used to explore relationships between vaccines in the vaccine ontology.",
    "venue": "Computational Methods and Algorithms for Medicine and Optimized Clinical Practice",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "007a635ad92443c701b381258e8e3892427665cd",
    "url": "https://www.semanticscholar.org/paper/007a635ad92443c701b381258e8e3892427665cd",
    "title": "A Semantic Neighborhood Approach to Relatedness Evaluation on Well-Founded Domain Ontologies",
    "abstract": "In the context of natural language processing and information retrieval, ontologies can improve the results of the word sense disambiguation (WSD) techniques. By making explicit the semantics of the term, ontology-based semantic measures play a crucial role to determine how different ontology classes have a similar or related meaning. In this context, it is common to use semantic similarity as a basis for WSD. However, the measures generally consider only taxonomic relationships, which negatively affects the discrimination of two ontology classes that are related by the other relationship types. On the other hand, semantic relatedness measures consider diverse types of relationships to determine how much two classes on the ontology are related. However, these measures, especially the path-based approaches, have as the main drawback a high computational complexity to calculate the relatedness value. Also, for both types of semantic measures, it is unpractical to store all similarity or relatedness values between all ontology classes in memory, especially for ontologies with a large number of classes. In this work, we propose a novel approach based on semantic neighbors that aim to improve the performance of the knowledge-based measures in relatedness analysis. We also explain how to use this proposal into the path and feature-based measures. We evaluate our proposal on WSD using an existent domain ontology for well-core description. This ontology contains 929 classes related to rock facies. Also, we use a set of sentences from four different corpora on the Oil&Gas domain. In the experiments, we compare our proposal with stateof-the-art semantic relatedness measures, such as path-based, feature-based, information content, and hybrid methods regarding the F-score, evaluation time, and memory consumption. The experimental results show that the proposed method obtains F-score gains in WSD, as well as a low evaluation time and memory consumption concerning the traditional knowledge-based measures.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "caad85e682f1d0a4716a22c52c6f0fafd0461858",
    "url": "https://www.semanticscholar.org/paper/caad85e682f1d0a4716a22c52c6f0fafd0461858",
    "title": "Making a Natural Wikipedia Category Scheme",
    "abstract": "Wikipedia is arguably the most important information source yet invented for natural language processing (NLP) and artificial intelligence, in addition to its role as humanity\u2019s largest encyclopedia. Wikipedia is the principal information source for such prominent services as IBM\u2019s Watson [1], Freebase [2], the Google Knowledge Graph [3], Apple\u2019s Siri [4], YAGO [5], and DBpedia [6], the core reference structure for linked open data [7]. Wikipedia information has assumed a prominent role in NLP applications in word sense disambiguation, named entity recognition, co-reference resolution, and multi-lingual alignments; in information retrieval in query expansion, multi-lingual retrieval, question answering, entity ranking, text categorization, and topic indexing; and in semantic applications in topic extraction, relation extraction, entity extraction, entity typing, semantic relatedness, and ontology building [8].",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "4116b467f5f9ad167917215214cfcbda7a002683",
    "url": "https://www.semanticscholar.org/paper/4116b467f5f9ad167917215214cfcbda7a002683",
    "title": "An Efficient Approach for Semantic Relatedness Evaluation Based on Semantic Neighborhood",
    "abstract": "In the context of natural language processing and information retrieval, ontologies can improve the results of the word sense disambiguation (WSD) techniques. By making explicit the semantics of the term, ontology-based semantic measures play a crucial role to determine how different ontology classes have a similar meaning. In this context, it is common to use semantic similarity as a basis for WSD. However, the measures generally consider only taxonomic relationships, which affects negatively the discrimination of two ontology classes that are related by the other relationship types. On the other hand, semantic relatedness measures consider diverse types of relationships to determine how much two classes on the ontology are related. However, these measures, especially the path-based approaches, has as the main drawback a high computational complexity to be calculated in query execution time. Also, for both types of semantic measures, it is unpractical to store all similarity or relatedness values between all ontology classes in memory, especially for large ontologies. In this work, we propose a novel approach based on semantic neighbors that aim to improve the query time in path-based semantic measures without losing their effectiveness in relatedness analysis. We also propose an efficient algorithm to calculate the semantic distance between two ontology classes. We evaluate our proposal in WSD using a pre-existent domain ontology for well-core description. This ontology contains 929 classes related to rock facies and a set of sentences from four different corpora about geology in the Oil\\&Gas domain. In the experiments, we compared our approach with state-of-the-art semantic relatedness measures, such as path-based, feature-based, information content, and hybrid methods regarding the F-score, query time and the total number of classes in memory. The experimental results show that the proposed method obtains F-score gains in WSD, as well as an improvement in the query time concerning the traditional path-based approaches. Also, we reduce the total number of classes stored in memory for each ontology class.",
    "venue": "2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1551899bc495b987e24ba86e4e9402ea720c2714",
    "url": "https://www.semanticscholar.org/paper/1551899bc495b987e24ba86e4e9402ea720c2714",
    "title": "A framework for enriching lexical semantic resources with distributional semantics",
    "abstract": "Abstract We present an approach to combining distributional semantic representations induced from text corpora with manually constructed lexical semantic networks. While both kinds of semantic resources are available with high lexical coverage, our aligned resource combines the domain specificity and availability of contextual information from distributional models with the conciseness and high quality of manually crafted lexical networks. We start with a distributional representation of induced senses of vocabulary terms, which are accompanied with rich context information given by related lexical items. We then automatically disambiguate such representations to obtain a full-fledged proto-conceptualization, i.e. a typed graph of induced word senses. In a final step, this proto-conceptualization is aligned to a lexical ontology, resulting in a hybrid aligned resource. Moreover, unmapped induced senses are associated with a semantic type in order to connect them to the core resource. Manual evaluations against ground-truth judgments for different stages of our method as well as an extrinsic evaluation on a knowledge-based Word Sense Disambiguation benchmark all indicate the high quality of the new hybrid resource. Additionally, we show the benefits of enriching top-down lexical knowledge resources with bottom-up distributional information from text for addressing high-end knowledge acquisition tasks such as cleaning hypernym graphs and learning taxonomies from scratch.",
    "venue": "Natural Language Engineering",
    "citationCount": 13,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a4745dadf7f9cfa59fa10467335bd599be6346d9",
    "url": "https://www.semanticscholar.org/paper/a4745dadf7f9cfa59fa10467335bd599be6346d9",
    "title": "WordNet-Based Information Retrieval Using Common Hypernyms and Combined Features",
    "abstract": "Text search based on lexical matching of keywords is not satisfactory due to polysemous and synonymous words. Semantic search that exploits word meanings, in general, improves search performance. In this paper, we survey WordNet-based information retrieval systems, which employ a word sense disambiguation method to process queries and documents. The problem is that in many cases a word has more than one possible direct sense, and picking only one of them may give a wrong sense for the word. Moreover, the previous systems use only word forms to represent word senses and their hypernyms. We propose a novel approach that uses the most specific common hypernym of the remaining undisambiguated multi-senses of a word, as well as combined WordNet features to represent word meanings. Experiments on a benchmark dataset show that, in terms of the MAP measure, our search engine is 17.7% better than the lexical search, and at least 9.4% better than all surveyed search systems using WordNet. \nKeywords Ontology, word sense disambiguation, semantic annotation, semantic search.",
    "venue": "ArXiv",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "de21ce14c3f3e126a3056edf7ae75536bda40814",
    "url": "https://www.semanticscholar.org/paper/de21ce14c3f3e126a3056edf7ae75536bda40814",
    "title": "Unsupervised Learning of Ontology-Linked Selectional Preferences",
    "abstract": null,
    "venue": "CIARP",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e28d203b326d6501ea8caf60220166a4b3f59622",
    "url": "https://www.semanticscholar.org/paper/e28d203b326d6501ea8caf60220166a4b3f59622",
    "title": "Educational Knowledge Management System Using NLP and Ontology Schema",
    "abstract": "The three concepts of information science are data, information and knowledge. The structure of one is different from another. The structure of knowledge is more complex than data and information. Knowledge management is complex for traditional information management techniques due to its complex structure and it is difficult to achieve common structure for knowledge captured from heterogeneous sources. Ontology is an upright technology to represent knowledge. Ontology provides homogeneous structure for knowledge acquired from heterogeneous sources. It enables knowledge sharing within and among organizations. Ontology based knowledge management provides a better support for integration of related knowledge sources and searching. The current work proposes an enhanced and clear framework for knowledge management using domain ontology. It addresses major issues of traditional and existing ontology based knowledge management systems. Many Natural Language Processing (NLP) techniques, including stemming, part of-speech tagging, compound recognition, de-compounding, chunking, word sense disambiguation and others, have been used in Information Retrieval (IR). The core IR task we are investigating here is document retrieval. Several other IR tasks use very similar techniques, e.g. document clustering, filtering, new event detection, and link detection, and they can be combined with NLP in a way similar to document retrieval. NLP and IR are very different areas of research, and recent major conferences only have a small number of papers investigating the use of NLP techniques for information retrieval. The moderate success contradicts the intuition that NLP should help IR, which is shared by a large number of researchers. This article reviews the research on combining the two areas and attempts to identify reasons for why NLP has not brought a breakthrough to IR. Natural language processing (NLP) is becoming much more robust and applicable in realistic applications. One area in which NLP has still not been fully exploite d is information retrieval (IR).",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "70cb5a663001829e617d72fa1da83195500c030a",
    "url": "https://www.semanticscholar.org/paper/70cb5a663001829e617d72fa1da83195500c030a",
    "title": "Domain Specific Word Extraction from Hierarchical Web Documents: A First Step Toward Building Lexicon Trees from Web Corpora",
    "abstract": "Domain specific words and ontological information among words are important resources for general natural language applications. This paper proposes a statistical model for finding domain specific words (DSW s) in particular domains, and thus building the association among them. When applying this model to the hierarchical structure of the web directories node-by-node, the document tree can potentially be converted into a large semantically annotated lexicon tree. Some preliminary results show that the current approach is better than a conventional TF-IDF approach for measuring domain specificity. An average precision of 65.4% and an average recall of 36.3% are observed if the top-10% candidates are extracted as domain-specific words. 1 Domain Specific Words and Lexicon Trees as Important NLP Resources Domain specific words (DSW s) are important anchoring words for natural language processing applications that involve word sense disambiguation (WSD). It is appreciated that multi-sense words appearing in the same document tend to be tagged with the same word sense if they belong to the same common domain in the semantic hierarchy (Yarowsky, 1995). The existence of some DSW s in a document will therefore be a strong evidence of a specific sense for words within the document. For instance, the existence of basketball in a document would strongly suggest the sport sense of the word ( Pistons ), rather than its mechanics sense. It is also a personal belief that DSW-based sense disambiguation, document classification and many similar applications would be easier than sense-based models since sense-tagged documents are rare while domain-aware training documents are abundant on the Web. DSW identification is therefore an important issue. On the other hand, the semantics hierarchy among words (especially among sets of domain specific words) as well as the membership of domain specific words are also important resources for general natural language processing applications, since the hierarchy will provide semantic links and ontological information (such as is-A and part-of relationships) for words, and, domain specific words belonging to the same domain may have the synonym or antonym relationships. A hierarchical lexicon tree (or a network, in general) (Fellbaum, 1998; Jurafsky and Martin, 2000), indicative of sets of highly associated domain specific words and their hierarchy, is therefore invaluable for NLP applications. Manually constructing such a lexicon hierarchy and acquiring the associated words for each node in the hierarchy, however, is most likely unaffordable both in terms of time and cost. In addition, new words (or new usages of words) are dynamically produced day by day. For instance, the Chinese word (pistons) is more frequently used as the sport or basketball sense (referring to the Detroit",
    "venue": "International Joint Conference on Natural Language Processing",
    "citationCount": 23,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "bcad67bc8178b9c69433d650ad00d45289a69f60",
    "url": "https://www.semanticscholar.org/paper/bcad67bc8178b9c69433d650ad00d45289a69f60",
    "title": "Neural Coding Model of Associative Ontology with Up/Down State and Morphoelectrotonic Transform",
    "abstract": null,
    "venue": "ICANN",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0870ea550fda6773d2430bac840b7dcd05b78386",
    "url": "https://www.semanticscholar.org/paper/0870ea550fda6773d2430bac840b7dcd05b78386",
    "title": "A Machine Reader for the Semantic Web",
    "abstract": "FRED is a machine reading tool for converting text into internally well-connected and quality linked-data-ready ontologies in web-service-acceptable time. It implements a novel approach for ontology design from natural language sentences, combining Discourse Representation Theory (DRT), linguistic frame semantics, and Ontology Design Patterns (ODP). The current version of the tool includes Earmark-based markup, and enrichment with word sense disambiguation (WSD) and named entity resolution (NER) off-the-shelf components.",
    "venue": "SEMWEB",
    "citationCount": 13,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "aadabea7b251c6323d8f4f85e82961625d3435f9",
    "url": "https://www.semanticscholar.org/paper/aadabea7b251c6323d8f4f85e82961625d3435f9",
    "title": "A knowledge-based approach to ontology learning and semantic annotation",
    "abstract": "The so-called Semantic Web vision will certainly benefit from automatic semantic annotation of words in documents. We present a method, called structural semantic interconnections (SSI), that creates structural specifications of the possible senses for each word in a context, and selects the best hypothesis according to a grammar G, describing relations between sense specifications. The method has been applied to different semantic disambiguation problems, like automatic ontology construction, sensebased query expansion, disambiguation of words in glossary definitions. Evaluation experiments have been performed on each disambiguation task, as well",
    "venue": "CAiSE Workshops",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1934aa0203281728b76b5f17deff7992360bfbff",
    "url": "https://www.semanticscholar.org/paper/1934aa0203281728b76b5f17deff7992360bfbff",
    "title": "Multiontology Semantic Disambiguation in Unstructured Web Contexts",
    "abstract": "The ability of computers to automatically determine the right sense of words, according to the context where they appear, can help bridge the gap between syntax and semantics required for the full development of the Semantic Web. However, the applicability of these techniques is sometimes hampered by the unrestricted way in which humans annotate web resources, especially in folksonomies. In such cases many context words are useless (or even harmful) to determine the right meaning of another one. Furthermore, these contexts lack well-formed sentences, thus preventing syntactic analysis and other features exploited by traditional disambiguation techniques from being used. In this paper we propose a technique for intelligent context selection, based on semantic relatedness computation, to detect the set of words that could induce an effective disambiguation. We use this technique as starting point of a disambiguation process that receives an ambiguous keyword and its context words as input, and provides a list of possible senses for the keyword, scored according to the probability of being the intended one. It combines different techniques to operate: Web-based relatedness, overlap of semantic descriptions, and frequency of use of senses. It accesses any pool of online ontologies as source of word senses, in addition to other available resources. Both our context selection technique and disambiguation method have been tested in this work, obtaining promising results.",
    "venue": "",
    "citationCount": 11,
    "fieldsOfStudy": null
  },
  {
    "paperId": "cca70bfbe15a8aa0366e5127d99b78a5f6b1c6e7",
    "url": "https://www.semanticscholar.org/paper/cca70bfbe15a8aa0366e5127d99b78a5f6b1c6e7",
    "title": "Wordnet Wordsense Disambigioution using an Automatically Generated Ontology",
    "abstract": "In this paper we present a word sense disambiguation method in which ambiguous words are first disambiguated to senses from an automatically generated ontology, and from there mapped to Wordnet senses. We use the \u201dclustering by committee\u201d algorithm to automatically generate sense clusters given untagged text. The content of each cluster is used to map ambiguous words from those clusters to Wordnet senses. The algorithm does not require any training data, but we suspect that performance could be improved by supplementing the text to be disambiguated with untagged text from a similar source. We compare our algorithm to a similar disambiguation scheme that does not make use of automatically generated senses, as well as too an intermediate algorithm that makes use of the automatically generated semantic categories, but does not limit itself to the actual sense clusters. While what results we were able to gather show that the direct disambiguator outperforms our other two algorithms, there are a number of reasons not to give up hope in the approach.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": null
  },
  {
    "paperId": "1dcee66494852538a554091f41e87740a6fd32d8",
    "url": "https://www.semanticscholar.org/paper/1dcee66494852538a554091f41e87740a6fd32d8",
    "title": "Language Independent and Practical Ontology in Korean-Japanese Machine Translation Systems",
    "abstract": "This paper presents a semi-automatic ontology construction method using various resources, and an ontology-based word sense disambiguation method in machine translation. To acquire a reasonably practical ontology in limited time and with less manpower, we extend the Kadokawa thesaurus by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. The former can be obtained by converting valency information and case frames from previously built computational dictionaries used in machine translation. The latter can be acquired from concept co-occurrence information, which is extracted automatically from large corpora. The ontology stores rich semantic constraints among 1110 concepts, and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology. In practical machine translation systems, our word sense disambiguation method achieved a 6.0 per cent and 7.9 per cent improvement over methods that do not use an ontology for each Japanese and Korean translation.",
    "venue": "Lit. Linguistic Comput.",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "52655a581ce2468ab28dbf56df0ca32cbf096fef",
    "url": "https://www.semanticscholar.org/paper/52655a581ce2468ab28dbf56df0ca32cbf096fef",
    "title": "An ontology-based approach in the literary research: two case-studies",
    "abstract": "The present paper suggests an application of ontologies in the analysis of a literary phenomenon. Starting from the needs of humanistic research and from the availibility on the Web of Language Resources, the adoption of such methodology, whose reference tool is an ontology, has led to a deeper analysis of the \u201cDualism Truth vs. Propaganda\u201d in Karl Kraus\u2019s writings and of the novel \u201cCloud, castle and lake\u201d of Vladimir Nabokov. The aim of this paper is to put in evidence the role of ontologies in the literary research and text analysis as helping tools, able to make domain assumptions explicit. For this purpose, the two case-studies will show how the application of the WordNet ontology for word sense disambiguation can help the understanding of a literary phenomenon (in the first example) and the interpretation of the core of a novel (in the second example), thus facilitating the translation.",
    "venue": "LREC",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "8fda2387cf7ce4a5c43333de4d92e524e40bbf36",
    "url": "https://www.semanticscholar.org/paper/8fda2387cf7ce4a5c43333de4d92e524e40bbf36",
    "title": "An Extensible Meta-Modelling Assistant",
    "abstract": "Meta-models play a pivotal role in Model-Driven Engineering (MDE). They are used to create domain-specific models, and to type model management operations like model transformations or code generators. However, even though creating meta-models is a common activity, it is currently mostly a manual activity, which does not profit from existing knowledge. In order to facilitate the meta-modelling task, we propose an extensible meta-modelling assistant. While primarily focussed on helping in the creation of meta- models, it can also help in creating models. The assistant permits the provision of heterogeneous data description sources (like ontologies, RDF data, XML schemas, database schemas and meta-models), and enables their uniform querying. Different kinds of queries are supported, and improved through synonym search. Query results are prioritized through sense disambiguation, can be graphically visualized, and incorporated into the (meta-)model being built. The assistant has been realized within Eclipse, and its architecture has been designed to be independent of the meta-modelling technology used. As a proof- of-concept, we show its integration within DSL-tao, a pattern-based meta-modelling tool built by our group, and two other tools developed by third-parties. The usefulness of the system is illustrated with a running example in the process modelling domain.",
    "venue": "IEEE International Enterprise Distributed Object Computing Conference",
    "citationCount": 16,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a9cf262ee51004890878f84962727e64ba6511ae",
    "url": "https://www.semanticscholar.org/paper/a9cf262ee51004890878f84962727e64ba6511ae",
    "title": "Combined Lesk-Based Method for Words Senses Disambiguation",
    "abstract": "Word sense disambiguation (WSD) is a linguistically-based mechanism for defining automatically the correct sense of a word in the context. Lesk-based methods use context information for WSD. In this paper, we propose a method of word sense disambiguation based on the combination of the original Lesk method and the simplified one with additional application of large lexical resources, like synonym dictionaries, ontologies, etc., for calculations of sense plausibility. We present experimental results that show that our method has better precision than the baseline Lesk-based methods",
    "venue": "2006 15th International Conference on Computing",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6743c210ffd0a828bb47379c6c22c9b0f905269e",
    "url": "https://www.semanticscholar.org/paper/6743c210ffd0a828bb47379c6c22c9b0f905269e",
    "title": "TaxoLearn: A Semantic Approach to Domain Taxonomy Learning",
    "abstract": "Building domain taxonomies is a crucial task in the domain of ontology construction. Domain taxonomy learning keeps getting more important as a form of automatically obtaining a knowledge representation of a certain domain. The alternative of manually developing domain taxonomies is not trivial. The main issues encountered when manually developing a taxonomy are the non-availability of a domain knowledge expert and the considerable amount of effort needed for this task. This paper proposes Taxo Learn, an approach to automatic construction of domain taxonomies. Taxo Learn is a new methodology that combines aspects from existing approaches, but also contains new steps in order to improve the quality of the resulted domain taxonomy. The contribution of this paper is threefold. First, we employ a word sense disambiguation step when detecting concepts in the text. Second, we show the use of semantics-based hierarchical clustering for the purpose of taxonomy learning. Third, we propose a novel dynamic labeling procedure for the concept clusters. We evaluate our approach by comparing the machine generated taxonomy with a manually constructed golden taxonomy. Based on a corpus of documents in the field of financial economics, Taxo Learn shows a high precision for the learned taxonomic concept relationships.",
    "venue": "2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology",
    "citationCount": 21,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "55a2a4d9e585bb2829a316e96e76f6a62bf5cf3b",
    "url": "https://www.semanticscholar.org/paper/55a2a4d9e585bb2829a316e96e76f6a62bf5cf3b",
    "title": "Automatic Conversion of Natural Language to 3D Animation",
    "abstract": "The purpose of this research is to investigate the process of mental imagery from a computational perspective, employing theories and resources from linguistics, natural language processing, and computer graphics about human language visualisation. This thesis presents our progress toward the automatic creation of 3D animation from natural language text. Lexical Visual Semantic Representation (LVSR) is proposed, which connects linguistic semantics to the visual semantics and is suitable for action execution (animation). We investigate visual semantics of verbs, and introduce the notion of visual valency which is used as a primary criterion to construct a visual semantic based ontology. The visual valency approach is a framework for modelling deeper semantics of verbs. Lexicon-based approaches used for word sense disambiguation are also discussed. The context and the senses of the ambiguous verb are analysed using hypernymy relations and word frequency information in WordNet and thematic roles in LCS (Lexical Conceptual Structure) database. The significance of this research is also related to an animation blending approach which combines precreated and dynamically generated animation facilities into a unified mechanism and an object-oriented object modelling approach for decentralising the control of animation engine. An intelligent storytelling system called CONFUCIUS, which visualizes single sentences into 3D animation, speech, and sound effects, has been implemented in Java and VRML. CONFUCIUS is an overall framework of language visualisation, using computer graphics techniques with NLP to achieve high-level animation generation. We conducted an evaluation experiment where subjects were asked to complete a questionnaire either rating agreement for the generated animation or selecting the closest text from four candidates which describes the animation best. The results show a low error rate of comprehension measures of animation (8.33%) and 3.82 average agreement score. We also evaluated the syntactic parsing by test-suite based diagnostic evaluation, and anaphora resolution and semantic analysis by corpus-based adequacy evaluation. CONFUCIUS gives promising results on word sense disambiguation (70% accuracy) with regard to the dataset it is tested on. Future work is suggested for extending the knowledge base and improving commonsense reasoning from lexicons to present more verb classes, extending language visualisation to discourse level, and applying physics, such as dynamics and kinematics, to 3D animation.",
    "venue": "",
    "citationCount": 57,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "50f0ad64c8cd845f0260859cacc40d1441fddb1e",
    "url": "https://www.semanticscholar.org/paper/50f0ad64c8cd845f0260859cacc40d1441fddb1e",
    "title": "ITRI-97-12 \u201d I don \u2019 t believe in word senses \u201d",
    "abstract": "Word sense disambiguation assumes word senses. Within the lexicography and linguistics literature, they are known to be very slippery entities. The paper looks at problems with existing accounts of \u2018word sense\u2019 and describes the various kinds of ways in which a word\u2019s meaning can deviate from its core meaning. An analysis is presented in which word senses are abstractions from clusters of corpus citations, in accordance with current lexicographic practice. The corpus citations, not the word senses, are the basic objects in the ontology. The corpus citations will be clustered into senses according to the purposes of whoever or whatever does the clustering. In the absence of such purposes, word senses do not exist. Word sense disambiguation also needs a set of word senses to disambiguate between. In most recent work, the set has been taken from a general-purpose lexical resource, with the assumption that the lexical resource describes the word senses of English/French/. . . , between which NLP applications will need to disambiguate. The implication of the paper is, by contrast, that word senses exist only relative to a task.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "794ca5bd78e38dafb89154807b1613b88fa788b2",
    "url": "https://www.semanticscholar.org/paper/794ca5bd78e38dafb89154807b1613b88fa788b2",
    "title": "What lexical sets tell us about conceptual categories",
    "abstract": "It is common practice in computational linguistics to attempt to use selectional constraints and semantic type hierarchies as primary knowledge resources to perform word sense disambiguation (cf. Jurafsky and Martin 2000). The most widely adopted methodology is to start from a given ontology of types (e.g. Wordnet, cf. Miller and Fellbaum 2007) and try to use its implied conceptual categories to specify the combinatorial constraints on lexical items. Semantic Typing information about selectional preferences is then used to guide the induction of senses for both nouns and verbs in texts. Practical results have shown, however, that there are a number of problems with such an approach. For instance, as corpus-driven pattern analysis shows (cf. Hanks et al. 2007), the paradigmatic sets of words that populate specific argument slots within the same verb sense do not map neatly onto conceptual categories, as they often include words belonging to different types. Also, the internal composition of these sets changes from verb to verb, so that no stable generalization seems possible as to which lexemes belong to which semantic type (cf. Hanks and Jezek 2008). In this paper, we claim that these are not accidental facts related to the contingencies of a given ontology, but rather the result of an attempt to map distributional language behaviour onto semantic type systems that are not sufficiently grounded in real corpus data. We report the efforts done within the CPA project (cf. Hanks 2009) to build an ontology which satisfies such requirements and explore its advantages in terms of empirical validity over more speculative ontologies.",
    "venue": "",
    "citationCount": 35,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4fe52c600e8298ab11d18c65968eb1feecc4f231",
    "url": "https://www.semanticscholar.org/paper/4fe52c600e8298ab11d18c65968eb1feecc4f231",
    "title": "Mapping Natural Language Labels to Structured Web Resources",
    "abstract": "Mapping natural language terms to a Web knowledge base enriches information systems without additional context, with new relations and properties from the Linked Open Data. In this paper we formally define such task, which is related to word sense disambiguation, named entity recognition and ontology matching. We provide a manually annotated dataset of labels linked to DBpedia as a gold standard for evaluation, and we use it to experiment with a number of methods, including a novel algorithm that leverages the specific characteristics of the mapping task. The empirical evidence confirms that general term mapping is a hard task, that cannot be easily solved by applying existing methods designed for related problems. However, incorporating NLP ideas such as representing the context and a proper treatment of multiword expressions can significantly boost the performance, in particular the coverage of the mapping. Our findings open up the challenge to find new ways of approaching term mapping to Web resources and bridging the gap between natural language and the Semantic Web.",
    "venue": "NL4AI@AI*IA",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9dad22621cdeb89bc2c67ed61db5b275e8c92497",
    "url": "https://www.semanticscholar.org/paper/9dad22621cdeb89bc2c67ed61db5b275e8c92497",
    "title": "Tourist review analytics using complex networks",
    "abstract": "A number of techniques for Natural Language Processing (shortly, NLP) based on graph representations were developed. Usually they target a specific NLP task, such as: text summarisation, syntactic parsing, word sense disambiguation, ontology construction, sentiment and subjectivity analysis, or text clustering. In this paper we explore complex network representation of tourist reviews for extracting lexical and quantitative features of the review text. The most important contribution of our proposal consists of defining a new method for keywords extraction using Complex Network ranking metrics.",
    "venue": "Balkan Conference in Informatics",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a07c3768cc40c78c5688855b38c74c7c244f6313",
    "url": "https://www.semanticscholar.org/paper/a07c3768cc40c78c5688855b38c74c7c244f6313",
    "title": "Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008, Aarhus, Denmark, September 17-19, 2008, Revised Selected Papers",
    "abstract": null,
    "venue": "Conference and Labs of the Evaluation Forum",
    "citationCount": 114,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4e2552c07848f96d51ced711f3a15542453428ce",
    "url": "https://www.semanticscholar.org/paper/4e2552c07848f96d51ced711f3a15542453428ce",
    "title": "Identifying Word Senses in Greek Text: A comparison of machine learning methods",
    "abstract": "In this paper we perform a comparative evaluation of machine learning methods on the task of identifying the correct sense of a word, based on the context in which it appears. This task is known as word sense disambiguation (WSD) and is one of the hardest and most interesting issues in language engineering. Research on the use of machine learning techniques for WSD has so far focused almost exclusively on English words, due to the scarcity of the required linguistic resources for other languages. The work presented here is the first attempt to apply machine learning methods to Greek words. We have constructed a semantically tagged corpus for two Greek words: a noun with clearly distinguishable senses and a verb with overlapping senses. This corpus is used to evaluate four different machine learning methods and three different representations of the context of the ambiguous word. Our results show that the simple na\u00efve Bayesian classifier and a method using Support Vector Machines outperform decision tree induction, even with the use of boosting. Furthermore, the use of a distance-based weighting function for the context of the ambiguous word does not seem to have a substantial effect on the performance of the methods.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": null
  },
  {
    "paperId": "649161055bb9e4101bbaa926594faa7f023da559",
    "url": "https://www.semanticscholar.org/paper/649161055bb9e4101bbaa926594faa7f023da559",
    "title": "Using BabelNet in Bridging the Gap Between Natural Language Queries and Linked Data Concepts",
    "abstract": "Many semantic search tool evaluations have reported a user preference for free natural language as a query input approach as opposed to controlled or view-based inputs. Although the exibility offered by this approach is a significant advantage, it can also be a major difficulty. Allowing users complete freedom in the choice of terms increases the difficulty for the search tools to match terms with the underlying data. This causes either a mismatch which affects precision, or a missing match which affects recall. In this paper, we present an empirical investigation on the use of named entity recognition, word sense disambiguation, and ontology-based heuristics in an approach attempting to bridge this gap between user terms and ontology concepts, properties and entities. We use the dataset provided by the Question Answering over Linked Data (QALD-2) workshop in our analysis and tests.",
    "venue": "NLP-DBPEDIA@ISWC",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e4383a0bab8a3459383be5d57820bc802abc86a4",
    "url": "https://www.semanticscholar.org/paper/e4383a0bab8a3459383be5d57820bc802abc86a4",
    "title": "Extracting Hidden Sense Probabilities from Bitexts",
    "abstract": "We propose a probabilistic model that is inspired by Diab & Resnik\u2019s algorithm to extract disambiguation information from aligned bilingual texts. Like Diab & Resnik\u2019s, the proposed model uses WordNet and the fact that word ambiguities are not always the same in the two languages. The generative model introduces a dependency between two translated words through a common ancestor in WordNet\u2019s ontology. Unlike Diab & Resnik\u2019s algorithm it does not suppose that the translation in the source language has a single meaning.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a29bac6cc7ecfec46eac697328db969b4f6ee849",
    "url": "https://www.semanticscholar.org/paper/a29bac6cc7ecfec46eac697328db969b4f6ee849",
    "title": "Knowledge Organisation Systems for Chatbots and Conversational Agents: A Review of Approaches and an Evaluation of Relative Value-Added for the User",
    "abstract": "Chatbots and smart digital assistants usually rely on some form of knowledge base for providing a source of answers to user queries. These may be more or less structured, ranging from free text, keyword search to interconnected knowledge graphs where answers are surfaced through structured query and/or dynamic logics. This paper will review the state of the art and probe the added value to the end user of using more structured KOSs as the knowledge base component of agent architectures. In addition to the extant literature, experimental development of a chat bot for student information based on a partial knowledge graph is described. A key conclusion is that ontology in a broad sense can be used to drive dialogue as much as to derive factually correct or organisationally-approved responses. 1.0 Introduction Text and speech-based dialogue agents are growing in popularity as a new or complementary interface to organisational systems and services. They offer high availability, convenience and the potential to identify, triage and satisfy common queries and interactions. As a window into an organisation\u2019s knowledge it seems natural that knowledge organisation systems (KOSs) should play a major role in supporting and directing exchanges, though this is a more sophisticated role than they usually play. For the sake of interoperability and portability, organisations have often erred on the side of simplicity and relatively semantically shallow KOSs to support discovery (Isaac and Baker 2015). However, this level of power is neglecting a wealth of experience in the implementation of Knowledge Representation and Knowledge Engineering coming from the computer science / AI disciplines (Giunchiglia, Dutta, and Maltese 2014). These somewhat deeper approaches allow the formal expression of conceptual structures and their interrelation, as well as the application of automated logical inference over a system\u2019s knowledge base to arrive at evidence-based conclusions. These affordances once again become potentially useful when it comes to conversational agents, so it is a good time to revisit standards and tools developed for knowledge representation and apply them to chatbots (Cameron et al. 2018) A similar driver has been the developments in natural language interfaces, where some progress has been made in mapping unstructured queries to formal data relations. Indeed, there was a call to action at ISKO 2004 for better connection between KO and computational linguistics (Jones, Cunliffe, and Tudhope 2004). Jone\u2019s et al\u2019s exploratory work showed how term disambiguation and the mapping of user queries to KO structures could be improved by using a general purpose thesaurus and expanding matching to include thesaurus scope notes. The application of KOS to conversational interfaces requires an active approach and a pragmatic view of semantics (as the concepts are action-oriented and application-specific) in addition to a unification of user and organisational warrant (Hj\u00f8rland 2007), as \u00a9 Ergon ein Verlag in der Nomos Verlagsgesellschaft",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "414d8017e6364d30446b39811c6a6573d9ce71ba",
    "url": "https://www.semanticscholar.org/paper/414d8017e6364d30446b39811c6a6573d9ce71ba",
    "title": "A Semantic Web-Based Approach for Building Personalized News Services",
    "abstract": "This article proposes Hermes, a Semantic Web-based framework for building personalized news services. It makes use of ontologies for knowledge representation, natural language processing techniques for semantic text analysis, and semantic query languages for specifying wanted information. Hermes is supported by an implementation of the framework, the Hermes News Portal, a tool which allows users to have a personalized online access to news items. The Hermes framework and its associated implementation aim at advancing the state-of-the-art of semantic approaches for personalized news services by employing Semantic Web standards, exploiting domain information, using a word sense disambiguation procedure, and being able to express temporal constraints for the desired news items.",
    "venue": "Int. J. E Bus. Res.",
    "citationCount": 93,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1b8a5eae3031acb666c21ee353cbcd9289efde63",
    "url": "https://www.semanticscholar.org/paper/1b8a5eae3031acb666c21ee353cbcd9289efde63",
    "title": "Shaping Wikipedia into a Computable Knowledge Base",
    "abstract": "Wikipedia is arguably the most important information source yet invented for natural language processing (NLP) and artificial intelligence, in addition to its role as humanity's largest encyclopedia. Wikipedia is the principal information source for such prominent services as IBM's Watson [1], Freebase [2], the Google Knowledge Graph [3], Apple's Siri [4], YAGO [5], and DBpedia [6], the core reference structure for linked open data [7]. Wikipedia information has assumed a prominent role in NLP applications in word sense disambiguation, named entity recognition, co-reference resolution, and multi-lingual alignments; in information retrieval in query expansion, multi-lingual retrieval, question answering, entity ranking, text categorization, and topic indexing; and in semantic applications in topic extraction, relation extraction, entity extraction, entity typing, semantic relatedness, and ontology building [8].",
    "venue": "",
    "citationCount": 5,
    "fieldsOfStudy": null
  },
  {
    "paperId": "d504fd98830fdc4a66e2b1e7c39d516e090f9d06",
    "url": "https://www.semanticscholar.org/paper/d504fd98830fdc4a66e2b1e7c39d516e090f9d06",
    "title": "Part-of-Speech Tagging by Latent Analogy",
    "abstract": "Part-of-speech tagging is often a critical first step in various speech and language processing tasks. High-accuracy taggers (e.g., based on conditional random fields) rely on well chosen feature functions to ensure that important characteristics of the empirical training distribution are reflected in the trained model. This makes them vulnerable to any discrepancy between training and tagging corpora, and, in particular, accuracy is adversely affected by the presence of out-of-vocabulary words. This paper explores an alternative tagging strategy based on the principle of latent analogy, which was originally introduced in the context of a speech synthesis application. In this approach, locally optimal tag subsequences emerge automatically from an appropriate representation of global sentence-level information. This solution eliminates the need for feature engineering, while exploiting a broader context more conducive to word sense disambiguation. Empirical evidence suggests that, in practice, tagging by latent analogy is essentially competitive with conventional Markovian techniques, while benefiting from substantially less onerous training costs. This opens up the possibility that integration with such techniques may lead to further improvements in tagging accuracy.",
    "venue": "IEEE Journal of Selected Topics in Signal Processing",
    "citationCount": 29,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0ea706a471fc622260d4abd8dd8104ecf34029c0",
    "url": "https://www.semanticscholar.org/paper/0ea706a471fc622260d4abd8dd8104ecf34029c0",
    "title": "Potential Uses in Breadth",
    "abstract": null,
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "7d355a37bc1266f25b7144565fd118863b65d7a2",
    "url": "https://www.semanticscholar.org/paper/7d355a37bc1266f25b7144565fd118863b65d7a2",
    "title": "Table of Contents.",
    "abstract": "On Semantic Classification of Modifiers p. 172 Evaluating a Spelling Support in a Search Engine p. 183 Opening Statistical Translation Engines to Terminological Resources p. 191 User-Centred Ontology Learning for Knowledge Management p. 203 A Multilevel Text Processing Model of Newsgroup Dynamics p. 208 Best Feature Selection for Maximum Entropy-Based Word Sense Disambiguation p. 213 Linguistics in Large-Scale Web Search p. 218 Similarity Model and Term Association for Document Categorization p. 223 Omnibase: Uniform Access to Heterogeneous Data for Question Answering p. 230 Automated Question Answering Using Question Templates That Cover the Conceptual Model of the Database p. 235",
    "venue": "The FEBS Journal",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Medicine"
    ]
  },
  {
    "paperId": "8fb8e7796a726d8bccdb26fc90d0639c99c1aaf8",
    "url": "https://www.semanticscholar.org/paper/8fb8e7796a726d8bccdb26fc90d0639c99c1aaf8",
    "title": "Evaluating the Semantic Web: A Task-Based Approach",
    "abstract": null,
    "venue": "ISWC/ASWC",
    "citationCount": 47,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "296f491a575ce9a2f4acfbdac08ac6b313dc1ee6",
    "url": "https://www.semanticscholar.org/paper/296f491a575ce9a2f4acfbdac08ac6b313dc1ee6",
    "title": "Ontoilper: an ontology- and inductive logic programming-based method to extract instances of entities and relations from texts",
    "abstract": "Information Extraction (IE) consists in the task of discovering and structuring information found in a semi-structured or unstructured textual corpus. Named Entity Recognition (NER) and Relation Extraction (RE) are two important subtasks in IE. The former aims at finding named entities, including the name of people, locations, among others, whereas the latter consists in detecting and characterizing relations involving such named entities in text. Since the approach of manually creating extraction rules for performing NER and RE is an intensive and time-consuming task, researchers have turned their attention to how machine learning techniques can be applied to IE in order to make IE systems more adaptive to domain changes. As a result, a myriad of state-of-the-art methods for NER and RE relying on statistical machine learning techniques have been proposed in the literature. Such systems typically use a propositional hypothesis space for representing examples, i.e., an attribute-value representation. In machine learning, the propositional representation of examples presents some limitations, particularly in the extraction of binary relations, which mainly demands not only contextual and relational information about the involving instances, but also more expressive semantic resources as background knowledge. This thesis attempts to mitigate the aforementioned limitations based on the hypothesis that, to be efficient and more adaptable to domain changes, an IE system should exploit ontologies and semantic resources in a framework for IE that enables the automatic induction of extraction rules by employing machine learning techniques. In this context, this thesis proposes a supervised method to extract both entity and relation instances from textual corpora based on Inductive Logic Programming, a symbolic machine learning technique. The proposed method, called OntoILPER, benefits not only from ontologies and semantic resources, but also relies on a highly expressive relational hypothesis space, in the form of logical predicates, for representing examples whose structure is relevant to the information extraction task. OntoILPER automatically induces symbolic extraction rules that subsume examples of entity and relation instances from a tailored graph-based model of sentence representation, another contribution of this thesis. Moreover, this graph-based model for representing sentences also enables the exploitation of domain ontologies and additional background knowledge in the form of a condensed set of features including lexical, syntactic, semantic, and relational ones. Differently from most of the IE methods (a comprehensive survey is presented in this thesis, including the ones that also apply ILP), OntoILPER takes advantage of a rich text preprocessing stage which encompasses various shallow and deep natural language processing subtasks, including dependency parsing, coreference resolution, word sense disambiguation, and semantic role labeling. Further mappings of nouns and verbs to (formal) semantic resources are also considered. OntoILPER Framework, the OntoILPER implementation, was experimentally evaluated on both NER and RE tasks. This thesis reports the results of several assessments conducted using six standard evaluation corpora from two distinct domains: news and biomedical. The obtained results demonstrated the effectiveness of OntoILPER on both NER and RE tasks. Actually, the proposed framework outperforms some of the state-of-the-art IE systems compared in this thesis.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ]
  },
  {
    "paperId": "192d084cd6d0b63dbbe56c3813b08e83cdaec5d9",
    "url": "https://www.semanticscholar.org/paper/192d084cd6d0b63dbbe56c3813b08e83cdaec5d9",
    "title": "Using Spreading Activation to Evaluate and Improve Ontologies",
    "abstract": "In this paper, we explore the relationship between the human-encoded semantics of ontologies and their application to natural language processing (NLP) tasks, such as word-sense disambiguation (WSD), for which such ontologies may not have been originally designed. We present a method for assessing the semantic content of an ontology with respect to a target domain, by spreading activation over a graph that represents instances of ontology concepts and relationships, in domain text. Our proposed method has several advantages beyond existing ontology metrics. By identifying bias or imbalance in the ontology, we can suggest target areas for improvement, and simultaneously facilitate the automated optimisation of the graph for use in the chosen NLP task. On applying this method to the Unified Medical Language System (UMLS) ontology, we significantly outperformed existing graph-based methods for WSD in biomedical NLP (0.82 accuracy). The subsequent introduction of a fall-back mechanism, using word-sense probability, achieved state of the art for unsupervised biomedical WSD (0.89 accuracy).",
    "venue": "International Conference on Computational Linguistics",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "18ef67f1ba0b5afcdc789edeb3a9f08c8554d634",
    "url": "https://www.semanticscholar.org/paper/18ef67f1ba0b5afcdc789edeb3a9f08c8554d634",
    "title": "TELIX: An RDF-Based Model for Linguistic Annotation",
    "abstract": null,
    "venue": "Extended Semantic Web Conference",
    "citationCount": 15,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "2672f4539b01cdcf6ce121c154da54f843adfb6e",
    "url": "https://www.semanticscholar.org/paper/2672f4539b01cdcf6ce121c154da54f843adfb6e",
    "title": "Popularity Based Web Service Search",
    "abstract": "Web service discovery is a main challenge despite the enhanced proposed methods based on information retrieval techniques (word sense disambiguation, stemming, etc.), domain knowledge and ontology. Unfortunately, the proposed approaches are, however, complex in practice. Despite the addition of extra information to WSDL documents, discovering a required web service looks like finding a needle in a haystack. Thus, we propose in this paper a seamless way to discover a more appropriate web service independently of its type (simple, composed or semantic). In fact, we consider the usage of the service and more particularly, users' requirements, and we invoke it using a generated GUI. This approach concerning a Web service search engine based on popularity is implemented and validated by performing an experiment.",
    "venue": "IEEE International Conference on Web Services",
    "citationCount": 12,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e2b6e3e5f2f7b2244585c2c4e8a88fc98043395c",
    "url": "https://www.semanticscholar.org/paper/e2b6e3e5f2f7b2244585c2c4e8a88fc98043395c",
    "title": "Lexicography and natural language processing",
    "abstract": "The chapter introduces links between lexicography and natural\nlanguage processing in both directions - first, how the\nlexicographic work benefits from techniques of natural language\nprocessing such as word sense disambiguation, semantic\nsimilarity measures, named entity recognition, relation\nextraction, or corpora good example search, and second,\nintensive exploitation of lexicographic data in language\nengineering technologies, in the form of general dictionaries\nor encyclopaedias as well as specialized lexicons, ontologies,\nor lexicographic databases.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "77d57c2a63f44f8956be3f493a42376969ea9533",
    "url": "https://www.semanticscholar.org/paper/77d57c2a63f44f8956be3f493a42376969ea9533",
    "title": "Enriching a lexical semantic net with selectional preferences by means of statistical corpus analysis",
    "abstract": "Broad-coverage ontologies which represent lexical semantic knowledge are being built for more and more natural languages. Such resources provide very useful information for word sense disambiguation, which is crucial for a variety of NLP tasks (e.g. semantic annotation of corpora, information retrieval, or semantic inferencing). Since the manual encoding of such ontologies is very labour-intensive, the development of (semi-)automatic methods for acquiring lexical semantic information is an important task. This paper addresses the automatic acquisition of selectional preferences of verbs by means of statistical corpus analysis. Knowledge about such preferences is essential for inducing thematic relations, which link verbal concepts to nominal concepts that are selectionally preferred as their complements. Several approaches for learning selectional preferences from corpora have been proposed in the last years. However, their usefulness for ontology building is limited. This paper introduces a modification of one of these methods (i.e. the approach of Li & Abe [1]) and evaluates it by employing a gold standard. The results show that the modified approach is much more appropriate for the given task.",
    "venue": "ECAI Workshop on Ontology Learning",
    "citationCount": 54,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "ca7f6b91ef1e8631bdf7679ed123847cbfb8f232",
    "url": "https://www.semanticscholar.org/paper/ca7f6b91ef1e8631bdf7679ed123847cbfb8f232",
    "title": "Linguistically Motivated Ontology-Based Information Retrieval",
    "abstract": "When Tim Berners-Lee proposed his vision of the Semantic Web in 2001, he thought of machines that automatically execute specific tasks based on available knowledge. The knowledge should be captured within ontologies which provide an unambiguous and semantically rich way to capture information. The information could further be used to enhance tasks like information retrieval, i.e., the retrieval of documents which match specific criteria. Over a decade later, technologies which are required for the Semantic Web have been established in several areas, e.g., the biological and medical domains. Both share a very constant pool of knowledge, which does not change as rapidly as in other domains, i.e., neither a lot of new knowledge must be added continuously nor the existing knowledge has to be updated very often. These circumstances make both domains suitable for manually creating ontologies. However, in case of a domain with constantly incoming new knowledge, it would be a great advantage if this knowledge could automatically be added or matched to an ontology. However, there is nearly no concept available on how ontological knowledge can be mapped to natural language precisely. We therefore developed the SE-DSNL approach. It provides experts with the ability to specify how ontological knowledge can be mapped to linguistic information of any known language. The concept provides a flexible and generic meta model which captures all the relevant information. In order to use this for parsing natural language text a prototypical implementation has been developed which takes the information of a SEDSNL model and applies it to a given input text. The result is a semantic interpretation of the input text which maps its lexical and syntactic elements to the ontology. The direct integration of semantic and linguistic information further allows using the semantic information at runtime. This yields certain advantages which are demonstrated by treating elaborate linguistic phenomena like pronominal anaphora resolution, word sense disambiguation, vagueness and reference transfer. To show the validity of the approach it has been evaluated using scenarios and two case studies.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "da85c944c65c3e062de08acea7c6ca89b2b9f133",
    "url": "https://www.semanticscholar.org/paper/da85c944c65c3e062de08acea7c6ca89b2b9f133",
    "title": "A Semantic Search Algorithm for Ontology Matching",
    "abstract": "Most of the ontology alignment tools use terminological techniques as the initial step and then apply the structural techniques to refine the results. Since each terminological similarity measure considers some features of similarity, ontology alignment systems require exploiting different measures. While a great deal of effort has been devoted to developing various terminological similarity measures and also developing various ontology alignment systems, little attention has been paid to develop similarity search algorithms which exploit different similarity measures in order to gain benefits and avoid limitations. We propose a novel terminological search algorithm which tries to find an entity similar to an input search string in a given ontology. This algorithm extends the search string by creating a matrix from its synonym and hypernyms. The algorithm employs and combines different kind of similarity measures in different situation to achieve a higher performance, accuracy, and stability in compare to previous methods which either use one measure or combine more measures in a naive ways such as averaging. We evaluated the algorithm using a subset of OAEI Bench mark data set. Results showed the superiority of proposed algorithm and effectiveness of different applied techniques such as word sense disambiguation and semantic filtering mechanism.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": null
  },
  {
    "paperId": "7dfc14cb073253f6cd4472bd31a221832e5cd3cb",
    "url": "https://www.semanticscholar.org/paper/7dfc14cb073253f6cd4472bd31a221832e5cd3cb",
    "title": "Enriching WordNet concepts with topic signatures",
    "abstract": "This paper explores the possibility of enriching the content of existing ontologies. The overall goal is to overcome the lack of topical links among concepts in WordNet. Each concept is to be associated to a topic signature, i.e., a set of related words with associated weights. The signatures can be automatically constructed from the WWW or from sense-tagged corpora. Both approaches are compared and evaluated on a word sense disambiguation task. The results show that it is possible to construct clean signatures from the WWW using some filtering techniques.",
    "venue": "ArXiv",
    "citationCount": 109,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "05a5cd2a1df104ca0a07f48e7cdd72a04eb13892",
    "url": "https://www.semanticscholar.org/paper/05a5cd2a1df104ca0a07f48e7cdd72a04eb13892",
    "title": "Concept Based Information Retrieval from Text Documents",
    "abstract": "This research is intended to develop a concept based information retrieval system for text documents in two phases: Therefore, this idea motivated us to develop a concept based information retrieval system for text documents. This system tries to provide additional semantics as conceptually related words with the help of glosses to the query words and keywords in the documents by disambiguating their meanings. Here, various senses provided by WSD algorithm have been used as semantics for indexing the documents to aid the information retrieval system. Later, this research has also been motivated to do ontology based information retrieval from Tamil text documents which improve the retrieval performance in a better way due to the incorporation of domain semantics. Here, the performance of IR has been improved by including more indexing information about the documents such as associated meaning with the words. The Word Sense Disambiguation is the process of finding correct senses of a word, among other senses associated with the words. The introduction of semantics in word level to improve Word Senses Disambiguation has been considered in this thesis specifically to improve the accuracy of WSD and thus in turn to improve the IR performance. In this work, the glosses of the indexed words in WordNet are utilized as conceptual information, which acts as an additional semantics for WSD. This concept based WSD, has been used in semantic chaining to cluster documents, which is used for IR performance.",
    "venue": "",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e475338979f8bae5a18495aa4d3e59599011d330",
    "url": "https://www.semanticscholar.org/paper/e475338979f8bae5a18495aa4d3e59599011d330",
    "title": "Exploiting semantics for filtering and searching knowledge in a software development context",
    "abstract": null,
    "venue": "Knowledge and Information Systems",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "318f6eb302d31238cd27041c4557c0630899c07d",
    "url": "https://www.semanticscholar.org/paper/318f6eb302d31238cd27041c4557c0630899c07d",
    "title": "A hybrid approach for extracting semantic relations from texts",
    "abstract": "We present an approach for extracting relations from texts that exploits linguistic and empirical strategies, by means of a pipeline method involving a parser, partof-speech tagger, named entity recognition system, pattern-based classification and word sense disambiguation models, and resources such as ontology, knowledge base and lexical databases. The relations extracted can be used for various tasks, including semantic web annotation and ontology learning. We suggest that the use of knowledge intensive strategies to process the input text and corpusbased techniques to deal with unpredicted cases and ambiguity problems allows to accurately discover the relevant relations between pairs of entities in that text.",
    "venue": "OntologyLearning@COLING/ACL",
    "citationCount": 37,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "fcbc8c412b0b6cda6eef918b40750d7dcf61b0bf",
    "url": "https://www.semanticscholar.org/paper/fcbc8c412b0b6cda6eef918b40750d7dcf61b0bf",
    "title": "From Word Alignment to Word Senses , via Multilingual Wordnets",
    "abstract": "Most of the successful commercial applications in language processing (text and/or speech) dispense of any explicit concern on semantics, with the usual motivations stemming from the computational high costs required, in case of large volumes of data, for dealing with semantics. With recent advances in corpus linguistics and statistical-based methods in NLP, revealing useful semantic features of linguistic data is becoming cheaper and cheaper and the accuracy of this process is steadily improving. Lately, there seems to be a growing acceptance of the idea that multilingual lexical ontologies might be the key towards aligning different views on the semantic atomic units to be used in characterizing the general meaning of various and multilingual documents. Depending on the granularity at which semantic distinctions are necessary, the accuracy of the basic semantic processing (such as word sense disambiguation) can be very high with relatively low complexity computing. The paper substantiates this statement by presenting a statistical/based system for word alignment and word sense disambiguation in parallel corpora. We describe a word alignment platform which ensures text pre-processing (tokenization, POS-tagging, lemmatization, chunking, sentence and word alignment) as required by an accurate word sense disambiguation. 1 The Pervasive Ambiguity of Natural Languages Most difficult problems in natural language processing stem from the inherent ambiguous nature of the human languages. Ambiguity is present at all levels of traditional structuring of a language system (phonology, morphology, lexicon, syntax, semantics) and not dealing with it at the proper level, exponentially increases the complexity of the problem solving. Currently, the state of the art taggers (combining various models, strategies and processing tiers) ensure no less than 97-98% accuracy in the process of morpho-lexical full disambiguation. For such taggers a 2-best tagging 1 is practically 100% correct. 1 In k-best tagging, instead of assigning each word exactly one tag (the most probable in the given context), it is allowed to have occasionally at most k-best tags attached to a word and if the correct tag is among the k-best tags, the annotation is considered to be correct. One further step is the word sense disambiguation (WSD) process. In the traditional compositional semantics, the meaning of a complex expression is supposed to be derivable from the meanings of its parts, and the way in which those parts are combined. Depending on the representation formalisms for the word-meaning representation, various calculi may be considered for computing the meaning of a complex expression from the atomic representations of the word senses. Obviously, one should be able, before hand, to decide for each word in a text which of its possible meanings is, contextually, the right one. Therefore, it is a generally accepted idea that the WSD task is highly instrumental (if not indispensable) in semantic processing of natural language documents. It is almost a truism that more decision makers, working together, are likely to find a common solution superior to each solution individually found. Dieterich [1] discusses conditions under which different decisions (in his case classifications) may be combined for obtaining a better result. Essentially, a successful automatic combination method would require comparable performance on behalf of the decision makers and, additionally, that they would not make the similar errors. This idea has been exploited by various NLP researchers in language modelling, statistical POS tagging, parsing, word alignment, word sense disambiguation, etc. The WSD problem can be stated as being able to associate to an ambiguous word (w) in a text or discourse, the sense (sk) which is distinguishable from other senses (s1, ..., sk-1, sk+1, ..., sn) prescribed for that word by a reference semantic lexicon. One such semantic lexicon (actually a lexical ontology) is Princeton WordNet [2] version 2.0 2 (henceforth PWN). PWN is a very finegrained semantic lexicon currently containing 203,147 sense distinctions, clustered in 115,424 equivalence classes (synsets). Out of the 145,627 distinct words, 119,528 have only one single sense. However, the remaining 26,099 words are those that one would frequently meet in a regular text and their ambiguity ranges from two senses up to 36. Several authors considered that sense granularity in PWN is too fine-grained for the computer use, arguing that even for a human (native speaker of English) the sense differences of some words are very hard to be reliably (and systematically) distinguished. There are several attempts to group the senses of the words in PWN in coarser grained senses \u2013 hyper-senses \u2013 so that clear-cut distinction among them is always possible for humans and (especially) computers. We will refer in this paper to two hyper-sense inventories used in the BalkaNet project [3]. A comprehensive review of the WSD state-of the art at the end of 90\u2019s can be found in [4]. Stevenson and Wilks [5] review several WSD systems that combined various knowledge sources to improve the disambiguation accuracy and address the 2 http://www.cogsci.princeton.edu/~wn/ issue of different granularities of the sense inventories. SENSEVAL 3 series of evaluation competitions on WSD is a very good source on learning how WSD evolved in the last 6-7 years and where is it nowadays. We describe a multilingual environment, containing several monolingual wordnets, aligned to PWN used as an interlingual index (ILI). The word-sense disambiguation method combines word alignment technologies, and interlingual equivalence relations in multilingual wordnets [6]. Irrespective of the languages in the multilingual documents, the words of interest are disambiguated by using the same sense-inventory labels. The aligned wordnets were constructed in the context of the European project BalkaNet. The consortium developed monolingual wordnets for five Balkan languages (Bulgarian, Greek, Romanian Serbian, and Turkish) and extended the Czech wordnet initially developed in the EuroWordNet project [6]. The wordnets are aligned to PWN, taken as an interlingual index, following the principles established by the EuroWordNet consortium. The version of the PWN used as ILI is an enhanced XML version where each synset is mapped onto one or more SUMO [7] conceptual categories and is classified under one of the IRST domains [8]. In the present version of the BalkaNet ILI there are used 2066 SUMO distinct categories and 163 domain labels. Therefore, for our WSD experiments we had at our disposal three sense-inventories, with very different granularities: PWN senses, SUMO categories and IRST Domains.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": null
  },
  {
    "paperId": "ffd5d30b438b2564cbd2b49ed2dbb3844b6a2976",
    "url": "https://www.semanticscholar.org/paper/ffd5d30b438b2564cbd2b49ed2dbb3844b6a2976",
    "title": "From Word Alignment to Word Senses, via Multilingual Wordnets",
    "abstract": "Most of the successful commercial applications in language processing (text and/or speech) dispense of any explicit concern on semantics, with the usual motivations stemming from the computational high costs required, in case of large volumes of data, for dealing with semantics. With recent advances in corpus linguistics and statistical-based methods in NLP, revealing useful semantic features of linguistic data is becoming cheaper and cheaper and the accuracy of this process is steadily improving. Lately, there seems to be a growing acceptance of the idea that multilingual lexical ontologies might be the key towards aligning different views on the semantic atomic units to be used in characterizing the general meaning of various and multilingual documents. Depending on the granularity at which semantic distinctions are necessary, the accuracy of the basic semantic processing (such as word sense disambiguation) can be very high with relatively low complexity computing. The paper substantiates this statement by presenting a statistical/based system for word alignment and word sense disambiguation in parallel corpora. We describe a word alignment platform which ensures text pre-processing (tokenization, POS-tagging, lemmatization, chunking, sentence and word alignment) as required by an accurate word sense disambiguation. 1 The Pervasive Ambiguity of Natural Languages Most difficult problems in natural language processing stem from the inherent ambiguous nature of the human languages. Ambiguity is present at all levels of traditional structuring of a language system (phonology, morphology, lexicon, syntax, semantics) and not dealing with it at the proper level, exponentially increases the complexity of the problem solving. Currently, the state of the art taggers (combining various models, strategies and processing tiers) ensure no less than 97-98% accuracy in the process of morpho-lexical full disambiguation. For such taggers a 2-best tagging 1 is practically 100% correct. 1 In k-best tagging, instead of assigning each word exactly one tag (the most probable in the given context), it is allowed to have occasionally at most k-best tags attached to a word and if the correct tag is among the k-best tags, the annotation is considered to be correct. One further step is the word sense disambiguation (WSD) process. In the traditional compositional semantics, the meaning of a complex expression is supposed to be derivable from the meanings of its parts, and the way in which those parts are combined. Depending on the representation formalisms for the word-meaning representation, various calculi may be considered for computing the meaning of a complex expression from the atomic representations of the word senses. Obviously, one should be able, before hand, to decide for each word in a text which of its possible meanings is, contextually, the right one. Therefore, it is a generally accepted idea that the WSD task is highly instrumental (if not indispensable) in semantic processing of natural language documents. It is almost a truism that more decision makers, working together, are likely to find a common solution superior to each solution individually found. Dieterich [1] discusses conditions under which different decisions (in his case classifications) may be combined for obtaining a better result. Essentially, a successful automatic combination method would require comparable performance on behalf of the decision makers and, additionally, that they would not make the similar errors. This idea has been exploited by various NLP researchers in language modelling, statistical POS tagging, parsing, word alignment, word sense disambiguation, etc. The WSD problem can be stated as being able to associate to an ambiguous word (w) in a text or discourse, the sense (sk) which is distinguishable from other senses (s1, ..., sk-1, sk+1, ..., sn) prescribed for that word by a reference semantic lexicon. One such semantic lexicon (actually a lexical ontology) is Princeton WordNet [2] version 2.0 2 (henceforth PWN). PWN is a very finegrained semantic lexicon currently containing 203,147 sense distinctions, clustered in 115,424 equivalence classes (synsets). Out of the 145,627 distinct words, 119,528 have only one single sense. However, the remaining 26,099 words are those that one would frequently meet in a regular text and their ambiguity ranges from two senses up to 36. Several authors considered that sense granularity in PWN is too fine-grained for the computer use, arguing that even for a human (native speaker of English) the sense differences of some words are very hard to be reliably (and systematically) distinguished. There are several attempts to group the senses of the words in PWN in coarser grained senses \u2013 hyper-senses \u2013 so that clear-cut distinction among them is always possible for humans and (especially) computers. We will refer in this paper to two hyper-sense inventories used in the BalkaNet project [3]. A comprehensive review of the WSD state-of the art at the end of 90\u2019s can be found in [4]. Stevenson and Wilks [5] review several WSD systems that combined various knowledge sources to improve the disambiguation accuracy and address the 2 http://www.cogsci.princeton.edu/~wn/ issue of different granularities of the sense inventories. SENSEVAL 3 series of evaluation competitions on WSD is a very good source on learning how WSD evolved in the last 6-7 years and where is it nowadays. We describe a multilingual environment, containing several monolingual wordnets, aligned to PWN used as an interlingual index (ILI). The word-sense disambiguation method combines word alignment technologies, and interlingual equivalence relations in multilingual wordnets [6]. Irrespective of the languages in the multilingual documents, the words of interest are disambiguated by using the same sense-inventory labels. The aligned wordnets were constructed in the context of the European project BalkaNet. The consortium developed monolingual wordnets for five Balkan languages (Bulgarian, Greek, Romanian Serbian, and Turkish) and extended the Czech wordnet initially developed in the EuroWordNet project [6]. The wordnets are aligned to PWN, taken as an interlingual index, following the principles established by the EuroWordNet consortium. The version of the PWN used as ILI is an enhanced XML version where each synset is mapped onto one or more SUMO [7] conceptual categories and is classified under one of the IRST domains [8]. In the present version of the BalkaNet ILI there are used 2066 SUMO distinct categories and 163 domain labels. Therefore, for our WSD experiments we had at our disposal three sense-inventories, with very different granularities: PWN senses, SUMO categories and IRST Domains.",
    "venue": "Comput. Sci. J. Moldova",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Mathematics",
      "Computer Science"
    ]
  },
  {
    "paperId": "226ff8156ea07cd8d2413bc5a1e2ecf77afb8f68",
    "url": "https://www.semanticscholar.org/paper/226ff8156ea07cd8d2413bc5a1e2ecf77afb8f68",
    "title": "Word Senses: The Stepping Stones in Semantic-Based Natural Language Processing",
    "abstract": null,
    "venue": "AIAI",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d0c8d7a6c29354df8b4bc14dea9fffdf886c8352",
    "url": "https://www.semanticscholar.org/paper/d0c8d7a6c29354df8b4bc14dea9fffdf886c8352",
    "title": "A novel semantic approach for Web service discovery using computational linguistics techniques",
    "abstract": "With the proliferation of available Web services, new approaches are needed to search for the most appropriate services matching user requirements. Existing UDDI based discovery techniques fail to recognize similarities and differences between Web service capabilities and thus greatly limit the scope of service discovery. Hence, in this paper we present a framework for discovering the vast amount of Web services present in Internet based on a novel semantic approach to automatically discover the most appropriate services to user requirements. Our framework is based on a natural language query to facilitate usage of our implemented discovery framework. For this purpose, we first apply some computational linguistics techniques such as part-of-speech tagging and word sense disambiguation to format and extract useful semantic information from user query. Then, we propose an innovative semantic matchmaking technique based on significant concepts properties in the referenced domain ontology and we show that it improves discovery efficiency compared to a similar related work.",
    "venue": "Fourth International Conference on Communications and Networking, ComNet-2014",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "7ef070a332f0266163a3be7462515bd105b08792",
    "url": "https://www.semanticscholar.org/paper/7ef070a332f0266163a3be7462515bd105b08792",
    "title": "A New Intelligent Topic Extraction Model on Web",
    "abstract": "We tackle the problem of topic extraction on Web. In this paper, we propose an approach to implementing ontology-based data access in WordNet with the distinguishing feature of optimizing density-based clustering OPTICS algorithm (DBCO) to extract topics. Our solution has the following two desirable properties: i) it uses WordNet for word sense disambiguation of words in the learning resources documents and ii) it mapping the data space of the original method to a vector space of sentence, improving the original OPTICS algorithm. We outline the interface between our scheme and the current data Web, and show that, in contrast to the existing approaches, no exponential blowup is produced by the DBCO. Based on the experiments with a number of real-world data sets of 310 users in three study sites, we demonstrate that topic extraction in the proposed approach is efficient, especially for large-scale web learning resources. According to the user ratings data of four learning sites in the 150 days, the average rate of increase of user rating after the system is used reaches 25.18%.",
    "venue": "J. Comput.",
    "citationCount": 14,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "375c141c25ce977740613cb1208a0f9b90bc85d9",
    "url": "https://www.semanticscholar.org/paper/375c141c25ce977740613cb1208a0f9b90bc85d9",
    "title": "Personalizing News Services Using Semantic Web Technologies",
    "abstract": "This chapter describes Hermes, a framework for building personalized news services using Semantic Web technologies. The Hermes framework consists of four phases: classification, which categorizes news items with respect to a domain ontology, knowledge base updating, which keeps the knowledge base up-to-date based on the news information, news querying, which allows the user to search the news with concepts of interest, and results presentation, which shows the news results of the search process. Hermes is supported by a framework implementation, the Hermes News Portal, a tool that enables users to have a personalized access to news items. The Hermes framework and its associated implementation aim at advancing the state-of-the-art of semantic approaches for personalized news services by employing Semantic Web standards, exploiting and keeping up-to-date domain information, using advanced natural language processing techniques (e.g., ontology-based gazetteering, word sense disambiguation, etc.), and supporting time-based queries for expressing the desired news items.",
    "venue": "",
    "citationCount": 14,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "36eee4126a1f1ddef1fbe7979ec8845028a5a28d",
    "url": "https://www.semanticscholar.org/paper/36eee4126a1f1ddef1fbe7979ec8845028a5a28d",
    "title": "Intelligent and fuzzy systems applied to language & knowledge engineering",
    "abstract": "Language & Knowledge Engineering is a very challenging area which is essential for the development of artificial intelligence in particular and Computer Science in general. These technologies are improving all areas of our daily life whether it is related to the education, health, production industries or something else. Thus, Recent Advancements in Intelligent and Fuzzy Systems applied to Language & Knowledge Engineering are the base for the society of tomorrow. The aim of this special issue of Journal of Intelligent and Fuzzy Systems is to present a collection of papers that cover recent research results on the topic of language and knowledge engineering. In particular, it aims to present technical papers in some of the following areas: Natural Language Processing, Knowledge engineering, Pattern recognition, Artificial Intelligence and Language, Scholarly Information Systems, Information Retrieval, Informetrics, Information Processing, Machine Learning Applied to Text Processing, Humanoids, Social Media Analytics and Fuzzy Systems for Text. Language engineering is an area of artificial intelligence and applications aiming to bridge the gap between traditional computational linguistics research and the implementation of potentially real-world applications. It looks to meet the needs of the research community working in all areas of automatic language processing, whether from a theoretical or applied perspective including some tasks such as machine translation, word sense disambiguation, reputation analysis, etc. As we will further describe, this thematic issue contains ten papers associated to the natural language engineering area, presenting specific natural language processing methods, tasks or applications. Knowledge engineering, on the other hand, refers to all technical, scientific and social aspects involved in designing, building, maintaining and using knowledge-based systems. The aim is to support human decisionmaking, learning and action, with emphases the practical significance, computer development and usage of knowledge-based systems including design process, models and methods, software tools, decision-support mechanisms, user interactions, organizational issues, knowledge acquisition and representation, and system architectures. The call for papers of this special issue received an overwhelming response from the community. After rigorous review only 49 papers representative of different tasks, techniques, and applications of language and knowledge engineering were selected from more than 150 papers submitted to the special issue. These papers represent the most up-to-date research work covering the aforementioned topics. We hope the reader will find this special issue informative and stimulating. The broad themes covered in this special issue are described as follows. First, we start this special issue with 33 papers devoted to the language engineering area. Their general description follows. Solovyev et al. in their paper \u201cPrediction of Reading Difficulty in Russian Academic Texts\u201d undertook a comparative analysis of academic texts features exemplified in textbooks on Social Science and examination texts of Russian as a foreign language. Experiments for 7 classifiers and 4 methods of linear regression on Russian Readability corpus demonstrated that ranking textbooks for native speakers is a much more difficult task than ranking examination texts written (or designed) for foreign students.",
    "venue": "Journal of Intelligent & Fuzzy Systems",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c28e8f8d3699761908ff5b0787d8f0ded4df240e",
    "url": "https://www.semanticscholar.org/paper/c28e8f8d3699761908ff5b0787d8f0ded4df240e",
    "title": "Using Domain Ontologies for Classification and Semantic Interpretation of Documents",
    "abstract": "The work presented in this paper addresses the problem of interpretation and semantic classification of documents. One of the issues faced by natural languages is related to the presence, in glossaries, of words with similar morphologies and different meanings. Our approach is based on the use of domain ontologies for nouns disambiguation. We begin our process with a global disambiguation, by linking the considered document to a semantic domain (represented by an ontology) which we select among several candidate ones. We define a candidate domain as any domain in which at least one significant word of the text can be considered and makes sense. We then perform a local disambiguation by using the selected ontology and finally build a semantic representation of the content of the document as a conceptual graph.",
    "venue": "Big Data 2016",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "bd2e9d421d97a4a2adebe5790a275fecfef1df32",
    "url": "https://www.semanticscholar.org/paper/bd2e9d421d97a4a2adebe5790a275fecfef1df32",
    "title": "INTEGRATING CONTEXTUAL INFORMATION INTO ONTOLOGIES FOR READING MATERIAL CLASSIFICATION",
    "abstract": "Reading Material Classification (RMC) classifies an unclassified text into the readability graded reading material based on its text readability. Recent approaches for RMC have used Natural Language Processing (NLP) methods such as machine-learning-based methods (e.g., Support Vector Machine, Multinomial Na\u00efve Bayes and Latent Semantic Indexing) to overcome disadvantages of using the syntactic features, i.e., insufficiency for modeling the levels of text reading difficulty. Ontologies have been used for sharing and reusing knowledge, and perhaps supporting the inference. It will be used for RMC. Concepts and contexts are treated separately in ontologies. By only using basic NLP techniques such as stemming and word sense disambiguation, integrating contextual information into ontologies, i.e., Contextual Ontology (CO), is proposed which aimed to improve the ontology\u2019s performance and possibly other types of quality for RMC. From the evaluation experiments, we do not claim that our proposed method is better than machine-learning based RMC, our system performance is just on a par with them. Rather than beating them, we aimed to use RMC to show that integrating contextual information into ontologies (RMC-CO) provides a considerable benefit for ontologies than not integrate it (RMC-O). 1.56% and 2.11% improvements can be obtained for validation and testing data, respectively.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "402f8430373176b43c883f99925347ca921e41ac",
    "url": "https://www.semanticscholar.org/paper/402f8430373176b43c883f99925347ca921e41ac",
    "title": "ADDING A CONTEXTUAL ONTOLOGICAL PROCESSING INTO RULE-BASED SYSTEMS FOR READING MATERIAL CLASSIFICATION",
    "abstract": "Determining the particular readability graded reading material from an unclassified text based on its text readability is called Reading Material Classification (RMC). Recently, RMC has used Natural Language Processing (NLP) methods, i.e., machine-learning-based RMC, to overcome disadvantages of using syntactic features, i.e., insufficiency for modelling the levels of text reading difficulty. Concepts can be varied somewhat between different contexts, therefore \u00abcontextual concept-variants\u00bb wanted in our ontologies which will result in Contextual Ontology (CO) by using basic NLP techniques such as stemming and word sense disambiguation. Rule-Based Systems (RBSs) are Knowledge-Based Systems whose knowledge is structured by rules. Given RMC as the test-bed, we propose to combine CO with RBSs for synergising their advantages, to improve RBS performance by adding contextual ontological processing into RBSs. Based on the evaluation experiments, we do not claim that our proposed method is better than machine-learning-based RMC. Instead, our system performance is just on a par with them. Rather than beating those methods, we aimed to use RMC to show that adding contextual ontological processing into RBSs (RMC-RBS + CO) presents a considerable benefit for RBSs than not adding it (RMC-RBS + O). 31.25% and 25.89% improvements can be obtained for validation and testing data, respectively.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "074a960ad3b842061a2aeddb4a157f2a4d48931c",
    "url": "https://www.semanticscholar.org/paper/074a960ad3b842061a2aeddb4a157f2a4d48931c",
    "title": "Wikipedia Mining Wikipedia as a Corpus for Knowledge Extraction",
    "abstract": "Wikipedia, a collaborative Wiki-based encyclopedia, has become a huge phenomenon among Internet users. It covers a huge number of concepts of various fields such as Arts, Geography, History, Science, Sports and Games. As a corpus for knowledge extraction, Wikipedia\u2019s impressive characteristics are not limited to the scale, but also include the dense link structure, word sense disambiguation based on URL and brief anchor texts. Because of these characteristics, Wikipedia has become a promising corpus and a big frontier for researchers. A considerable number of researches on Wikipedia Mining such as semantic relatedness measurement, bilingual dictionary construction, and ontology construction have been conducted. In this paper, we take a comprehensive, panoramic view of Wikipedia as a Web corpus since almost all previous researches are just exploiting parts of the Wikipedia characteristics. The contribution of this paper is triple-sum. First, we unveil the characteristics of Wikipedia as a corpus for knowledge extraction in detail. In particular, we describe the importance of anchor texts with special emphasis since it is helpful information for both disambiguation and synonym extraction. Second, we introduce some of our Wikipedia mining researches as well as researches conducted by other researches in order to prove the worth of Wikipedia. Finally, we discuss possible directions of Wikipedia research.",
    "venue": "",
    "citationCount": 24,
    "fieldsOfStudy": null
  },
  {
    "paperId": "0bbf3f03f56f741df066593b7f5dfdde873c7902",
    "url": "https://www.semanticscholar.org/paper/0bbf3f03f56f741df066593b7f5dfdde873c7902",
    "title": "Shimmering Lexical Sets",
    "abstract": "0. Abstract For word sense disambiguation and other pedagogical and NLP applications, it has long seemed desirable to group words together according to their essential semantic type 1 \u2014 [[Human]], [[Animate]], [[Artefact]], [[Physical Object]], [[Event]], etc.\u2014and to arrange these types into a hierarchy. Vast lexical and conceptual ontologies such as WordNet (Miller and Fellbaum 2007) and the Brandeis Semantic Ontology (Pustejovsky et. al. 2006) have been built on this foundation. However, the expectation that semantic types can serve word sense disambiguation purposes is disappointed by the fact that, as corpus-driven pattern analysis shows, semantic types do not map neatly onto lexical sets (Hanks et al. 2007). Firstly, lexical sets that pick out a particular sense of a verb may cut across semantic types. Secondly, as lexical sets move from verb to verb, some words drop out and others come in. In this paper we examine the implications of these inconvenient observations, and ask how lexical sets of co-occurring words can be organized for purposes of predicting the meaning of words in context. We discuss two steps aimed at dealing with this problem. Firstly, a new type of shallow ontology of nouns is being developed, based on work that was first reported in Pustejovsky, Hanks, and Rumshisky (2004). Secondly, each semantic type is populated by a set of canonical lexical items, which are identified by statistical contextual information. Noncanonical lexical items are classed as exploitations. Exploitations include words that are coerced into \"honorary\" membership of a semantic type in particular contexts. In this paper, we show how coercion phenomena are encoded in the Pattern Dictionary of English Verbs (Hanks and Pustejovsky, 2005). We show what the CPA Shallow Ontology looks like in its new form, discuss how it is populated, and explore its advantages in terms of empirical validity over more homogeneous, speculative ontologies.",
    "venue": "",
    "citationCount": 24,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "30c5ea39c906e3299607818dcab36d9dd7fbba55",
    "url": "https://www.semanticscholar.org/paper/30c5ea39c906e3299607818dcab36d9dd7fbba55",
    "title": "The Philosophical Foundations of Technological and Engineering Literacy",
    "abstract": "The purpose of this paper is to discuss the importance of philosophy in discussions of technological literacy, and to point out that actionable definitions of technological literacy are not possible without philosophy. Technological literacy has been broadly conceived as relating to the designed world, which exists in conjunction with the natural and social worlds. Definitions of technology tacitly include the social world since social institutions produce technologies, governments regulate them, and engineers design them. Within this broad sphere, however, there are competing definitions of technological literacy that confuse the issue of how to best develop technological literacy in students through education. When one also considers engineering literacy, scientific literacy, math literacy and information literacy and the more recent push for economic and media literacy, these confusions are magnified. To make sense of the many definitions of technological literacy it helps to look broadly at the groups that promote them. Each group has an explicit or tacit epistemology, and considering the definitions that arise from these views help to illuminate the underlying aims and objectives of teaching technological or engineering literacy. Here we briefly look at five perspectives: understanding technology in society, training students to manage technology in their lives, the need to have a technically literate workforce, philosophy of technology, and engineering. While each viewpoint has underlying aims and philosophies, the perspective of understanding technology in society provides a sufficiently expansive view so that meaningful problems can be posed and addressed by students. We explore some of the problems this view suggests and find that technological literacy can be taught as a transdisciplinary area of study (science, technology, and society programs), an area of philosophical inquiry (philosophy of technology), or in ways that organize inquiry across disciplines so students develop a personal philosophy. Background Interest in technological literacy as a concept has grown since the late 1950\u2019s. Krupczak and Blake (Blake & Krupczak Jr., 2014) have charted development of the concept, looking particularly at the intersection of technological literacy with engineering literacy. It should be noted that the term \u201ctechnological literacy\u201d is more commonly used in the United States than other nations. A sense of how technological literacy has become more prevalent in conversations on education can be seen by looking at the word frequency of the term using the Ngram viewing tool (Google, 2010). While this tool has significant biases and limitations (Pechenick, Danforth, & Dodds, 2015) the relatively high representation of scientific and technical literature in the corpus and the fact that \u201ctechnological literacy\u201d has a specific meaning that is not generally found in fiction permits a qualitative view of the rise in use of the term over a time frame of decades, as shown in Figure 1. Figure 1: Google NGram word frequency vs. time graph of the terms \u201ctechnological literacy\u201d, \u201cscience literacy\u201d, and \u201cinformation literacy\u201d (value divided by five) for 2012 dataset of American English with a smoothing of 1 for the time period 1970 2008. The term technological literacy seems to have first come into use around the 1950\u2019s where it was used more by happenstance than deliberately. Before 1980 technological literacy most often referred to the benefits of, or need for: vocational education, skills for living in a world with rapid technological change, and to denote measures of technical competence. The steady rise starting around 1980 coincides with the time that personal computers became both popular and affordable; for example the IBM PC was introduced in 1981. The accessibility of technology to all age groups has only grown since then (Mawson, 2007). The 1980\u2019s was also the decade technological literacy began to come under increasing consideration in higher education by policy makers (The Committee to Idenfity Critical Issues in Federal Support for Science and Technology, 1986), foundations such as Sloan (Florman, 1987), and the AAAS through Project 2061 (Rutherford, 1989). The 1990\u2019s saw increasing interest in technological literacy at the policy level (The Board for Engineering Education, 1994) where elements in government began to consider the role of technological literacy as a requirement for broad education and elements of technological literacy were integrated into the National Science Education Standards (National Research Council, 1996). The use of the term became even more prevalent in the US after 2000 when it was extended to K-12 education with the introduction of No Child Left Behind. This farreaching legislation required schools to use technology to increase student achievement, ensure students were technologically literate by the completion of the eighth grade, and have teachers to adopt technology in the classroom (Knezek, 2003). The International Technology Education Association (now ITEEA), a group that had represented industrial arts for decades, developed technological literacy standards as they sought to reframe their relevance in a changing educational environment. During the same period the National Academy of Engineering released several influential publications that broadly sought to clarify technological literacy and its distinction from engineering literacy (Garmire & Pearson, 2006; National Academy of Engineering & National Research Council, 2002). It is worth noting that a parallel term, \u201cscientific literacy\u201d is used with roughly the same frequency and the same rapid rise from the 1980 onward, see Figure 1. Both of these terms are often associated with perceived crises in science, engineering, or technology education. Such crises have been discussed within engineering in the United States for almost as long as there have been engineering programs (Cheville, 2014) due to the perception that STEM education is closely tied to economic growth and security (Committee on Prospering in the Global Economy of the 21st Century, 2006, 2010). Despite the large role engineering plays in the economy (Carnevale, Smith, & Melton, 2011) there are far fewer references to \u201cengineering literacy\u201d, a term promoted by ASEE\u2019s Technological and Engineering Literacy and Philosophy of Engineering Division (Krupczak Jr. et al., 2012). Other terms related to technological literacy are \u201cinformation literacy\u201d which is about five to ten times more prevalent, \u201cmath literacy\u201d, and more recently \u201cmedia literacy\u201d. The increasing interest in technological literacy has led to many attempts to define what technological literacy actually is (Blake & Krupczak Jr., 2014; Cheek, 1992; Dakers, 2006; Dyrenfurth, 1992; Gagel, 1997). Converging to a single, concise definition has been difficult. Rather than precise definitions the existing literature focuses more on sets of descriptors that are adopted by particular communities such as K-12 educators, STS researchers, or policy makers. In the remainder of this paper we briefly contrast existing descriptions and discuss the positions and belief systems underlying these definitions, showing that each adopts particular philosophies. We then explore several reasons for why technological literacy may be difficult to define, finding that technological literacy is better identified as a set of issues or problems rather than definitions. In this analysis we first assume that every attempt to define technological literacy is made in support of an existing belief system. Second, we recognize that the needs of educators who operate under real and pressing constraints are different from those who view technological literacy at a distance from the reality of teaching. Disparate Definitions of Technological Literacy As technological literacy has come to drive high stakes educational outcomes, attempts to define what is meant by the term have increased. Here we focus on a set of definitions created by different groups who have some stake in defining technological literacy. Although we discuss them separately, in reality these groups are not isolated; a Venn diagram would be highly intersecting. Science, Technology, and Society: One group consists of the researchers and academics who work in the area broadly defined as Science, Technology, and Society (STS). This group seeks to understand relationships between, and the impacts of, intersections of social and technical systems. They also engage with history, sociology, and philosophy, as exemplified by the work of Mitcham (Mitcham, 1994), as they seek to understand how the value systems that drive technology\u2019s impact with society. An early review of curricular frameworks that incorporated STS content or ideas identified 29 descriptors drawn from four general categories\u2014knowledge, skills, ethics and values, and action/involvement\u2014that can be applied to many forms of literacy (Cheek, 1992). A later study (Gagel, 1997) defined technological literacy as being able to: \u201c(a) accommodate and cope with rapid and continuous technological change, (b) generate creative and innovative solutions for technological problems, (c) act through technological knowledge both effectively and efficiently, and (d) assess technology and its involvement with the human lifeworld judiciously.\u201d Generally STS-derived definitions of technological literacy address the need to educate individuals for living in a technologically mediated world and are system oriented, involve elements of the ethical and moral dimensions of society, and are both critical and integrative, questioning the often accepted formula that \u201cscience + technology + democracy (+ capitalism) + education = progress\u201d (Wonacott, 2001). In essence the STS perspective is ontological, exploring how technology interfaces with and is an organizing force in society. Educators: Another set of definitions arise from educators and the",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Sociology"
    ]
  },
  {
    "paperId": "349df1e0b90bcb111757f323b3cc103f781fb347",
    "url": "https://www.semanticscholar.org/paper/349df1e0b90bcb111757f323b3cc103f781fb347",
    "title": "Graph-based Methods for Natural Language Processing Introduction to Textgraphs-8 Jobimtext Visualizer: a Graph-based Approach to Contextualizing Distributional Similarity Reconstructing Big Semantic Similarity Networks from Global to Local Similarities: a Graph-based Contextualization Method Using D",
    "abstract": "For the past 7 years, the series of TextGraphs workshops have exposed and encouraged the synergy between the field of Graph Theory (GT) and Natural Language Processing (NLP). The mix between the two started small, with graph theoretical framework providing efficient and elegant solutions for NLP applications that focused on single documents for part-of-speech tagging, word sense disambiguation and semantic role labeling. It then got progressively larger with ontology learning and information extraction from large text collections, and have reached web scale through the new fields of research that focus on information propagation in social networks, rumor proliferation, e-reputation, multiple entity detection, language dynamics learning and future events prediction to name but a few. The 8th edition of the TextGraphs workshop aimed to be a new step in the series, focused on issues and solutions for large-scale graphs, such as those derived for web-scale knowledge acquisition or social networks. We encouraged the description of novel NLP problems or applications that have emerged in recent years which can be addressed with graph-based solutions, as well as novel graph-based solutions to known NLP tasks. Continuing to bring together researchers interested in Graph Theory applied to Natural Language Processing, provides an environment for further integration of graph-based solutions into NLP tasks. A deeper understanding of new theories of graph-based algorithms is likely to help create new approaches and widen the usage of graphs for NLP applications. This volume contains papers accepted for presentation at the workshop. We issued calls for regular papers, short late\u2013breaking papers, and demos. After careful review by the program committee of the 15 submissions received \u2013 12 regular papers, 2 short papers and 1 demo \u2013 8 regular papers, 2 short papers and 1 demo were accepted for presentation. The accepted papers address varied problems \u2013 from theoretical and general considerations, to NLP and also \"real-world\" applications-through interesting variations in known and also novel graph-based methods. We are thankful to the members of the program committee, who have provided high quality reviews in a timely fashion despite the holiday season, and all submissions have benefited from this expert feedback. We were lucky to have two excellent speakers for this year's event. We thank Oren Etzioni and Pedro Domingos for their enthusiastic acceptance and presentations. Abstract Traditional information retrieval models assume keyword-based queries and use unstruc-tured document representations. There is an abundance of event-centered texts (e.g., breaking news) and event-oriented \u2026",
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": null
  },
  {
    "paperId": "31153d46755d3af1a50a256486cf6a417c023960",
    "url": "https://www.semanticscholar.org/paper/31153d46755d3af1a50a256486cf6a417c023960",
    "title": "Word Senses: The Stepping Stones in Semantic-Based Natural Language Processing",
    "abstract": "Most of the successful commercial applications in language processing (text and/or speech) dispense of any explicit concern on semantics, with the usual motivations stemming from the computational high costs required by dealing with semantics in case of large volumes of data. With recent advances in corpus linguistics and statistical-based methods in NLP, revealing useful semantic features of linguistic data is becoming cheaper and cheaper and the accuracy of this process is steadily improving. Lately, there seems to be a growing acceptance of the idea that multilingual lexical ontologies might be the key towards aligning different views on the semantic atomic units to be used in characterizing the general meaning of various and multilingual documents. Depending on the granularity at which semantic distinctions are necessary, the accuracy of the basic semantic processing (such as word sense disambiguation) can be very high with relatively low complexity computing. The paper substantiates this statement by presenting a statistical/based system for word alignment (WA) and word sense disambiguation (WSD) in parallel corpora.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": null
  },
  {
    "paperId": "c7e410eed82acc0cf15c2427b7d812489ba6012f",
    "url": "https://www.semanticscholar.org/paper/c7e410eed82acc0cf15c2427b7d812489ba6012f",
    "title": "Knowledge Engineering: Practice and Patterns, 16th International Conference, EKAW 2008, Acitrezza, Italy, September 29 - October 2, 2008. Proceedings",
    "abstract": null,
    "venue": "International Conference Knowledge Engineering and Knowledge Management",
    "citationCount": 118,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "89afcbbf79d195a5947216895fb717ed349f2a12",
    "url": "https://www.semanticscholar.org/paper/89afcbbf79d195a5947216895fb717ed349f2a12",
    "title": "Conceptual Clustering of Documents for Automatic",
    "abstract": "In Information retrieval, Keyword based retrieval is unsatisfactory for user needs since it can't always retrieve relevant words according to the concept. Since different words can represent the same concept (polysemy) and one word can represent different concepts (homonymy), mapping problem will lead to word sense Disambiguation. Through the implementation of domain dependent ontology, concept based information retrieval (IR) can be achieved. Since Semantic concept extraction from keywords is the initial phase for automatic construction of ontology process, this paper propose an effective method for it. Reuters21578 is used as the input of this process, followed by indexing, training and clustering using self-Organizing Map. Based on the feature vector, the clustering of documents are formed using automatic concept selections, in order to make the hierarchy. Clusters are represented hierarchically based on the topics assigned .Ontology will be generated automatically for each cluster, based on the topic assigned.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d481ab3c32987c757e80b114f6a7b0ee80e4288c",
    "url": "https://www.semanticscholar.org/paper/d481ab3c32987c757e80b114f6a7b0ee80e4288c",
    "title": "Comparison of WebService Retrieval with Syntactic and Semantic Approach Using Similarity Measures",
    "abstract": "Web Service is a client server application that can communicate over an HTTP protocol and it provides interoperability between the applications which can be run on the different platforms. Web Service Discovery is to find the suitable Web Service for a user given query. Parser extracts the words from the Web Service Description. Then match the user query with the semantic Web Service Descriptions given in Web Service Modeling Ontology Language (WSML) file. Part-of-speech tagging is applied to find the nouns, verb and adverbs for the words extracted from the Web Service descriptions. WordNet tool is used to discover the synonym and semantic relation for each word in the user query and the Web Service description then the corresponding senses are determined. Words will have multiple meanings. Word sense Disambiguation is used to identify the correct context for the given user query. After finding the senses of suitable words gathered from web service descriptions and from the user query, sense matching will takes place which results in a list of relevant web services.SSI algorithm performs better and list the suitable web service.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0a2d8df6f9b6f493d97e35ae1243410378377e7a",
    "url": "https://www.semanticscholar.org/paper/0a2d8df6f9b6f493d97e35ae1243410378377e7a",
    "title": "Proceedings of the ACL-2000 workshop on Word senses and multi-linguality - Volume 8",
    "abstract": "With an increasingly global economy and the explosive growth of the \"World\" in \"World Wide Web\", the computational linguistics community is faced as never before with the challenges and opportunities of mulfi-linguality. At the same time, the community has returned with renewed enthusiasm to problems of word meaning, especially the delineation and discrimination of word senses. An intimate relationship between the two issues is becoming apparent - for example, in the consideration of translation equivalence in parallel corpora, the construction of mullilingual ontologies, and the examination of senses in relation to specific natural language applications such as machine translation, information retrieval, summarization, etc. The issue of multi-lingual approaches to sense distinctions was also a central topic of discussion at the first SENSEVAL conference in 1998, and is one of the areas to be covered at SENSEVAL-2 (to be held in Spnng 2001).This workshop addresses problems of word sense disambiguation and delineation of appropriate sense distinctions, with specific emphasis on approaches that involve more than one language and the ways in which observations about cross-linguistic equivalence affect our consideration of sense divisions in the individual languages. More generally, we seek to foster discussion and exchanges of insight in any area of computational linguistics where a non-monolingual approach to word sense issues is being taken.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "65c90e576782445b836ee67bc64c2dab8d3e1809",
    "url": "https://www.semanticscholar.org/paper/65c90e576782445b836ee67bc64c2dab8d3e1809",
    "title": "Automatic Extraction and Enrichment of a Multilingual Domain Model",
    "abstract": "This thesis presents a framework for automated learning of a semantic, multilingual domain model. It tackles the problem of Ontology Learning by combining automatic extraction of expert knowledge from an online glossary with Linked Open Data. The obtained model contains lexical and semantic information on the domain of probability theory and mathematical statistics in three languages. During the linking of its constituent concepts to DBpedia, a linked open dataset, the problem of ambiguity arises. For one query term, several candidate concepts may be suggested. A multi-pass algorithm is presented which combines different techniques to disambiguate the word sense. Several approaches have been evaluated to come up with the best possible assembly of methods enabling the best precision and recall of the word sense disambiguation process.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "6ed692db2e5fec097c5321c9ccc7b420dc669d84",
    "url": "https://www.semanticscholar.org/paper/6ed692db2e5fec097c5321c9ccc7b420dc669d84",
    "title": "Using openWordnet-PT to improve VIVO",
    "abstract": "VIVO is an open source semantic web application for research discovery. The power of VIVO relies mainly on the VIVO-ISF ontology and its expressivity to represent all information about researchers and the research domain. VIVOISF makes all those types of information interconnected and browsable in the VIVO application. Nevertheless, although VIVO has good support for faceted search across disciplines, it is still not anything more than a keywordbased search engine. That is; it is still not using the power of semantics for information retrieving. WordNet [5] is an extremely valuable resource for research in Computational Linguistics and Natural Language Processing in general. WordNet has been used for a number of different purposes in information systems, including word sense disambiguation, information retrieval, automatic text classification, automatic text summarization, and dozens of other knowledge intensive projects. WordNets model the semantic relationships between words and they are crucial in all those applications because computational systems are not aware of the fact that salario (salary) and contra-cheque (paycheck) both refer to salary, or that beneficio (benefit) is also related to these words as a common more general hypernym. Wordnets have been distributed in a wide range of different incompatible data formats. An increasingly popular way of addressing the issue of interoperability is to rely on Linked Data and Semantic Web standards such as RDF [2] and OWL [7], which have led to the emergence of a number of Linked Data projects for lexical resources [3, 1]. OpenWordnet-PT 1 is a lexical-semantic resource describing (Brazilian) Portuguese words and their relationships. It is modelled after and fully interoperable with the original Princeton WordNet for English [5], relying",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "b8527e74631f07e57e21208b3765cfe5af07a9fe",
    "url": "https://www.semanticscholar.org/paper/b8527e74631f07e57e21208b3765cfe5af07a9fe",
    "title": "Graph Based Algorithm for Automatic Domain Segmentation of WordNet",
    "abstract": "We present a graph based algorithm for automatic domain segmentation of Wordnet. We pose the problem as a Markov Random Field Classification problem and show how existing graph based algorithms for Image Processing can be used to solve the problem. Our approach is unsupervised and can be easily adopted for any language. We conduct our experiments for two domains, health and tourism. We achieve F-Score more than .70 in both domains. This work can be useful for many critical problems like word sense disambiguation, domain specific ontology ex-",
    "venue": "GWC",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "45204df1c51a4b3f0221282f87409425852d2799",
    "url": "https://www.semanticscholar.org/paper/45204df1c51a4b3f0221282f87409425852d2799",
    "title": "UNIVERSIDADE FEDERAL DO ESTADO DO RIO DE JANEIRO CENTRO DE CI\u00caNCIAS EXATAS E TECNOLOGIA PROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM INFORM\u00c1TICA EXPANDING THE SEMANTIC KNOWLEDGE OF WORDNET THROUGH SEMANTIC TYPES AND UFO",
    "abstract": "Computational linguistics is a field of study concerned with the computational aspects of the human language faculty. It has been largely embraced by researchers of many different areas such as Natural Language Processing, Information Retrieval and Machine Learning, comprising applications like sentence understanding, machine translation, ontology learning, word sense disambiguation, and automatic question answering. WordNet is a large lexical database that exposes different types of semantic relations between words regarding their meanings, being used by an uncountable number of applications for computational linguistics. Nonetheless, its ability of expressing semantic knowledge about synsets (its basic knowledge unit) is still too limited. Previous proposals have attempted to add semantic knowledge to the lexical database, but only approaching common properties, which are more limited in scope than philosophical meta-properties. Relating WordNet synsets to this valuable kind of knowledge would enable algorithms using it as knowledge base to enhance their output, generating results with more accuracy, better mappings and more expressive models overall. This research proposes the expansion of WordNet\u2019s semantic knowledge by defining mapping rules between its synsets and UFO, a foundational ontology capable of exposing philosophical meta-properties of concepts. The proposed mapping rules rely on the use of a linguistic set of categories called Semantic Types. Once mapping rules from synsets to Semantic types have been established, a previous mapping of semantic types to UFO can be applied, achieving a more expressive WordNet. The proposal has been validated through an experiment where specialists manually evaluated over 5.500 sample mappings of WordNet synsets to semantic types, demonstrating the high accuracy level of the established rules.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Art"
    ]
  },
  {
    "paperId": "47eab3587f25ca0f4189a2f4bcb42cffcbcccf33",
    "url": "https://www.semanticscholar.org/paper/47eab3587f25ca0f4189a2f4bcb42cffcbcccf33",
    "title": "Estimating happiness of most populous cities by mining user- generated content in Twitter",
    "abstract": "1Bachelor of Engineering, Department of Electrical and Electronics Engineering, PSG College of Technology, Coimbatore, India 2Bachelor of Engineering, Department of Electrical and Electronics Engineering, PSG College of Technology, Coimbatore, India ---------------------------------------------------------------------***--------------------------------------------------------------------Abstract In online networking, individuals around the globe produce limitless measures of content substance. Online networking users broadcast their state of mind, suppositions, and sentiments on twitter as tweets. In this paper we evaluate the happiness of the general population living in the top 40 most populous cities around the globe. We gather and analyze a huge corpus of geo referenced messages or tweets, posted by online networking users through twitter information provided by Twitter API. We analyze the text content information using Natural Language Processing (NLP) methods and classify the sentiment of individual tweets using various lexicon based methods associated with Word Sense Disambiguation (WSD) algorithm for computing the happiness coefficients of individual cities and form a happiness index based on it.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "aaea8278bdcf4e3e481e2a31c1f57eb685f415fc",
    "url": "https://www.semanticscholar.org/paper/aaea8278bdcf4e3e481e2a31c1f57eb685f415fc",
    "title": "LOOK4: Enhancement of Web Search Results with Universal Words and WordNet",
    "abstract": "In this paper we present some results gained from an ongoing experimental project called Look4 that is aimed to enhance the results that we get from popular web search engines by means of a network of concepts, derived from the ontology of WordNet 2.1. We also present some sample statistics and achievements in the fields of multilingual search (Karande Jalindar Baban, 2007) and word sense disambiguation. The key approach for the construction of the concept network is the use of a network from Universal Words (UW) of Universal Networking Language (UNL) (Uchida H. and Zhu M, 2005).",
    "venue": "",
    "citationCount": 5,
    "fieldsOfStudy": null
  },
  {
    "paperId": "ede59e9606b0e5e2948afaa9ebe441e5988bb239",
    "url": "https://www.semanticscholar.org/paper/ede59e9606b0e5e2948afaa9ebe441e5988bb239",
    "title": "Ontologies and Query expansion",
    "abstract": "This master thesis will explore the use of ontologies in information retrieval and in query expansion in particular. Ontologies are usually huge, hand-coded repositories of concepts and relations between them so using them in information retrieval seems to be a reasonable goal. We feel that the use of ontologies for query expansion in particular has been overlooked in contemporary literature, as the main related papers date before 2000. In this thesis we will attempt to present a query expansion method using ontologies that outperforms non-ontological query expansion methods. Note, however, that the presented approach is not purely ontological but is rather a hybrid approach as it uses non-ontological steps. We also propose a method for purely probabilistic query expansion that outperforms all methods tested. Finally we explore word sense disambiguation based on ontologies as that is a prerequisite step for ontological query expansion. The ontology used was WordNet. The results of our experiments were based on standard TREC conferences data and showed that an ontological approach can cause improvement over non-ontological methods.",
    "venue": "",
    "citationCount": 29,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f0cd388dde4cf8be0d2ab154d162e9acb6f53a6d",
    "url": "https://www.semanticscholar.org/paper/f0cd388dde4cf8be0d2ab154d162e9acb6f53a6d",
    "title": "OntoSeek: Using Large Linguistic Ontologies for Accessing On-Line Yellow Pages and Product Catalogs",
    "abstract": "based on WordNet (namely SENSUS, developed at ISIUSC) for content matching. In general, with respect to standard word-matching systems, expressing the content structure by means of a simple representation language increases the precision of the retrieval, while adopting a hierarchy of keywords increases both recall and precision. In OntoSeek, the use of a linguistic ontology results in two further advantages: a decoupling between the user vocabulary and the encoding terminology, and an additional increase of recall and precision due to synonymy handling and sense disambiguation. Our conclusion is that yellow pages and product catalogs constitute a strategic niche, where retrieval techniques based on simple representation capabilities and large linguistic ontologies appear to be particularly effective.",
    "venue": "",
    "citationCount": 39,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "df4b472d70af0203a7ec84ff7448a50eb700d561",
    "url": "https://www.semanticscholar.org/paper/df4b472d70af0203a7ec84ff7448a50eb700d561",
    "title": "Web Information Systems Engineering \u2013 WISE 2013",
    "abstract": null,
    "venue": "Lecture Notes in Computer Science",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "b5458196c160e7e8f5928282b047d62738a809dc",
    "url": "https://www.semanticscholar.org/paper/b5458196c160e7e8f5928282b047d62738a809dc",
    "title": "Semantic document engineering with WordNet and PageRank",
    "abstract": "This paper describes Natural Language Processing techniques for document engineering in combination with graph algorithms and statistical methods. Google's PageRank and similar fast-converging recursive graph algorithms have provided practical means to statically rank vertices of large graphs like the World Wide Web. By combining a fast Java-based PageRank implementation with a Prolog base inferential layer, running on top of an optimized WordNet graph, we describe applications to word sense disambiguation and evaluate their accuracy on standard benchmarks.",
    "venue": "ACM Symposium on Applied Computing",
    "citationCount": 9,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "051aa6200fd70bdbb47578c44cc2d853c015336f",
    "url": "https://www.semanticscholar.org/paper/051aa6200fd70bdbb47578c44cc2d853c015336f",
    "title": "PUNJABI SHAHMUKHI LEXICAL RESOURCE",
    "abstract": "Pakistan is a multilingual country where different languages are being spoken by its people. So for the cross lingual information processing there should be some centralized repository of words, usually known as a lexical database. Natural language processing or natural language engineering has many tasks such as word sense disambiguation, machine translation, part of speech tagging and such others. All these tasks also need large scale lexical databases. So there is a rich need to develop such resources. The lexical databases for the developed languages are already built, but less attention is given to less resourced or under resourced languages like Punjabi, Saraiki etc. These are the main motivations behind this research. English WordNet developed by Princeton University is a best example of lexical database. Our work includes design and construction of such a database for Pakistani regional languages. We have studied different approaches adopted for the construction of lexical databases of different languages in the world. In the proposed system, we have developed a web interface that facilitates the updation and query-based results retrieval of entries from lexical database.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "77d223b9a6a0e0e7c0bd034d89832dfa08464c56",
    "url": "https://www.semanticscholar.org/paper/77d223b9a6a0e0e7c0bd034d89832dfa08464c56",
    "title": "Exploiting semantics for filtering and searching knowledge in a software development context",
    "abstract": null,
    "venue": "Knowledge and Information Systems",
    "citationCount": 1,
    "fieldsOfStudy": null
  },
  {
    "paperId": "751bb2069b225989bc93421c91efb4d89d5f9297",
    "url": "https://www.semanticscholar.org/paper/751bb2069b225989bc93421c91efb4d89d5f9297",
    "title": "Non-Learning Semantic Analysis for Context Discovery and Sentiment Estimation: Transportation Application",
    "abstract": "Non-Learning Semantic Analysis for Context Discovery and Sentiment Estimation: Transportation Application by Himanshu Verma \u3008Dr. Pushkin Kachroo\u3009, Examination Committee Chair Professor of Electrical and Computer Engineering University of Nevada, Las Vegas With enormous amount of linguistic data present on web, text analysis has become one of the major fields of interest today. This field includes sentiment analysis, information retrieval, text document classification, knowledge based modeling, content similarity measure, data clustering, words prediction/correction, decision making etc. Managing and processing such data has vital importance. The field being quite broad, our focus is mainly on transportation related social media(Twitter) data extraction, text categorization/classification which can be further sub-divided into concept discovery, word sense disambiguation and sentiment analysis to analyze performance of existing transportation system worldwide. Concept discovery is the method of extracting the actual concept/context in which the text is about. This also allows us to filter irrelevant data. Word sense disambiguation is to find the correct sense in which a word is being used in a sentence. It is the basic necessity for concept discovery. iii A lot of research has been done in this field with major improvements. However, when it comes to short texts, the field still seems in nascent stage. Moreover, most of the methods today require huge amount training corpus(database). Arranging such corpus is a cumbersome task and requires a lot of human effort. The other problem with the existing methods are that they require a set of defined concepts from which a concept is chosen and labeled to text. We will consider the case of finding a general context. In this work a novel approach has been proposed for word sense disambiguation which in turn allows us to find general context. For this purpose, I have used the existing knowledge based semantic dictionary called WordNet. This methodology helps in avoiding the use of huge corpus and works for general context recognition. Our focus is on short-texts(Tweets) but the concept is easily applicable to text documents as well. Sentiment measuring technique was applied on extracted data and the scores were mapped to google maps based on the location information present in the tweets. This clearly points out the locations where people are more frustrated with the existing transportation system and need immediate attention for improvements.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "7410a709254ab7bc27315e4f38fd7119eef5b489",
    "url": "https://www.semanticscholar.org/paper/7410a709254ab7bc27315e4f38fd7119eef5b489",
    "title": "Ontology Extraction using Social Network",
    "abstract": "This paper proposes integration of a social network with the tripartite model of ontologies by P. Mika. That model is based on three dimensions, i.e. ac- tors, concepts and instances, and illustrates ontol- ogy emergence using actor-concept and concept- instance relations. However, another important in- gredient is the actor-actor relation. For example, a vocabulary is sometimes shared within a commu- nity, which consists of dense relations among per- sons. Through considering of who knows whom (as described in FOAF) and who collaborates with whom, the extracted ontology might be improved. We propose an advanced model based on Mika's work, and describe a case study using the model. We show an application of an extracted ontology for information recommendation for academic con- ferences. of people and concepts, we can extract a hierarchy based on sub-community relationships. Several researchers have suggested emergent semantics. A community might evolve and their commitments might change because members continually leave and enter the group. Towards the dynamic change of ontologies, it is im- portant to develop a method to extract an ontology. The ex- pectation is that numerous individual interactions would en- gender global effects that are observable as semantics. This approach can support realization of a scalable and easily maintainable Semantic Web. Mika's model provides meth- ods to grasp emergence of semantics through introduction of a structure of members in a community. Although Mika's study proposes an elegant model of actors-concepts-instances, we can advance it further. We call Mika's model as the Mika model in this paper. One impor- tant factor that we explore in our analysis is actors-actors re- lation. Although Mika mentionssocial networks, the network that was conceptualized in that study is an affiliation network (or a two-mode network) that shows the relation between ac- tors and concepts. The affiliation network can be folded to generate an association of concepts in terms of overlapping instances or concepts. We integrate the Mika model with an- other kind of social network, called an adjacent network .I t is the social network to which we refer in a more usual senses, e.g., when mentioning social networking in SNSs or a social network by friend-of-a-friend (FOAF) aggregation. An adjacent network, in which each tie represents a rela- tion between actors such as knows, collaborates, and being friends with, can enhance the Mika model. The major ad- vantages of using the Mika model along with the adjacent networks are to give solutions for two problems: the data sparsity problem and word-sense disambiguation. In this paper, we propose an advanced model for ontology extraction based on the Mika model. Moreover, we show a case study using the model: emergent ontology at academic conferences. By considering who knows whom (as described in FOAF) and who collaborates with whom, the extracted on- tology can be improved. The contribution of this paper is summarized as follows:",
    "venue": "",
    "citationCount": 23,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "5972b34e313a03dcdf3bd81589cd54069d8f439d",
    "url": "https://www.semanticscholar.org/paper/5972b34e313a03dcdf3bd81589cd54069d8f439d",
    "title": "Using Background Knowledge to Support Coreference Resolution",
    "abstract": "Systems based on statistical and machine learning methods have been shown to be extremely effective and scalable for the analysis of large amount of textual data. However, in the recent years, it becomes evident that one of the most important direction of improvement in natural language processing (NLP) tasks, like word sense disambiguation, coreference resolution, relation extraction, and other tasks related to knowledge extraction, is by exploiting semantics. While in the past, the unavailability of rich and complete semantic descriptions constituted a serious limitation of their applicability, nowadays, the Semantic Web made available a large amount of logically encoded information (e.g. ontologies, RDF(S)-data, linked data, etc.), which constitute a valuable source of semantics. However, web semantics cannot be easily plugged into machine learning systems. Therefore the objective of this paper is to define a reference methodology for combining semantics information available in the web under the form of logical theories, with statistical methods for NLP. The major problems that we have to solve to implement our methodology concern (i) the selection of the correct and minimal knowledge among the large amount available in the web, (ii) the representation of uncertain knowledge, and (iii) the resolution and the encoding of the rules that combine knowledge retrieved from Semantic Web sources with semantics in the text. In order to evaluate the appropriateness of our approach, we present an application of the methodology to the problem of intra-document coreference resolution, and we show by means of some experiments on the ACE 2005 dataset, how the injection of knowledge is correlated to the improvement of the performance of our approach on this tasks.",
    "venue": "European Conference on Artificial Intelligence",
    "citationCount": 34,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "8510089c10f9fbedcfed69c33feb857c926dd4f4",
    "url": "https://www.semanticscholar.org/paper/8510089c10f9fbedcfed69c33feb857c926dd4f4",
    "title": "Rule-Based Semantic Tagging. An Application Undergoing Dictionary Glosses",
    "abstract": "The project presented in this article aims to formalize criteria and procedures in order to extract semantic information from parsed dictionary glosses. The actual purpose of the project is the generation of a semantic network (nearly an ontology) issued from a monolingual Italian dictionary, through unsupervised procedures. Since the project involves rule-based Parsing, Semantic Tagging and Word Sense Disambiguation techniques, its outcomes may find an interest also beyond this immediate intent. The cooperation of both syntactic and semantic features in meaning construction are investigated, and procedures which allows a translation of syntactic dependencies in semantic relations are discussed. The procedures that rise from this project can be applied also to other text types than dictionary glosses, as they convert the output of a parsing process into a semantic representation. In addition some mechanism are sketched that may lead to a kind of procedural semantics, through which multiple paraphrases of an given expression can be generated. Which means that these techniques may find an application also in 'query expansion' strategies, interesting Information Retrieval, Search Engines and Question Answering Systems.",
    "venue": "ArXiv",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4c91a9be7c8be7865a8f426df2885903f144d1af",
    "url": "https://www.semanticscholar.org/paper/4c91a9be7c8be7865a8f426df2885903f144d1af",
    "title": "Best First Search Planning of Service Composition Using Incrementally Refined Context-Dependent Heuristics",
    "abstract": null,
    "venue": "MATES",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "084c93f5e432b0f494ef5c2019eaf940e2e58a05",
    "url": "https://www.semanticscholar.org/paper/084c93f5e432b0f494ef5c2019eaf940e2e58a05",
    "title": "Ontology Clarification by Using WordNet",
    "abstract": "Semantic Web technology highly depends on the quality of ontology as it reduces or eliminates conceptual confusion and reuses knowledge.In order to enhance quality of ontology,a vast amount of research has focused on concept modeling task,but there is one major problem with lexical representation of ontology.Current lexical representation is term which may have different meanings;this can result in frustrating misunderstanding and ambiguity during the management and application of ontology.To solve this problem,sense is used to replace term as the lexical representation of concepts and properties for its unique meaning.We call ontology clarification the process of automatically disambiguating terms in ontology by using its surrounding ontology elements and its nearby terms in annotated documents using this ontology.The right sense is assigned to a target term by maximizing the relatedness between the target and its neighbors for semantic relatedness between them.Experiments show our ontology clarification method has good performance.Comparing with the best word sense disambiguation method,the concept precision is almost 2 times than the precision of noun,and the property precision is almost 3 times than the precision of verb.The last experiment proves that our method is also effective in a semi-automatic ontology clarification process.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c586fabf9d8a10fba8cd46f11c83f58998b95bae",
    "url": "https://www.semanticscholar.org/paper/c586fabf9d8a10fba8cd46f11c83f58998b95bae",
    "title": "SYNTAGMA. A Linguistic Approach to Parsing",
    "abstract": "SYNTAGMA is a rule-based parsing system, structured on two levels: a general parsing engine and a language specific grammar. The parsing engine is a language independent program, while grammar and language specific rules and resources are given as text files, consisting in a list of constituent structuresand a lexical database with word sense related features and constraints. Since its theoretical background is principally Tesniere's Elements de syntaxe, SYNTAGMA's grammar emphasizes the role of argument structure (valency) in constraint satisfaction, and allows also horizontal bounds, for instance treating coordination. Notions such as Pro, traces, empty categories are derived from Generative Grammar and some solutions are close to Government&Binding Theory, although they are the result of an autonomous research. These properties allow SYNTAGMA to manage complex syntactic configurations and well known weak points in parsing engineering. An important resource is the semantic network, which is used in disambiguation tasks. Parsing process follows a bottom-up, rule driven strategy. Its behavior can be controlled and fine-tuned.",
    "venue": "ArXiv",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "2c34718cac684974bddbbf8858cda8dddedcf3a0",
    "url": "https://www.semanticscholar.org/paper/2c34718cac684974bddbbf8858cda8dddedcf3a0",
    "title": "WSD Implementation for Processing Improvement of Structured Documents",
    "abstract": "The term of word sense disambiguation, WSD, is introduced in the context of text document processing. A knowledge based approach is conducted using WordNet lexical ontology, describing its structure and components used for the process of identification of context related senses of each polysemy words. The principal distance measures using the graph associated to WordNet are presented, analyzing their advantages and disadvantages. A general model for aggregation of distances and probabilities is proposed and implemented in an application in order to detect the context senses of each word. For the non-existing words from WordNet, a similarity measure is used based on probabilities of co-occurrences.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a01397a9f100d7e7d7c4f48920595c7672d81726",
    "url": "https://www.semanticscholar.org/paper/a01397a9f100d7e7d7c4f48920595c7672d81726",
    "title": "A Novel Matchmaking Approach for Automated Semantic Web Service Discovery",
    "abstract": "With the increasing number of available Web services, the need for a sophisticated discovery mechanism becomes essential. Existing UDDI based discovery techniques fail to recognize similarities and differences between Web service capabilities and thus greatly limit the scope of service discovery. Hence, in this paper we present a framework for discovering the vast amount of Web services present in Internet based on a novel semantic approach to automatically discover the most appropriate services to user requirements. Our framework is based on a natural language query to facilitate usage of our implemented discovery framework. For this purpose, we first apply some computational linguistics techniques such as part-of-speech tagging and word sense disambiguation to format and extract useful semantic information from user query. Then, we propose an innovative semantic matchmaking technique based on significant concepts properties in the referenced domain ontology. The proposed technique is implemented in a prototype system to evaluate our system performance and compare it to a related work. The results obtained demonstrate the effectiveness of our approach.",
    "venue": "International Conference on Information Integration and Web-based Applications & Services",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "19b3a06feb3d475aad94ad7e6c41df3557773d2a",
    "url": "https://www.semanticscholar.org/paper/19b3a06feb3d475aad94ad7e6c41df3557773d2a",
    "title": "Natural Language and Information Systems, 13th International Conference on Applications of Natural Language to Information Systems, NLDB 2008, London, UK, June 24-27, 2008, Proceedings",
    "abstract": null,
    "venue": "International Conference on Applications of Natural Language to Data Bases",
    "citationCount": 47,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a5809727de1118351a230daea0f9639fc370f826",
    "url": "https://www.semanticscholar.org/paper/a5809727de1118351a230daea0f9639fc370f826",
    "title": "Computational Approaches to Assistive Technologies for People with Disabilities",
    "abstract": "Assistive technologies have become increasingly important for people with disabilities in recent years. This book is the result of over a decade of research into computational approaches to assistive technology. Its chapters are based on a number of graduate theses, successfully completed over the past dozen or so years under the supervision of Kanlaya Naruedomkul of Mahidol University in Bangkok, Thailand and Nick Cercone of York University, Toronto, Canada. Some applications in the chapters use Thai language examples, but the techniques employed are not restricted to any single language. Each chapter is based on the Ph.D. work of a former or current student, suitably updated and presented for interested readers. The book is divided into four sections. Following an introduction, which includes a review of assistive technology products, part two covers applications, and includes chapters on alternative sign text MT for language learning, lexical simplification using word sense disambiguation and detecting and rating dementia through lexical analysis of spontaneous speech. Part three deals with theories and systems, and includes: granules for learning behavior, rough sets methods and applications for medical data and multimedia support systems as assistive technology for hearing impaired students. Part four presents a conclusion which includes a look into the future. Although this book is not a comprehensive treatise on assistive technology, it nevertheless provides a fascinating look at recent research, and will be of interest to all those whose work involves the application of assistive technologies for people with disabilities.IOS Press is an international science, technical and medical publisher of high-quality books for academics, scientists, and professionals in all fields. Some of the areas we publish in: -Biomedicine -Oncology -Artificial intelligence -Databases and information systems -Maritime engineering -Nanotechnology -Geoengineering -All aspects of physics -E-governance -E-commerce -The knowledge economy -Urban studies -Arms control -Understanding and responding to terrorism -Medical informatics -Computer Sciences",
    "venue": "Computational Approaches to Assistive Technologies for People with Disabilities",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ]
  },
  {
    "paperId": "a247eec2a82c4d115ba22880fc2ad94815be786f",
    "url": "https://www.semanticscholar.org/paper/a247eec2a82c4d115ba22880fc2ad94815be786f",
    "title": "Scalable Semantic Annotation of Text Using Lexical and Web Resources",
    "abstract": null,
    "venue": "Hellenic Conference on Artificial Intelligence",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4559600f2ba644d41a7580362d3fcfd5c363b814",
    "url": "https://www.semanticscholar.org/paper/4559600f2ba644d41a7580362d3fcfd5c363b814",
    "title": "The STEM-ECR Dataset: Grounding Scientific Entity References in STEM Scholarly Content to Authoritative Encyclopedic and Lexicographic Sources",
    "abstract": "We introduce the STEM (Science, Technology, Engineering, and Medicine) Dataset for Scientific Entity Extraction, Classification, and Resolution, version 1.0 (STEM-ECR v1.0). The STEM-ECR v1.0 dataset has been developed to provide a benchmark for the evaluation of scientific entity extraction, classification, and resolution tasks in a domain-independent fashion. It comprises abstracts in 10 STEM disciplines that were found to be the most prolific ones on a major publishing platform. We describe the creation of such a multidisciplinary corpus and highlight the obtained findings in terms of the following features: 1) a generic conceptual formalism for scientific entities in a multidisciplinary scientific context; 2) the feasibility of the domain-independent human annotation of scientific entities under such a generic formalism; 3) a performance benchmark obtainable for automatic extraction of multidisciplinary scientific entities using BERT-based neural models; 4) a delineated 3-step entity resolution procedure for human annotation of the scientific entities via encyclopedic entity linking and lexicographic word sense disambiguation; and 5) human evaluations of Babelfy returned encyclopedic links and lexicographic senses for our entities. Our findings cumulatively indicate that human annotation and automatic learning of multidisciplinary scientific concepts as well as their semantic disambiguation in a wide-ranging setting as STEM is reasonable.",
    "venue": "International Conference on Language Resources and Evaluation",
    "citationCount": 16,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c0b9306d6f14271ffb633352e210ac81b8922c9f",
    "url": "https://www.semanticscholar.org/paper/c0b9306d6f14271ffb633352e210ac81b8922c9f",
    "title": "\u0395\u03bd\u03bd\u03bf\u03b9\u03bf\u03bb\u03bf\u03b3\u03b9\u03ba\u03ae \u03b1\u03c0\u03bf\u03c3\u03b1\u03c6\u03ae\u03bd\u03b9\u03c3\u03b7 XML \u03b5\u03b3\u03b3\u03c1\u03ac\u03c6\u03c9\u03bd \u03bc\u03b5 \u03bf\u03bd\u03c4\u03cc\u03c4\u03b7\u03c4\u03b5\u03c2 \u03c4\u03b7\u03c2 Wikipedia",
    "abstract": "The ability to exploit the enormous amount of information that exists in the natural language form is a hot topic and it has been a point of study for many research groups. To make it possible, you first need to convert the natural language into a form that can be processed and analyzed by computer systems. A promising form is the XML language, an extensible markup language, which is the one of the dominant proposed solutions serving the wider vision of Semantic Web / Web 3.0. The simultaneous presence of structure and content information in XML documents allows us to devise many ways of data management and knowledge discovery, either using them separately or both. \n \nWord Sense Disambiguation (WSD), the task which is able to identify which sense of a word (i.e. meaning) is used in a sentence, when the word has multiple meanings has also been a major research topic for a long time. This process usually includes the task of identifying the semantically dominant concepts in a text segment and then, attaching them with a sense from a thesaurus inventory or semantic ontology. \n \nIn this paper, a new method of Word Sense Disambiguation of XML documents is proposed, using Wikipedia as a thesaurus inventory and considering its articles as sense entities. The specificity of XML documents, which is the simultaneous coexistence of content and structure information, as opposed to the unstructured text, requires a balanced task of disambiguation process at these two levels as the structure greatly affects the semantics of a term in the XML document. For example, the \u201cBeethoven\u201d word under the node \u201ccomposer\u201d imply the meaning of the music composer, while under the node \u201cmovie\u201d, we are most likely referring to the comedy movie. Thus, documents with a similar structure may have different semantic content or the opposite (a different structure with similar semantic content). The method proposed in this paper, through an iterative approach succeeds in disambiguating the tag name of a node of an XML document, matching it with a Wikipedia entity. To evaluate our proposed method, we created through an automated way a new XML dataset that contains words and phrases as hyperlinks to Wikipedia articles.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "3f4d5f68f2546d7f71d4960069dc886b8f86c4a3",
    "url": "https://www.semanticscholar.org/paper/3f4d5f68f2546d7f71d4960069dc886b8f86c4a3",
    "title": "Passage Scoring for Question Answering via Bayesian Inference on Lexical Relations",
    "abstract": "Many researchers have used lexical networks and ontologies to mitigate synonymy and polysemy problems in Question Answering (QA), systems coupled with taggers, query classifiers, and answer extractors in complex and ad-hoc ways. We seek to make QA systems reproducible with shared and modest human effort, carefully separating knowledge from algorithms. To this end, we propose an aesthetically \u201cclean\u201d Bayesian inference scheme for exploiting lexical relations for passage-scoring for QA . The factors which contribute to the efficacy of Bayesian Inferencing on lexical relations are soft word sense disambiguation, parameter smoothing which ameliorates the data sparsity problem and estimation of joint probability over words which overcomes the deficiency of naive-bayes-like approaches.",
    "venue": "Text Retrieval Conference",
    "citationCount": 70,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "39a2abdc61a258b1337a72d388ddf48bfe32096c",
    "url": "https://www.semanticscholar.org/paper/39a2abdc61a258b1337a72d388ddf48bfe32096c",
    "title": "Multi-Fusion Chinese WordNet (MCW) : Compound of Machine Learning and Manual Correction",
    "abstract": "Princeton WordNet (PWN) is a lexicon-semantic network based on cognitive linguistics, which promotes the development of natural language processing. Based on PWN, five Chinese wordnets have been developed to solve the problems of syntax and semantics. They include: Northeastern University Chinese WordNet (NEW), Sinica Bilingual Ontological WordNet (BOW), Southeast University Chinese WordNet (SEW), Taiwan University Chinese WordNet (CWN), Chinese Open WordNet (COW). By using them, we found that these word networks have low accuracy and coverage, and cannot completely portray the semantic network of PWN. So we decided to make a new Chinese wordnet called Multi-Fusion Chinese Wordnet (MCW) to make up those shortcomings. The key idea is to extend the SEW with the help of Oxford bilingual dictionary and Xinhua bilingual dictionary, and then correct it. More specifically, we used machine learning and manual adjustment in our corrections. Two standards were formulated to help our work. We conducted experiments on three tasks including relatedness calculation, word similarity and word sense disambiguation for the comparison of lemma's accuracy, at the same time, coverage also was compared. The results indicate that MCW can benefit from coverage and accuracy via our method. However, it still has room for improvement, especially with lemmas. In the future, we will continue to enhance the accuracy of MCW and expand the concepts in it.",
    "venue": "ArXiv",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e2da4029b5d5c8bb6863582ef3e706beb302c2a6",
    "url": "https://www.semanticscholar.org/paper/e2da4029b5d5c8bb6863582ef3e706beb302c2a6",
    "title": "Concept Based Information Access Using Ontologies and Latent Semantic Analysis",
    "abstract": "Concept-based access to information promises important benefits over keyword-based access. One of these benefits is the ability to take advantage of semantic relationships among concepts in finding relevant documents. Another benefit is the elimination of irrelevant documents by identifying conceptual mismatches. Concepts are mental structures. Words and phrases are the linguistic representatives of concepts. Due to the inherent conciseness of natural language, words can represent multiple concepts and different words may represent the same or very similar concepts. Word Sense Disambiguation attempts to resolve this ambiguity by pinpointing which concept is represented by a word or phrase in a context. The use of an ontology facilitates identification of related concepts and their linguistic representatives given a key concept. Latent semantic analysis, on the other hand, attempts to reveal the hidden conceptual relationships among words and phrases based on linguistic usage patterns. In this work we explore the potential of concept-based information access via these two mechanisms. We apply these techniques in three domains and examine under what circumstances concept-based access becomes feasible and improves user experience.",
    "venue": "",
    "citationCount": 24,
    "fieldsOfStudy": null
  },
  {
    "paperId": "5b6f28bff521b3eb16f260e96a243c70e80701a3",
    "url": "https://www.semanticscholar.org/paper/5b6f28bff521b3eb16f260e96a243c70e80701a3",
    "title": "Automatic Ontology Learning : Supporting a Per-Concept Evaluation by Domain Experts",
    "abstract": "Ontology evaluation is a critical task, even more so when the ontology is the output of an automatic system, rather than the result of a conceptualisation effort produced by a team of domain specialists and knowledge engineers. This paper provides an evaluation of the OntoLearn ontology learning system. The proposed evaluation strategy is twofold: first, we provide a detailed quantitative analysis of the ontology learning algorithms, in order to compute the accuracy of OntoLearn under different learning circumstances. Second, we automatically generate natural language descriptions of formal concept specifications, in order to facilitate per-concept qualitative analysis by domain specialists. 1 EVALUATING ONTOLOGIES Automatic methods for ontology learning and population have been proposed in recent literature (e.g. ECAI-2002 and KCAP2003 workshops3) but a co-related issue then becomes the evaluation of such automatically generated ontologies, not only with the goal of comparing the different approaches [5] and ontology-based tools [1], but also to verify whether an automatic process may actually compete with the typically human process of converging on an agreed conceptualization of a given domain. Ontology construction, apart from the technical aspects of a knowledge representation task (i.e. choice of representation languages, consistency and correctness with respect to axioms, etc.), is a consensus building process, one that implies long and often harsh discussions among the specialists of a given domain. Can an automatic method simulate this process? Can we provide domain specialists with a means to measure the adequacy of a specific set of concepts as a model of a given domain?, Specialists are often unable to evaluate the formal content of a computational ontology (e.g. the denotational theory, the formal notation, the knowledge representation system capabilities like property inheritance, consistency, etc.). Evaluation of the formal content is better tackled by computational scientists, or by automatic verification systems. The role of the specialists instead is to compare their 1 Dipartimento di Informatica, Universit La Sapienza , Roma, Italy, {velardi,navigli}@di.uniroma1.it 2 DIIGA, Universit Politecnica delle Marche, Ancona, Italy, {cucchiarelli, neri}@diiga.univpm.it 3 ECAI-2002 http://www-sop.inria.fr/acacia/WORKSHOPS/ECAI 2002-OLT/accepted-papers.html KCAP-2003 http://km.aifb.uni-karlsruhe.de/ws/semannot2003/papers .html intuition of a domain with the description of this domain, as provided by the ontology concepts. To facilitate one such qualitative per-concept evaluation, we devised a method for automatic generation of textual explanations (glosses) of automatically learned concepts. Glosses provide a description, in natural language, of the formal specifications assigned to the learned concepts. An expert can easily compare his intuition with these natural language descriptions. The objective of the gloss-based evaluation is, as previously remarked, to obtain a judgement, by domain specialists, concerning the adequacy of an automatically derived domain conceptualisation. On the computational side, an ontology learning tool is based on a battery of software programs aimed at extracting and formalising domain knowledge, usually starting from unstructured data. Therefore, it is equally important to produce a detailed evaluation of these programs, on a quantitative ground, in order to gain insight on the internal and external contingencies that may affect the result of an ontology learning process. In what follows, we firstly provide a quantitative evaluation of the OntoLearn ontology learning system, under different learning circumstances. Secondly, we describe the gloss-based per-concept evaluation method. Both evaluation strategies are experimented in two application domains: Tourism and Economy. The subsequent section provides a sketchy description of the OntoLearn algorithms. Details are found in [7,8]. Sections 3 and 4 are dedicated to the quantitative and qualitative analyses of OntoLearn. 2 SUMMARY OF THE ONTOLEARN SYSTEM OntoLearn is an ontology population method based on text mining and machine learning techniques. OntoLearn starts with an existing generic ontology (we use WordNet, though other choices are possible) and a set of documents in a given domain, and produces a domain extended and trimmed version of the initial ontology. The ontology generated by OntoLearn is anchored to texts, it can be therefore classified as a linguistic ontology [4]. OntoLearn has been applied to different domains (tourism, computer networks, economy) and in several European projects4. Concept learning is achieved in the following three phases: 4 E.g.: Harmonize IST-2000-29329 and the INTEROP network of excellence, started on december 2003. 1 ) Terminology Extraction: A list of domain terms is extracted from a set of documents that are judged representative of a given domain. Terms are extracted using natural language processing and statistical techniques. Contrastive corpora and glossaries in different domains are used to prune terminology which is not domain-specific. Domain terms are selected also on the basis of an entropy-based measure that simulates specialist consensus on concepts choice: in words, the probability distribution of a good domain term must be uniform across the individual documents of the domain corpus. 2 ) Semantic interpretation of terms: Semantic interpretation is based on a principle, compositional interpretation, and on a novel algorithm, called structural semantic interconnections (SSI) . Compositional interpretation signifies that the meaning of a complex term can be derived compositionally from its components5, e.g. the meaning of business plan is derived first, by associating the appropriate concept identifier, with reference to the initial top ontology, to the component terms (i.e. sense #2 of business and sense #1 of plan in WordNet), and then, by identifying the semantic relations holding among the involved concepts (e.g. plan#1 topic \uf8e7 \u2192 \uf8e7 \uf8e7 business# 2 ). 3) Extending and trimming the initial ontology: Once the terms have been semantically interpreted, they are organized in sub-trees, and appended under the appropriate node of the initial ontology, e.g. business _ plan# 1 kind _ of \uf8e7 \u2192 \uf8e7 \uf8e7 \uf8e7 \uf8e7 plan# 1 . Furthermore, certain upper and lower nodes of the initial ontology are pruned to create a domain-view of the ontology. The final ontology is output in OWL language. SSI lies in the area of syntactic pattern matching algorithms [2]. It is a word sense disambiguation algorithm used to determine the correct sense (with reference to the initial ontology) for each component of a complex term. The algorithm is based on building a graph representation for alternative senses of each term component6, and then selecting the appropriate senses on the basis of detected semantic interconnection patterns between graph pairs. The SSI algorithm seeks for semantic interconnections among the words of a context T. Contexts Ti are generated from groups of partially overlapping complex terms (extracted during phase 1 of the OntoLearn procedure) sharing the same syntactic head. For example, given the list of complex terms securities portfolio, investment portfolio, real-estate portfolio, junk-bond portfolio, diversified portfolio, stock portfolio, bond portfolio, loan portfolio, the following list of term components is created: T= [security, investment, real-estate, estate, bond, junkbond, diversified, stock, portfolio, loan ] Relevant pattern types are described by a context free grammar G. An example of rule in G is the following (S1 S2 and 5 Compositional interpretation works well (see also the evaluation section) in domains which are not overly technical, like tourism, economy, sport, politics. In other domains like medicine, or computer science, other strategies must be adopted, like glossary parsing. This is an in-progress research. 6 We remark again that a detailed description of the SSI algorithm is in [7,8]. Graphs are generated on the basis of lexico-semantic information in WordNet and in a variety of on-line resources, see the mentioned papers for details. S are concepts, i.e. synsets in WordNet): Rule Name:gloss+hyperonymy/meronymy(S1,S2). Def: \u2203 \u2208 \uf8e7 \u2192 \uf8e7\uf8e7 G Synsets S S gloss : 1 and there is a hyperonymy /meronymy path between S and S2. For instance, in railway companies, the gloss of railway#1 contains the word organization, and there is an hyperonymy path of length 2 between company#1 and organization#1. Tha t i s : railway organization gloss # # 1 1 \uf8e7 \u2192 \uf8e7\uf8e7 a n d company institution organization kind of kind of # # # 1 1 1 \u2212 \u2212 \uf8e7 \u2192 \uf8e7\uf8e7\uf8e7 \uf8e7 \u2192 \uf8e7\uf8e7\uf8e7 This pattern (an instance of the gloss+hypeonymyr/meronymy rule) cumulates evidence for senses #1 of both railway and company. In SSI, the correct sense St for a term t\u2208 T is selected depending upon the number and weight of patterns matching with rules in G. The weights of patterns are automatically learned using a perceptron7 model. The weight function is given by: weight pattern length pattern j j j j ( ) ( _ ) = + \u03b1 \u03b2 1 (1) where \u03b1j is the weight of rule j in G, and the second addend is a smoothing parameter inversely proportional to the length of the matching pattern (e.g. 2 in the previous example, since 2 is the minimal length of the rule, and the actual length of the pattern is 3). The perceptron has been trained on the SemCor8 semantically annotated corpus. In order to complete the semantic interpretation process, OntoLearn then attempts to determine the semantic relations that hold between the components of a complex concept. To do this, it was first necessary to select an inventory of semantic relations. We examined several proposals, like EuroWordnet [10], DOLCE [6], FrameNet [9] and others. As also remarked in [5], no systematic methods are available in literature to compare the different sets of relations. Since our objective wa",
    "venue": "",
    "citationCount": 14,
    "fieldsOfStudy": null
  },
  {
    "paperId": "f6aedfc594eb256daaceb89a8e64b81fc0307ce7",
    "url": "https://www.semanticscholar.org/paper/f6aedfc594eb256daaceb89a8e64b81fc0307ce7",
    "title": "Modelling Context to Solve Conflicts in SentiWordNet",
    "abstract": "Sentiment analysis and affect detection algorithms are generally based on annotated data, structured into dictionaries, ontologies or word nets. Among other research problems, two issues are considered very important in this field: 1) word sense disambiguation and 2) accuracy of affect detection. Most of the current approaches use annotated resources based on word nets. Their structure, founded on synonymic relations, makes the disambiguation process very difficult. Our model uses contextonyms, which simplify the decision process. Therefore, the disambiguation issue is transformed into a context matching problem. The second focus is on the manual annotation of the data followed by a semantic valence propagation. This approach enables the generation of new affective labels from a set of initial ones, through the expansion process. Unfortunately, this is usually done to the detriment of precision. We use an existing linguistic resource, SentiWordNet, which is one of the largest dictionaries available for sentiment analysis. Using our disambiguation model, we manage to solve all the SentiWordNet ambiguities and inconsistencies, which increases the accuracy of the classification process. This is the first of our major contributions. Second, we manage to reduce the disagreement percentage computed against well known linguistic resources to less than half of the original rate.",
    "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0af0a902968f0ef53da25442506ba8b158125d0d",
    "url": "https://www.semanticscholar.org/paper/0af0a902968f0ef53da25442506ba8b158125d0d",
    "title": "Supporting Natural Language Processing with Background Knowledge: Coreference Resolution Case",
    "abstract": null,
    "venue": "SEMWEB",
    "citationCount": 21,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4e89fa2fac461683a58a9adb19b403e006616600",
    "url": "https://www.semanticscholar.org/paper/4e89fa2fac461683a58a9adb19b403e006616600",
    "title": "Natural Language Query Parser using First Order Logic for Querying Relational Databases",
    "abstract": "Relational database management systems are mostly used for effective representation and retrieval of data. For the user, it is hard to learn the database interface language to deal with various operations on databases. Hence there is a need to construct a bridge between natural language query and database understandable query which is a major challenge. In this paper, we have proposed a Natural Language Parser for Natural Language Interface to customer database. The parser converts the Natural Language query into First order Logic and then the First order logic query is converted into structured query. This paper also addresses the word sense disambiguation problem using ontologies and n-grams. The lexical meaning of the natural language query can be captured with n contiguous characters or words of the query. The proposed system is able to handle extraction, insertion, deletion and updation queries. It is also able to process join, conditional, single and multiple column retrieval queries. The performance of the system is measured using precision, recall and F-measure. The results are progressive.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1a8f4eec71b4c0dbb566ade2ef45268914ce5971",
    "url": "https://www.semanticscholar.org/paper/1a8f4eec71b4c0dbb566ade2ef45268914ce5971",
    "title": "Networks and Natural Language Processing",
    "abstract": "Over the last few years, a number of areas of natural language processing have begun applying graph-based techniques. These include, among others, text summarization, syntactic parsing, word-sense disambiguation, ontology construction, sentiment and subjectivity analysis, and text clustering. In this paper, we present some of the most successful graph-based representations and algorithms used in language processing and try to explain how and why they work.",
    "venue": "The AI Magazine",
    "citationCount": 17,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "08729b4fa1388963dcde819f81dcf04fb29ec3ff",
    "url": "https://www.semanticscholar.org/paper/08729b4fa1388963dcde819f81dcf04fb29ec3ff",
    "title": "A Linguistic Approach for Semantic Web Service Discovery",
    "abstract": null,
    "venue": "International Symposium in Management Intelligent Systems",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "521a161a139efe6a8cbe5a15699f2593370074b2",
    "url": "https://www.semanticscholar.org/paper/521a161a139efe6a8cbe5a15699f2593370074b2",
    "title": "What\u2019s the Matter? Knowledge Acquisition by Unsupervised Multi-Topic Labeling for Spoken Utterances",
    "abstract": "Systems such as Alexa, Cortana, and Siri appear rather smart.\nHowever, they only react to predefined wordings and do not actually\ngrasp the user\u2019s intent. To overcome this limitation, a system must understand the topics the user is talking about. Therefore, we apply unsupervised multi-topic labeling to spoken utterances. Although topic labeling is a well-studied task on textual documents, its potential for spoken\ninput is almost unexplored. Our approach for topic labeling is tailored\nto spoken utterances; it copes with short and ungrammatical input.\nThe approach is two-tiered. First, we disambiguate word senses. We utilize Wikipedia as pre-labeled corpus to train a na\u00efve-bayes classifier.\nSecond, we build topic graphs based on DBpedia relations. We use two\nstrategies to determine central terms in the graphs, i.e. the shared topics. One focuses on the dominant senses in the utterance and the other\ncovers as many distinct senses as possible. Our approach creates multiple\ndistinct topics per utterance and ranks results.\nThe evaluation shows that the approach is feasible; the word sense disambiguation achieves a recall of 0.799. Concerning topic labeling, in a user\nstudy subjects assessed that in 90.9% of the cases at least one proposed\ntopic label among the first four is a good fit. With regard to precision,\nthe subjects judged that 77.2% of the top ranked labels are a good fit or\ngood but somewhat too broad (Fleiss\u2019 kappa \u03ba = 0.27).\nWe illustrate areas of application of topic labeling in the field of programming in spoken language. With topic labeling applied to the spoken\ninput as well as ontologies that model the situational context we are able\nto select the most appropriate ontologies with an F1-score of 0.907.",
    "venue": "International Journal of Humanized Computing and Communication",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "aa2d1ee478b4b8e4d0eb784ad3bec89319ac4543",
    "url": "https://www.semanticscholar.org/paper/aa2d1ee478b4b8e4d0eb784ad3bec89319ac4543",
    "title": "BOwL: exploiting Boolean operators and lesk algorithm for linking ontologies",
    "abstract": "BOwL applies word sense disambiguation techniques for tagging ontology entities with WordNet words. Boolean operators that appear in names of ontology entities are interpreted based on their semantics and are used during the ontology matching stage accordingly. Experimental results are shown, demonstrating the feasibility of the approach.",
    "venue": "SAC '12",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "dbb957b670efaab707c5ae30c0674a0f9216ab95",
    "url": "https://www.semanticscholar.org/paper/dbb957b670efaab707c5ae30c0674a0f9216ab95",
    "title": "The semantic Web : research and applications : 6th European Semantic Web Conference, ESWC 2009, Heraklion, Crete, Greece, May 31-June 4, 2009 : proceedings",
    "abstract": "Invited Talks.- Tonight's Dessert: Semantic Web Layer Cakes.- Discovering and Building Semantic Models of Web Sources.- Video Semantics and the Sensor Web.- Keys, Money and Mobile Phone.- Research Track.- Querying Trust in RDF Data with tSPARQL.- RadSem: Semantic Annotation and Retrieval for Medical Images.- Semanta - Semantic Email Made Easy.- The Sile Model - A Semantic File System Infrastructure for the Desktop.- Who the Heck Is the Father of Bob?.- Benchmarking Fulltext Search Performance of RDF Stores.- A Heuristics Framework for Semantic Subscription Processing.- Towards Linguistically Grounded Ontologies.- Frame Detection over the Semantic Web.- Word Sense Disambiguation for XML Structure Feature Generation.- Improving Ontology Matching Using Meta-level Learning.- Ontology Integration Using Mappings: Towards Getting the Right Logical Consequences.- Using Partial Reference Alignments to Align Ontologies.- Semantic Matching Using the UMLS.- Embedding Knowledge Patterns into OWL.- A Core Ontology of Knowledge Acquisition.- ONTOCOM Revisited: Towards Accurate Cost Predictions for Ontology Development Projects.- Ranking Approximate Answers to Semantic Web Queries.- Tempus Fugit.- Representing, Querying and Transforming Social Networks with RDF/SPARQL.- Applied Temporal RDF: Efficient Temporal Querying of RDF Data with SPARQL.- ReduCE: A Reduced Coulomb Energy Network Method for Approximate Classification.- Hybrid Reasoning with Forest Logic Programs.- SIM-DLA: A Novel Semantic Similarity Measure for Description Logics Reducing Inter-concept to Inter-instance Similarity.- Decidability of with Transitive Closure of Roles.- FO(ID) as an Extension of DL with Rules.- A Tableau Algorithm for Handling Inconsistency in OWL.- How to Trace and Revise Identities.- Concept Search.- Semantic Wiki Search.- Applying Semantic Social Graphs to Disambiguate Identity References.- Middleware for Automated Implementation of Security Protocols.- Can RDB2RDF Tools Feasibily Expose Large Science Archives for Data Integration?.- A Flexible API and Editor for SKOS.- An Ontology of Resources: Solving the Identity Crisis.- Mining Semantic Descriptions of Bioinformatics Web Resources from the Literature.- Hybrid Adaptive Web Service Selection with SAWSDL-MX and WSDL-Analyzer.- Enhancing Service Selection by Semantic QoS.- Towards an Agent Based Approach for Verification of OWL-S Process Models.- Leveraging Semantic Web Service Descriptions for Validation by Automated Functional Testing.- Neighborhood-Based Tag Prediction.- User Evaluation Study of a Tagging Approach to Semantic Mapping.- Fuzzy Annotation of Web Data Tables Driven by a Domain Ontology.- An Integrated Approach to Extracting Ontological Structures from Folksonomies.- Reducing Ambiguity in Tagging Systems with Folksonomy Search Expansion.- Semantic Web In-Use Track.- Ontology-Based Service Discovery Front-End Interface for GloServ.- A Resource List Management Tool for Undergraduate Students Based on Linked Open Data Principles.- SCOVO: Using Statistics on the Web of Data.- Media Meets Semantic Web - How the BBC Uses DBpedia and Linked Data to Make Connections.- Creating Digital Resources from Legacy Documents: An Experience Report from the Biosystematics Domain.- Collaborative Ocean Resource Interoperability: Multi-use of Ocean Data on the Semantic Web.- ONKI SKOS Server for Publishing and Utilizing SKOS Vocabularies and Ontologies as Services.- Ontology Libraries for Production Use: The Finnish Ontology Library Service ONKI.- Demo Track.- SAscha: Supporting the Italian Public Cooperation System with a Rich Internet Application for Semantic Web Services.- Folksonomy Enrichment and Search.- K-Tools: Towards Semantic Knowledge Management.- The XMediaBox: Sensemaking through the Use of Knowledge Lenses.- Controlled Natural Language for Semantic Annotation.- Multilingual and Localization Support for Ontologies.- WSMX 1.0: A Further Step toward a Complete Semantic Execution Environment.- MoKi: The Enterprise Modelling Wiki.- The Personal Knowledge Workbench of the NEPOMUK Semantic Desktop.- Utilizing Semantics in the Production of iTV Shows.- Knowledge Applications for Life Events: How the Dutch Government Informs the Public about Rights and Duties in the Netherlands.- CultureSampo: A National Publication System of Cultural Heritage on the Semantic Web 2.0.- A Rule System for Querying Persistent RDFS Data.- RaDON - Repair and Diagnosis in Ontology Networks.- Supporting the Reuse of Global Unique Identifiers for Individuals in OWL/RDF Knowledge Bases.- Modeling and Enforcement of Business Policies on Process Models with Maestro.- A Reasoning-Based Support Tool for Ontology Mapping Evaluation.- Semanta - Semantic Email in Action.- KiWi - A Platform for Semantic Social Software (Demonstration).- Pattern-Based Annotation of HTML-Streams.- OntoComP: A Protege Plugin for Completing OWL Ontologies.- Demo: HistoryViz - Visualizing Events and Relations Extracted from Wikipedia.- Ontology Evolution with Evolva.- Cupboard - A Place to Expose Your Ontologies to Applications and the Community.- PhD Symposium.- Effects of Using a Research Context Ontology for Query Expansion.- Towards a Semantic Infrastructure for User Generated Mobile Services.- Relational Databases as Semantic Web Endpoints.- The Relevance of Reasoning and Alignment Incoherence in Ontology Matching.- Towards a Semantic Service Broker for Business Grid.- Evolva: A Comprehensive Approach to Ontology Evolution.- A Context-Aware Approach for Integrating Semantic Web Technologies onto Mobile Devices.- Dealing with Inconsistencies in DL-Lite Ontologies.",
    "venue": "",
    "citationCount": 13,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "54f51ce797d28b9de6e35fd2989ed200daa52d6b",
    "url": "https://www.semanticscholar.org/paper/54f51ce797d28b9de6e35fd2989ed200daa52d6b",
    "title": "Exploiting Synonymy and Hypernymy to Learn Efficient Meaning Representations",
    "abstract": null,
    "venue": "ICADL",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "70b72ae6c6c1346fa6e9ab301960bb3ff9e02374",
    "url": "https://www.semanticscholar.org/paper/70b72ae6c6c1346fa6e9ab301960bb3ff9e02374",
    "title": "Proceedings of the LexSem+Logics Workshop 2016",
    "abstract": "Lexical semantics continues to play an important role in driving research directions in NLP, with the recognition and understanding of context becoming increasingly important in delivering successful outcomes in NLP tasks. Besides traditional processing areas such as word sense and named entity disambiguation, the creation and maintenance of dictionaries, annotated corpora and resources have become cornerstones of lexical semantics research and produced a wealth of contextual information that NLP processes can exploit. New efforts both to link and construct from scratch such information - as Linked Open Data or by way of formal tools coming from logic, ontologies and automated reasoning - have increased the interoperability and accessibility of resources for lexical and computational semantics, even in those languages for which they have previously been limited. \nLexSem+Logics 2016 combines the 1st Workshop on Lexical Semantics for Lesser-Resources Languages and the 3rd Workshop on Logics and Ontologies. The accepted papers in our program covered topics across these two areas, including: the encoding of plurals in Wordnets, the creation of a thesaurus from multiple sources based on semantic similarity metrics, and the use of cross-lingual treebanks and annotations for universal part-of-speech tagging. We also welcomed talks from two distinguished speakers: on Portuguese lexical knowledge bases (different approaches, results and their application in NLP tasks) and on new strategies for open information extraction (the capture of verb-based propositions from massive text corpora).",
    "venue": "ArXiv",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "7d5379b6ac795a0d02fe476a9be42cc62dd862e2",
    "url": "https://www.semanticscholar.org/paper/7d5379b6ac795a0d02fe476a9be42cc62dd862e2",
    "title": "Towards Automating Inter-organizational Workflow Semantic Resolution",
    "abstract": "Interoperability in sharing work through services between organizations requires understanding the perspective of each partner. Available resources and requirements are widely distributed with heterogeneous descriptions and enactments. No centralized registration and discovery process exists, nor automated means to map the variety of formal organizations within various overlapping domains, some defined and many not. While drawing from a common language, the mission and culture of each organization shapes their particular collection and definition of symbols, vocabulary and signals. Mining execution logs of historical inter-organizational workflows and tapping into explicitly specified domain and organizational knowledge ontologies provides a corpus of information useful to identify emergent patterns. This paper proposes a novel mediator approach using feature alignment demonstrated in previous partnerships, identified through the employment of topic modeling and word sense disambiguation methods, to infer semantic matches between parties without a priori relationships. This model demonstrates an inter-organizational workflow middleware proof of concept coupling workflow management systems interoperating between a set of organizations to automatically resolve meaning, providing the right services for requirements.",
    "venue": "2015 IEEE International Conference on Services Computing",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "b46b90b258a3099ec0b9eefa489aba795c421f05",
    "url": "https://www.semanticscholar.org/paper/b46b90b258a3099ec0b9eefa489aba795c421f05",
    "title": "Clustered Hierarchical Concept Based Semantic Closeness Between Two Concepts Using WordNet",
    "abstract": "The search engine needs relatedness to measure closeness between two concepts for determining optimal results in major applications like information retrieval, information integration and of many more in natural language processing tasks i.e. text classification, word sense disambiguation, matching problems in artificial intelligence etc,. The clustered hierarchical concept network helps to overcome the fuzzy variations in different levels of granularity in measures of closeness based on weights, frequency or distances but these measures are not considered since no method takes the actual context of the user intention, user query or context domain subject fields. Clustered hierarchical concept network has three steps: Elicitation: extract the concepts of user query using concept extraction algorithm and name the output as context domain. Construction: building hierarchical clusters based on context or concept domain with re lated concepts as nodes and relations as edges. Matching: determine the matching concepts like Least Common General Concept (LCGC) and Least Common Specific Concept (LCSC). Clustered hierarchical concept based semantic closeness has three features i.e., context domain, concept net and common concepts. These features are used to calculate the relatedness. The primary goal of hierarchical concept network is to include the semantic of the concept by including its three features. The extraction of concepts are not only related to individual concepts, but it is also an organizational structure of the concepts that are combined in the ontology i.e. WorNet. In this paper, we propose a method for computing semantic closeness of two concepts in which the holonyms, meronyms, instances of concepts are considered synthetically. By calculating test data, the experiment results show that the method can compute concepts closeness effectively. The human judgments on a set of concept pairs led our approach to be more effective and have shown one of the best performance than the measures based on concept vector.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Mathematics"
    ]
  },
  {
    "paperId": "1015fb3d097fedeb117fa4177c036abb0015ca88",
    "url": "https://www.semanticscholar.org/paper/1015fb3d097fedeb117fa4177c036abb0015ca88",
    "title": "\u0410\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043d\u043e\u0432\u044b\u0445 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043d\u043e-\u0441\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u043d\u044b\u0445 \u0442\u0435\u0440\u043c\u0438\u043d\u043e\u0432",
    "abstract": "Most of the state-of-the-art approaches for word sense disambiguation (WSD) are based on knowledge bases, or ontologies \u2014 databases of terms, their concepts and relations between them. One of the standing problems of knowledge bases is their incompleteness, i.e. the lack of appropriate concepts for terms occurred in some contexts; the problem is mostly actual for domain-specific terms. The consequence is that systems produce incorrect results because existing WSD algorithms simply assign one of the a-priori incorrect concepts to the terms. This paper describes a novel approach for recognition of domain-specific terms that exist in the knowledge base but represent new concepts. In contrast to previous approaches requiring formal ontologies with hierarchical structure and different relation types, our method can be applied to informal knowledge bases \u2014 it requires only semantic similarity between concepts and statistics of terms extracted from the domain-specific corpus. We show that our method performs better than existing approaches and achieves 74% precision and 83% recall for the collection of domain-specific terms not fully covered by our knowledge base. Also our method improves precision of WSD from 52% to 78% for the considered terms.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "8af45e306d7dac261c7cc3746b91ed03f3362432",
    "url": "https://www.semanticscholar.org/paper/8af45e306d7dac261c7cc3746b91ed03f3362432",
    "title": "Description Extracting an ontology of persons from Wikipedia",
    "abstract": "The focus of this research is the automatic extraction of an ontology of persons in Information Technology. Our approach involves the extraction of a categorization hierarchy of Wikipedia, the extraction of information about persons and the extraction of relations between persons. We have investigated the suitability of Wikipedia to extract social relations. Our research indicates that the infoboxes are reliable sources to extract attributes and relations, however extracting relations from the article text itself is much more difficult. The Web is an enormous resource to gather information about people and the relations between them. Since the rise of the Web a lot of research has been carried out to mine social networks from it. Previous research has mainly focused on the use of normal web pages to extract such a network (e.g. [1], [2]). We investigate the use of Wikipedia as a resource to extract an ontology of people. In particular, we will create an ontology of people in Information Technology. We have mainly focused on the extraction of social relations. Besides acquiring relations among people, we will also extract other information, for example the field they are working in. Such an ontology could give a quick overview of a particular field (e.g. who has a lot of relations) or might be a good start ontology which could be extended with other information. Wikipedia has several advantages compared to other resources. Information about people and their relations is often explicitly stated in predefined templates, which are present on the pages as an \u2018infobox\u2019. Moreover, we can avoid the problem of word sense disambiguation for the names of people, since people in Wikipedia are uniquely identified by the URI of their corresponding page. Our approach can be applied to construct an ontology of all people present in Wikipedia. It consists of the following steps: First, we extract the categorization structure of Wikipedia of people (in our case, we restrict our approach by only extracting a categorization hierarchy of persons in IT). Then we use this categorization structure to determine if a particular page in Wikipedia actually represents a person (i.e. it has to belong to the previous extracted categorization structure). Once we have acquired all pages that represent a person, we extract attributes of these persons (e.g. name and institution) and their relations with others. Relations extracted from an infobox can be labeled directly. For internal links, we first use the Stanford Parser to obtain a parse tree of the sentence in which it occurs. We use this tree to extract the relation when the particular link is part of a prepositional phrase. The final ontology is represented in OWL. In total we extracted 151 categories. We retrieved 2843 persons, 787 out of them had an infobox on their page. The extracted ontology can be found below. Fig. 1: Sub ontology of persons in IT Fig. 2: Extracted ontology of people in IT We used 300 articles to adjust our approach. We then took a set of 700 articles from which we manually evaluated 48 labeled relations. Extracting information and relations from infoboxes proved to be very reliable. However, extracting relations using the other links is not trivial. The main problem is the correct labeling of the relations. Sentences in Wikipedia are sometimes not grammatically correct and in many cases very complex. Therefore parsing such a sentence often does not result in a correct parse tree. The relations extracted between people are looser than the conditions normally used for two persons to be related. Persons in our ontology can be related under loose circumstances, for example person X was influenced by person Y or X said something about Y. Some relations between persons were hardly relevant, and might be a too loose relation to be taken up in an ontology. We believe that Wikipedia alone is not a sufficient resource to obtain an ontology of persons. Most persons are not represented in Wikipedia by an article, thus the created ontology gives a very limited view. However, we do believe that the created ontology might be a very suitable starting ontology to enrich with information from other resources (e.g. normal websites), and that especially information and relations extracted from the infoboxes are very reliable. These relations also indicate strong social relationships (e.g. spouse, doctoral student, etc.), while the links in the articles itself are sometimes hardly related to the person the article is about. In our evaluation, we observed that some pages were wrongly categorized as persons. This could be filtered by applying heuristics such as only including pages which contain a birth date. Furthermore, a more interesting ontology can easily be constructed by combining the obtained categorization hierarchy with the extracted person instances. [1] Yutaka Matsuo, Hironori Tomobe, Koiti Hasida, Mitsuru Ishizuka. Mining Social Network of Conference Participants from theWeb. In Proceedings of the International Conference on Web Intelligence, pp.190\u2013194, 2003. [2] Lada A. Adamic and Eytan Adar. Friends and neighbors on the web. Social Networks, 25(3):211-230, July 2003.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "10439e3c350ecb4d7dd40800aa4cce41b2786e26",
    "url": "https://www.semanticscholar.org/paper/10439e3c350ecb4d7dd40800aa4cce41b2786e26",
    "title": "A Semantic Method for Searching Knowledge in a Software Development Context",
    "abstract": "The FACIT-SME European FP-7 project targets to facilitate the use and sharing of Software Engineering (SE) methods and best practices among software developing SMEs. In this context, we present an automatic semantic document searching method based on Word Sense Disambiguation which exploits both syntactic and semantic information provided by external dictionaries and is easily applicable for any SME. 1 Introduction and Motivation Over the last years, in Europe, software development is becoming a bottleneck in the development of the Information Society, especially for SMEs (Small and Medium Enterprises) which need to allocate mostly all of their available resources on its production rather than on new technology training. The main goal of the European FP7 3 years project \u201cFacilitate IT-providing SMEs by Operationrelated Models and Methods (FACIT-SME)\u201d is to facilitate IT SMEs in sharing and (re)using SE methods, tools, and experiences for systematically designing and developing their applications integrated with the business processes. In order to achieve this goal, the project proposes a novel Open Reference Model (ORM) [4] serving as an underlying knowledge backbone which stores existing reference knowledge for software-developing SMEs, including different engineering methods, tools, quality model requirements, and enterprise model fragments of IT SMEs in a computer-processable form. On top of the ORM repository, a customizable Open Source Enactment System (OSES) [3] provides IT support for the project-specific application of the ORM. As key part of the OSES, specific query-based search methods support the organizations in: finding a new methodology, by selecting ORM elements that best match given specific enterprise objectives (i.e., \u201cFrom Scratch\u201d scenario); improving a given existing methodology, suggesting the ORM information most relevant to it (i.e., \u201cFrom methodology\u201d scenario). ? This extended abstract summarizes the research work we performed in the first two years of the FACIT-SME project, including a summarization of the preliminary results described in [12] (SEKE 2011). It was partially supported by the European Community\u2019s Seventh Framework Programme managed by REA Research Executive Agency (http://ec.europa.eu/research/rea)([FP7/2007-2013][FP7/2007 2011]) ? This extended abstract summa izes the research work we performed in the first two years of the FACIT-SM project, including a summarization of the preliminary results described in [12] (SEKE 2011). It was parti lly supported by the European Community\u2019s Sev nth Fram work P ogramme managed by REA Research Executive Agency (http://ec.europa.eu/research/rea)([FP7/2007-2013][FP7/2007 2011]) Nicola Ferro and Letizia Tanca (Eds.): SEBD 2012, Edizioni Libreria Progetto, Padova, Italia ISBN: 978-88-96477-23-6, Copyright (c) 2012 Edizioni Libreria Progetto and the authors 116 S. Bergamaschi, R. Martoglia, and S. Sorrentino In our research work, we focus on search/filtering methods by taking advantage of the textual information (which we will refer to as documents) stored in the ORM and/or already available in each enterprise. In this context, queries are provided in textual form, e.g. keywords/sentences about the company background and project requirements, or even existing methodology descriptions (for the second scenario). Standard search methods based on syntactic techniques [5] are often inadequate to capture the similarity between documents, as they do not consider the semantics associated with the terms composing documents. For instance, without exploiting semantics, i.e., synonyms and related terms, the piece of document D1 \u201c...clients for your small business enterprise...\u201d would wrongly be deemed as irrelevant to the query fragment Q1 \u201c....product requirements specified by the customer...\u201d. Moreover, terms might be ambiguous, i.e., they may have more than one possible meaning. For instance, even if the piece of document D2 \u201cDistributed applications partition workloads between servers and clients...\u201d contains \u201cclient\u201d, the term is used in a completely different context, thus it should not be presented among the results. In this paper, we propose a semantic method, implemented in the Semantic Helper component of the FACIT-SME solution, for searching ORM documents. It exploits a standard information retrieval weighting/ranking scheme extended to take into account synonyms and related terms information, together with Word Sense Disambiguation (WSD) techniques, and leads to the following achievements: (1) it is a fully automatic and semantic method that overcomes the standard syntactic technique limitations; (2) it is devised for IT SMEs, providing them with a flexible and easy-to-apply method that does not require big investments or knowledge prerequisites. The rest of the paper is organized as follows: in Sections 2, we describe the Semantic Helper and the phases of the processes it supports; in Section 3, we describe the experimental evaluation of our method, while Section 4 concludes the work and briefly analyzes related works. 2 The Semantic Helper The Semantic Helper supports the FACIT-SME solution by performing two main processes (see Figure 1): 1. Semantic Glossary Population: during this off-line process, statistical and semantic information are automatically extracted from the ORM documents and stored in a repository called Semantic Glossary; 2. Relevant Document Ranking/Retrieval: in this online process, user queries are processed and relevant documents are identified by exploiting the information provided by the Semantic Glossary. In these two processes, we can identify three main phases: (a) keyword extraction and enrichment; (b) Semantic Glossary generation; (c) semantic similarity computation. Keyword Extraction and Enrichment. The goal of this phase (involved in both processes) is to automatically extract, normalize and disambiguate terms Nicola Ferro and Letizia Tanca (Eds.): SEBD 2012, Edizioni Libreria Progetto, Padova, Italia ISBN: 978-88-96477-23-6, Copyright (c) 2012 Edizioni Libreria Progetto and the authors A Semantic Method for Searching Knowledge in Software Development 117 Keyword Extrac.on and Enrichment ORM Repository Sema0c Glossary WordNet IEEE Vocabulary New Document Semantic Glossary Population (offline process) Keyword Extrac.on and Enrichment Sema0c Glossary Relevant Document Ranking/Retrieval (online process) Query Seman.c Similarity Computa.on ORM Document Ranking Knowledge Sources WordNet IEEE Vocabulary",
    "venue": "Sistemi Evoluti per Basi di Dati",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "feda33fe9c369b386909ed4ef9d4fb45b131d23f",
    "url": "https://www.semanticscholar.org/paper/feda33fe9c369b386909ed4ef9d4fb45b131d23f",
    "title": "TERMINAE : a method and a tool to build a domain ontology",
    "abstract": "The purpose of TERMINAE is to help building an ontology, both from scratch and from texts, without control by any task. Requirements have been defined for a methodology on the basis of real experiments. TERMINAE fulfills these requirements, involving theoretical bases from linguistics and knowledge representation. Its strong points are integration of a terminological approach and an ontology management, precise definition of concept types reflecting modeling choices, and traceability facilities. This paper presents briefly the underlying methodology of the tool, which is under development in Java, to introduce the demo. A more comprehensive presentation is given in [2]: 1 A terminological point We use the term \u201ddomain ontology\u201d, with the meaning of \u201dontology\u201d in [12]: \u201d An ontology is a hierarchically structured set of terms for describing a domain that can be used as a skeletal foundation for a knowledge base \u201d. This definition seems to be totally compatible with that of [7]: \u201d An ontology is a logical theory accounted for the intended meaning of a formal vocabulary, i.e. its ontological commitment to a particular conceptualization of the world \u201d. We use also the same distinction between top-level ontology, domain ontology, task ontology and application ontology as in [7]. We agree too with Guarino\u2019s definition of a knowledge base as being obtained by specialization of an ontology to a particular state of the world. In TERMINAE, the computer-aided tool presented in this paper, we often use the term \u201dknowledge base\u201d for \u201dontology of a generic knowledge base\u201d because what we speak about is always state-independent, and the term \u201dontology management\u201d is not yet widely used. TERMINAE is used as an \u201dontology management\u201d tool, even if its representation language allows the description of facts by individual concepts. 2 A lesson from experiments The lesson we learnt through our modeling experiments reinforces the literature on ontology: when modeling is controlled by the final task and when the domain is well established or concrete, modeling is easier than without any application bias or when the domain is new, informal, and hardly investigated. When there is no task to drive the domain building, the modeling looks like the work of a linguist, lexicologist or terminologist. The major difference is that the final modeling has to be used not only for human understanding or translation, but also for automatic inferences, which need more formal modeling. Researchers from computational lexical semantics ([10],[8]) have begun to investigate the close relations between these two approaches, domain ontology modeling and computational lexical semantics. The description of all the possible semantic uses of a word may be possible if it is restricted to a specialized domain relatively to a corpus, while it seems an inaccessible goal in general language. What is needed in both domains is understanding, i.e making \u201d reasonable \u201d inferences. Linguistic methods to define lexical items or terms (lexical items in a specialized domain) are usually introspection (traditional in classical lexicography) and, more recently, corpus analysis (traditional in classical terminology). Even if they are not formal, linguistic methods are rigorous, and there are now usable linguistic tools to help the work. We think, as others ([11], [1], [9]), that domain modeling would benefit from a close interaction between linguistic methods or tools and computer-aided knowledge engineering methods or tools. Ontology, terminology, and lexical semantics aim to describe the world through the words of the language, in all language\u2019s generality for lexical semantics, restricted to a technical domain for terminology and for ontology as we have defined it. Our idea is to push the integration of these disciplines as far as possible into a tool, TERMINAE. 3 Some requirements for TERMINAE Since the beginning of the 90\u2019s, a lot of principles have been elicited for the design of ontologies; the best known may be those of [5], [6]. A lot of ontologies have been designed in big or small projects (see [4] for a review of worldwide known projects and the general literature from the recent conferences or workshops on ontologies or modeling [13], [14] ,[15] ,[16]. But researchers are still asking for guidelines and methodologies, and building usable or reusable ontologies faces the same difficulties. To these existing principles, we propose to add some requirements that fit the need we met during our work on modeling. All the experiments faced the problem of building an ontology of a domain from texts and we needed a tool to help us. Some exist, but none fitting our needs. We wanted to have a linguistic approach, to take advantage of the method and techniques existing in the terminology domain. We wanted a CAKE tool to help the human task as much as possible. We wanted a formal ontology to help validate the ontology, while avoiding most of the common mistakes such as redundancy and inconsistency, and we also wanted to be able to query the ontology and make inferences. This led to the following requirements for building a domain ontology from scratch and from texts without being task-driven. * Linguistic-based methods: Linguistic methods such as the study of terminology are required. Terminology is studied from domain texts, that is to say a description of a term is elaborated from its occurrences in the texts. * A typology of concepts to highlight the modeling choices: When modeling an ontology, different types of concepts are elaborated. Some come from the text, others from the type of text, from the domain, from metaknowledge, from common-sense knowledge. Some are introduced to structure the ontology bottom-up or top-down. It is important to be able to distinguish the modeling choices in order to understand and maintain the ontology. * Formality to avoid as far as possible incoherence and inconsistencies: The support of an ontology has to be formal to avoid incoherence and to allow further inferences. The drawback of this option is a loss of meaningful substance but this is the price of correctness and automation. * Traceability, maintainability, back linking to texts: A condition of usability is the ability to understand the ontology, i.e. to be able to decide if the underlying conceptualization fits the addressed problem or domain or not. This implies a documented ontology, with links to its sources and comments on the modeling process. TERMINAE has been built to meet these requirements.",
    "venue": "",
    "citationCount": 16,
    "fieldsOfStudy": null
  },
  {
    "paperId": "46a35809417b183122cd7010dfcb30eddc095702",
    "url": "https://www.semanticscholar.org/paper/46a35809417b183122cd7010dfcb30eddc095702",
    "title": "Natural Language Processing and Information Systems, 15th International Conference on Applications of Natural Language to Information Systems, NLDB 2010, Cardiff, UK, June 23-25, 2010. Proceedings",
    "abstract": null,
    "venue": "International Conference on Applications of Natural Language to Data Bases",
    "citationCount": 9,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "ef43c228b0f3b2d7da4bec8c3cde46e35fb97307",
    "url": "https://www.semanticscholar.org/paper/ef43c228b0f3b2d7da4bec8c3cde46e35fb97307",
    "title": "A Fully Semantic Approach to Large Scale Text Categorization",
    "abstract": null,
    "venue": "ISCIS",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "afbf416461ee135b60d350379777309bfa6868bc",
    "url": "https://www.semanticscholar.org/paper/afbf416461ee135b60d350379777309bfa6868bc",
    "title": "An Approach to Improve the Representation of the User Model in the Web-Based Systems",
    "abstract": "A major shortcoming of content-based approaches exists in the representation of the user model. Content-based approaches often employ term vectors to represent each user's interest. In doing so, they ignore the semantic relations between terms of the vector space model in which indexed terms are not orthogonal and often have semantic relatedness between one another. In this paper, we improve the representation of a user model during building user model in content-based approaches by performing these steps. First is the domain concept filtering in which concepts and items of interests are compared to the domain ontology to check the relevant items to our domain using ontology based semantic similarity. Second, is incorporating semantic content into the term vectors. We use word definitions and relations provided by WordNet to perform word sense disambiguation and employ domain-specific concepts as category labels for the semantically enhanced user models. The implicit information pertaining to the user behavior was extracted from click stream data or web usage sessions captured within the web server logs. Also, our proposed approach aims to update user model, we should analysis user's history query keywords. For a certain keyword, we extract the words which have the semantic relationships with the keyword and add them into the user interest model as nodes according to semantic relationships in the WordNet. Keywords-User model; Domain ontology; Semantic Similarity; Wordnet.",
    "venue": "",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Chemistry",
      "Computer Science"
    ]
  },
  {
    "paperId": "fc56a60a921c51d5fc483acb7f5fd6bb414b34cf",
    "url": "https://www.semanticscholar.org/paper/fc56a60a921c51d5fc483acb7f5fd6bb414b34cf",
    "title": "A New Hybrid Semantic Similarity Measure Based on WordNet",
    "abstract": null,
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "45ff749bad13ec1973ec47563ba42eef970c1b21",
    "url": "https://www.semanticscholar.org/paper/45ff749bad13ec1973ec47563ba42eef970c1b21",
    "title": "TopX: efficient and versatile top-k query processing for semistructured data",
    "abstract": null,
    "venue": "The VLDB journal",
    "citationCount": 110,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4ecf1f8c380f0e75dbf597b550e8193df368e926",
    "url": "https://www.semanticscholar.org/paper/4ecf1f8c380f0e75dbf597b550e8193df368e926",
    "title": "Semantic pattern learning through maximum entropy-based WSD technique",
    "abstract": "This paper describes a Natural Language Learning method that extracts knowledge in the form of semantic patterns with ontology elements associated to syntactic components in the text. The method combines the use of EuroWordNet\u2019s ontological concepts and the correct sense of each word assigned by a Word Sense Disambiguation(WSD) module to extract three sets of patterns: subject-verb, verb-direct object and verb-indirect object. These sets define the semantic behaviour of the main textual elements based on their syntactic role. On the one hand, it is shown that Maximum Entropy models applied to WSD tasks provide good results. The evaluation of the WSD module has revealed a accuracy rate of 64% in a preliminary test. On the other hand, we explain how an adequate set of semantic or ontological patterns can improve the success rate of NLP tasks such us pronoun resolution. We have implemented both modules in C++ and although the evaluation has been performed for English, their general features allow the treatment of other languages like Spanish. This paper has been partially supported by the Spanish Government (CICYT) project number TIC2000-0664-C0202.",
    "venue": "CoNLL",
    "citationCount": 21,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "9cfcc454a1e112c1878e48ea0eec9a3ff8ffe37c",
    "url": "https://www.semanticscholar.org/paper/9cfcc454a1e112c1878e48ea0eec9a3ff8ffe37c",
    "title": "Knowledge Extraction from Classification Schemas",
    "abstract": null,
    "venue": "CoopIS/DOA/ODBASE",
    "citationCount": 20,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0e3ae2e41b49574c901ab02bd612f6043d33d98a",
    "url": "https://www.semanticscholar.org/paper/0e3ae2e41b49574c901ab02bd612f6043d33d98a",
    "title": "Engineering lexical semantics for natural language processing systems",
    "abstract": "Educated adults apply a vocabulary of at least 100,000 words when they read a domain independent text such as a newspaper. Building a lexicon large enough to handle domain independent text is one of the major engineering problems in Natural Language Processing (NLP). Generating the semantic information for a lexicon, including selectional restrictions on the subjects and objects of verbs, is especially difficult because the information is not readily available from a single source such as a machine readable dictionary or sample text in a corpus. Selectional restrictions are important for domain independent text because they can help disambiguate frequently occurring words which tend to have many word senses. Generating a lexicon with semantics involves a typical engineering tradeoff between computing resources (e.g., processing and memory) and performance on an application (e.g., percent correct word sense disambiguation). \nMy research focused on 3 key questions: (1) How does an NLP engineer build a large lexicon that contains semantic information? (2) How does an NLP engineer choose appropriate semantic information for selectional restrictions? (3) How does an NLP engineer automate the acquisition of selectional restrictions? \nI implemented a program to automatically convert and merge knowledge from WordNet, Semcor and CELEX into a lexicon that can be used by an efficient NLP system called Register Vector Grammar (RVG). I proposed a general process for choosing appropriate semantic information for selectional restrictions and demonstrated the application of the process to domain independent text. I developed a system to learn selectional restrictions from samples of text corpora. My approach built on the work of Resnik and others by taking into account the sense of the verb predicate and the noun argument. I applied a novel procedure for testing selectional restrictions for over-restriction and under-restriction. Experiments were performed to refine the lexicon learning process and test the resulting selectional restrictions on new sentences that were not used in the learning process. Results show that the learned selectional restrictions: (1) show a reasonable engineering tradeoff between over-restriction and under-restriction. (2) show a level of performance on word sense disambiguation that is close to state-of-the-art techniques which are not yet implemented in an efficient NLP system.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "8db4e74574ff65ea51da9851c3b691d24c0afe52",
    "url": "https://www.semanticscholar.org/paper/8db4e74574ff65ea51da9851c3b691d24c0afe52",
    "title": "Classifying Domain-Specific Terms Using a Dictionary",
    "abstract": "Automatically building domain-specific ontologies is a highly challenging task as it requires extracting domain-specific terms from a corpus and assigning them relevant domain concept labels. In this paper, we focus on the second task: i.e., assigning domain concepts to domain-specific terms. Motivated by previous approaches in related research (such as word sense disambiguation (WSD) and named entity recognition (NER)) that use semantic similarity among domain concepts, we explore three types of features - contextual, domain concepts, topics - to measure the semantic similarity of terms; we then assign the domain concepts from the best matching terms. As evaluation, we collected domainspecific terms from FOLDOC, a freely available on-line dictionary for the the Computing domain, and defined 9 domain concepts for this domain. Our results show that beyond contextual features, using domain concepts and topics derived from domain-specific terms helps to improve assigning domain concepts to the terms.",
    "venue": "Australasian Language Technology Association Workshop",
    "citationCount": 10,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6b851ccfb14ac31ce62621faab0595a626ebafab",
    "url": "https://www.semanticscholar.org/paper/6b851ccfb14ac31ce62621faab0595a626ebafab",
    "title": "Automatic Extraction of Patterns Displaying Hyponym-Hypernym Co-Occurrence from Corpora",
    "abstract": "Many of the tasks in computational linguistics, such as information retrieval, document classification, automatic summaries, word sense disambiguation, resolving prepositional phrase attachment, etc. (see Vossen 2003 for a presentation of the uses of various ontologies in solving different tasks in Natural Language Processing) need good ontologies for their success. The manual development of an ontology requires considerable time and money investments. An alternative way for their development is to extract the relevant content from (domain-specific) corpora. A prerequisite in such an experiment is the inventory of patterns which allow for the instantiation in text of the taxonomic relation organizing the ontology. We ran an experiment in which we identify such patterns in corpora and classify them from a lexical point of view. Another resource on which we rely is WordNet, whose already encoded hyponymy relations help us to identify the patterns in which they occur in corpus.",
    "venue": "",
    "citationCount": 14,
    "fieldsOfStudy": null
  },
  {
    "paperId": "9474032d4701508bb9eae09a5909bf3498ab9d15",
    "url": "https://www.semanticscholar.org/paper/9474032d4701508bb9eae09a5909bf3498ab9d15",
    "title": "Automating ontological annotation with WordNet",
    "abstract": "Semantic Web applications require robust and accurate annotation tools that are capable of automating the assignment of ontological classes to words in naturally occurring text (ontological annotation). Most current ontologies do not include rich lexical databases and are therefore not easily integrated with word sense disambiguation algorithms that are needed to automate ontological annotation. WordNet provides a potentially ideal solution to this problem as it offers a highly structured lexical conceptual representation that has been extensively used to develop word sense disambiguation algorithms. However, WordNet has not been designed as an ontology, and while it can be easily turned into one, the result of doing this would present users with serious practical limitations due to the great number of concepts (synonym sets) it contains. Moreover, mapping WordNet to an existing ontology may be difficult and requires substantial labor. We propose to overcome these limitations by developing an analytical platform that (1) provides a WordNet-based ontology offering a manageable and yet comprehensive set of concept classes, (2) leverages the lexical richness of WordNet to give an extensive characterization of concept class in terms of lexical instances, and (3) integrates a class recognition algorithm that automates the assignment of concept classes to words in naturally occurring text. The ensuing framework makes available an ontological annotation platform that can be effectively integrated with intelligence analysis systems to facilitate evidence marshaling and sustain the creation and validation of inference models.",
    "venue": "",
    "citationCount": 16,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "ded53e69bbbc6c61f78545a4b90f19a43887082d",
    "url": "https://www.semanticscholar.org/paper/ded53e69bbbc6c61f78545a4b90f19a43887082d",
    "title": "Context based information retrieval based on ontological concepts",
    "abstract": "Ontology has rich internal structure. There are relations and constraints between the concepts that is why Ontology has a richer internal structure. Ontology can be used for information retrieval. Ontology is a halfway determination of a conceptual vocabulary to be utilized for formulating knowledge-level hypothesis around a domain of discourse. The key part of ontology is to help knowledge sharing and reuse. The process of allotting descriptions to documents in an IRS is called indexing. In this paper a technique is proposed which improves results. In this technique web pages are stored in xml database. WordNet is used as dictionary for finding synonyms of user's query. The technique is based on context of word. Using context of words helps improvise the search results. The technique is called ontological indexing. This technique is compared with text based search. The words on the web pages are mapped to concepts in ontology. Mapping score is generated for each word. Results of search depend on value of mapping score. Moreover the issue of word sense disambiguation is solved up to some extent using parts of speech tagger.",
    "venue": "International Conference on Information Photonics",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0951b1dcd3da2548699ca0e71b0b7789eb0f7e9c",
    "url": "https://www.semanticscholar.org/paper/0951b1dcd3da2548699ca0e71b0b7789eb0f7e9c",
    "title": "Information retrieval based on semantic query on RDF annotated resources",
    "abstract": "The development of the semantic Web, where Web resources would be semantically annotated based on RDF and shared ontology, provides an opportunity for more effective meaning-based search and retrieval of information available through the Web. In such environment, a free text query can be posted and resolved into its semantics, which direct the information retrieval. One important factor in the performance of such semantic query is finding the right interpretation for the query words (i.e. word sense disambiguation). In this paper, we examine the performance of a new approach to such semantic query in the context of semantic Web.",
    "venue": "2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)",
    "citationCount": 16,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "38d8917e0b8dc7df21763d9c6454d28c3d84975e",
    "url": "https://www.semanticscholar.org/paper/38d8917e0b8dc7df21763d9c6454d28c3d84975e",
    "title": "World WordNet Database Structure: An Efficient Schema for Storing Information of WordNets of the World",
    "abstract": "\n \n WordNet is an online lexical resource which expresses unique concepts in a language. English WordNet is the first WordNet which was developed at Princeton University. Over a period of time, many language WordNets were developed by various organizations all over the world. It has always been a challenge to store the WordNet data. Some WordNets are stored using file system and some WordNets are stored using different database models. In this paper, we present the World WordNet Database Structure which can be used to efficiently store the WordNet information of all languages of the World. This design can be adapted by most language WordNets to store information such as synset data, semantic and lexical relations, ontology details, language specific features, linguistic information, etc. An attempt is made to develop Application Programming Interfaces to manipulate the data from these databases. This database structure can help in various Natural Language Processing applications like Multilingual Information Retrieval, Word Sense Disambiguation, Machine Translation, etc.\n \n",
    "venue": "AAAI",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "bc01a1a27b3a2f154d2a8cc9032f6ea957c6723b",
    "url": "https://www.semanticscholar.org/paper/bc01a1a27b3a2f154d2a8cc9032f6ea957c6723b",
    "title": "Harnessing the Expertise of 70, 000 Human Editors: Knowledge-Based Feature Generation for Text Categorization",
    "abstract": "Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-specific and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two significant problems in natural language processing---synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets confirm improved performance compared to the bag of words document representation.",
    "venue": "Journal of machine learning research",
    "citationCount": 86,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "042a95eca688f6c215bcd9d6570ad38f18b227a9",
    "url": "https://www.semanticscholar.org/paper/042a95eca688f6c215bcd9d6570ad38f18b227a9",
    "title": "Discovering Semantic Relations Using Prepositional Phrases",
    "abstract": null,
    "venue": "ISMIS",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a9387b4cde1250112092a7defa7ea898048d52cf",
    "url": "https://www.semanticscholar.org/paper/a9387b4cde1250112092a7defa7ea898048d52cf",
    "title": "Publishing Knowledge and Transformation of Conventional Web into Semantic Web using Metadata",
    "abstract": "In Semantic Web research, most of the work has focused around the OWL ontology, natural language processing, word sense disambiguation and semantic matching. Few efforts have been made to provide a publishing methodology for metadata-rich Web content for end users. This work encompasses all possible aspects of publishing pure digital content, such as HTML documents, images, video, and other media documents free from metadata. Afterwards the content could be integrated with metadata to provide optimal capability and scalability for document organization, search and navigation in the aim of explicit knowledge publishing. The effort is made on our specific framework to publish documents which are enriched with metadata for better machine reasoning. The first and the second version of the prototype is a Mozilla Firefox extension called Semantic Web (SWeb) browser. SWeb is intended for managing and publishing knowledge and information, as \u201cpublishing knowledge\u201d was missing in the system where we got the opportunity to contribute.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "2e009c1420153cadde2086cbfd532d6777dc1988",
    "url": "https://www.semanticscholar.org/paper/2e009c1420153cadde2086cbfd532d6777dc1988",
    "title": "Novel Semantics-based Distributed Representations for Message Polarity Classification using Deep Convolutional Neural Networks",
    "abstract": ": Unsupervised learning of distributed representations (word embeddings) obviates the need for task-speci\ufb01c feature engineering for various NLP applications. However, such representations learned from massive text datasets do not faithfully represent \ufb01ner semantic information in the feature space required by speci\ufb01c applications. This is owing to the fact that (a) models learning such representations ignore the linguistic structure of the sentences, (b) they fail to capture polysemous usages of the words, and (c) they ignore pre-existing semantic information from manually-created ontologies. In this paper, we propose three semantics-based distributed representations of words and phrases as features for message polarity classi\ufb01cation: Sentiment-Speci\ufb01c Multi-Word Expressions Embeddings(SSMWE) are sentiment encoded distributed representations of multi-word expressions (MWEs) ; Sense-Disambiguated Word Embeddings(SDWE) are sense-speci\ufb01c distributed representations of words; and WordNet embeddings(WNE) are distributed representations of hypernym and hyponym of the correct sense of a given word. We examine the effects of these features incorporated in a convolutional neural network(CNN) model for evaluation on the SemEval benchmarked dataset. Our approach of using these novel features yields 14.24% improvement in the macro-averaged F1 score on SemEval datasets over existing methods. While we have shown promising results in twitter sentiment classi\ufb01cation, we believe that the method is general enough to be applied to many NLP applications where \ufb01ner semantic analysis is required.",
    "venue": "KDIR",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "8545d7798e878f9bb88f7905803b13e85921cf59",
    "url": "https://www.semanticscholar.org/paper/8545d7798e878f9bb88f7905803b13e85921cf59",
    "title": "Ontological Annotation with WordNet",
    "abstract": "Semantic Web applications require robust and accurate annotation tools that are capable of automating the assignment of ontological classes to words in naturally occurring text (ontological annotation). Most current ontologies do not include rich lexical databases and are therefore not easily integrated with word sense disambiguation algorithms that are needed to automate ontological annotation. WordNet provides a potentially ideal solution to this problem as it offers a highly structured lexical conceptual representation that has been extensively used to develop word sense disambiguation algorithms. However, WordNet has not been designed as an ontology, and while it can be easily turned into one, the result of doing this would present users with serious practical limitations due to the great number of concepts (synonym sets) it contains. Moreover, mapping WordNet to an existing ontology may be difficult and requires substantial labor. We propose to overcome these limitations by developing an analytical platform that (1) provides a WordNet-based ontology offering a manageable and yet comprehensive set of concept classes, (2) leverages the lexical richness of WordNet to give an extensive characterization of concept class in terms of lexical instances, and (3) integrates a class recognition algorithm that automates the assignment of concept classes to words in naturally occurring text. The ensuing framework makes available an ontological annotation platform that can be effectively integrated with intelligence analysis systems to facilitate evidence marshaling and sustain the creation and validation of inference models.",
    "venue": "SemAnnot@ISWC",
    "citationCount": 15,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "b22512e106a0c916b2062a644634a7c5e645c3d3",
    "url": "https://www.semanticscholar.org/paper/b22512e106a0c916b2062a644634a7c5e645c3d3",
    "title": "An insight into semantic similarity aspects using WordNet",
    "abstract": "Semantic similarity is becoming a generic issue in variety of applications in areas of information retrieval, computational linguistic and AI, both in the academia and industry. Examples include: computing semantic similarity, word sense disambiguation, text segmentation, multimodal document retrieval, image retrieval, etc. However, semantic similarity measures have been used showing mixed chances of success. The basic problem is that if semantic measures are used bluntly without understanding, they might decrease retrieval efficiency. There is a need to investigate semantic similarity approaches in order to have better understanding of these approaches. Several semantic methods for determining semantic similarity between terms have been proposed in the literature and most of them have been tested on WordNet. In this paper, we investigate the approaches to compute semantic similarity by mapping word concepts to WordNet ontology and by examining their relationship in that ontology. The paper then provides specific examples for explaining these approaches Further, the paper categorises and compares various approaches for measuring semantic similarity using WordNet ontology.",
    "venue": "International Journal of Information and Communication Technology",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "70ea782a7fc01c7e13da050171b158967da12048",
    "url": "https://www.semanticscholar.org/paper/70ea782a7fc01c7e13da050171b158967da12048",
    "title": "Merging Global and Specialized Linguistic Ontologies Bernardo Magnini and",
    "abstract": "There is an increasing interest in linguistic ontologies (e.g. WordNet) for a variety of content-based tasks, including conceptual indexing, word sense disambiguation and cross-language information retrieval. A relevant contribution in this direction is represented by linguistic ontologies with domain specific coverage, which are a crucial topic for the development of concrete application systems. This paper tries to go a step further in the direction of the interoperability of specialized linguistic ontologies, by addressing the problem of their integration with global ontologies. This scenario poses some simplifications with respect to the general problem of merging ontologies, since it enables to define a strong precedence criterion so that terminological information overshadows generic information whenever conflicts arise. We assume the EuroWordNet model and propose a methodology to \u201cplug\u201d specialized linguistic ontologies into global ontologies. Experimental data related to an implemented algorithm, which has been tested on a global and a specialized linguistic ontology for the Italian language, are provided.",
    "venue": "",
    "citationCount": 33,
    "fieldsOfStudy": null
  },
  {
    "paperId": "8beca2441d862fb2960ad4713077e8ccbb896560",
    "url": "https://www.semanticscholar.org/paper/8beca2441d862fb2960ad4713077e8ccbb896560",
    "title": "Concept-based information access",
    "abstract": "Concept-based access to information promises important benefits over keyword-based access. One of these benefits is the ability to take advantage of semantic relationships among concepts in finding relevant documents. Another benefit is the elimination of irrelevant documents by identifying conceptual mismatches. Concepts are mental structures. Words and phrases are the linguistic representatives of concepts. Due to the inherent conciseness of natural language, words can represent multiple concepts and different words may represent the same or very similar concepts. Word sense disambiguation attempts to resolve this ambiguity using contextual information. The use of an ontology facilitates identification of related concepts and their linguistic representatives given a key concept. Latent semantic analysis, on the other hand, attempts to reveal the hidden conceptual relationships among words and phrases based on linguistic usage patterns. In this work we explore the potential of concept-based information access via these two methods. We examine under what circumstances concept-based access becomes feasible and improves user experience.",
    "venue": "International Conference on Information Technology: Coding and Computing (ITCC'05) - Volume II",
    "citationCount": 16,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e19801ac027492f06b2ef10fbf3d0c3ed9737b24",
    "url": "https://www.semanticscholar.org/paper/e19801ac027492f06b2ef10fbf3d0c3ed9737b24",
    "title": "Naive semantics for natural language understanding",
    "abstract": null,
    "venue": "",
    "citationCount": 167,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "eb7f3901844aac05e3786290c25d687caaa0dfe0",
    "url": "https://www.semanticscholar.org/paper/eb7f3901844aac05e3786290c25d687caaa0dfe0",
    "title": "Clustering by committee",
    "abstract": "Text contains a wealth of knowledge about who we are, what we know, how we think, and how we communicate. We are just beginning to tap into the information that is available in the tales we read to our children, the narratives that capture our thoughts, and the stories that shape our world. In this work, we present some recent advances in automatically acquiring knowledge from text. We propose a general-purpose clustering algorithm called CBC (Clustering By Committee) from which we will organize documents according to topics as well as discover concepts and word senses. We will explore the value of these systems by experimenting with two novel evaluation methodologies that attempt to define what a word sense is and define the quality of a particular clustering. \nCBC addresses the general goal of clustering, which is to group data elements such that the intra-group similarities are high and the inter-group similarities are low. Using sets of representative elements called committees, CBC attempts to discover cluster centroids that unambiguously describe the members of a possible class. CBC will be shown to outperform several common clustering algorithms in document clustering and concept discovery tasks. Document clustering is practical in many information retrieval tasks such as document browsing and the organization and viewing of retrieval results. Broad-coverage lexical resources such as WordNet are extremely useful but are mostly hand generated. They often include many rare senses while missing domain-specific senses. Automatically generating them is useful for many applications such as word sense disambiguation, question answering and ontology construction. Sample concepts discovered by CBC include baking ingredients, symptoms, academic departments, Impressionists, Canadian provinces, musical instruments, and emotions. \nWe present two novel evaluation methodologies. The first is based on the editing distance between output clusters and a manually constructed answer key. It defines how much work is necessary in order to convert from one to the other. For the word sense discovery system, we present an evaluation methodology for measuring the precision and recall of discovered senses. Using WordNet, we formulate what is a correct sense of a word.",
    "venue": "",
    "citationCount": 118,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "5b0ff699ea478b6ea997a41f5ca68836f0228aae",
    "url": "https://www.semanticscholar.org/paper/5b0ff699ea478b6ea997a41f5ca68836f0228aae",
    "title": "Semantic Metrics , Conceptual Metrics , and Ontology Metrics : An Analysis of Software Quality Using IR-based Systems , Potential Applications and Collaborations",
    "abstract": "Similarities and differences between \u201csemantic metrics\u201d (metrics defined on a knowledge-based IR system) and \u201cconceptual metrics\u201d (metrics defined on a Latent Semantic Indexing-based IR system) are discussed. Potential collaboration areas between research groups are identified. Potential application and collaboration areas of a new research area called \u201contology metrics,\u201d metrics calculated on the ontologies that form part of an ontology-based software system, are also discussed. Currently ontology metrics are calculated using techniques similar to semantic metrics, but other semantically-based expansions, including some similar to conceptual metrics are possible. Application areas of Information Retrieval (IR) based approaches have in the past included concept location [13], clone detection [8], traceability link recovery [11], and software reuse [5,6], among others. One relatively new application area is the following: in the past few years, the use of IR based techniques has been extended to examine software quality, through metrics based on the output of IR-based systems. This new field, introduced by Etzkorn et al. [7,8] and furthered by Stein et al. [16,17,18] and Cox et al.[3], and called \u201csemantic metrics,\u201d has recently been extended by Marcus and Poshyvanyk [12,20], and by them called \u201cconceptual metrics.\u201d \u201cConceptual metrics\u201d are different from \u201csemantic metrics\u201d in that semantic metrics are defined in the context of comment and identifier analysis within a program understanding system based on knowledge-based natural language understanding, whereas the \u201cconceptual metrics\u201d are defined in the context of comment and identifier analysis within a program understanding system based on Latent Semantic Indexing. Latent Semantic Indexing (LSI) is a corpus-based statistical method for representing the meaning of natural language words and passages (sentences, paragraphs, etc.). With LSI, meanings are derived from usage rather than from a predefined dictionary[11]. Both types of IR-based metrics, semantic metrics and conceptual metrics, have the advantage that they can be used to measure some aspects of software design, such as cohesion, which are difficult to measure [7] using traditional syntactic metrics [2] based solely on counting items in the source code. Both types of IR-based metrics could also be used to analyze design qualities in, for example, software design documents [18] as well as in source code, which would expand metrics capability far beyond that provided by most traditional syntactic metrics. The advantages and disadvantages between semantic metrics and conceptual metrics would seem to be primarily based on the differences between the underlying natural language-based program understanding systems: knowledge-based or LSI based. For example, one primary disadvantage of a knowledge-based natural language understanding system is the necessity to create knowledge bases for each new domain examined (or possibly one large knowledge base to cover several domains). An additional related disadvantage of the knowledge-based system approach would be a potential lack of scalability due to the difficulties involved in word sense disambiguation in large knowledge-bases. Therefore, the LSI approach is possibly more easily scalable than the knowledge based system approach. (We note here, however, that there are a number of variations of Word Sense Disambiguation (WSD) techniques used for IR that do not depend on complete knowledge bases. Ozcan and Aslandogan [15] performed a study that employed various WSD and LSI techniques separately with success.) However, LSI can be criticized for not making use of word order, morphology, or the syntactic relations between words [13], so although overall it has shown to work well [13], potentially it could work better on some input data than others. For example, Deetwester et al. [4] found a substantial improvement over traditional term (keyword style) indexing using latent semantic indexing in one corpus (13% improvement on the MED corpus) of two that they tested, while they found no improvement on the other corpus (the CISI corpus). They suggested the poor performance in the CISI corpus was potentially due to the homogeneous nature of the dataset. However, in areas where both semantic metrics and conceptual metrics can be collected easily (data areas where both knowledge-based program understanding and LSI-based program understanding will easily operate; for example, where knowledge bases are available and the corpus involved is nonhomogeneous), a comparison between semantic metrics and conceptual metrics intended to measure the same software qualities would be interesting. Each type of metric has, in the past, been used to measure the same quality. For example, the software quality \u201ccohesion\u201d has been measured using semantic metrics [3,17] and conceptual metrics [12]. In these papers, the semantic and conceptual metrics have been compared to existing syntactic metrics, but not (to the author of this paper\u2019s knowledge) to each other. One interesting expansion of both kinds of metrics would be to investigate applying variations of already proposed semantic or conceptual metrics to comment and identifier understanding that employs other variations of IR, and preferably industrial strength IR implementations, or possibly combinations of existing IR approaches. Furthering this idea would be an examination of how previously defined semantic metrics and cognitive metrics would be defined differently when based on different kinds of IR systems. For example, semantic metrics are defined in the context of a knowledge-base loosely based on the conceptual graph representation format [7]. How would the definition of the semantic metrics change when defined on different knowledge representation formats, for example, a knowledgebase defined in the Resource Description Framework (RDF) knowledge representation format? Similar questions could potentially be raised for conceptual metrics if applied to different kinds of IR systems. This question leads into another research area, that of metrics to measure ontology-based systems, otherwise known as ontology metrics, or ontology-based metrics [14,19]. In ontology-based systems, ontologies provide a common, shared interface definition between applications. One example application area is internet-based systems providing Business to Business or Business to Consumer services. These types of service-oriented architectures, as well as other web based systems, often employ ontology data in a description of web services. Such ontology data is often represented in Resource Description Format (RDF), and stored in XML [14]. The same kinds of software processes are used to develop ontology-based systems as are used for more traditional software systems that do not employ ontologies. However, an integral part of an ontology-based system is the development of an ontology. Therefore, an important part of measuring the quality of an ontology-based system is measuring the quality of the ontologies themselves. Recently, Orme, Yao, and Etzkorn [14] and Yao, Orme, and Etzkorn [19] examined ontology cohesion and coupling, using techniques similar to the calculation of the earlier semantic metrics within the knowledge base of the program understanding system, but in this case the calculations were performed directly on the ontologies themselves, which were stored in RDF format. The intent of this work was to examine ontology cohesion and coupling as they might affect a software system that employed those ontologies. Other work, by Burton-Jones, et al. [1], examined ontology quality of individual ontologies based on a semiotic framework derived from linguistics theory that includes general elements of quality based on linguistic-style concepts related to the analysis of signs and symbols. However, the work of BurtonJones et al. was not directly targeted toward the use of these ontologies in a software system. In addition to web services employed for internet based systems, bioinformatics and genomics researchers also make heavy use of ontologies. Orme, Yao, and Etzkorn [14] performed a bioinformatics case study in which they examined the integration of existing ontologies to reduce failures that could occur in the runtime use of these ontologies. Substantial work remains in ontology metrics, particularly validating the metrics in various application areas, but also determining how these metrics can best be employed in software maintenance on the new kinds of software applications that make heavy use of ontologies. As mentioned earlier, the ontology metrics of Orme, Yao, and Etzkorn [14] and Yao, Orme, and Etzkorn [19] were calculated similarly to how semantic metrics are calculated within the knowledge base of a program understanding system. However, other more advanced semantic techniques are possible (in the case of these ontology metrics as currently defined, an IR-based program understanding system is not employed; rather, the metrics are calculated based on the relationships as specified in the ontologies themselves). Recently, Kotis, Vouros, and Stergiou [9] examined the automated mapping/merging of ontologies using the meaning of concepts by mapping them to WordNet senses using LSI. This kind of approach could lead to conceptual metrics to measure ontology qualities. Similarly, combinations of the original ontology metrics of Orme, Yao, and Etzkorn [14] and new conceptual metrics for ontologies could potentially be useful.",
    "venue": "",
    "citationCount": 4,
    "fieldsOfStudy": null
  },
  {
    "paperId": "2f40268f6524a247af12749c410ff187579ff9a1",
    "url": "https://www.semanticscholar.org/paper/2f40268f6524a247af12749c410ff187579ff9a1",
    "title": "A Genetic Fuzzy Semantic Web Search Agent Using Granular Semantic Trees for Ambiguous Queries",
    "abstract": "For most Web searching applications, queries are commonly ambiguous because words or phrases have different linguistic meanings for different Web users. The conventional keyword-based search engines cannot disambiguate queries to provide relevant results matching Web users\u2019 intents. Traditional Word Sense Disambiguation (WSD) methods use statistic models or ontology-based knowledge systems to measure associations among words. The contexts of queries are used for disambiguation in these methods. However, due to the fact that numerous combinations of words may appear in queries and documents, it is difficult to extract concepts\u2019 relations for all possible combinations. Moreover, queries are usually short, so contexts in queries do not always provide enough information to disambiguate queries. Therefore, the traditional WSD methods are not sufficient to provide accurate search results for ambiguous queries. In this chapter, a new model, Granular Semantic Tree (GST), is introduced for more conveniently representing associations among concepts than the traditional WSD methods. Additionally, users\u2019 preferences are used to provide personalized search results that better adapt to users\u2019 unique intents. Fuzzy logic is used to determine the most appropriate concepts related to queries based on contexts and users\u2019 preferences. Finally, Web pages are analyzed by the GST model. The concepts of pages for the queries are evaluated, and the pages are re-ranked according to similarities of concepts between pages and queries. DOI: 10.4018/978-1-60566-324-1.ch018",
    "venue": "",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "b76a333553817cf4441cc55f5558f7fa47cfc6ab",
    "url": "https://www.semanticscholar.org/paper/b76a333553817cf4441cc55f5558f7fa47cfc6ab",
    "title": "Semantic Resources and their Applications in Hungarian Natural Language Processing",
    "abstract": "This thesis is about the creation and application of semantic resources in Hungarian natural language processing. The first part of my work deals with applying automatic methods in order to generate a WordNet ontology \u2013 a hierarchical lexicon of word meanings \u2013 for Hungarian. I used several methods to generate automatic Hungarian translation for English WordNet synsets in what is called the expand model of building wordnets. I applied methods described in the literature and also developed new ones specific to Hungarian based on the available machine-readable dictionaries and other resources. The second part of my work focuses on word meaning in the context of polysemy and machine translation. I developed a word-sense disambiguation system to improve the lexical translation quality of an English-to-Hungarian rule-based machine translation system. My WSD system uses classifiers applying supervised machine learning, each trained with local and global features extracted from the training contexts. The classes are Hungarian translations of the ambiguous English lexical items, which improves disambiguation accuracy. I also showed a way to semi-automatically generate training instances for such classifiers using an aligned parallel corpus. In this approach, I have shown that it is essential to recognize idiomatic multi-word expressions formed with the target word in the corpus. In the third part of my work, I proposed a system for noun-phrase coreferenceand possessor-relationship resolution in Hungarian texts. The system uses rules relying on several knowledge sources, among them Hungarian WordNet. I also present shortly applications of my results both in research & development and in industrial projects.",
    "venue": "",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "687138a9f2609844eb5ffc66bda8ec73c0a50c09",
    "url": "https://www.semanticscholar.org/paper/687138a9f2609844eb5ffc66bda8ec73c0a50c09",
    "title": "Handling polysemy in description logic ontologies",
    "abstract": "Semantic Web technology highly depends on the quality of ontology as it reduces or eliminates conceptual confusion and reuses knowledge. In order to enhance quality of ontology, there is one major problem with lexical representation of ontology. Current lexical representation is term which may have different meanings, i.e., the term is polysemous, this can result in frustrating misunderstanding and ambiguity during the management and application of ontology. To solve this problem, sense is used to replace term as the lexical representation of concepts and properties for its unique meaning. The process of handling polysemy is to automatically disambiguate terms in ontology by using its surrounding ontology symbols and its nearby terms in annotated documents using this ontology. The right sense is assigned to a target term by maximizing the relatedness between the target and its neighbors. Experiments show our method has good performance. Comparing with the best word sense disambiguation method, the concept precision is almost 2 times than the precision of noun, and the property precision is almost 3 times than the precision of verb. The last experiment proves that our method is also effective in a semi-automatic process.",
    "venue": "2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "ee9800a1608a78a5a325af33c2b59747e9a40e00",
    "url": "https://www.semanticscholar.org/paper/ee9800a1608a78a5a325af33c2b59747e9a40e00",
    "title": "Web-Based Measure of Semantic Relatedness",
    "abstract": null,
    "venue": "WISE",
    "citationCount": 165,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "68b33cb1bc8e441ccd534ad2d3bc4cd218197eec",
    "url": "https://www.semanticscholar.org/paper/68b33cb1bc8e441ccd534ad2d3bc4cd218197eec",
    "title": "Meaningful Results for Information Retrieval in the MEANING Project",
    "abstract": "The goal of the MEANING project (IST-2001-34460) is to develop tools for the automatic acquisition of lexical knowledge that will help Word Sense Disambiguation (WSD). The acquired lexical knowledge from various sources and various languages is stored in the Multilingual Central Repository (MCR) (Atserias et al 04), which is based on the design of the EuroWordNet database. The MCR holds wordnets in various languages (English, Spanish, Italian, Catalan and Basque), which are interconnected via an Inter-LingualIndex (ILI). In addition, the MCR holds a number of ontologies and domain labels related to all concepts. During the MEANING project, the MCR has been enriched in various cycles. This paper describes the integration and evaluation of the MCR in a commercial classification and (cross-lingual) information retrieval system, developed by Irion Technologies. We carried out a series of task-based evaluations on English and Spanish news collections, for which indexes were built with and without the results of MEANING. The evaluations show that both recall and precision are significantly higher when using the enriched semantic networks in combination with WSD.",
    "venue": "",
    "citationCount": 23,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "b4c867db442d4628640c87c601ae15a40b8bc903",
    "url": "https://www.semanticscholar.org/paper/b4c867db442d4628640c87c601ae15a40b8bc903",
    "title": "AUTOMATIC ANNOTATION OF AMBIGUOUS PERSONAL NAMES ON THE WEB",
    "abstract": "Personal name disambiguation is an important task in social network extraction, evaluation and integration of ontologies, information retrieval, cross\u2010document coreference resolution and word sense disambiguation. We propose an unsupervised method to automatically annotate people with ambiguous names on the Web using automatically extracted keywords. Given an ambiguous personal name, first, we download text snippets for the given name from a Web search engine. We then represent each instance of the ambiguous name by a term\u2010entity model (TEM), a model that we propose to represent the Web appearance of an individual. A TEM of a person captures named entities and attribute values that are useful to disambiguate that person from his or her namesakes (i.e., different people who share the same name). We then use group average agglomerative clustering to identify the instances of an ambiguous name that belong to the same person. Ideally, each cluster must represent a different namesake. However, in practice it is not possible to know the number of namesakes for a given ambiguous personal name in advance. To circumvent this problem, we propose a novel normalized cuts\u2010based cluster stopping criterion to determine the different people on the Web for a given ambiguous name. Finally, we annotate each person with an ambiguous name using keywords selected from the clusters. We evaluate the proposed method on a data set of over 2500 documents covering 200 different people for 20 ambiguous names. Experimental results show that the proposed method outperforms numerous baselines and previously proposed name disambiguation methods. Moreover, the extracted keywords reduce ambiguity of a name in an information retrieval task, which underscores the usefulness of the proposed method in real\u2010world scenarios.",
    "venue": "International Conference on Climate Informatics",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f59e67f1b655f2bcb255ca57a51b9c44898829dc",
    "url": "https://www.semanticscholar.org/paper/f59e67f1b655f2bcb255ca57a51b9c44898829dc",
    "title": "The semantic Knowledge-base of Contemporary Chinese and Its Applications in WSD",
    "abstract": "The Semantic Knowledge-base of Contemporary Chinese (SKCC) is a large scale Chinese semantic resource developed by the Institute of Computational Linguistics of Peking University. It provides a large amount of semantic information such as semantic hierarchy and collocation features for 66,539 Chinese words and their English counterparts. Its POS and semantic classification represent the latest progress in Chinese linguistics and language engineering. The descriptions of semantic attributes are fairly thorough, comprehensive and authoritative. The paper introduces the outline of SKCC, and indicates that it is effective for word sense disambiguation in MT applications and is likely to be important for general Chinese language processing.",
    "venue": "SIGHAN",
    "citationCount": 15,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "8a7f3a27915fb7858250d83faf492008cdd48b19",
    "url": "https://www.semanticscholar.org/paper/8a7f3a27915fb7858250d83faf492008cdd48b19",
    "title": "Gloss in Sanskrit Wordnet",
    "abstract": null,
    "venue": "Sanskrit Computational Linguistics",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "750c161a148961139328dd5ec162fbd1badfceb5",
    "url": "https://www.semanticscholar.org/paper/750c161a148961139328dd5ec162fbd1badfceb5",
    "title": "Robust Statistical Translation Models: The Case for Word Alignment",
    "abstract": "The success of the statistical approaches in language processing has resurrected the interest in machine translation, which on its turn generated very useful results in many areas of language engineering. Within the architecture of a typical Statistical Machine Translation system, using a \"noise-channel\" paradigm, two basic information sources fundamentally influence the automatic translation accuracy: the translation model and the target language model. The translation model encodes statistical information on how words or phrases from the source language are translated in the target language (including wording, local grouping of the translated words, part-of-speech mapping, etc). The references to target and source language texts are generic, identifying the direction of the processing in trying to reveal the equivalence relations over the two parts of a parallel text (bitext). The word alignment is an explicit representation of the pairs of words <wL1 wL2> (called translation equivalence pairs) co-occurring in corresponding parts of a bitext and representing mutual translations. The general word alignment problem includes the cases where words in one part of the bitext are not translated in the other part (these are called null alignments) and the cases where multiple words in one part of the bitext are translated as one or more words in the other part (these are called expression alignments). The target language model encodes the distributional properties of the words in the output language (the grammatical ordering of the words depending on their part-of-speech (POS), idiosyncratic properties of the lexical items (such as case markers). The estimation of the accuracy of a language model is easily evaluated by a POS-tagging validation while for a translation model one could draw meaningful conclusions on the robustness of the model through a word-alignment exercise. Neither POS-tagging nor word-alignment is an end in itself but necessary at one level or another to accomplish most natural language processing tasks. Because of this, is no surprise to see that the natural language research community invested and continue to invest a lot of energy in evaluating the progress in tagging and in word/phrase alignment. The technological competitions for integrated tasks, such as information retrieval, cross-language information retrieval, summarization, question-answering, machine translation, were systematically complemented by organized evaluations of the performances of the critical modules (tagging, chunking, parsing, name or time entity recognition, anaphora resolution, word aligning, word-sense disambiguation, etc). The paper describes and evaluates our state-of-the-art word alignment system that combines two different word aligners, developed with independent motivations. The aligner combination achieves a significantly better result than each individual aligner does. We will report on the latest developments of our word-alignment system, winner of the second word-alignment competition organized in Ann Arbor, Michigan, USA, 2005. It implements a different approach from its predecessor, also a winner in the first word-alignment competition held in Edmonton, Canada, 2003.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "579655f3ece39455ab5b4afed626606aacff2bc7",
    "url": "https://www.semanticscholar.org/paper/579655f3ece39455ab5b4afed626606aacff2bc7",
    "title": "A Hybrid Approach for Relation Extraction Aimed at the Semantic Web",
    "abstract": null,
    "venue": "International Conference on Flexible Query Answering Systems",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e3829f0ed23bb4488eda92fc576ff1fff8700f48",
    "url": "https://www.semanticscholar.org/paper/e3829f0ed23bb4488eda92fc576ff1fff8700f48",
    "title": "Natural Research Front",
    "abstract": "language is most common way to communicate with each other but it's not possible to understand all the languages. To understand diff erent languages machine translation (MT) is required. MT is the most excellent application which helps to understand any other language in very less time and cost. Related to this context some problems are faced by researchers like words which pronounce same but having totally diff erent meaning, few words spelled diff erent but having identical meaning, while in some cases combination of words may change the meaning. Thus Word Sense Disambiguation (WSD) is needed to resolve such kind of problems. Word Sense Disambiguation is used to understand the correct meaning of the word with respect to context in which that is used. WSD is essentially a task of classifi cation. Where word senses are the classes and the context provides the evidence. Every incidence of a word is assigned to one or more of its possible classes based on the evidence. Words are assumed to have a fi nite and discrete set of senses from ontology, a dictionary or a lexical knowledge base. WSD has apparent relationships with other fi elds such as lexical semantics, whose main aim is to defi ne, analyze, and realize the relationships between \" word \" , \" meaning \" , and \" context \" [1]. Signifi cance of WSD has been widely acknowledged in computational linguistics. Obviously WSD is not thought of as an end in itself other than as an enabler for other tasks and applications of natural language processing (NLP) and computational linguistics such as parsing, machine translation, text mining, semantic interpretation, knowledge acquisition and information retrieval. On the other hand, along with its theoretical signifi cance, explicit WSD has not always demonstrated benefi ts in real applications. In general, the WSD module is a black box surrounding an explicit process of WSD that can be dropped into any function, greatly like a syntactic parser or a (POS) part-of-speech tagger. The alternative is to include WSD as a task specifi c \" module \" of a particular application in a precise domain and included so completely into a system that it is hard to isolate. Explicit WSD has not yet been persuasively demonstrated to have a signifi cant positive eff ect on any function [2]. Selection of Word Senses A commonly accepted meaning of a word is a word sense. \u2026",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "43e256c214c40fe73ff49eb51c1896e881a3af70",
    "url": "https://www.semanticscholar.org/paper/43e256c214c40fe73ff49eb51c1896e881a3af70",
    "title": "Natural Language Processing Using Recurrent Neural Networks",
    "abstract": null,
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "3166c22d48939370a35c8ec7ba783aab79e9529b",
    "url": "https://www.semanticscholar.org/paper/3166c22d48939370a35c8ec7ba783aab79e9529b",
    "title": "Language Production, Cognition, and the Lexicon",
    "abstract": null,
    "venue": "Text, Speech and Language Technology",
    "citationCount": 19,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e7fc2c37d09f5fead993bf712a387de966d98cc3",
    "url": "https://www.semanticscholar.org/paper/e7fc2c37d09f5fead993bf712a387de966d98cc3",
    "title": "Exploiting Prolog and NLP Techniques for Matching Ontologies and for Repairing Correspondences",
    "abstract": "Providing efficient ontology matching algorithms is one of the means for pursuing semantic interoperability. In this paper we discuss an algorithm that exploits natural language processing techniques for matching ontologies and that post-processes the obtained alignment in order to find semantic inconsistencies. The algorithm has been entirely implemented in Prolog, whose usefulness was mainly evident in the post-processing phase. A careful analysis of the recent state- of-the art witnesses the originality of our matching algorithm which is based on the \"Adapted Lesk Algorithm\" for word sense disambiguation. The experiments we carried out, although in their early stages, are encouraging.",
    "venue": "",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "2221bee063a183d7fb1d8952f3da0ffb062acd48",
    "url": "https://www.semanticscholar.org/paper/2221bee063a183d7fb1d8952f3da0ffb062acd48",
    "title": "A Terminological Search Algorithm for Ontology Matching",
    "abstract": "Most of the ontology alignment tools use terminological techniques as the initial step and then apply the structural techniques to refine the results. Since each terminological similarity measure considers some features of similarity, ontology alignment systems require exploiting different measures. While a great deal of effort has been devoted to developing various terminological similarity measures and also developing various ontology alignment systems, little attention has been paid to develop similarity search algorithms which exploit different similarity measures in order to gain benefits and avoid limitations. We propose a novel terminological search algorithm which tries to find an entity similar to an input search string in a given ontology. This algorithm extends the search string by creating a matrix from its synonym and hypernyms. The algorithm employs and combines different kind of similarity measures in different situations to achieve a higher performance, accuracy, and stability in comparison with previous methods which either use one measure or combine more measures in a naive ways such as averaging. We evaluated the algorithm using a subset of OAEI Bench mark data set. Results showed the superiority of proposed algorithm and effectiveness of different applied techniques such as word sense disambiguation and semantic filtering mechanism.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ]
  },
  {
    "paperId": "ce537138732832c50cf978378e60d4df69013097",
    "url": "https://www.semanticscholar.org/paper/ce537138732832c50cf978378e60d4df69013097",
    "title": "Towards Automatic Dialogue Understanding",
    "abstract": "In this paper we will present work carried out to scale up the system for text understanding called GETARUNS, and port it to be used in dialogue understanding. The current goal is that of extracting automatically argumentative information in order to build argumentative structure. The long term goal is using argumentative structure to produce automatic summarization of spoken dialogues. Very much like other deep linguistic processing systems(see Allen et al.), our system is a generic text/dialogue understanding system that can be used in connection with an ontology \u2013 WordNet and other similar repositories of commonsense knowledge. Word sense disambiguation takes place at the level of semantic interpretation and is represented in the Discourse Model. We will present the adjustments we made in order to cope with transcribed spoken dialogues like those produced in the ICSI Berkely project. The low level component is organized according to LFG theory; at this level, the system does pronominal binding, quantifier raising and temporal interpretation. The high level component is where the Discourse Model is created from the Logical Form. For longer sentences the system switches from the topdown to the bottomup system. In case of failure it will backoff to the partial system which produces a very lean and shallow semantics with no inference rules. In a final section we present preliminary evaluation of the system on two tasks: the task of automatic argumentative labeling and another frequently addressed task: referential vs. non-referential pronominal detection. Results obtained fair much higher than those reported in similar experiments with machine learning approaches.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Psychology"
    ]
  },
  {
    "paperId": "93afcedc67f405a69a5a7d0ebe9973090fa91fdf",
    "url": "https://www.semanticscholar.org/paper/93afcedc67f405a69a5a7d0ebe9973090fa91fdf",
    "title": "Natural Language Generation with Markov Chains and Grammar",
    "abstract": "How do we assign meaning to words? This project investigates semantics from a lexical perspective, using the WordNet and OpenCyc ontologies to create a semiotic map of our consensus reality. Given a list of words, how can we find the word least like the others? Through a heuristicial search across the hypernym ontology, computational semantics can discover the contextual meaning of words, even when the only context given is the other words from which it must differentiate itself. This method, which has not been given a name previously, will hitherto be known as dynamic word sense disambiguation. Language can be generated stochastically using Markov Chain databases. This project explores the use thereof in junction with the aforementioned semantic web.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "5108c02c950195546ea75983848cb9b0dc98411c",
    "url": "https://www.semanticscholar.org/paper/5108c02c950195546ea75983848cb9b0dc98411c",
    "title": "Information Extraction from Concise Passages of Natural Language Sources",
    "abstract": null,
    "venue": "ADBIS",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "eb70fb7ff4b84b7392c51db3f8af0e59173d300f",
    "url": "https://www.semanticscholar.org/paper/eb70fb7ff4b84b7392c51db3f8af0e59173d300f",
    "title": "Context in the Large: Contextual Issues when Dealing with Thousands of Ontologies",
    "abstract": "Back in 2005 we launched a research programme called \u201cNext Generation Semantic Web Applications\u201d, which pioneered the vision of exploiting the Semantic Web as a source of background knowledge to enable the development of a new class of intelligent applications. This research programme was based on some rather radical tenets, envisaging applications relying not simply on a few hand-picked ontologies, but on the dynamic selection and use of knowledge acquired from thousands of online ontologies . Five years later, this programme has produced a variety of applications, tools and techniques, concretely realizing the paradigm in scenarios such as ontology evolution, relation discovery, word sense disambiguation, and semantic enrichment of folksonomies. An important challenge when building applications that consume knowledge dynamically sourced from thousands of ontologies concerns dealing with the contextual nature of ontological resources. Specifically, new methods were needed to identify automatically whether the context of the original ontology still holds when the knowledge is reused in the application in hand. In addition, we have also looked at this issue in an application-independent way and carried out analytical studies that consider the Semantic Web itself as an object of study and attempt to characterize and understand the relations between the different epistemologies which are at the basis of the published conceptualizations. In particular, we have developed formal notions of agreement and disagreement between ontologies and used these to cluster ontologies which seem to share similar world views. To our knowledge these studies represent the first ever empirical analysis of large scale distributed conceptualizations and provide useful insights into the concrete practices used by ontology engineers. Specifically, these studies make it possible for us to see which contextual viewpoints tend to occur more frequently and which communities share specific conceptualizations. Finally, such results can be used further to develop highly structured ontology repositories, identifying and resolving contextual discrepancies between ontologies to facilitate and improve the efficiency of both automatic and manual access to resources on the Semantic Web.",
    "venue": "CIAO@EKAW",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e010061f99bcdc008b93f1feaab4690c4cf0bc66",
    "url": "https://www.semanticscholar.org/paper/e010061f99bcdc008b93f1feaab4690c4cf0bc66",
    "title": "Extracting Structured Knowledge for Semantic Web by Mining Wikipedia",
    "abstract": "Since Wikipedia has become a huge scale database storing wide-range of human knowledge, it is a promising corpus for knowledge extraction. A considerable number of researches on Wikipedia mining have been conducted and the fact that Wikipedia is an invaluable corpus has been confirmed. Wikipedia's impressive characteristics are not limited to the scale, but also include the dense link structure, URI for word sense disambiguation, well structured Infoboxes, and the category tree. One of the popular approaches in Wikipedia Mining is to use Wikipedia's category tree as an ontology and a number of researchers proved that Wikipedia's categories are promising resources for ontology construction by showing significant results. In this work, we try to prove the capability of Wikipedia as a corpus for knowledge extraction and how it works in the Semantic Web environment. We show two achievements; Wikipedia Thesaurus, a huge scale association thesaurus by mining the Wikipedia's link structure, and Wikipedia Ontology, a Web ontology extracted by mining Wikipedia articles.",
    "venue": "International Workshop on the Semantic Web",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0fc8a28d71d3af8917fe66e0a3340fee7a3dd615",
    "url": "https://www.semanticscholar.org/paper/0fc8a28d71d3af8917fe66e0a3340fee7a3dd615",
    "title": "Conceptual Indexing for Multilingual Information Retrieval",
    "abstract": null,
    "venue": "CLEF",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "7a29a9116d2d3e8f95a4f7c4da1b7782dd91602d",
    "url": "https://www.semanticscholar.org/paper/7a29a9116d2d3e8f95a4f7c4da1b7782dd91602d",
    "title": "An Approach to Reduce Part of Speech Ambiguity Using Semantically Annotated Lexicon Definitions",
    "abstract": "In computational linguistics, the problem of word-sense disambiguation (WSD) is a difficult one and methods using a flat topology of the tokens are not very effective. One solution to this is to use a Part of Speech (POS) tagger before starting the WSD process. However, POS taggers show their limitations when high precision tagging is required or large texts are processed. This paper presents a technique to reduce the POS ambiguity using semantic information. As benchmarks we use as following standard WSD corpuses: Senseval2, Senseval3 and Semcor. Moreover, we tested our approach on WordNet semantically tagged glosses for English and on our own semantically tagged lexicon glosses for Romanian language. Introduction \u201cAll-Words\u201d task for word sense disambiguation (WSD) is a complex pursuit in the field of natural language processing (NLP). WSD systems have improved over time and achieve now 6570% accuracy for the fined-grained all-words task and 78-83% accuracy when a coarse-grained sense inventory is used [8]. Systems using knowledge-based methods are beginning to be the predominant research direction for WSD ([7],[9],[11]). Such knowledge-based methods have the property of low or no variation in decision making when solving sense ambiguity. Best-known results for this type of systems can go up to an accuracy of 83% on coarse-grained all-words task using an algorithm called Structural Semantic Interconnections [3]. However, obtaining a high accuracy comes at a price, i.e. large space and time consumption is needed. For tasks using fined-grained sense inventory, this problem is usually solved with heuristic approaches. Even if the POS tagging process has a lower complexity than the WSD process, its complexity is high enough to become problematic for most NLP applications. Moreover, comprehensive grammar analysis becomes difficult to accomplish [6]. Based on the above observations, we decided to investigate a partial WSD analysis before the POS tagger process. We do this in a research project in our company, codenamed SenDiS (Sense Disambiguation System). The purpose of this paper is to significantly reduce the word sense ambiguity and thus the POS ambiguity, but still to preserve all or most of POS tagging solutions. For this purpose, we adjusted the methods and algorithms used in the SenDiS project such that the system will provide WSD variants, for a given text, with semantic similarity scores greater than a threshold relative to the maximum discovered. SenDiS WSD approach The SenDiS research project addresses the WSD process in a knowledge-based fashion. It mainly relies on semantic networks, especially semantic networks built from semantically annotated lexicon glosses, for establishing the sense semantic similarity costs used to solve sense ambiguity. The main WSD usage scenario in the SenDiS system is: 1. the text is tokenized in text items; 2. each text items is matched with sense interpretations; 2nd International Conference on Management Science and Industrial Engineering (MSIE 2013) \u00a9 2013. The authors Published by Atlantis Press 443 3. for each sense interpretation assigned to the text items, a sense semantic signature is built based on the lexicon network; 4. relevant sense pairs with senses of different text items, are identified and then sense semantic signatures are compared; 5. semantic similarity costs of the senses in each pair are further used to compute the best WSD variant or variants. Reference [10] describes the main ingredients of WSD approach used in the SenDiS project. This WSD approach mainly consists of the following steps detailed below: A. Lexicon nework A lexicon network is obtained from semantically annotated lexicon glosses [5]. This is similar to other hierarchical networks built on lexicons [4]. However, it best resembles the Lesk algorithm approach ([1],[2]) in the way that it extends the definition domain of a word sense from a set of words in a gloss to a spanning tree like structure inside this lexicon network. Significant efforts were undertaken to achieve high quality semantic annotation of lexicon glosses. Semi-automated annotation is generally employed, but manual annotation is the gold standard even if the cost is much higher. B. Ordering the lexicon network The original lexicon network can be preprocessed in order to better fit different WSD methods that operate on it. This optimization task is often challenging considering the large dimensions of such networks. C. Building sense semantic signatures Using this large lexicon network, sense semantic signatures can be built having one of the forms: \u2022 spanning tree with node (sense) and relation information embedded \u2022 sets of nodes or/and relations \u2022 sequences of nodes or/and relations \u2022 combinations of the above. D. Comparing sense semantic signatures Semantic similarity cost for two senses can be obtained by comparing their semantic signatures. Various comparisons algorithms can be imagined depending on the form of the sense semantic signatures. E. Computation of WSD variants The final step in this WSD approach is to use these semantic similarity costs between the senses of the text items to compute best WSD variant or variants. One method will be to compute the complete sub-graph for the text, consisting of senses as nodes and edges between them with semantic similarity cost as edge rank, which maximizes the semantic similarity score of a variant. Reducing POS ambiguity using semantic information We propose, in analyzing a text, the computation of specific several WSD variants, especially those with strong semantic similarity scores. These should preserve the POS solutions for the text with high precision and, at the same time, should reduce the POS ambiguity for the POS tagger process. The normal output of the SenDiS system after disambiguating a text is a WSD variant or a set of WSD variants that have the highest score of semantic similarity. We modified the last step in the system, i.e. the computation of WSD variants, in order to obtain more WSD variants with a semantic similarity score close to the highest one determined. The last step of the SenDiS system, the computation of WSD variants with the highest semantic similarity score, has as input a set of sense pairs from the system\u2019s input text and each sense pair is associated with a semantic similarity cost. These pairs can be seen as edges in a graph where words senses are nodes. In fact this graph is actually an N-partite graph, as seen in Figure 1, where N is the number of words in the text and each partition of nodes consists of nodes representing senses of the",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "fac1434944ef68176ed957d9b0323cf6e83840fd",
    "url": "https://www.semanticscholar.org/paper/fac1434944ef68176ed957d9b0323cf6e83840fd",
    "title": "Uncovering the Deep Web: Transferring Relational Database Content and Metadata to OWL Ontologies",
    "abstract": null,
    "venue": "KES",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0e182e9b05b687c5ddc2d162b45dea80923e025f",
    "url": "https://www.semanticscholar.org/paper/0e182e9b05b687c5ddc2d162b45dea80923e025f",
    "title": "Artificial Intelligence for the Early Design Phases of Space Missions",
    "abstract": "Recent introduction of data mining methods has led to a paradigm shift in the way we can analyze space data. This paper demonstrates that Artificial Intelligence (AI), and especially the field of Knowledge Representation and Reasoning (KRR), could also be successfully employed at the start of the space mission life cycle via an Expert System (ES) used as a Design Engineering Assistant (DEA). An ES is an AI-based agent used to solve complex problems in particular fields. There are many examples of ES being successfully implemented in the aeronautical, agricultural, legal or medical fields. Applied to space mission design, and in particular, in the context of concurrent engineering sessions, an ES could serve as a knowledge engine and support the generation of the initial design inputs, provide easy and quick access to previous design decisions or push to explore new design options. Integrated to the User design environment, the DEA could become an active assistant following the design iterations and flagging model inconsistencies. Today, for space missions design, experts apply methods of concurrent engineering and Model-Based System Engineering, relying both on their implicit knowledge (i.e., past experiences, network) and on available explicit knowledge (i.e., past reports, publications, data sheets). The former knowledge type represents still the most significant amount of data, mostly unstructured, non-digital or digital data of various legacy formats. Searching for information through this data is highly time-consuming. A solution is to convert this data into structured data to be stored into a Knowledge Graph (KG) that can be traversed by an inference engine to provide reasoning and deductions on its nodes. Knowledge is extracted from the KG via a User Interface (UI) and a query engine providing reliable and relevant knowledge summaries to the Human experts. The DEA project aims to enhance the productivity of experts by providing them with new insights into a large amount of data accumulated in the field of space mission design. Natural Language Processing (NLP), Machine Learning (ML), Knowledge Management (KM) and Human-Machine Interaction (HMI) methods are leveraged to develop the DEA. Building the knowledge base manually is subjective, time-consuming, laborious and error bound. This is why the knowledge base generation and population rely on Ontology Learning (OL) methods. This OL approach follows a modified model of the Ontology Layer Cake. This paper describes the approach and the parameters used for the qualitative trade-off for the selection of the software to be adopted in the architecture of the ES. The study also displays the first results of the multi-word extraction and highlights the importance of Word Sense Disambiguation for the identification of synonyms in the context. This paper includes the detailed software architecture of both front and back-ends, as well as the tool requirements. Both architectures and requirements were refined after a set of interviews with experts from the European Space Agency. The paper finally presents the preliminary strategy to quantify and mitigate uncertainties within the ES.",
    "venue": "IEEE Aerospace Conference",
    "citationCount": 9,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "7ca8e98e9c52e7d26b4af0f9f9ec57794b76e17a",
    "url": "https://www.semanticscholar.org/paper/7ca8e98e9c52e7d26b4af0f9f9ec57794b76e17a",
    "title": "Building a Bilingual Representation of the Roget Thesaurus for French to English Machine Translation",
    "abstract": "This paper describes a solution to lexical transfer as a trade-off between a dictionary and an ontology. It shows its association to a translation tool based on morpho-syntactical parsing of the source language. It is based on the English Roget Thesaurus and its equivalent, the French Larousse Thesaurus, in a computational framework. Both thesaurii are transformed into vector spaces, and all monolingual entries are represented as vectors, with 1,000 components for English and 873 for French. The indexing concepts of the respective thesaurii are the generation families of the vector spaces. A bilingual data structure transforms French entries into vectors in the English space, by using their equivalencies representations. Word sense disambiguation consists in choosing the appropriate vector among these \u0093bilingual\u0094 vectors, by computing the contextualized vector of a given word in its source sentence, wading it in the English vector space, and computing the closest distance to the different entries in the bilingual data structure beginning with the same source string (i.e. French word). The process has been experimented on a 20,000 words extract of a French novel, Le Petit Prince, and lexical transfer results were found quite encouraging with a recall of 71% and a precision of 86%.",
    "venue": "International Conference on Language Resources and Evaluation",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "5a46cdd3a302e35babfbca5924f29db706e2a452",
    "url": "https://www.semanticscholar.org/paper/5a46cdd3a302e35babfbca5924f29db706e2a452",
    "title": "Mapping terms to UMLS concepts of the same semantic type.",
    "abstract": "We are interested in mapping terms from the biomedical literature to controlled terminologies. For clinical and related terms, we rely on the MetaMap program for mapping terms to the UMLS Metathesaurus, accepting term assignments that have a reasonable match score. In a sizable number of cases, terms are ambiguous, and MetaMap proposes several mapping candidates. To address these cases prior studies investigated Word Sense Disambiguation (WSD) strategies for selecting between concepts of different semantic types. Here, we investigated the situation where MetaMap proposes concepts that share the same semantic type. We present an ontology-based strategy for selecting between these concepts.",
    "venue": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ]
  },
  {
    "paperId": "b7c455f02a44a629b45e4f01e775cc504a76c9b9",
    "url": "https://www.semanticscholar.org/paper/b7c455f02a44a629b45e4f01e775cc504a76c9b9",
    "title": "Automatic acquisition of concepts from domain texts",
    "abstract": "Domain specific concept extraction is a key com- ponent in ontology construction for Semantic Web applications. Manual concept extraction is costly both in time and labor. In this paper, we present several heuristic methods for automatic concepts extraction from domain texts. These methods aim to improve the precision and recall over the word frequency-based techniques. Precision is improved by elimination of irrelevant terms using word sense information. Recall is enhanced by adding new concepts formed by composition of relevant words. Our methods are domain independent, and can be applied in fully automatic way to the concept extraction task. Experimental results on the electronic voting domain texts (from New York Times) are presented which show the promise of the proposed methods. Index Terms\u2014 Concept extraction, ontology engineering, text processing, WordNet, WordNet Senses.",
    "venue": "IEEE International Conference on Granular Computing",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "97334c9d5544b8ed893e5f9ec796489015e9aa2b",
    "url": "https://www.semanticscholar.org/paper/97334c9d5544b8ed893e5f9ec796489015e9aa2b",
    "title": "Data Mining and the Text Categorization Framework",
    "abstract": "The aim of this contribution is to show one of the most important application of text mining. According to a wide part of the literature regarding the aforementioned field, great relevance is given to the classification task (Drucker et al., 1999, Nigam et al., 2000). The application contexts are several and multitask, from text filtering (Belkin & Croft, 1992) to word sense disambiguation (Gale et al., 1993) and author identification ( Elliot and Valenza, 1991), trough anti spam and recently also anti terrorism. As a consequence in the last decade the scientific community that is working on this task, has profuse a big effort in order to solve the different problems in the more efficient way. The pioneering studies on text categorization (TC, a.k.a. topic spotting) date back to 1961 (Maron) and are deeply rooted in the Information Retrieval context, so declaring the engineering origin of the field under discussion. Text categorization task can be briefly defined as the problem of assigning every single textual document into the relative class or category on the basis of the content and employing a classifier properly trained. In the following parts of this contribution we will formalize the classification problem detailing the main issues related.",
    "venue": "Encyclopedia of Data Warehousing and Mining",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1d1f78acfec3ee8a3610cd4d22a301a87ec9b697",
    "url": "https://www.semanticscholar.org/paper/1d1f78acfec3ee8a3610cd4d22a301a87ec9b697",
    "title": "The semantic annotated documents: from HTML to the semantic web",
    "abstract": "The current circumstance of the Semantic Web is that there is not much of a Semantic Web due to the lack of annotated web pages. There is such a lack because annotating web pages currently does not provide much practical benefit. In this work an automated approach to semantics extraction and annotation on textual data is proposed. Word sense disambiguation technique is used to identify the concepts, and RDF is used to annotate the semantics. A corresponding approach to retrieve data via ontology is also discussed. Finally a framework to integrate and automate these processes is demonstrated. In this fashion all the existing data on the Web can be processed and brought to the Semantic Web.",
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4687ad58a053fe9ca91b6ba100b8ef0850ef6644",
    "url": "https://www.semanticscholar.org/paper/4687ad58a053fe9ca91b6ba100b8ef0850ef6644",
    "title": "Semantics, Web and Mining, Joint International Workshops, EWMF 2005 and KDO 2005, Porto, Portugal, October 3 and 7, 2005, Revised Selected Papers",
    "abstract": null,
    "venue": "EWMF/KDO",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c277f87ed5f11531139bbe8c74e3bc31f13107ce",
    "url": "https://www.semanticscholar.org/paper/c277f87ed5f11531139bbe8c74e3bc31f13107ce",
    "title": "Role of Semantics in Question Answering",
    "abstract": "This chapter contains sections titled: Introduction to Question Answering Overview of Semantics in Question Answering Word Sense Disambiguation Extended Word Net Knowledge Base Lexical Chains Semantic Relations Contexts Temporal Expressions and Events Ontologies Semantic Reasoner for Question Answering Discussion References",
    "venue": "",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c01c63997810fecebd9acc91c6158e8b492c1658",
    "url": "https://www.semanticscholar.org/paper/c01c63997810fecebd9acc91c6158e8b492c1658",
    "title": "The EB-ANUBAD translator: A hybrid scheme",
    "abstract": null,
    "venue": "",
    "citationCount": 13,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f4b38807b0408162d1f2532ec61579eac7aeecbd",
    "url": "https://www.semanticscholar.org/paper/f4b38807b0408162d1f2532ec61579eac7aeecbd",
    "title": "The TALP systems for disambiguating WordNet glosses",
    "abstract": "This paper describes the TALP systems presented at Senseval-3 task 12 \u201cWord-Sense Disambiguation of WordNet Glosses\u201d. Our method combines a set of knowledge-based heuristics integrating several information sources and techniques. Using large scale lexico\u2013semantic knowledge bases, such as WN, has become a usual, often necessary, practice for most current Natural Language Processing systems. Building appropriate resources of this nature for broad\u2013coverage semantic processing is a hard and expensive task, involving large research groups during long periods of development. For example, dozens of person\u2013 years are been invested world\u2013wide into the development of wordnets for various languages (Fellbaum, 1998), (Atserias et al., 1997), (Agirre et al., 2002), (Pianta et al., 2002). Dictionaries are special texts describing the meaning of a language. They provide a wide range of information of words by giving definitions of the word senses and as, a side effect, they supply knowledge about the world itself. WordNet (WN) (Fellbaum, 1998) can be also seen as an structured dictionary with thouthands of semantic relations, defining the most common concepts of the English language. Although the importance of (WN) has widely exceeded the purpose of its creation (Miller et al., 1990), and it has become an essential semantic resource for many applications, at the moment is not rich enough to directly support advanced semantic processing (Harabagiu et al., 1999). Sense disambiguation of definitions in any lexical resource is an important objective in the language engineering community because this process can increase the semantic conectivity among concepts. The first significant disambiguation of dictionary definitions took place 20 years ago (see (Rigau, 1998) for an extended survey on acquiring lexical knowledge from Machine Readable Dictionaries). Recently, several research groups have presented different approaches to perform this process on WN. In the eXtended WordNet1 (Mihalcea and Moldovan, 2001) the WN glosses have been syntactically parsed, transformed into logic forms and the content words are also semantically disambiguated. Being derived from an automatic process, disambiguated words included into the glosses have assigned a confidence label indicating the quality of the annotation (gold, silver or normal).",
    "venue": "SENSEVAL@ACL",
    "citationCount": 13,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "817fb6fda2f0bfe743156d72ae16cdd67195141b",
    "url": "https://www.semanticscholar.org/paper/817fb6fda2f0bfe743156d72ae16cdd67195141b",
    "title": "AI*IA 2009: Emergent Perspectives in Artificial Intelligence, XIth International Conference of the Italian Association for Artificial Intelligence, Reggio Emilia, Italy, December 9-12, 2009, Proceedings",
    "abstract": null,
    "venue": "International Conference of the Italian Association for Artificial Intelligence",
    "citationCount": 26,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4e8fd548e28be1f131f05a70d8bbe1a909c6391a",
    "url": "https://www.semanticscholar.org/paper/4e8fd548e28be1f131f05a70d8bbe1a909c6391a",
    "title": "Polysemy in Controlled Natural Language Texts",
    "abstract": null,
    "venue": "Controlled Natural Language",
    "citationCount": 9,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "8d9a996b7374dfb3dce215112a41071e3e7bf8e1",
    "url": "https://www.semanticscholar.org/paper/8d9a996b7374dfb3dce215112a41071e3e7bf8e1",
    "title": "Processing Tools and Services from iLexIR Ltd",
    "abstract": "We describe the text processing tools that iLexIR has developed in collaboration with the Universities of Cambridge and Sussex. iLexIR has sole commercial rights to an extensive toolkit for English text processing applications and undertakes additional software development as well as tool tuning and porting for SMEs marketing applications with a text processing component. To date, we have worked with clients to develop sentiment classification systems, mobile phone based question-answering services, and text mining tools for use in ESOL examination design and biomedical information extraction. Our toolkit has been extensively deployed for non-commercial research and proven its utility in projects on ontology and lexicon construction, anonymisation, anaphora resolution, word sense disambiguation, and many forms of text classification at the document, passage and sentence levels.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": null
  },
  {
    "paperId": "779fbe8bc01edb40ecdb02fb59c8b2c5ed01073f",
    "url": "https://www.semanticscholar.org/paper/779fbe8bc01edb40ecdb02fb59c8b2c5ed01073f",
    "title": "Similarity Measures for Query Expansion in TopX",
    "abstract": "TopX is a top-k retrieval engine for text and XML data. Unlike some other engines, TopX includes an ontology. This ontology allows TopX to use techniques like word sense disambiguation and query expansion, to search for words similar to the original query terms. These techniques allow finding data items which would be ignored for the original source query, due to missing of words similar to the query terms. The similarity of words is given via the weights of the relations connecting words. The underlying ontology of TopX is the WordNet ontology, but in 2007 there was a further ontology integrated, the YAGO ontology. This thesis has three main focuses: \u2022 Import of a new version of the YAGO ontology. \u2022 Similarity computation for YAGO relations. \u2022 Adaptations of the TopX procedures for word sense disambiguation and query expansion to the differences between the WordNet ontology and the YAGO ontology. We demonstrate the improvement of our approach for TopX, with center to the newly available YAGO ontology.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "d2e135edb1e9839523ceaed262c1045a19c15b7e",
    "url": "https://www.semanticscholar.org/paper/d2e135edb1e9839523ceaed262c1045a19c15b7e",
    "title": "Transparency and Building Lexical Dependency Graphs",
    "abstract": "The authors are participants in a project, called \"FrameNet\" which is aimed at building a large computer lexicon of contemporary written English and making it accessible through the World Wide Web for both computational and lexicographic interests. In the process of designing this resource, we have to keep in mind how it can serve its intended applications. A prerequisite to most imaginable NLP applications is word sense disambiguation\u00a0 (WSD), the automatic process by which a word in a linguistic context can be (probabilistically) assigned its locally intended meaning. This paper will introduce FrameNet and will characterize a facility, based on its tools and data, that could in principle be directed to WSD efforts, and will suggest how both technical (engineering) and linguistic considerations must be called on to build it.",
    "venue": "",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "23db444f9ab395cab4a15e12c328b30e99655094",
    "url": "https://www.semanticscholar.org/paper/23db444f9ab395cab4a15e12c328b30e99655094",
    "title": "E-LEARNING TOOLS: CONCEPTUALISATION OF DOMAIN KNOWLEDGE FOR FUTURE USE IN E-LEARNING CONTEXT",
    "abstract": "Semantic Web is a Web of new generation. The main difference from the Web of the first generation is that information presented is understandable not only for humans, but also for software agents or other software modules. Ontologies are most often defined as basic component in Semantic Web infrastructure.\u00a0 Domain ontologies provide shared and common understanding of a specific domain. They, as engineering artefacts, are used in different fields, including e-learning.\u00a0 In this paper, we present the development of domain ontology for future use in e-learning context.\u00a0 Domain of \u201cE-learning tools\u201d was chosen for implementation. The distance learning course \u201cE-learning technologies\u201d (3 credits) is elective and oriented not only for students with strong background of information technologies. Among others, the expected ability of the study module is formulated as follows: students will be able to analyse, compare and in the real context to choose the most suitable tools for development of study materials, delivering distance learning course or making other decision in e-learning context. Our domain consists of three large parts: tools (software products), technologies in a wider sense of this word and didactics. The obstacle of our solution is that the domain is evolving quickly. But since we agree that \u201cthere is no single correct ontology for any domain\u201d (Noy, 2001), we can freely experiment and foresee further use of developed ontology in e-learning.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "ad82062647ba64518c788a0ce999c976425d4170",
    "url": "https://www.semanticscholar.org/paper/ad82062647ba64518c788a0ce999c976425d4170",
    "title": "Extracting Ontological Selectional Preferences for Non-Pertainym Adjectives from the Google Corpus",
    "abstract": "\n \n While there has been much research into using selectional preferences for word sense disambiguation (WSD), much difficulty has been encountered. To facilitate study into this difficulty and aid in WSD in general, a database of the selectional preferences of non-pertainym prenomial adjectives extracted from the Google Web 1T 5-gram Corpus is proposed. A variety of methods for computing the preferences of each adjective over a set of noun categories from WordNet have been evaluated via simulated disambiguation of pseudohomonyms. The best method of these involves computing for each noun category the ratio of single-word common (i.e. not proper) noun lemma types which can co-occur with a given adjective to the number of single-word common noun lemmata whose estimated frequency is greater than a threshold based on the frequency of the adjective. The database produced by this procedure will be made available to the public.\n \n",
    "venue": "AAAI Conference on Artificial Intelligence",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "315d9883e411d0c8d37c0c17318d8a45cc17d5ac",
    "url": "https://www.semanticscholar.org/paper/315d9883e411d0c8d37c0c17318d8a45cc17d5ac",
    "title": "Distributional Semantics and the Lexicon",
    "abstract": "The lexicons used in computational linguistics systems contain morphological, syntactic, and occasionally also some semantic information (such as definitions, pointers to an ontology, verb frame filler preferences, etc.). But the human cognitive lexicon contains a great deal more, crucially, expectations about how a word tends to combine with others: not just general information-extraction-like patterns, but specific instantial expectations. Such information is very useful when it comes to listening in bad aural conditions and reading texts in which background information is taken for granted; without such specific expectation, one would be hard-pressed (and computers are completely unable) to form coherent and richly connected multi-sentence interpretations. Over the past few years, NLP work has increasingly treated topic signature word distributions (also called \u2018context vectors\u2019, \u2018topic models\u2019, etc.) as a de facto replacement for semantics. Whether the task is wordsense disambiguation, certain forms of textual entailment, information extraction, paraphrase learning, and so on, it turns out to be very useful to consider a word(sense) as being defined by the distribution of word(senses) that regularly accompany it (in the classic words of Firth, \u201cyou shall know a word by the company it keeps\u201d). And this is true not only for individual wordsenses, but also for larger units such as topics: the product of LDA and similar topic characterization engines is similar. In this talk I argue for a new kind of semantics, which is being called Distributional Semantics. It combines traditional symbolic logicbased semantics with (computation-based) statistical word distribution information. The core resource is a single lexico-semantic lexicon that can be used for a variety of tasks, provided that it is reformulated accordingly. I show how to define such a semantics, how to build the appropriate lexicon, how to format it, and how to use it for various tasks. The talk pulls together a wide range of related topics, including Pantel-style resources like DIRT, inferences / expectations such as those used in Schank-style expectation-based parsing and expectation-driven NLU, PropBank-style word valence lexical items, and the treatment of negation and modalities. I conclude by arguing that the human cognitive lexicon has to have the same kinds of properties as the Distributional Semantics lexicon, given the ways people do things with words.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6d44a7340103ee5bc0488ac94963b4ec2d682d83",
    "url": "https://www.semanticscholar.org/paper/6d44a7340103ee5bc0488ac94963b4ec2d682d83",
    "title": "Natural Language Query in the Biochemistry and Molecular Biology Domains Based on Cognition Search\u2122",
    "abstract": null,
    "venue": "Summit on translational bioinformatics",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ]
  },
  {
    "paperId": "4ed2a6f85d40f6c8cbb81447dfd353715bc4c32d",
    "url": "https://www.semanticscholar.org/paper/4ed2a6f85d40f6c8cbb81447dfd353715bc4c32d",
    "title": "Ontological Semantics and the Study of Meaning in Linguistics , Philosophy and Computational Linguistics",
    "abstract": "This chapter contains a very brief survey of the study of meaning in linguistics and philosophy (see Raskin 1983 and 1986 for a more detailed discussion). Its purpose is limited to placing ontological semantics in the realm of linguistic and philosophical semantics. 3.1 Prehistory of semantics Before the study of meaning emerged as a separate linguistic discipline in the late 19th century, a number of disjoint ideas about meaning had accumulated over the millennia. For instance, Plato's \" Kratylos \" is devoted essentially to a discussion about whether words are natural and necessary expressions of notions underlying them or merely arbitrary and conventional signs for these notions, that might be equally well expressed by any other collection of sounds. The closely related problem of sound symbolism has recurred ever since. In modern times, de Saussure's (1916), Jakobson's (1965) and Benveniste's (1939) debate on the arbitrariness of the linguistic sign develops the same issue. The currently active area of word sense disambiguation can be traced back at least to Democritus who commented on the existence of polysemy and synonymy (1717; cf. Lurfle 1970). Modern work on diachronic changes in word meaning was anticipated by Proclus (1987, 1989). Aristotle (1968) contributed to the definition of what we would now call the distinction between open-and closed-class lexical items, a taxonomy of parts of speech and another one for metaphors (or tropes). An ancient Indian (see, for instance, Zvegintzev 1964) school of linguistic thought was preoccupied with the question of whether the word possesses a meaning in isolation or acquires it only in a sentence. This argument was taken up by Gardiner (1951) and Grice (1957). Practical work with meaning can be traced back to the Middle Ages and the trailblazing lexicographic and thesaurus-building work by Arab scholars (see, for instance, Zvegintsev 1958). 3.2 Diachrony of word meaning In 1883, a French classical philologist Michel Br\u00e9al (1832-1915) published an article (see Br\u00e9al 1997) which contained the following passage: \" The study where we invite the reader to follow us is of such a new kind that it has not even yet been given a name. Indeed, it is on the body and the form of words that most linguists have exercised their acumen: the laws governing changes in meaning, the choice of new expressions, the birth and death of idioms, have been left in the dark or have only been casually indicated. Since this \u2026",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "eb305853130d45c09f5530df741f35392192bb97",
    "url": "https://www.semanticscholar.org/paper/eb305853130d45c09f5530df741f35392192bb97",
    "title": "Automatic Generation of Glosses in the OntoLearn System",
    "abstract": "OntoLearn is a system for automatic acquisition of specialized ontologies from domain corpora, based on a syntactic pattern matching technique for word sense disambiguation, called structural semantic interconnection (SSI). We use SSI to extract from corpora complex domain concepts and create a specialized version of WordNet. In order to facilitate the task of domain specialists who inspects and evaluate the newly acquired domain ontology, we defined a method to automatically generate glosses for the learned concepts. Glosses provide an informal description, in natural language, of the formal specifications of a concept, facilitating a perconcept evaluation of the ontology by domain specialists, who are usually unfamiliar with the formal language used to describe a computational ontology. The proposed evaluation framework has been tested in a financial domain.",
    "venue": "LREC",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1d6b85caa17c2d39a1b3d65ca5418450ded2cfc3",
    "url": "https://www.semanticscholar.org/paper/1d6b85caa17c2d39a1b3d65ca5418450ded2cfc3",
    "title": "Symmetry, phase modulation and nonlinear waves",
    "abstract": "and techniques from a very broad range of disciplines from biology to meteorology via chemistry, physics and mathematics. Despite having been subject to detailed scrutiny for decades if not centuries much remains to be done in order to fully comprehend the intricacies of nature. Such studies are often curiosity-driven but often have serious practical implications. Disease control and weather prediction exemplify areas of research where interdisciplinary innovation is required in order to provide global benefits \u2013 in the most literal sense. The intimate relationship between weather and the state of the oceans impacts everyone on the planet either directly or indirectly. The devastation wreaked by rogue water waves in many localities has been one of the factors which has brought home (to most people) the need to control atmospheric emissions. The study of waves in water, or in fluids more generally, has been a main stay of applied mathematical research for a considerable period of time. Those studies have resulted in the development of sophisticated mathematical techniques as well as advances in computational methodologies within the discipline of computational fluid dynamics. One of the particular aspects which has excited mathematical interest is the need to treat phenomena which derive from nonlinearities. The thrust of the present book is to offer a natural approach to the modelling of nonlinear waves from the viewpoint of phase modulation. Phase modulation has a number of meanings in science and engineering and will be most familiar in the context of electronic communications. For the treatment of nonlinear waves, modulation equations arise from nonlinear partial differential equations governing the envelope of a wave. The treatment of phase modulation promulgated here is utilised in conservative systems where a basic state, dependent on a phase and one or more other parameters, can be described as a slowly varying function of space and time. Such an approach is contained within so-called Whitham modulation theory which itself is underpinned by conservation of wave action and conservation of waves. The service performed by this volume is to reformulate Whitham theory to allow \u2018greater flexibility on the phase, scaling and use of singularities, generating new modulation equations with nonlinearity and dispersion\u2019. This book has been written by a well-established researcher in the field. His expertise is evidenced by the deft exposition of relatively challenging material. In that regard, one of the very useful functions of this book is its provision of a number of background mathematical techniques in Hamiltonians systems, symplectic geometry, Noether theory and Lagrangian field theory. (Some of these aspects are considered in greater detail in a number of useful appendices to the main text.) This overview of techniques and concepts is provided very early in the book, and will require particular attention by readers unfamiliar with such techniques. In a word, some initial effort may be needed but the payback from that effort will be considerable. A particular outcome of the author\u2019s work is a new theory for the emergence of the Korteweg\u2013de Vries (KdV) equation. interpretation of quantum mechanics\u2019 by D. Bohm and B. Hiley, both works published in 1993, an exploration of the physical meaning of wave function where new ideas and interpretations are reviewed and discussed has been missing. Shan Gao\u2019s book is addressed to fill such lack of information. It offers a readable and comprehensive discussion on the everlasting debate about the reality of the mathematical object that is used to represent states of quantum systems. With a profound influence on ontological interpretations, the measurement problem is faced in a very original form that, I believe, will provoke renewed discussions on the matter. The text, consisting on eight chapters, is an ambitious attempt to relieve one of the worst features of quantum mechanics textbooks: the exclusion of all historical and philosophical grounds. It includes well-founded concepts to interpret experiments and avoids the purely calculation problems. The first five chapters integrate the first part of the book, they are addressed to review the different interpretations of the wave function and introduce arguments for the ontic view based on protective measurements. The ontology of quantum mechanics is discussed in chapters six and seven, which integrate the second part of the monograph. The random discontinuous motion of particles is analysed and the discussion is addressed to propose an ontological interpretation of the wave function. Chapters eight and nine cover the third part of the book. In the former a discussion on the solution of the measurement problem in terms of the suggested quantum ontology is given. The latter concerns the possible extension of the ontological approach to the relativistic domain. In summary, I found the present book well-written and organised, with very stimulating discussions on the meaning of the wave function that was introduced by Schr\u00f6dinger, as a reaction to the formulation of quantum mechanics given by Werner Heisenberg, and that was found mysterious and subtle by Schr\u00f6dinger himself. This monograph represents an excellent resource for students and researchers interested in the foundations and philosophy of quantum mechanics.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Physics"
    ]
  },
  {
    "paperId": "498c5d3e0845350b1abc3fd5536e2ef3cdecbc18",
    "url": "https://www.semanticscholar.org/paper/498c5d3e0845350b1abc3fd5536e2ef3cdecbc18",
    "title": "Tag semantics for the retrieval of XML documents",
    "abstract": "Word Sense Disambiguation (WSD), in the field of Natural Language Processing (NLP), consists in assigning the correct sense (semantics) to a word form (lexeme) by means of the context in which the lexeme is found. In this paper we investigate the possibility of applying WSD techniques to the field of Information Retrieval, especially to the retrieval of XML documents. We consider two methods to automatically assign semantic values to XML tags on the grounds of the tagged text contained. Such methods rely on the bayesian supervised approach and on an automatic unsupervised approach and exploit the WordNet ontology. Results show that the applicability of both methods is hampered by the habit of use abbreviation or shortcuts as tags.",
    "venue": "International Symposium on Information and Communication Technologies",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "b186aac304a3708389538f998f8ffc86e87500cb",
    "url": "https://www.semanticscholar.org/paper/b186aac304a3708389538f998f8ffc86e87500cb",
    "title": "Extracting Ontological Relations of Korean Numeral Classifiers from Semi-structured Resources Using NLP Techniques",
    "abstract": null,
    "venue": "OTM Workshops",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a8de5528bb300461cd13e1298c10bf1bc26cd56b",
    "url": "https://www.semanticscholar.org/paper/a8de5528bb300461cd13e1298c10bf1bc26cd56b",
    "title": "A Relation-Centric View of Semantic Representation Learning",
    "abstract": "Much of NLP can be described as mapping of a message from one sequence of symbols to another. Examples include word surface forms, POS-tags, parse trees, vocabularies of different languages, etc. Machine learning has been applied successfully to many NLP problems by adopting the symbol mapping view. No knowledge of the sets of symbols is required: only a rich feature representation to map between them. The task of coming up with expressive features is typically a knowledge-intensive and time-consuming endeavor of human creativity. However the representation learning paradigm (Bengio et al., 2013) offers an alternative solution, where an optimal set of features are automatically learned from data for a target application. It has been successfully used in many applied fields of machine learning including speech recognition (Dahl et al., 2010), vision and object recognition (Krizhevsky et al., 2012), and NLP (Socher et al., 2010; Schwenk et al., 2012). One particular area of NLP that has received a lot of interest, from the perspective of representation learning, is lexical semantics. Many efforts have focussed on attempting to learn high-dimensional numerical vector representations for units of meaning (most often words) in a vocabulary. While the current slew of research uses neural network techniques (Collobert and Weston, 2008; Mikolov et al., 2013a), word-embeddings have been around much longer in the form of distributional vector space models (Turney and Pantel, 2010). We use the term semantic representation learning to encompass all these views and techniques. Semantic representations learned automatically from data have proven to be useful for many downstream applications such as question answering (Weston et al., 2015), word-sense discrimination and disambiguation (McCarthy et al., 2004; Sch\u00fctze, 1998), and selectional preference modeling (Erk, 2007). The successes of representation learning, along with its unsupervised and data-driven nature make it a compelling modelling solution for semantics. However, unlike manual feature engineering, it remains difficult to inject linguistic or world knowledge into the process of learning semantic representations automatically. This has led many models to still fail to account for many basic properties of human languages, such as antonymy, polysemy, semantic composition, and negation, among others. We attempt to outline solutions to some of these shortcomings in this thesis. When considering semantic representation learning models, two questions may be asked: firstly, how is the model learned; and secondly; what does the model represent? The research community has largely focussed on the first of the two questions \u2013 the how \u2013 proposing many different machine learning solutions that yield semantic vector representations. In this thesis, we instead focus on the second of the two questions \u2013 the what \u2013 namely gaining insight into the underlying semantic information that is actually captured by models. To explain this information we introduce the two connected notions of semantic relations and contextual relations. Semantic relations are properties of language that we generally wish to capture with semantic models, such as similarity and relatedness. However, given some data, unsupervised representation learning techniques have no way of directly optimizing and learning these semantic relations. At the same time, a crucial component of every representation learning model, is the notion of a context. This is the basis upon which models extract count statistics to process and produce representations. It is, effectively a model\u2019s view of the world and one of the few ways of biasing the unsupervised learner\u2019s perception of semantics, with data. A contextual relation is the operationalization that yields the contexts, upon which a semantic model can be trained. Therefore, to understand what information a model represents one must understand and articulate the key contextual relation that defines its learning process. In other words, being able to specify what is being counted leads to insights into what is being represented. Conversely, if we wish to learn a model that captures a specific semantic relation it is important to define a contextual relation that captures the intuition of the semantic relation through contextual views of data. The connection between semantic and corresponding contextual relation may not always be easy or evident, but in this thesis we present some examples where we can make such a connection and also provide a framework for thinking about future solutions. To clarify the idea of semantic and contextual relations let us consider the example of learning a model of semantic similarity. In this case the semantic relation that we wish to learn is similarity, but how do we bias the learner to capture a specific relation in an unsupervised way from unannotated data? We need to start with an intuition about semantic similarity, and make it concrete with a contextual relation. Harris (1954) and Firth (1957) intuited that words that appear in similar contexts tend to have similar meaning; this is known as the distributional hypothesis. This hypothesis is often operationalized to yield contexts over word window neighborhoods, and many existing word vector learning techniques (Collobert and Weston, 2008; Mnih and Teh, 2012; Mikolov et al., 2013a) use just such contexts. The wordneighborhood-word contextual relation thus produces models that capture similarity. From this point of view the distributional hypothesis is first and foremost a contextual relational, only one among many possible contextual relations. It becomes clear why models that use this relation to extract counts and process information are incapable of capturing all of semantics. The contextual relation based on the distributional hypothesis is simply not the right or most expressive one for many linguistic phenomena or semantic problems \u2013 for example composition, negation, antonymy etc. Turney and Pantel (2010) describe other contextual relations that define and yield contexts such as entire documents or patterns. Models that use these contextual relations capture different information (i.e. semantic relations) because they define views of the data via contexts differently. This thesis hypothesizes that relations and structure play a primary role in creating meaning in language, and that each linguistic phenomenon has its own set of relations and characteristic structure. We further hypothesize that by understanding and articulating the principal contextual relations",
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "5a86fec77f0c2da40b1f72ce967d7b9888e462ec",
    "url": "https://www.semanticscholar.org/paper/5a86fec77f0c2da40b1f72ce967d7b9888e462ec",
    "title": "Entity-centric knowledge discovery for idiosyncratic domains",
    "abstract": "Technical and scientific knowledge is produced at an ever-accelerating pace, leading to increasing issues when trying to automatically organize or process it, e.g., when searching for relevant prior work. Knowledge can today be produced both in unstructured (plain text) and structured (metadata or linked data) forms. However, unstructured content is still the most dominant form used to represent scientific knowledge. In order to facilitate the extraction and discovery of relevant content, new automated and scalable methods for processing, structuring and organizing scientific knowledge are called for. In this context, a number of applications are emerging, ranging from Named Entity Recognition (NER) and Entity Linking tools for scientific papers to specific platforms leveraging information extraction techniques to organize scientific knowledge. In this thesis, we tackle the tasks of Entity Recognition, Disambiguation and Linking in idiosyncratic domains with an emphasis on scientific literature. Furthermore, we study the related task of co-reference resolution with a specific focus on named entities. We start by exploring Named Entity Recognition, a task that aims to identify the boundaries of named entities in textual contents. We propose a new method to generate candidate named entities based on n-gram collocation statistics and design several entity recognition features to further classify them. In addition, we show how the use of external knowledge bases (either domain-specific like DBLP or generic like DBPedia) can be leveraged to improve the effectiveness of NER for idiosyncratic domains. Subsequently, we move to Entity Disambiguation, which is typically performed after entity recognition in order to link an entity to a knowledge base. We propose novel semi-supervised methods for word disambiguation leveraging the structure of a community-based ontology of scientific concepts. Our approach exploits the graph structure that connects different terms and their definitions to automatically identify the correct sense that was originally picked by the authors of a scientific publication. We then turn to co-reference resolution, a task aiming at identifying entities that appear using various forms throughout the text. We propose an approach to type entities leveraging an inverted index built on top of a knowledge base, and to subsequently re-assign entities based on the semantic relatedness of the introduced types. Finally, we describe an application which goal is to help researchers discover and manage scientific publications. We focus on the problem of selecting relevant tags to organize collections of research papers in that context. We experimentally demonstrate that the use of a community-authored ontology together with information about the position of the concepts in the documents allows to significantly increase the precision of tag selection over standard methods.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Psychology"
    ]
  },
  {
    "paperId": "c533fc1b75166f16604611240950fbfb7ec2b019",
    "url": "https://www.semanticscholar.org/paper/c533fc1b75166f16604611240950fbfb7ec2b019",
    "title": "Constrained Semi-supervised Learning in the Presence of Unanticipated Classes",
    "abstract": "Traditional semi-supervised learning (SSL) techniques consider the missing labels of unlabeled datapoints as latent/unobserved variables, and model these variables, and the parameters of the model, using techniques like Expectation Maximization (EM). Such semisupervised learning techniques are widely used for Automatic Knowledge Base Construction (AKBC) tasks.\n We consider two extensions to traditional SSL methods which make it more suitable for a variety of AKBC tasks. First, we consider jointly assigning multiple labels to each instance, with a flexible scheme for encoding constraints between assigned labels: this makes it possible, for instance, to assign labels at multiple levels from a hierarchy. Second, we account for another type of latent variable, in the form of unobserved classes. In open-domain webscale information extraction problems, it is an unrealistic assumption that the class ontology or topic hierarchy we are using is complete. Our proposed framework combines structural search for the best class hierarchy with SSL, reducing the semantic drift associated with erroneously grouping unanticipated classes with expected classes. Together, these extensions allow a single framework to handle a large number of knowledge extraction tasks, including macro-reading, noun-phrase classification, word sense disambiguation, alignment of KBs to wikipedia or on-line glossaries, and ontology extension.\n To summarize, this thesis argues that many AKBC tasks which have previously been addressed separately can be viewed as instances of single abstract problem: multiview semisupervised learning with an incomplete class hierarchy. In this thesis we present a generic EM framework for solving this abstract task.",
    "venue": "SIGF",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "af684bbfb1ae29847f2c05a0cacb1fbef7387d81",
    "url": "https://www.semanticscholar.org/paper/af684bbfb1ae29847f2c05a0cacb1fbef7387d81",
    "title": "The Role of Qualitative Research in Education 4.0: Reflections from a State-Funded Model-Building Qualitative Research",
    "abstract": "The major shift in educational contour brought about by the Fourth Industrial Revolution (4IR) challenges every component of the traditional educational landscape (Morales, Anito, Avilla, Abulon, et al., 2019; Shahroom & Hussin, 2018; Chao, 2017). In higher education, the relevance of qualitative research in Education 4.0 is a significant point of interest. This paper primarily presents our reflection points after conducting a three-tier qualitative data analysis, which is a component of a state-funded research project (Morales, Anito, Avilla, Sarmiento, et al., 2019). \nThe research component described in this paper aimed at developing a pedagogical model of the current perspectives, ideals, and practices of Philippine Higher Education Institutions (HEIs) on Science, Technology, Engineering, Agri-Fisheries, and Mathematics (STEAM) Education. Likewise, this paper briefly describes the methodological rigors the we underwent in the development of the model. The developed model is also defined and elaborated in this paper to provide a context of the reflections. Our reflections mainly focus on the role of qualitative research in Philippine higher education as it deals with the possibilities, opportunities, and challenges of the 4IR. \nThese reflections were drawn both from the pedagogical model that we developed and from our experience in employing qualitative techniques in developing the model (Anito, Morales, Torres, Gonzales, & Ganeb, 2019). In this paper, we posit that qualitative research is highly relevant in Philippine higher education primarily because qualitative research develops and nurtures skills that are essential in the 4IR. These skills include sense-making, social intelligence, adaptive thinking, cognitive flexibility, and knowledge management. Sense-making refers to our ability to determine the underlying concepts and deeper meaning of what is being expressed by the research participants (Davies, Fidler, & Gorbis, 2011; World Economic Forum, 2018). \nSense-making is inherent in any qualitative research approach. On one hand, it entails having a mental dialogue with anything that appeals to our senses during data gathering, data encoding, and data analysis. On the other hand, it is demonstrating sensitivity to the implications of the analysis outcomes to the extant literature and the current and future social infrastructure. Sense-making is essential in the 4IR because it complements what the machines in the era of automation are not capable of doing. Apparently, sense-making is an ability that is unique to human beings thereby making it significant in a period where humans thrive with machines. \nQualitative research likewise advances social intelligence. Social intelligence is our ability to connect to other human beings in a deep, direct, and harmonious way (Davies, Fidler, & Gorbis, 2011; World Economic Forum, 2018; Bughin, et al., 2018; Xing & Marwala, 2017). It is a skill to sense reactions and stimulate the desired interactions. In qualitative research, social intelligence is best demonstrated during interviews, focus group discussions, and immersions. It is during these processes that we exhibit keenness to every action, emotion, word, and gesture of the people around us such that we tailor our own actions, emotions, words, and gestures to blend with theirs. Social intelligence is an indispensable skill in the 4IR as workers need to collaborate and build relationships with people across social and cultural settings. \nLike sense-making, social intelligence is another advantage of humans over machines which makes it relevant in the 4IR. Adaptive thinking is another skill that qualitative research builds among researchers. It is our ability to respond to unique and unexpected circumstances and to figure out solutions appropriate to emerging problems (Davies, Fidler, & Gorbis, 2011; World Economic Forum, 2018; Bughin, et al., 2018; Xing & Marwala, 2017). \nThe emergent nature of a qualitative investigation requires us to deal with serendipities across all stages of the research process thus inducing adaptive thinking skills. Adaptive thinking likewise pertains to our ability to deal with conceptual variations especially in addressing discrepancies between generated knowledge and existing social theories and constructs. The disruptive nature of the 4IR proffers uncertainties and randomness in all aspects of life thereby making the adaptive thinking skill essential. Qualitative research also nurtures cognitive flexibility (Xing & Marwala, 2017) and transdisciplinarity (Davies, Fidler, & Gorbis, 2011). \nQualitative research necessitates understanding of concepts across multiple disciplines such that researchers are able to think of multiple perspectives simultaneously during data collection and data analysis. This is demonstrated as our ability to analyze data with openness, especially those data that represent diverse epistemological and ontological groundings. \nConsequently, this unravels every conceptual variation and discrepancy in the form of a conceptual model. While some qualitative approaches require us to suspend our conceptual prejudices in dealing with data, framing the data in a specific disciplinal context helped us in understanding the responses of our participants in our research. Cognitive flexibility is necessary in the 4IR. The seamless interaction of skills and opportunities requires graduates whose competencies represent an amalgamation of multiple perspectives. They are graduates who are able to speak languages of multiple disciplines. As experienced in our research, being able to converse in the language of a broader range of disciplines proved helpful in analyzing verbal data. \nKnowledge management is another skill that is deemed relevant in the 4IR.\u00a0 Knowledge management, the ability to discriminate and categorize information, is fundamental in qualitative data analysis (Xing & Marwala, 2017; Davies, Fidler, & Gorbis, 2011). We discriminate information according to the conceptual demands of the emerging categories and themes and according to what is essential in building our thesis or model. At certain point in the study described in this paper, we encountered ideas that are important but are not relevant in our ongoing analysis. Hence, knowledge management is imperative. The upsurge of sophisticated information sources and knowledge exchange platforms in the 4IR results to a massive influx of data that we need to discriminate according to importance and relevance. \nThis makes knowledge management an essential skill in the 4IR. Qualitative research nurtures skills that are crucial to the economic, industrial, and social infrastructure in the 4IR. It deals with uncertainties and randomness which characterize the fourth industrial revolution. We argue that qualitative research is an essential component of Education 4.0, especially in Philippine higher education. Hence, integrating the principles and practices of qualitative research in Philippine higher education curricula, especially in STEAM, is deemed appropriate.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "631ddb6051c83f7502dde911ed3fd708aa7f1e97",
    "url": "https://www.semanticscholar.org/paper/631ddb6051c83f7502dde911ed3fd708aa7f1e97",
    "title": "Resources , Tools and Algorithms for the Semantic Web",
    "abstract": "As the vast majority of knowledge existing on the web is still embedded into written texts, it is obvious that any attempt to intelligently solve the problems of exploiting the web without heavily relying on Natural Language Processing (NLP) technologies is doomed to failure. It is generally known that the key issue in NLP is ambiguity. Ambiguity is present at all levels of traditional structuring of a language system (phonology, morphology, lexicon, syntax, semantics) and not dealing with it at the proper level exponentially increases the complexity of the problem solving. Currently, the state-of-the-art taggers (combining various models, strategies and processing tiers) ensure no less than 97-98% accuracy in the process of morpholexical full disambiguation. For such taggers a 2-best tagging is practically 100% accurate. One further step is the word sense disambiguation (WSD) process. In the Fregean compositional semantics, the meaning of a complex expression is supposed to be derivable from the meanings of its parts, and the way in which those parts are combined. Depending on the representation formalisms for the word-meaning representation, various calculi may be considered for computing the meaning of a complex expression from the atomic representations of the word senses. Obviously, one should be able, beforehand, to decide for each word in a text which of its possible meanings is the contextually right one. Therefore, it is a generally accepted idea that the WSD task is highly instrumental (if not indispensable) in semantic processing of natural language documents. SENSEVAL series of evaluation competitions on WSD is a very good source for learning how WSD evolved in the last 6-7 years and where is it nowadays. However, most of the successful commercial applications in language processing (text and/or speech) dispense of any explicit concern on semantics, with the usual motivations stemming from the computational high costs required by dealing with semantics in case of large volumes of data. With recent advances in corpus linguistics and statistical-based methods in NLP, revealing useful semantic features of linguistic data is becoming cheaper and cheaper and the accuracy of this process is steadily improving. Lately, there seems to be a growing acceptance of the idea that multilingual lexical ontologies might be the key towards aligning different views on the semantic atomic units to be used in characterizing the general meaning of various and multilingual documents. Depending on the granularity at which semantic distinctions are necessary, the accuracy of the basic semantic processing (such as word morphosyntactic disambiguation and word sense discrimination) can be very high with relatively low complexity computing. The current understanding of the Semantic Web concept depicts mainly two things: sharing and retrieval of information. Sharing information is about encoding information in such a way that any independent software application can query the data store and receive both information and information type and structure. Information retrieval is thought of as being an effective way of accessing any piece of data given that the software agent \u2018knows\u2019 already the structure it must crawl in order to find it. The authoritative statement about the Semantic Web belongs to Tim Berners-Lee ([15]): \u201cThe goal of the Semantic Web initiative is to create a universal medium for the exchange of data where data can be shared and processed by automated tools as well as by people. The Semantic Web is designed to smoothly interconnect personal information management, enterprise application integration, and the global sharing of commercial, scientific and cultural data. We are talking about data here, not human documents.\u201d",
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": null
  },
  {
    "paperId": "cb9b3303d094417a5c682b101c28e9ce74318622",
    "url": "https://www.semanticscholar.org/paper/cb9b3303d094417a5c682b101c28e9ce74318622",
    "title": "The SENSEVAL-2 Panel on Domains, Topics and Senses",
    "abstract": "An important aspect of sense disambiguation is the wider semantic space (domain, topic) in which the ambiguous word occurs. This may be most clearly illustrated by some cross-lingual examples, as they would appear in (machine) translation. Consider for instance the English word housing. In a more general \"sense\", this translates in German into Wohnung. In an engineering setting however it translates into Gehause. Also verbs may be translated differently (i.e. have a different sense) according to the semantic space in which they occur. For instance, English warming up translates into erhitzen in a more general sense, but into aufwarmen in the sports domain.",
    "venue": "*SEMEVAL",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c61d74519290ab66ad76b28b7d67da2808419bfa",
    "url": "https://www.semanticscholar.org/paper/c61d74519290ab66ad76b28b7d67da2808419bfa",
    "title": "PRICAI 2008: Trends in Artificial Intelligence, 10th Pacific Rim International Conference on Artificial Intelligence, Hanoi, Vietnam, December 15-19, 2008. Proceedings",
    "abstract": null,
    "venue": "Pacific Rim International Conference on Artificial Intelligence",
    "citationCount": 48,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d1b7d9937c9c155efc46e0b5dd1495242c09f943",
    "url": "https://www.semanticscholar.org/paper/d1b7d9937c9c155efc46e0b5dd1495242c09f943",
    "title": "Introducing Semantics in Web Personalization: The Role of Ontologies",
    "abstract": null,
    "venue": "EWMF/KDO",
    "citationCount": 42,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4db580c9132b90879f7a0f96e5d8bf29901a6543",
    "url": "https://www.semanticscholar.org/paper/4db580c9132b90879f7a0f96e5d8bf29901a6543",
    "title": "Lexical Bottleneck in Machine Translation and Natural Language Processing: A Case Study",
    "abstract": "This paper emphasises the need to develop efficient lexical knowledge acquisition techniques in order to tackle problems related to the so-called lexical bottleneck. Bearing this in mind, a semi-automatic technique for semantic clustering and word sense disambiguation is proposed. The main principles behind this method are the extraction of knowledge on a sublanguage basis and from actual corpora. Clustering and disambiguation are carried out by means of the similarity measure Dynamic Matching. Further, the development of a domain- specific semantic ontology is also reported.",
    "venue": "TC",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1d9f56932d26f629a9966db6ca396022324ae1b6",
    "url": "https://www.semanticscholar.org/paper/1d9f56932d26f629a9966db6ca396022324ae1b6",
    "title": "Commonsense Reasoning with Verbs",
    "abstract": "KT encodes commonsense knowledge (CK) about entities and relations in a cross-classified Ontology and a database of generic (prototypical and inherent) information. The verb Ontology combines linguistic and psycholinguistic classification. The generic information for verbs encodes typical associations with actions and events. This knowledge base is useful for word-sense disambiguation, inferencing without scripts, discourse reasoning, and post-verbal PP attachment.",
    "venue": "IJCAI",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f848a43698d3893e58d41bb8b38354d88b0a48c3",
    "url": "https://www.semanticscholar.org/paper/f848a43698d3893e58d41bb8b38354d88b0a48c3",
    "title": "\u57fa\u65bc\u300a\u77e5\u7db2\u300b\u7684\u8fad\u5f59\u8a9e\u7fa9\u76f8\u4f3c\u5ea6\u8a08\u7b97",
    "abstract": "Word similarity is broadly used in many applications, such as information retrieval, information extraction, text classification, word sense disambiguation, example-based machine translation, etc. There are two different methods used to compute similarity: one is based on ontology or a semantic taxonomy; the other is based on collocations of words in a corpus. As a lexical knowledgebase with rich semantic information, How-net has been employed in various researches. Unlike other thesauri, such as WordNet and Tongyici Cilin, in which word similarity is defined based on the distance between words in a semantic taxonomy tree, How-net defines a, word in a complicated multi-dimensional knowledge description language. As a result, a series of problems arise in the process of word similarity computation using How-net. The difficulties are outlined below: 1. The description of each word consists of a group of sememes. For example, the Chinese word \u201d\u6697\u7bb1(camera obscura)\u201d is described as: \u201dpart\u90e8\u4ef6, #TakePicture|\u62cd\u651d, %tool|\u7528\u5177, body|\u8eab\u201d, and the Chinese word \u201d\u5beb\u4fe1(write a letter)\u201d is described as: \u201dwrite|\u5beb, ContentProduct=letter|\u4fe1\u4ef6\u201d; 2. The meaning of a word is not a simple combination of these sememes. Sememes are organized using a specific knowledge description language. To meet these challenges, our work includes: 1. A study on the How-net knowledge description language. We rewrite the How-net definition of a word in a more structural format, using the abstract data structure of set and \u201cfeature structure\u201d. 2. A study on the algorithm used to compute word similarity based on How-net. The similarity between sememes, that between sets, and that between feature structures are given. To compute the similarity between two sememes, we use the distance between the sememes in the semantic taxonomy, as is done in Wordnet and Tongyici Cilin. To compute the similarity between two sets or two feature structures, we first establish a one-to-one mapping between the elements of the sets or the feature structures. Then, the similarity between the sets or feature structures is defined as the weighted average of the similarity between their elements. For feature structures, a one-to-one mapping is established according to the attributes. For sets, a one-to-one mapping is established according to the similarity between their elements. 3. Finally, we give experiment results to show the validity of the algorithm and compare them with results obtained using other algorithms. Our results for word similarity agree with people's intuition to a large extent, and they are better than the results of two comparative experiments.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "36c2a1478cdb5c0197221a3670219f13cabcd6c7",
    "url": "https://www.semanticscholar.org/paper/36c2a1478cdb5c0197221a3670219f13cabcd6c7",
    "title": "A Novel View on Information Content of Concepts in Extremely Large Ontologies",
    "abstract": "Semantic distance and semantic similarity are two important information retrieval measures used in word sense disambiguation as well as for the assessment of how relevant concepts are with respect to the documents in which they are found. A variety of calculation methods have been proposed in the literature, whereby methods taking into account the information content of an individual concept outperform those that don't. In this paper, we present a novel recursive approach to calculate a concept's information content based on the information content of the concepts to which it relates. The method is applicable to extremely large ontologies containing several million concepts and relationships amongst them. It is shown that a concept's information content as calculated by this method provides additional information with respect to an ontology that cannot be approximated by hierarchical edge-counting or human insight. In addition, it is suggested that the method can be used for quality control within large ontologies.",
    "venue": "MIE",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ]
  },
  {
    "paperId": "07b0f31048bedf8eba2ce99856da96378e6f40e1",
    "url": "https://www.semanticscholar.org/paper/07b0f31048bedf8eba2ce99856da96378e6f40e1",
    "title": "\ud55c\uad6d\uc5b4 \uc758\ubbf8\ub9dd \uad6c\ucd95\uacfc \ud65c\uc6a9",
    "abstract": "It is important to construct Knowledge Base(like Thesaurus, Ontology, Semantic Network, etc) which can be applied to the whole field of natural language processing. For example, WordNet, Kadokawa Thesaurus, and Lexical FreeNet represent the most typical Knowledge Base in natural language processing. Many Knowledge Bases constructed in many fields does not come up to our expectations in Korean language processing. In order to construct an effective Knowledge Base, various language resources such as corpus, dictionary, synonym dictionary and WordNet have to be integrated one another, and the knowledge base has to consist of chain of morpheme-word-phrase-collocation-idiom-corpus. This paper presents a construction method and application of Korean Semantic Network (KSN). The KSN is based on Korean dictionary and Sejong corpus, and is applied to text processing, word sense disambiguation (WSD). semantic analysis, query pattern analysis in information retrieval, and so on. This paper deals with the following contents: (1) We point out problems of thesaurus and semantic network that look like a hierarchical structure of words, and compare KSN with them. The KSN has 1:1 relationship between word and sense, not 1:n relationship that an existing thesaurus and semantic network has (2) We present KSN component parts and a construction method. The KSN has noun semantic hierarchy structure linked to predicates, semantic class, proper noun, semantic information, and so on. The links are resulted from consideration of a paradigmatic relation and a syntagmatic relation within sentence. For reference, the KSN consists of dictionary, morpheme information, parts of speech information, construction information, proper noun information (name entity), noun semantic hierarchical structure, predicates classification structure, semantic class relation, idiom, semantic information, and so on. (3) We Present that WSD using the KSN is more effective than one using an existing thesaurus and semantic network.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Geography",
      "Computer Science"
    ]
  },
  {
    "paperId": "240ecb7f417ec87332632366b1d2e977e15f01cf",
    "url": "https://www.semanticscholar.org/paper/240ecb7f417ec87332632366b1d2e977e15f01cf",
    "title": "From Noun Clusters to Taxonomies on Untagged Corpora",
    "abstract": "Clustering nouns can be useful in construction of a set of synonyms for word sense disambiguation, to perform query expansion in a QA system [10], to build ontology from a text, in data mining, etc., especially for languages different to English for which doesn\u2019t exist such hierarchy as WordNet (as in Romanian language case). On the other hand, syntactic analysis provides some potentially relevant information for clustering [11]. In this paper we propose two algorithms of obtaining the taxonomies for nouns. First, , replaces the syntactical analysis by constructing some special context vectors, while second algorithm, , does not consider syntactical relations between nouns and verbs.A comparison between constructing clusters and taxonomies for nouns with the algorithm , and with the algorithm is done as well.The experiments are made using a Romanian corpus from [10].",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "22d329a84b7848582ee6d2b04f0d859fe28596d7",
    "url": "https://www.semanticscholar.org/paper/22d329a84b7848582ee6d2b04f0d859fe28596d7",
    "title": "\uc5f0\uad6c\ub17c\ubb38 : \ud55c\uad6d\uc5b4 \uc758\ubbf8\ub9dd \uad6c\ucd95\uacfc \ud65c\uc6a9 - \uba85\uc0ac\ub97c \uc911\uc2ec\uc73c\ub85c -",
    "abstract": "It is important to construct Knowledge Base(like Thesaurus, Ontology, Semantic Network, etc) which can be applied to the whole field of natural language processing. For example, WordNet, Kadokawa Thesaurus, and Lexical FreeNet represent the most typical Knowledge Base in natural language processing. Many Knowledge Bases constructed in many fields does not come up to our expectations in Korean language processing. In order to construct an effective Knowledge Base, various language resources such as corpus, dictionary, synonym dictionary and WordNet have to be integrated one another, and the knowledge base has to consist of chain of morpheme-word-phrase-collocation-idiom-corpus. This paper presents a construction method and application of Korean Semantic Network (KSN). The KSN is based on Korean dictionary and Sejong corpus, and is applied to text processing, word sense disambiguation (WSD). semantic analysis, query pattern analysis in information retrieval, and so on. This paper deals with the following contents: (1) We point out problems of thesaurus and semantic network that look like a hierarchical structure of words, and compare KSN with them. The KSN has 1:1 relationship between word and sense, not 1:n relationship that an existing thesaurus and semantic network has (2) We present KSN component parts and a construction method. The KSN has noun semantic hierarchy structure linked to predicates, semantic class, proper noun, semantic information, and so on. The links are resulted from consideration of a paradigmatic relation and a syntagmatic relation within sentence. For reference, the KSN consists of dictionary, morpheme information, parts of speech information, construction information, proper noun information (name entity), noun semantic hierarchical structure, predicates classification structure, semantic class relation, idiom, semantic information, and so on. (3) We Present that WSD using the KSN is more effective than one using an existing thesaurus and semantic network.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0c0d1816ebe35fa9f00d4f7c720b6a89c9acdb58",
    "url": "https://www.semanticscholar.org/paper/0c0d1816ebe35fa9f00d4f7c720b6a89c9acdb58",
    "title": "A Proposal for Screening Inconsistencies in Ontologies based on Query Languages using WSD",
    "abstract": "In this paper, we discuss a method to screen inconsistencies in ontologies by applying a natural language processing (NLP) technique, especially, those used for word sense disambiguation (WSD). In the database research field, it is claimed that queries over target ontologies should play a significant role because they represent every aspect of the terms described in each ontology. According to (Calvanese et al., 2001), considering the global and the local ontologies, the terms in the global ontology can be viewed as the query over the local ontology, and the mapping between the global and the local ontologies is given by, associating each term in the global ontology with a view. On the other hand, ontology screening systems should be able to take advantage of some popular techniques for WSD, which is supposed to decide the right sense where the target word is used in a specific context. We present several examples regarding inconsistencies in ontologies with the aid of DAML+OIL notation(DAML+OIL, 2001), and propose that WSD can be one of the promising method to screen such as inconsistencies.",
    "venue": "NLPXML@COLING",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "55f6af40a84d9fdc88166275be315245cf591afe",
    "url": "https://www.semanticscholar.org/paper/55f6af40a84d9fdc88166275be315245cf591afe",
    "title": "Natural language processing and information systems : 5th International Conference on Applications of Natural Language to Information Systems, NLDB 2000, Versailles, France, June 28-30, 2000 : revised papers",
    "abstract": "Linguistics in Information Systems Design.- Word Net++: A Lexicon Supporting the Color-X Method.- Temporal Databases.- Generating Narratives from Plots Using Schema Information.- Temporal Granularity Enlightened by Knowledge.- Word-Sense Disambiguation.- Coping with Different Types of Ambiguity Using a Uniform Context Handling Mechanism.- WSD Algorithm Applied to a NLP System.- \"Beijing Frowns and Washington Pays Close Attention\" Computer Processing of Relations between Geographical Proper Names in Foreign Affairs.- Semantic Relationships in Databases.- Understanding and Representing Relationship Semantics in Database Design.- An Ontology for Classifying the Semantics of Relationships in Database Design.- A Very Large Database of Collocations and Semantic Links.- Semantic and Contextual Document Retrieval, Traceability.- Using Terminology Extraction to Improve Traceability from Formal Models to Textual Requirements.- Natural Language Analysis for Semantic Document Modeling.- Domain Analysis and Queries in Context.- Natural Language Generat on for Answering E-Mail and OLAP.- Using Information Extraction and Natural Language Generation to Answer E-Mail.- Using OLAP and Data Mining for Content Planning in Natural Language Generation.- NLP Techniques for Informat on Retrieval.- Toward an Enhancement of Textual Database Retrieval Using NLP Techniques.- Document Identification by Shallow Semantic Analysis.- Automated Information Extraction out of Classified Advertisements.- WEB Information Retrieval.- A Smart Web Query Engine for Semantic Retrieval of Web Data and Its Application to E-Trading.- GETESS: Constructing a Linguistic Search Index for an Internet Search Engine.- Using Semantics for Efficient Information Retrieval.- Guidelines for NL-Based Requirements Specifications in NIBA.- Developing Document Analysis and Data Extraction Tools for Entity Modelling.- On the Automatization of Database Conceptual Modelling through Linguistic Engineering.- Technical Databases.- The REVERE Project: Experiments with the Application of Probabilistic NLP to Systems Engineering.- Presenting Mathematical Concepts as an Example for Inference-Rich Domains.- Users and Interactions in WEB Querying.- Modeling Interaction and Media Objects.- Conceptual Patterns.- Validating Conceptual Models - Utilising Analysis Patterns as an Instrument for Explanation Generation.- Conceptual Patterns - A Consolidation of Coad's and Wohed's Approaches.- Patterns Retrieval System: A First Attempt.- Posters and Demonstrations.- Ontology Learning from Text.- ISIS: Interaction through Speech with Information Systems.- Effects of Hypertext Navigational Structure on Knowledge Acquisition.- MEDIEVAL : A Navigation Tool Through Medieval Documents.- MESIA: A Web Site Assistant for Document Filtering.- LExIS A Query Language to Scan Information Flow.- A Financial Data Mining Trading System.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a28f0689dd74c55c9a6e8c2def8393d55b002601",
    "url": "https://www.semanticscholar.org/paper/a28f0689dd74c55c9a6e8c2def8393d55b002601",
    "title": "An Abstraction-Based Data Model for Information Retrieval",
    "abstract": null,
    "venue": "Australasian Conference on Artificial Intelligence",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4a1b7392c9a8d905f27b2e8c90f6b48d1c661f16",
    "url": "https://www.semanticscholar.org/paper/4a1b7392c9a8d905f27b2e8c90f6b48d1c661f16",
    "title": "Automatic annotation of local data sources for data integration systems",
    "abstract": "In this article we present CWSD (Combined Word Sense Disambiguation) a method and a software tool for enabling automatic annotation of local structured and semi-structured data sources, with lexical information, in a data integration system. CWSD is based on the exploitation of WordNet Domains, structural knowledge and on the extension of the lexical annotation module of the MOMIS data integration system. The distinguishing feature of the algorithm is its low dependence of a human intervention. Our approach is a valid method to satisfy two important tasks: (1) the source annotation process, i.e. the operation of associating an element of a lexical reference database (WordNet) to all source elements, (2) the discover of mappings among concepts of distributed data sources/ontologies.",
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f7732096c02bd5074bc6a203378972b4485a4570",
    "url": "https://www.semanticscholar.org/paper/f7732096c02bd5074bc6a203378972b4485a4570",
    "title": "PORTABLE LANGUAGE TECHNOLOGY: A RESOURCE-LIGHT APPROACH TO MORPHO-SYNTACTIC TAGGING DISSERTATION Presented in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy in the Graduate School of The Ohio State University By",
    "abstract": "Morpho-syntactic tagging is the process of assigning part o f speech (POS), case, number, gender, and other morphological information to each word in a corpus. Morpho-syntactic tagging is an important step in natural language processing . Corpora that have been morphologically tagged are very useful both for linguistic res earch, e.g. finding instances or frequencies of particular constructions in large corpora, and for further computational processing, such as syntactic parsing, speech recognition, st emming, and word-sense disambiguation, among others. Despite the importance of morphol ogical tagging, there are many languages that lack annotated resources. This is almost ine vi able because these resources are costly to create. But, as described in this thesis, it is p os ible to avoid this expense. This thesis describes a method for transferring annotation from a morphologically annotated corpus of a source language to a corpus of a related target language. Unlike unsupervised approaches that do not require annotated data at ll and, as a consequence, lack precision, the approach proposed in this dissertation relies on linguistic knowledge, but avoids large-scale grammar engineering. The approach need s neither a parallel corpus nor a bilingual lexicon, and requires much less linguistic labo r than the standard technology. This dissertation describes experiments with Russian, Cze ch, Polish, Spanish, Portuguese, and Catalan. However, the general method proposed can be applied to any fusional language.",
    "venue": "",
    "citationCount": 6,
    "fieldsOfStudy": null
  },
  {
    "paperId": "f1e2687c45976eaac25e9de49d05c7b00546e1e6",
    "url": "https://www.semanticscholar.org/paper/f1e2687c45976eaac25e9de49d05c7b00546e1e6",
    "title": "A Large-scale Lexical Semantic Knowledge-base of Chinese",
    "abstract": "The Semantic Knowledge-base of Contemporary Chinese (SKCC) is a large scale Chinese semantic resource developed by the Institute of Computational Linguistics of Peking University. It provides a large amount of semantic information such as semantic hierarchy and collocation features for 66,539 Chinese words and their English counterparts. Its POS and semantic classification represent the latest progress in Chinese linguistics and language engineering. The descriptions of semantic attributes are fairly thorough, comprehensive and authoritative. The main work in this paper is to introduce the outline of SKCC, and establish a multi-level WSD model based on it. The results indicate that the SCK is effective for word sense disambiguation in Chinese and are likely to be important for general NLP.",
    "venue": "Pacific Asia Conference on Language, Information and Computation",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "4b08cfee38542915c3ad1d041a3b8cba074186d4",
    "url": "https://www.semanticscholar.org/paper/4b08cfee38542915c3ad1d041a3b8cba074186d4",
    "title": "Automatic annotation for mappings discovery in data integration systems ( Extended abstract ) ?",
    "abstract": "In this article we present CWSD (Combined Word Sense Disambiguation), a method and a software tool for enabling automatic annotation with lexical information of local structured and semi-structured data sources in a data integration system. CWSD is based on the exploitation of WordNet Domains and the lexical and structural knowledge of the data sources. The method extends the lexical annotation module of the MOMIS data integration system with an automatic annotation. The distinguishing feature of the method is its independence or low dependence of a human intervention. CWSD is a valid method to satisfy two important tasks: (1) the source annotation process, i.e. the operation of associating an element of a lexical reference database (WordNet) to all source elements, (2) the discover of mappings among concepts of distributed data sources/ontologies.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "c7d67cc23f28cde5f9ad26b31264235ff28d6a19",
    "url": "https://www.semanticscholar.org/paper/c7d67cc23f28cde5f9ad26b31264235ff28d6a19",
    "title": "On Semantic Similarity and Relatedness for Knowledge-Driven Discovery in Biomedical Data",
    "abstract": "A great variety of tasks, from word sense disambiguation and document retrieval to assessing the functional similarity of gene products and validating protein-protein interaction networks, depend on the ability to measure the semantic similarity between concepts organized in ontologies. This report is a comprehensive study of classic and recent computational methods measuring semantic relatedness. Motivational arguments set the stage for a survey of the methods, applications and a critical assessment of the methods and of the various evaluation strategies adopted in the literature. Proposals for future directions in the areas of measuring semantic similarity and relatedness, as well as suggestions for improvement, curently under investigation, are offered.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": null
  },
  {
    "paperId": "b54d65b1a03232d16bf82b778c89160b3c7aabb7",
    "url": "https://www.semanticscholar.org/paper/b54d65b1a03232d16bf82b778c89160b3c7aabb7",
    "title": "Criteria for Manual Clustering of Verb Senses",
    "abstract": "Criteria for Manual Clustering of Verb Senses Cecily Jill Duffield, Jena D. Hwang, Susan Windisch Brown, Dmitriy Dligach, Sarah E. Vieweg, Jenny Davis, Martha Palmer ({cecily.duffield, hwangd, susan.brown, dmitriy.dligach, sarah.vieweg, jennifer.davis, martha.palmer}@colorado.edu) Departments of Linguistics and Computer Science University of Colorado Boulder, C0 80039-0295, USA Key words: word sense disambiguation, annotation, inter-annotator agreement, syntactic/semantic features Introduction Word sense ambiguity poses significant obstacles to accurate and efficient information extraction and automatic translation. Successful disambiguation of polysemous words in NLP applications depends on determining an appropriate level of granularity of sense distinctions, especially for verbs. WordNet, an important and widely used lexical resource, uses fine-grained distinctions that provide subtle information about the particular usages of various lexical items (Felbaum, 1998). When used as a resource for annotation of various genres of text, this fine level of granularity has not been conducive to high rates of inter-annotator agreement (ITA) or high automatic tagging performance. Annotation of verb senses as described by coarse-grained Proposition Bank framesets may result in higher ITA scores, but the blurring of distinctions between verb senses with similar argument structures may fail to alleviate the problems posed by ambiguity. Our goal in this project is to create verb sense distinctions at a middle level of granularity that allow us to capture as much information as possible from a lexical item while still attaining high ITA scores and high system performance in automatic sense disambiguation. We have demonstrated that clear sense distinctions improve annotator productivity and accuracy, which results in a corresponding improvement in system performance. Training on this new data, Chen, Schein, Ungar and Palmer, (2006) report 86.7% accuracy for verbs using a smoothed maximum entropy model and rich linguistic features (just over 70% for fine-grained senses). This paper focuses on the methodology used to create the sense groupings, with a particular emphasis on the types of features that are most accessible to human annotators who are not linguists. Various criteria are considered when disambiguating senses and creating sense groupings for the verbs, including frequent lexical usages and collocations, syntactic features and alternations, and semantic features, similarly to the groupings for Senseval2 (Palmer, Dang & Felbaum, 2007). Our highest priority is to create clear distinctions among sense groupings that will be easily understood by the annotators and consequently result in high rates of inter- annotator agreement. We have found that the most successful approach is to cluster senses intuitively on a verb-by-verb basis, distinguishing sense groupings with features that are easily grasped by all annotators. Such features include specific domain usages, as in legal, financial, and social uses (distinguishing two senses of integrate in, \u201cOver two-thirds of the teachers report they integrated the arts into their subjects,\u201d and \u201cThe movie is set in 1971, when TC Williams High integrated blacks and whites.\u201d); a specific syntactic construction, such as a required locative prepositional phrase (distinguishing the sense of open in, \u201cThe master bedroom opens to a large terrace,\u201d from that in \u201cThe door won't open.\u201d); and the features of nominal arguments, such as an agentive subject (separating senses of indicate in \u201cThese symptoms indicate a serious illness,\u201d and \u201cHe indicated the right road by nodding towards it.\u201d) More theoretical features for distinguishing groupings have proven to be less successful. Annotators not familiar with linguistics were confused by concrete/abstract distinctions and such aspectual features as continuative or stative. Therefore, they are now rarely used to label sense groupings. Such concepts, when used, are more likely to be described in prose commentary. Certain compositional features of verbs such as manner and path have also proven to be confusing for annotators, and resulted in decreased annotator agreement. Verb sense groupings that do not receive high ITA scores in initial rounds of annotation are revised, often prioritizing the use of the more successful features illustrated above with examples. Acknowledgements We gratefully acknowledge the support of the National Science Foundation Grant NSF-0415923, Word Sense Disambiguation, and the Defense Advanced Research Projects Agency GALE program, Contract No. HR0011-06- C-0022, a subcontract from the BBN-AGILE Team. References Chen, J., Schein, A., Ungar, L., & Palmer, M. (2006). An empirical study of the behavior of word sense disambiguation. Proceedings of NAACL-HLT 2006. New York City, NY. Fellbaum, C. (Ed.). (1998). WordNet: An on-line lexical database and some of its applications. MIT Press, Cambridge, MA. Palmer, M., Dang, H.T., and Fellbaum, C. (to appear, 2007). Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Journal of Natural Language Engineering",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f3314a3b6287bdb6baff233d4d2d743d2a8b9676",
    "url": "https://www.semanticscholar.org/paper/f3314a3b6287bdb6baff233d4d2d743d2a8b9676",
    "title": "Using Contexts of One Trillion Words for WSD",
    "abstract": "This paper describes how features derived from one trillion words of text can be used in a practical supervised Word Sense Disambiguation system. Our system is based on substituting into the context of the target word, others ontologically related to it. Words related to the correct sense of the word are likely to occur with its context more frequently in the corpus than relatives of other senses. The Pointwise Mutual Information arising from these frequencies is used to form features for supervised learning. The system retains many of the advantages of supervised systems, while using the Web1T corpus to ameliorate sparse data problems. Using the Web1T data presents significant processing challenges, however, to which we describe practical solutions. We demonstrate that the features we describe improve the performance of systems when combined with existing local context features.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": null
  },
  {
    "paperId": "359345fdf0f10246bc2c8986bae15ace858b9057",
    "url": "https://www.semanticscholar.org/paper/359345fdf0f10246bc2c8986bae15ace858b9057",
    "title": "Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition",
    "abstract": "This volume contains the papers accepted for presentation at the ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisition, held at the University of Michigan, Ann Arbor, USA, on the 30th of June, 2005. \n \nThis workshop is supported by SIGLEX, the Special Interest Group on the Lexicon of the Association for Computational Linguistics (http://www.clres.com/siglex.html). Its goal is to bring together researchers interested in different facets of the automatic acquisition of deep lexical information, e.g. in the areas of computational grammars, computational lexicography, machine translation, information retrieval, question-answering, and text mining. \n \nDeep lexical resources include lexicons for linguistically-precise grammars, template sets for information extraction systems, and ontologies for word sense disambiguation. Such resources are critical for enhancing the performance of systems and for improving their portability between domains. Most deep lexical resources in current use have been developed manually by lexicographers at considerable cost, and yet have limited coverage and require labour-intensive porting to new tasks. Automatic lexical acquisition is a more promising and cost-effective approach to take, and is increasingly viable given recent advances in NLP and machine learning technology, and corpus availability. However, a number of important challenges still need addressing before benefits can be reaped in practical language engineering, such as the (multilingual) acquisition of deep lexical information from corpora and the implementation of accurate, large-scale, portable acquisition techniques.",
    "venue": "ACL 2005",
    "citationCount": 7,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "d668ef64b04e245ee18a8a392ce95eebef4b6ddf",
    "url": "https://www.semanticscholar.org/paper/d668ef64b04e245ee18a8a392ce95eebef4b6ddf",
    "title": "The role of data in NLP: the case for dataset profiling",
    "abstract": "[About the book] \n \nThis volume brings together selected and revised papers from the international conference on \u201cRecent Advances in Natural Language Processing\u201d, held in Borovets, Bulgaria, in September 2005. The best papers have been selected for this volume with the aim to reflect the most promising and significant trends in natural language processing. The volume covers a wide variety of topics in Natural Language Processing, including information extraction, indexing, latent semantic analysis, dependency parsing, anaphora and referring expressions, spam analysis, document classification, rhetorical relations, textual entailment, question answering, ontologies, word sense disambiguation, machine translation, treebanks and corpora.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "7324f9fdc093be012c71569043ed1d0bb6089191",
    "url": "https://www.semanticscholar.org/paper/7324f9fdc093be012c71569043ed1d0bb6089191",
    "title": "Sociopolitical Domain As a Bridge from General Words to Terms of Specic Domains",
    "abstract": "In the paper we argue that there exists a polythematic domain which is situated in an intermediate area between senses of a general language area and specic domains. The concepts of this domain can be naturally added to general wordnets together with publicly known technical terms. Such enhanced wordnets can provide much more considerable preliminary coverage of domain specic texts, improve efcienc y of word sense disambiguation procedures. Majority of the texts in electronic collections contain as general words as terms from specic domains. To effectively organize automatic text processing, knowledge resources have to include descriptions of both types of language expressions. However for years general words and domain terms were studied by different research communities. Lexicology and lexicography studied meanings of general words, technical terms were considered by terminologists in the general theory of terminology. Wuster wrote that the main difference in consideration of general words by lexicologists and terms by terminologists was as follows: terminologists begin consideration from a concept, but lexicologists from a form of a linguistic expression (15). He wrote that terminological research starts from the concept which has to be precisely delimited and that in terminology concepts are considered to be independent from their designations. This explains the fact that terminologists talk about 'concepts' while linguists talk about 'word meanings'. But now when linguists began to develop wordnets for various languages, the situation is changing. Creating wordnets linguists construct hierarchical semantic networks, try to nd similar isynsetsi for different languages, build the top ontology of language-independent concepts (2). These directions of lexical research are much closer to the study of concepts, therefore the distinction between approaches seems to be considerably less serious. Recently researchers began development of wordnets for specic domains (1,14). From this point of view it is very important to understand how a general wordnet and domain specic wordnets interact with each other, how development of domain specic wordnets correlates with terminology research, if it is possible to combine lexical and terminological knowledge in the same linguistic resource. In this paper we argue that there exists a polythematic domain which is situated in an intermediate area between senses of general language and concepts of specic domains and partially intersects with both ones. The concepts of this domain can be naturally added to",
    "venue": "",
    "citationCount": 6,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c445e30d3becc2952cb2c31e85ebeb7b7977b5da",
    "url": "https://www.semanticscholar.org/paper/c445e30d3becc2952cb2c31e85ebeb7b7977b5da",
    "title": "Keynote address: computers versus common sense",
    "abstract": "Suppose that it is the year 2020, and the \u2018enterprise wide knowledge utility\u2019 is a reality. Is it dataor inferenceintensive? Where does it sit on the expressiveness versus efficiency tradeoff curve? What was the \u201cAI bottleneck\u201d and the \u201cDB bottleneck\u201d to building it? How local/guaranteed/distributed/heteroge~leous/parallel is it? Looking back thirty years from now, where should we have spent our limited resources (people and dollars) in the 90\u2019s, to have accelerated its coming into existence? I\u2019ll give representative answers from today\u2019s Data Base and Artificial Intelligence communities, and my own opinions which differ at times from both of those. The upshot of that is a mandate to AI, to roll up their sleeves and begin to build and experiment with very large knowledge bases. I came to this conclusion in 1984, and with no little trepidation took my own advice. I left the field of machine learning to embark on a decade-long effort to construct a Knowledge Base with on the order of ten million assertions, spanning all of human consensus reality, down to some reasonable level of depth. Why that particular KB, instead of, say, one that focused on engineering electromechanical devices? Intelligent behavior, especially in unexpected situations, requires being able to fall back on general knowledge, and being able to analogize to specific but far-flung knowledge. Natural language understanding, even in seemingly narrow domains such as \u201cWall St. Journal articles about mergers and acquisitions,\u201d also requires broad general knowledge, to resolve word sense ambiguities, elhpses, pronominal referents, and of course metaphors and analogies. And machine learning (automated discovery) occurs at the fringe of what one already knows, hence also would be empowered by such a KB. Aside from the magnitude of the task, the person-centuries it would take to build the KB, my concern stemmed from the dangers of getting stuck on representation thorns along the way (representing and reasoning with time, space, substances, belief, intention, emotion, causality, parts, awareness, contexts, counterfactuals, and so on.) Our approach had to be a pragmatic one: for each of those issues, we found a set of partial solutions which, in aggregate, efficiently covered the common cases. Notice this is a violation of the 1-2-infinity taboo which has so dominated most sciences in the past century (i.e., separately handling 27 special cases of phenomenon x is considered unesthetic, unscientific, etc.) Some of the other interesting issues in building the large KB are: how we decide what knowledge to include; how it accesses, deduces, or guesses answers; how it deals with uncertainty and contradictions; and how we coordinate a large team of knowledge enterers without having them \u201cdiverge.\u201d We\u2019ve gotten pretty far along already, in the process of building up an already-large KB (over a million rules and general assertions). I\u2019ll present some of those solutions, giving the flavor of our representation language, ontology, and methodology, Finally I\u2019ll relate this back to the general issue of producing a global information infrastructure. I invite those of you who wish additional detail to read the article by R. V. Guha and myself in the August \u201990 (2A CM.",
    "venue": "ACM SIGMOD Conference",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "946399cd7b6248c4c5d9c7be5f4f65694eb25a8e",
    "url": "https://www.semanticscholar.org/paper/946399cd7b6248c4c5d9c7be5f4f65694eb25a8e",
    "title": "Text, Speech and Dialogue, 10th International Conference, TSD 2007, Pilsen, Czech Republic, September 3-7, 2007, Proceedings",
    "abstract": null,
    "venue": "International Conference on Text, Speech and Dialogue",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "315e3bbb8efc96e849cbabe6786ec3ddc4932d80",
    "url": "https://www.semanticscholar.org/paper/315e3bbb8efc96e849cbabe6786ec3ddc4932d80",
    "title": "Financial event detection using semantics",
    "abstract": "Nowadays, emerging news on economic events like acquisitions substantially impacts \ufb01nancial markets. Hence, it is important to automatically and accurately identify economic events in news in a timely manner. For this, one has to be able to process a large amount of heterogeneous sources of unstructured data to extract knowledge useful for guiding decision making pro-cesses. We propose a Semantics-based Pipeline for Economic Event Detection (SPEED), with which we aim to extract \ufb01nancial events from emerging news gathered from RSS feeds, and to annotate these with machine-understandable meta-data, while retaining a speed that is high enough to make real-time use possible. Our framework is modeled as a pipeline which takes news messages as input and is driven by a \ufb01nancial ontology developed by domain experts, containing infor-mation extracted from Yahoo! Finance on NASDAQ-100 companies. In our implementation, we have reused some of the components of the existing General Archi-tecture for Text Engineering (GATE) framework and additionally, developed new ones, e.g., an Ontology Gazetteer and a Word Sense Disambiguator. Experiments on 200 news messages fetched from the Yahoo! Business and Technology RSS feeds show fast, sub-second gazetteering and a precision and recall for concept identi\ufb01cation of 86% and 81%,",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e55a1e4061c2202d67cd0772470309f7c826a213",
    "url": "https://www.semanticscholar.org/paper/e55a1e4061c2202d67cd0772470309f7c826a213",
    "title": "Ontological Dependency",
    "abstract": "Successful ontological analysis depends upon having the right underlying theory. The work described here, exploring how to understand organisations as systems of social norms found that the familiar objectivist position did not work, eventually replacing it with a radically subjectivist ontology which treats every thing, relationship and attribute as a repertoire of behaviour as understood by some responsible agent. Gibson\u2019s Theory of Affordances supports this view in relation to our physical reality and the concept of norms extends the theory naturally into the social domain. A formalism, Norma, which captures the need always to specify the responsible agent and some more or less complex repertoire of behaviour, introduces the concept of ontological dependency where one repertoire depends for its existence on another. This unusual logical relationship allows one to devise schemas which can generate systems as by-products; the paper ends with an example dealing with health insurance. To emphasise the validity of the underlying philosophical position, the paper is written in E-Prime (Borland 1974), English without the verb \"to be\" which forces one to abandon the objectivist way of thinking in favour of one that accounts for the world in terms of the actions of agents. The commitment we make concerning what kinds of things exist must be the deepest we make in forming a view of the world, so deep as a rule that it tends usually to remain implicit in what we say. Habitual ways of thinking and language itself can keep our ontological assumptions submerged in the unconscious and, by doing so, these influences can hold us prisoner to a set of ideas. This paper explains a radically subjectivist ontological position which we eventually adopted to replace the old objectivist view which did not work. We inteded our research on the problem of describing organisations accurately as information systems to lead to better ways of analysing, specifying and developing computer-based information systems. In this we have now succeeded but in many other surprising ways, in particular in developing a method of semantic analysis that appears capapable of generating a cannonical semantic form. The crucial ingredient in this method, the concept of ontological dependency, I shall explain in this paper. Ontological theory versus ontological engineering Our research into methods of specifying organisations as information systems made use of legal norms as experimental material. (Organised behaviour implies regularities caused by people tending to follow norms, hence to understand and specify organistion we could try to do so by treating norms as our primitive concept. No more of this here see Stamper 1994) Naturally we encountered the same problem as Cyc, and in much the same light as Lenat and Guha (1990, p.23): \"Choosing a set of representation primitives (predicates, objects and functions) has been called ontological engineering that is, defining the categories and relationships of the domain. (This is empirical, experimental engineering, as contrasted with ontological theorizing, which philosophers have done for millenia.)\" and we made significant progress, demonstrating an expert system shell at the workshop on computers and law in Swansea in 1979. But this attitude towards ontology will not work in the longrun for large human systems. If you want a machine to pass the Turing test for artificial intelligence, then you can build your own local solution for manipulating symbols in ways that will pull enough wool over the eyes of the individuals in the neighbouring room recruited to judge the machine/person undergoing the test. Turing\u2019s test avoids the real difficulties of ontology (pure nominalism suffices in a world of character-strings), semantics (treated intuitively by the judge) and intentional behaviour (no serious commitments made) by limiting itself to desktop activities. In the world of practical human affairs where laws, organisational norms and culture matter, we exchange signs not as tokens on a kind of chess board but as instruments for action. I propose a tougher criterion for intelligence outside the laboratory or the school examination room: the subject and the judge must enter into serious social relationships in which the subject will have to discharge the commitments it accepts and account for its failures in a responsible way. The engineering of character-string manipulation in a laboratory would not suffice in for systems that must play in integral role in an organisation or society. Using the law as experimental material brought with it more than the benefits of an endless supply of complex but fairly clear norms to study; it forced us to accept some rather obvious premisses to which a training in natural science or mathematics tends to blind one. In an attempt to create a legally oriented language (Legol) that ordinary users would understand fairly easily we adopted as a quality goal the hiding of as much structure as possible, perhaps an arbitrary criterion but one that inadvertanty pointed us in the right direction. It soon became evident that every thing in the domain of everyday human affairs seems to involve time and also some responsible agent, because timelessness places things in a distant, abstract world, and we know nothing without involving someone at least as an observer. The central role of the agent began to emerge but our understanding of time had some way to go. The emergent structure owed not a little to an analogy with the notion of spatial coordinates but extended to a space of social as well as physical reality. Thus: to own a licence to publish this paper in the proceedings of a conference one had to find one\u2019s way first to the literary work in question, then to the copyright in it, then to someone\u2019s ownership of the copyright and the licence incorporating a limited portion of those rights, then to the ownership of the licence by someone else and to the inclusion of one literary work within another and to its publication; the licence in question makes no sense without all these other things making sense, just as a point in three dimensional space makes no sense without understanding how to move in each of the three dimensions. Clearly, at the origin of this elaborate, social coordinate system we find society itself acting as the agent. Progress was good and the quality criterion of having a uniform, hidable structure behind every thing grew into a goal of creating a semantic normal form. Given a cannonical structure to capture meanings, every analyst would contribute his or her share towards a grand, coherent structure and computer-based systems developed for different parts of an organisation would not remain island of automation, but would naturally link up to one another as their functions began to overlap. This goal did not seem unattainable. So, at that stage, everything mentioned in a norm had, besides a label (natural lanuage word or a name), a sort (the kind of universal it instantiated), optional start and finish events, an authority (an agent or a norm) and any number of antecedents one had to know about to understand the thing in question. We implemented a new interpreter for the Legol formalism (Tagg 1979) which had the nesting of antecedents on the basis of a paper (Stamper 1978) on the goal of a semantic normal form. The next step suggested itself: make the start and finish mandatory. Sociologists had not problem with the idea of the social construction of at least the social part of our reality (Berger and Luckmann, 1967); theologians might object but we could leave them to their own account; more troublesome were mathematicians who study an eternal reality but a number of writers (Lakatos 1976, Bloor 1976 and 1983, Davis and Hersh 1983, Kitcher 1984, De Millo et al 1979) made clear that one might argue for an underlying empirical and social foundation for mathematics, thus bringing its concepts under the rule that everything has a finite period of existence. This made it easier to impose on the predicates and functions the constraint that they could only exist during the coexistence of their immediate antecedents, but things of a social kind would not obey this constraint without making it difficult to handle such things as a tax liability for income in the previous year, certainly, in my experience when the income has all gone, the tax liability remains in existence. Time remained an enigma. But rescue came in the form of semiotics, the doctrine of signs, which makes clear the role that signs (information) play in the construction of our social reality. The blinding ontological insight that we can experience only the here-and-now and that we can only bind the present to the past using signs transformed the picture. As any practiced taxevader will tell you, the liablity only exists by virtue of the revenue service demanding payment on the basis of the record of one\u2019s income; so, avoid the demand, or better still, rid the world of the record of income, and the liability vanishes. Henceforth we applied the cooexistence rule to all antecedents. The education we all receive in natural science and mathematics encourages us to believe in an objective reality composed of individuals and all kinds of set-theoretic structures build from them, and to which our words point when we ask for their meanings. We still held that view. Nevertheless our emerging theory had all the building bricks for constructing an ontology upon quite a different basis. Clearly we can know of no reality without involving an agent of some kind, ourselves or another person or group, even for purposes of establishing commonsense knowledge we must admit a group agent as large as society as a whole; indeed none of us would never have learned much about the world without assimilating the perceptual framework built up over centuries by society at large. This makes possible the leap from an objectivist paradigm to a ",
    "venue": "",
    "citationCount": 21,
    "fieldsOfStudy": null
  },
  {
    "paperId": "0a1627c4c811446651b555ce5b06e267826ede8d",
    "url": "https://www.semanticscholar.org/paper/0a1627c4c811446651b555ce5b06e267826ede8d",
    "title": "Web technologies and applications : 17th Asia-Pacific Web Conference, APWeb 2015, Guangzhou, China, September 18-20, 2015 : proceedings",
    "abstract": "On The Marriage of SPARQL and Keywords.- An Online Inference Algorithm for Labeled Latent Dirichlet Allocation.- Efficient Buffer Management for PCM-Enhanced Hybrid Memory Architecture.- Efficient Location-dependent Skyline Queries in Wireless Broadcast Environments.- Distance and Friendship: A Distance-based Model for Link Prediction in Social Networks.- Multi-Label Emotion Tagging for Online News by Supervised Topic Model.- Distinguishing Specific and Daily topics.- Matching Reviews to Object Based on 2-Stage CRF.- Discovering Restricted Regular Expressions with Interleaving.- Efficient Algorithms for Distance-based Representative Skyline Computation in 2D Space.- Trustworthy Collaborative Filtering through Downweighting Noise and Redundancy.- A Co-Ranking Framework to Select Optimal Seed Set for Influence Maximization in Heterogeneous Network.- Hashtag Sense Induction Based on Co-Occurrence Graphs.- Hybrid-LSH for Spatio-Textual Similarity Queries.- Sleep Quality Evaluation of Active Microblog Users.- Distributed XML Twig Query Processing using MapReduce.- Sentiment Word Identification with Sentiment Contextual Factors.- Large-Scale Graph Classification based on Evolutionary Computation with Map Reduce.- Multiple Attribute Aware Personalized Ranking.- Knowledge Base Completion Using Matrix Factorization.- MATAR: Keywords Enhanced Multi-Label Learning for Tag Recommendation.- Reverse Direction-based Surrounder Queries.- Learning to Hash for Recommendation with Tensor Data.- Spare Part Demand Prediction based on Context-aware Matrix Factorization.- Early Outbreak Detection in Social Networks.- Location Sensitive Friend Recommendation in Social Network.- A Compression-Based Filtering Mechanism in Content-Based Publish/Subscribe System.- Sentiment Classification for Chinese Product Reviews Based on Semantic Relevance of Phrase.- Overlapping Schema Summarization based on Multi-label Propagation.- Analysis of Subjective City Happiness Index Based on Large Scale Microblog Data.- Tree-Based Metric Learning for Distance Computation in Data Mining.- GSCS - Graph Stream Classification with Side Information.- A Supervised Parameter Estimation Method of LDA.- Learning Similarity Functions for Urban Events Detection by Mining Hotline Phone Records.- Answering Spatial Approximate Keyword Queries on Disks.- Hashing Multi-Instance Data from Bag and Instance Level.- A Multi-news Timeline Summarization Algorithm Based on Aging Theory.- Extended Strategies for Document Clustering with Word Co-occurrences.- A Lightweight Evaluation Framework for Table Layouts in MapReduce Based Query Systems.- An Extended Graph-based Label Propagation Method for Readability Assessment.- Simple is Beautiful: An Online Collaborative Filtering Recommendation Solution with Higher Accuracy.- Random-based Algorithm for Efficient Entity Matching.- A New Similarity Measure Between Semantic Trajectories based on Road Networks.- Effective Citation Recommendation by Unbiased Reference Priority Recognition.- UserGreedy: Exploiting the Activation Set to Solve Influence Maximization Problem.- AILabel: A Fast Interval Labeling Approach for Reachability Query on Very Large Graphs.- Graph-based Hybrid Recommendation Using Random Walk and Topic Modeling.- RDQS: A Relevant and Diverse Query Suggestion Generation Framework.- Minimizing the cost to win competition in social network.- Ad Dissemination Game in Ephemeral Networks.- User Generated Content Oriented Chinese Taxonomy Construction.- Mining Weighted Frequent Itemsets with the Recency Constraint.- Boosting Explicit Semantic Analysis by Clustering Paragraph Vectors of Wikipedia Articles.- Research on Semantic Disambiguation in Treebank.- PDMA: A Probabilistic Framework for Diversifying Recommendation Lists.- User Behavioral Context-Aware Service Recommendation for Personalized Mashups in Pervasive Environments.- On Coherent Indented Tree Visualization of RDF Graphs.- Online Feature Selection based on Passive-Aggressive Algorithm with Retaining Features.- Online Personalized Recommendation Based on Streaming Implicit User Feedback.- A Self-Learning Rule-based Approach for Sci-tech Compound Phrase Entity Recognition.- Batch Mode Active Learning For Geographical Image Classification.- A Multi-view Retweeting Behaviors Prediction in Social Networks.- Probabilistic Frequent Pattern Mining by PUH-Mine.- A Secure and Efficient Framework for Privacy Preserving Social Recommendation.- DistDL: A Distributed Deep Learning Service Schema with GPU Accelerating.- A Semi-Supervised Solution for Cold Start Issue on Recommender Systems.- Hybrid Cloud Deployment of an ERP-based Student Administration System.- A Benchmark Evaluation of Enterprise Cloud Infrastructure.- A Fast Data Ingestion and Indexing Scheme for Real-time Log Analytics.- A Fair Data Market System with Data Quality Evaluation and Repairing Recommendation.- HouseIn: A Housing Rental Platform with Non-Redundant Information Integrated from Multiple Sources.- ONCAPS: An Ontology-based Car Purchase Guiding System.- A Multiple Trust Paths Selection Tool in Contextual Online Social Networks.- EPEMS: An Entity Matching System For E-Commerce Products.- PPS-POI-Rec: A Privacy Preserving Social Point-of-Interest Recommender System.- Incorporating Contextual Information into a Mobile Advertisement Recommender System.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "fedbb4a9f151f820ab4aad386e9bef282db69fbb",
    "url": "https://www.semanticscholar.org/paper/fedbb4a9f151f820ab4aad386e9bef282db69fbb",
    "title": "Proceedings of the COLING-2000 Workshop on Semantic Annotation and Intelligent Content, SAIC@COLING 2000, Centre Universitaire, Luxembourg, August 5-6, 2000",
    "abstract": "SEMANTIC ANNOTATION is augmentation of data to facilitate automatic recognition of the underlying semantic structure. A common practice in this respect is labeling of documents with thesaurus classes for the sake of document classification and management. In the medical domain, for instance, there is a long-standing tradition in terminology maintenance and annotation/classification of documents using standard coding systems such as ICD, MeSH and the UMLS meta-thesaurus. Semantic annotation in a broader sense also addresses document structure (title, section, paragraph, etc.), linguistic structure (dependency, coordination, thematic role, co-reference, etc.), and so forth. In NLP, semantic annotation has been used in connection with machine-learning software trainable on annotated corpora for parsing, word-sense disambiguation, co-reference resolution, summarization, information extraction, and other tasks. A still unexplored but important potential of semantic annotation is that it can provide a common I/O format through which to integrate various component technologies in NLP and AI such as speech recognition, parsing, generation, inference, and so on. \n \nINTELLIGENT CONTENT is semantically structured data that is used for a wide range of content-oriented applications such as classification, retrieval, extraction, translation, presentation, and question-answering, as the organization of such data provides machines with accurate semantic input to those technologies. Semantically annotated resources as described above are typical examples of intelligent content, whereas another major class includes electronic dictionaries and inter-lingual or knowledge-representation data. Some ongoing projects along these lines are GDA (Global Document Annotation), UNL (Universal Networking Language) and SHOE (Simple HTML Ontology Extension), all of which aim at motivating people to semantically organize electronic documents in machine-understandable formats, and at developing and spreading content-oriented application technologies aware of such formats. Along similar lines, MPEG-7 is a framework for semantically annotating audiovisual data for the sake of content-based retrieval and browsing, among others. Incorporation of linguistic annotation into MPEG-7 is in the agenda, because linguistic descriptions already constitute a main part of existing metadata. In short, semantic annotation is a central, basic technology for intelligent content, which in turn is a key notion in systematically coordinating various applications of semantic annotation. In the hope of fueling some of the developments mentioned above and thus promoting the linkage between basic researches and practical applications, the workshop invites researchers and practitioners from such fields as computational linguistics, document processing, terminology, information science, and multimedia content, among others, to discuss various aspects of semantic annotation and intelligent content in an interdisciplinary way.",
    "venue": "SAIC@COLING",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "46d82377c4167d4443132780b04da40fef347399",
    "url": "https://www.semanticscholar.org/paper/46d82377c4167d4443132780b04da40fef347399",
    "title": "TGD Inspired Comments about Integrated Information Theory of Consciousness",
    "abstract": "Integrated Information Theory (IIT) is a theory of consciousness originally proposed by Giulio Tononi. The basic goal of IIT is to abstract from neuroscience axioms about consciousness hoped to provide constraints on physical models. IIT relies strongly on information theory. The basic problem is that the very definition of information is not possible without introducing conscious observer so that circularity cannot be avoided. IIT identifies a collection of few basic concepts and axioms such as the notions of mechanism (computer program is one analog for mechanism), information, integration and maximally integrated information (maximal interdependence of parts of the system), and exclusion. Also the composition of mechanisms as kind of engineering principle of consciousness is assumed and leads to the notion of conceptual structure, which should allow to understand not only cognition but entire conscious experience. A measure for integrated information (called \u03a6) assignable to any partition of system to two parts is introduced in terms of relative entropies. Consciousness is identified with a maximally integrated decomposition of the system to two parts (\u03a6 is maximum). The existence of this preferred decomposition of the system to two parts besides computer and program running in it distinguishes IIT from the computational approach to consciousness. Personally I am however afraid that bringing in physics could bring in physicalism and reduce consciousness to an epiphenomenon. Qualia are assigned to the links of network. IIT can be criticized for this assignment as also for the fact that it does not say much about free will nor about the notion of time. Also the principle fixing the dynamics of consciousness is missing unless one interprets mechanisms as such. In this article IIT is compared to the TGD vision relying on physics and on general vision about consciousness strongly guided by the new physics predicted by TGD. At classical level this new physics involves a new view about space-time and fields (in particular the notion of magnetic body central in TGD inspired quantum biology and quantum neuroscience). At quantum level it involves Zero Energy Ontology (ZEO) and the notion of causal diamond (CD) defining 4-D perceptive field of self; p-adic physics as physics of cognition and imagination and the fusion of real and various p-adic physics to adelic physics; strong form of holography (SH) implying that 2-D string world sheets and partonic surfaces serve as \u201cspace-time genes\u201d; and the hierarchy of Planck constants making possible macroscopic quantum coherence. Number theoretic entanglement entropy (EE) makes sense as number theoretic variant of Shannon entropy in the p-adic sectors of the adelic Universe. Number theoretic EE can be negative and corresponds in this case to genuine information: one has negentropic entanglement (NE). TGD inspired theory of consciousness reduces to quantum measurement theory in ZEO. Negentropy Maximization Principle (NMP) serves as the variational principle of consciousness and implies that NE can can only increase this implies evolution. By SH real and p-adic 4-D systems are algebraic continuations of 2-D systems (\u201cspace-time genes\u201d) characterized by algebraic extensions of rationals labelling evolutionary levels with increasing algebraic complexity. Real and p-adic sectors have common Hilbert space with coefficients in algebraic extension of rationals so that the state function reduction at this level can be said to induce real and p-adic 4-D reductions as its shadows. NE in the p-adic sectors stabilizes the entanglement also in real sector (the sum of real (ordinary) and various p-adic negentropies tends to increase) the randomness of the ordinary state function reduction is tamed by cognition and mind can be said to rule over matter. Quale corresponds in IIT to a link of a network like structure. In TGD quale corresponds to the eigenvalues of observables measured repeatedly as long as corresponding sub-self (mental image, quale) remains conscious. In ZEO self can be seen as a generalized Zeno effect. What happens in death of a conscious entity (self) can be understood and it accompanies re-incarnation of time reversed self in turn making possible re-incarnation also in the more conventional sense of the word. The death of mental image (sub-self) can be also interpreted as motor action involving signal to geometric past: this in accordance with Libet\u2019s findings. There is much common between IIT and TGD at general structural level but also profound differences. Also TGD predicts restricted pan-psychism. NE is the TGD counterpart for the integrated information. The combinatiorial structure of NE gives rise to quantal complexity. Mechanisms correspond to 4-D self-organization patterns with self-organization interpreted in 4-D sense in ZEO. The decomposition of system to two parts such that this decomposition can give rise to a maximal negentropy gain in state function reduction is also involved but yields two independent selves. Engineering of conscious systems from simpler basic building blocks is predicted. Indeed, TGD predicts infinite self hierarchy with sub-selves identifiable as mental images. Exclusion postulate is not needed in TGD framework. Also network like",
    "venue": "",
    "citationCount": 6,
    "fieldsOfStudy": null
  },
  {
    "paperId": "39fa78ca5cbb16a0f6dfab75ba42fc7171fed113",
    "url": "https://www.semanticscholar.org/paper/39fa78ca5cbb16a0f6dfab75ba42fc7171fed113",
    "title": "Intelligent Integrated Health Management for a System of Systems",
    "abstract": "An intelligent integrated health management system (IIHMS) incorporates major improvements over prior such systems. The particular IIHMS is implemented for any system defined as a hierarchical distributed network of intelligent elements (HDNIE), comprising primarily: (1) an architecture (Figure 1), (2) intelligent elements, (3) a conceptual framework and taxonomy (Figure 2), and (4) and ontology that defines standards and protocols. Some definitions of terms are prerequisite to a further brief description of this innovation: A system-of-systems (SoS) is an engineering system that comprises multiple subsystems (e.g., a system of multiple possibly interacting flow subsystems that include pumps, valves, tanks, ducts, sensors, and the like); 'Intelligent' is used here in the sense of artificial intelligence. An intelligent element may be physical or virtual, it is network enabled, and it is able to manage data, information, and knowledge (DIaK) focused on determining its condition in the context of the entire SoS; As used here, 'health' signifies the functionality and/or structural integrity of an engineering system, subsystem, or process (leading to determination of the health of components); 'Process' can signify either a physical process in the usual sense of the word or an element into which functionally related sensors are grouped; 'Element' can signify a component (e.g., an actuator, a valve), a process, a controller, an actuator, a subsystem, or a system; The term Integrated System Health Management (ISHM) is used to describe a capability that focuses on determining the condition (health) of every element in a complex system (detect anomalies, diagnose causes, prognosis of future anomalies), and provide data, information, and knowledge (DIaK) not just data to control systems for safe and effective operation. A major novel aspect of the present development is the concept of intelligent integration. The purpose of intelligent integration, as defined and implemented in the present IIHMS, is to enable automated analysis of physical phenomena in imitation of human reasoning, including the use of qualitative methods. Intelligent integration is said to occur in a system in which all elements are intelligent and can acquire, maintain, and share knowledge and information. In the HDNIE of the present IIHMS, an SoS is represented as being operationally organized in a hierarchical-distributed format. The elements of the SoS are considered to be intelligent in that they determine their own conditions within an integrated scheme that involves consideration of data, information, knowledge bases, and methods that reside in all elements of the system. The conceptual framework of the HDNIE and the methodologies of implementing it enable the flow of information and knowledge among the elements so as to make possible the determination of the condition of each element. The necessary information and knowledge is made available to each affected element at the desired time, satisfying a need to prevent information overload while providing context-sensitive information at the proper level of detail. Provision of high-quality data is a central goal in designing this or any IIHMS. In pursuit of this goal, functionally related sensors are logically assigned to groups denoted processes. An aggregate of processes is considered to form a system. Alternatively or in addition to what has been said thus far, the HDNIE of this IIHMS can be regarded as consisting of a framework containing object models that encapsulate all elements of the system, their individual and relational knowledge bases, generic methods and procedures based on models of the applicable physics, and communication processes (Figure 2). The framework enables implementation of a paradigm inspired by how expert operators monitor the health of systems with the help of (1) DIaK from various sources, (2) software tools that assist in rapid visualization of the condition of the system, (3) analical software tools that assist in reasoning about the condition, (4) sharing of information via network communication hardware and software, and (5) software tools that aid in making decisions to remedy unacceptable conditions or improve performance.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Engineering"
    ]
  },
  {
    "paperId": "99ea002876b44a0c72a2407ed0ad80381989ee42",
    "url": "https://www.semanticscholar.org/paper/99ea002876b44a0c72a2407ed0ad80381989ee42",
    "title": "Semantic TagPrint - Tagging and Indexing Content for Semantic Search and Content Management",
    "abstract": "Existing search and content management technology is facing a challenge of locating desired content with the exponentially growing volume of documents. An approach for mitigating this issue is to make use of user-generated tags. However, the improvements are limited because tags are (1) free from context and form, (2) user generated, (3) used for purposes other than description, and (4) often ambiguous. Since tagging is a voluntary action, some documents are not tagged at all. Furthermore, the interpretation of the tags associated with tagged documents also remains a challenge. To overcome these challenges, semantic web resources and technologies can be utilized to automatically generate semantic tags. Semantic tags not only reflect document content more accurately, they also enable better search results. Ontology coverage, ontology mapping and weighting significant ontological entities within a context are key challenges in semantic tagging systems. To address these challenges, this paper presents a semantic tagging system - Semantic TagPrint - to map a text document to semantic tags defined as entities in an ontology. Semantic TagPrint uses a linear time lexical chaining Word Sense Disambiguation (WSD) algorithm for real time concept mapping. In addition, it utilizes statistical metrics and ontological features of the ontology for weighting and recommending the semantic tags. A comparative evaluation shows that our mapping algorithm is fairly accurate and our tag recommendation algorithm performs better than other systems and algorithms.",
    "venue": "2010 IEEE Fourth International Conference on Semantic Computing",
    "citationCount": 10,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "2b2ef66d498a423c357b47dc7fcb2c6128d2063c",
    "url": "https://www.semanticscholar.org/paper/2b2ef66d498a423c357b47dc7fcb2c6128d2063c",
    "title": "Exploring Co-Reference Chains for Concept Annotation of Domain Texts",
    "abstract": "The paper explores the co-reference chains as a way for improving the density of concept annotation over domain texts. The idea extends authors\u0092 previous work on relating the ontology to the text terms in two domains \u2015 IT and textile. Here IT domain is used. The challenge is to enhance relations among concepts instead of text entities, the latter pursued in most works. Our ultimate goal is to exploit these additional chains for concept disambiguation as well as sparseness resolution at concept level. First, a gold standard was prepared with manually connected links among concepts, anaphoric pronouns and contextual equivalents. This step was necessary not only for test purposes, but also for better orientation in the co-referent types and distribution. Then, two automatic systems were tested on the gold standard. Note that these systems were not designed specially for concept chaining. The conclusion is that the state-of-the-art co-reference resolution systems might address the concept sparseness problem, but not so much the concept disambiguation task. For the latter, word-sense disambiguation systems have to be integrated.",
    "venue": "LREC",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "89643199e60c02adf4c7d1192bc70ec7812859b3",
    "url": "https://www.semanticscholar.org/paper/89643199e60c02adf4c7d1192bc70ec7812859b3",
    "title": "Computational Linguistics and Intelligent Text Processing, 11th International Conference, CICLing 2010, Iasi, Romania, March 21-27, 2010. Proceedings",
    "abstract": null,
    "venue": "CICLing",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "130e93bc9b98fa991050849b70c0e717c1b90e8b",
    "url": "https://www.semanticscholar.org/paper/130e93bc9b98fa991050849b70c0e717c1b90e8b",
    "title": "Briefly Noted",
    "abstract": "In Les linguistiques de corpus, newcomers as well as experts in the field will find a very interesting, well-written, and concise survey of corpus linguistics. The book gives a good overview of the multiple methods used and the problems involved in the creation and the annotation of a corpus. It also presents proposed applications for these corpora. The authors do not relate their own work, but present to the reader in an organized way (by topic) the work of many researchers. The book is written in French, but most of the works presented are based on corpora of English texts. Among other things, the authors discuss what makes a \"good corpus,\" how representative and reliable a corpus can be, and what corpora annotated at the lexical, syntactic, or semantic level presently exist. They analyze the interesting problem of the dependency of a corpus annotation on the foreseen applications for the corpus. The authors present applications such as word sense disambiguation and lexical acquisition from text, but they also present different applications that are of more concern to the literary community, such as discovering changes in the usage of a word between periods or between authors. In the authors' view (which I certainly share and consider very positive), corpus linguistics leaves linguists (and language specialists) and computer scientists no choice but to finally come together and use each other 's strengths (deep understanding of language and deep understanding of computer technology); they must share their expertise to push the field forward.--Caroline Barri~re, School of Information Technology and Engineering, University of Ottawa, Canada Text Retrieval and Filtering: Analytic Models of Performance",
    "venue": "Consciousness and Cognition",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "fd13bb769c9f0cd79682559940e6e47fd3c685ef",
    "url": "https://www.semanticscholar.org/paper/fd13bb769c9f0cd79682559940e6e47fd3c685ef",
    "title": "Identifying Words to Explain to a Reader: A Preliminary Study",
    "abstract": "In previous work we tried to automatically annotate text with semantic assistance on words. We augmented text with short \"factoids\" about words such as \"cheetah can be a kind of cat. Is it here?\" We used WordNet (Fellbaum 1998) to retrieve synonyms (assistance is like aid), hypernyms (cheetah is a kind of cat), and antonyms (beautiful is the opposite of ugly), and focused on the \"low-hanging fruit\" of words with only one or two senses. During design, development and deployment of these automatic annotations, we uncovered a number of challenges to our automated approach. \u2022 Appropriate language. Comprehensive language resources aimed at adults may not be appropriate for children because of \"improper\" language, too-difficult words, or archaic vocabulary. \u2022 Multiple senses. We intended to skirt the problem of multiple senses by looking only at words with few senses, and presenting the remaining word sense disambiguation problem as an exercise to the reader. (In the hope that telling the difference between e.g. elephant the animal and elephant the political symbol would be easy enough to leave to the student.) Such phrases were sometimes confusing, and unfortunately the computer never gave the \"answer\" of what the correct sense for the word was in this context. \u2022 Textual context. For some words it may be fine to explain them the same way regardless of the textual context in which they appear (e.g. asparagus, aardvark). However, some uses of a word are better explained by one synonym rather than others. Even harder, a word may be best explained by analogy with a word that is not a synonym, but a functionally related word. For example, one teacher in one of the schools we work with explained the word slate in a story about frontier schools in early America as being like a chalkboard -you write on it in school. (In WordNet, slate and chalkboard are most closely related through artifact, fairly high up in the ontology.) Together, these problems led us to seek instead a computer-assisted (but human-controlled) solution.",
    "venue": "AAAI/IAAI",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0817a32f6d88dbe761f22ca94b39206a87a8564e",
    "url": "https://www.semanticscholar.org/paper/0817a32f6d88dbe761f22ca94b39206a87a8564e",
    "title": "Text, speech and dialogue : third International Workshop, TSD 2000, Brno, Czech Republic, September 13-16, 2000 : proceedings",
    "abstract": "Text.- The Linguistic Basis of a Rule-Based Tagger of Czech.- Harnessing the Lexicographer in the Quest for Accurate Word Sense Disambiguation.- An Integrated Statistical Model for Tagging and Chunking Unrestricted Text.- Extending Bidirectional Chart Parsing with a Stochastic Model.- Ensemble of Classifiers for Noise Detection in PoS Tagged Corpora.- Towards a Dynamic Syntax for Language Modelling.- A Word Analysis System for German Hyphenation, Full Text Search, and Spell Checking, with Regard to the Latest Reform of German Orthography.- Automatic Functor Assignment in the Prague Dependency Treebank.- Categories, Constructions, and Dependency Relations.- Local Grammars and Parsing Coordination of Nouns in Serbo-Croatian.- Realization of Syntactic Parser for Inflectional Language Using XML and Regular Expressions.- A Rigoristic and Automated Analysis of Texts Applied to a Scientific Abstract by Mark Sergot and Others.- Evaluation of Tectogrammatical Annotation of PDT.- Probabilistic Head-Driven Chart Parsing of Czech Sentences.- Aggregation and Contextual Reference in Automatically Generated Instructions.- Information Retrieval by Means of Word Sense Disambiguation.- Statistical Parameterisation of Text Corpora.- An Efficient Algorithm for Japanese Sentence Compaction Based on Phrase Importance and Inter-Phrase Dependency.- Word Senses and Semantic Representations Can We Have Both?.- Automatic Tagging of Compound Verb Groups in Czech Corpora.- Sensitive Words and Their Application to Chinese Processing.- Testing a Word Analysis System for Reliable and Sense-Conveying Hyphenation and Other Applications.- The Challenge of Parallel Text Processing.- Selected Types of Pg-Ambiguity: Processing Based on Analysis by Reduction.- Cohesive Generation of Syntactically Simplified Newspaper Text.- TEA: A Text Analysis Tool for the Intelligent Text Document Filtering.- Competing Patterns for Language Engineering.- Speech.- Recognition and Labelling of Prosodic Events in Slovenian Speech.- Rules for Automatic Grapheme-to-Allophone Transcription in Slovene.- An Adaptive and Fast Speech Detection Algorithm.- Optimal Pitch Path Tracking for More Reliable Pitch Detection.- FlexVoice: A Parametric Approach to High-Quality Speech Synthesis.- The Continuous and Discontinuous Styles in Czech TTS.- Automatic Speech Segmentation with the Application of the Czech TTS System.- Speaker Identification Using Autoregressive Hidden Markov Models and Adaptive Vector Quantisation.- Morpheme Based Language Models for Speech Recognition of Czech.- A Large Czech Vocabulary Recognition System for Real-Time Applications.- Building a New Czech Text-to-Speech System Using Triphone-Based Speech Units.- Acoustic and Perceptual Properties of Syllables in Continuous Speech as a Function of Speaking Rate.- NL-Processor and Linguistic Knowledge Base in a Speech Recognition System.- Russian Phonetic Variability and Connected Speech Transcription.- Database Processing for Spanish Text-to-Speech Synthesis.- Topic-Sensitive Language Modelling.- Design of Speech Recognition Engine.- Combining Multi-band and Frequency-Filtering Techniques for Speech Recognition in Noisy Environments.- Allophone- and Suballophone-Based Speech Synthesis System for Russian.- Diphone-Based Unit Selection for Catalan Text-to-Speech Synthesis.- Analysis of Information in Speech and Its Application in Speech Recognition.- What Textual Relationships Demand Phonetic Focus?.- Speaker Identification Using Kalman Cepstral Coefficients.- Belarussian Speech Recognition Using Genetic Algorithms.- A Discriminative Segmental Speech Model and Its Application to Hungarian Number Recognition.- Comparison of Frequency Bands in Closed Set Speaker Identification Performance.- Recording and Annotation of the Czech Speech Corpus.- Dialogue.- A Text Based Talking Face.- Dialogue Control in the Alparon System.- ISIS: Interaction through Speech with Information Systems.- Centering-Based Anaphora Resolution in Danish Dialogues.- Some Improvements on the IRST Mixed Initiative Dialogue Technology.- Dictionary-Based Method for Coherence Maintenance in Man-Machine Dialogue with Indirect Antecedents and Ellipses.- Reconstructing Conversational Games in an Obligation-Driven Dialogue Model.- Prosody Prediction from Tree-Like Structure Similarities.- A Speaker Authentication Module in TelCorreo.- TelCorreo: A Bilingual E-mail Client over the Telephone.- A Syntactical Model of Prosody as an Aid to Spoken Dialogue Systems in Italian Language.- What Do You Mean by \"What Do You Mean\"?.- Simplified Processing of Elliptic and Anaphoric Utterances in a Train Timetable Information Retrieval Dialogue System.- Pragmatic and Grammatical Aspects of the Development of Dialogue Strategies.- An Annotation Scheme for Dialogues Applied to Anaphora Resolution Algorithms.- Cooperative Information Retrieval Dialogues through Clustering.- Acoustic Cues for Classifying Communicative Intentions in Dialogue Systems.- Active and Passive Strategies in Dialogue Program Generation.- Architecture of Multi-modal Dialogue System.- The Utility of Semantic-Pragmatic Information and Dialogue-State for Speech Recognition in Spoken Dialogue Systems.- Word Concept Model for Intelligent Dialogue Agents.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "215098ac9efc3bc834b2336f56733cd9ccfbd809",
    "url": "https://www.semanticscholar.org/paper/215098ac9efc3bc834b2336f56733cd9ccfbd809",
    "title": "Text, speech and dialogue : 4th International Conference, TSD 2001, \u017delezn\u00e1 Ruda, Czech Republic, September 11-13, 2001 : proceedings",
    "abstract": "Text.- Knowledge Sources for Word Sense Disambiguation.- The Current Status of the Prague Dependency Treebank.- Language Corpora: The Czech Case.- A Hybrid Logic Formalization of Information Structure Sensitive Discourse Interpretation.- he Possibilities of Automatic Detection/Correction of Errors in Tagged Corpora: A Pilot Study on a German Corpus.- Grammatical Agreement and Automatic Morphological Disambiguation of Inflectional Languages.- Modelling Semantic Association and Conceptual Inheritance for Semantic Analysis.- Influence of Conditional Independence Assumption on Verb Subcategorization Detection.- Morphological Guesser of Czech Words.- Clustering Technique Based on Semantics for Definite Description Resolution.- Pronoun Resolution in Spanish from Full Parsing.- A Method of Accurate Robust Parsing of Czech.- A New Czech Morphological Analyser ajka.- Finding Semantically Related Words in Large Corpora.- Syntactic-Based Methods for Measuring Word Similarity.- The ATRACT Workbench: Automatic Term Recognition and Clustering for Terms.- Augmented Auditory Representation of e-Texts for Text-to-Speech Systems.- Enhancing the Valency Dictionary of Czech Verbs: Tectogrammatical Annotation.- An Interactive Graph Based Legal Information Retrieval System.- Text Segmentation into Paragraphs Based on Local Text Cohesion.- Building a Digital Collection of Web-Pages: Access and Filtering Information with Textual Expansion.- Three Approaches to Word Sense Disambiguation for Czech.- Method for WordNet Enrichment Using WSD.- Speech.- Human Speech Perception: Some Lessons from Automatic Speech Recognition.- Speech Technology in Reality - Applications, Their Challenges and Solutions.- Generative Phoneme-Threephone Model for ASR.- Algebraic Models of Speech Segment Databases.- Bayesian Noise Compensation of Time Trajectories of Spectral Coefficients for Robust Speech Recognition.- The Influence of a Filter Shape in Telephone-Based Recognition Module Using PLP Parameterization.- System for Speech Communication through Internet.- Phoneme Based ASR System for Slovak SpeechDat Database.- Large Vocabulary Continuous Speech Recognizer for Slovenian Language.- A Nonlinearized Discriminant Analysis and Its Application to Speech Impediment Therapy.- Speech Recognition Issues for Dutch Spoken Document Retrieval.- Multichannel Sound Acquisition with Stress Situations Determination for Medical Supervision in a Smart House.- Two-Pass Recognition of Czech Speech Using Adaptive Vocabulary.- Simulation of Speaking Styles with Adapted Prosody.- The Utterance Type and the Size of the Intonation Unit.- Estimation of Boundaries between Speech Units Using Bayesian Changepoint Detectors.- Data Driven Design of Filter Bank for Speech Recognition.- Minimization of Transition Noise and HNM Synthesis in Very Low Bit Rate Speech Coding.- Chinese Radicals and Top Ontology in EuroWordNet.- Towards a Dynamic Adjustment of the Language Weight.- Speech Recognition Based on Feature Extraction with Variable Rate Frequency Sampling.- The Phonectic SMS Reader.- Dialouge.- Research Issues for the Next Generation Spoken Dialogue Systems Revisited.- Determining User Interface Semantics Using Communicating Agents.- Agent-Based Adaptive Interaction and Dialogue Management Architecture for Speech Applications.- Dialogue for Web Search Utilizing Automatically Acquired Domain Knowledge.- A Development Tool for Spoken Dialogue Systems and Its Evaluation.- Using XML for Representing Domain Dependent Knowledge in Dialogos.- Dialogue Generation of Program Source Codes.- Semi-automatic Identification of Danish Discourse Deictics.- Language Understanding Using Two-Level Stochastic Models with POS and Semantic Units.- Shallow Processing and Cautious Incrementality in a Dialogue System Front End: Two Steps towards Robustness and Reactivity.- Creation of a Corpus of Training Sentences Based on Automated Dialogue Analysis.- Dialogue Manager in Dialogue System for Visually Impaired Programmers.- Help System in Dialogue Grammar Based Programming Environment for Visually Impaired Programmers.- Automatic Generation of Dialogue Interfaces for Web-Based Applications.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "0f8c77b44ec46dd1f5aeedfc7343e26ca2ce64be",
    "url": "https://www.semanticscholar.org/paper/0f8c77b44ec46dd1f5aeedfc7343e26ca2ce64be",
    "title": "AI*IA 2007: Artificial Intelligence and Human-Oriented Computing, 10th Congress of the Italian Association for Artificial Intelligence, Rome, Italy, September 10-13, 2007, Proceedings",
    "abstract": null,
    "venue": "AI*IA",
    "citationCount": 40,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ]
  },
  {
    "paperId": "243d3163f8e70561714e70fe1d0707c70c5c188c",
    "url": "https://www.semanticscholar.org/paper/243d3163f8e70561714e70fe1d0707c70c5c188c",
    "title": "Machine Learning of Natural Language",
    "abstract": null,
    "venue": "Springer London",
    "citationCount": 26,
    "fieldsOfStudy": null
  },
  {
    "paperId": "d9a097e72ebeb00c5c2c1be9c4de00c117802c07",
    "url": "https://www.semanticscholar.org/paper/d9a097e72ebeb00c5c2c1be9c4de00c117802c07",
    "title": "Research \u2013 China",
    "abstract": "The contemporary Chinese terms for research are yanjiu ( research in the general sense) and keyan ( research in areas of science and engineering); the formation of these terms, however, was relatively recent and owed a great deal to the influence of the notions of research in the Western tradition. Perhaps the traditional Chinese terms that most closely resemble the English word research are kaoju ( ) or kaozheng ( ) (evidential research), which refer to the tradition of philological scholarship that reached its height in the 18th century (Qing dynasty). It took the form of scrupulous textual analysis on a range of classics \u2013 the originals of which were lost over time \u2013 in terms of their internal inconsistencies and factual errors attributable to scribes and printers, and its credibility rested on the mastery of a vast amount of information; its purpose was to restore the classics to their \u2018original\u2019 and \u2018authoritative\u2019 forms in which they were to be understood and respected. In paying homage to classics as the substance of a tradition, this scholarship reaffirmed the classics with a profound respect for \u2018timeless\u2019 truthfulness coupled with a deep sense of filial piety. In this form, kaoju was central to Chinese scholarly tradition for many centuries. In the late 19th century, the philological rigor inherent in this tradition was allied with reform; through kaoju, Kang Youwei, founder of the so-called New Text School, pronounced Confucius a \u2018reformer\u2019, and Liang Qichao, in his \u2018Outline of Qing Scholarship\u2019, saw in this tradition a development comparable to that of philology and textual criticism centered on the Greek and Roman antiquity in Renaissance Europe. The practice of kaoju emerged with very different intellectual assumptions from those found in the ontological inquiries of Plato and Aristotle. Instead of truth, form and idea, the primary goal in kaoju research was the establishment of authenticity, which served crucial functions in sustaining institutional, moral and aesthetic forms. Two types of theoretical formulations occupied the grounds of Platonic or Aristotelian metaphysical discourse: analogical abstractions that are capable of mapping the natural world with a certain degree of sophistication (such as Book of Changes), and ethical codes of conduct founded on the principles of \u2018hierarchy\u2019 and \u2018moderation\u2019 as paths to familial and social harmony (such as Confucian classics). Quite separate from metaphysical inquiries, \u2018useful knowledge\u2019 in the Chinese context would also constitute what we mean today by \u2018research\u2019; in this regard, there was an astonishing range of discoveries, inventions and theoretical formulations in mathematics, medicine, ship-building, navigation, agriculture, art, techniques of construction and so on. During the European Renaissance, Chinese technologies, such as printing, navigation and gun-powder, played unacknowledged but critical roles in the formation of notions of modernity. One of the most notable recent publicizers of these achievements is Joseph Needham, whose monumental Science and Civilization in China contributed a great deal to the understanding and dissemination of traditional Chinese research in the English language. Despite these achievements, the divide between the discourse of metaphysics and that of useful knowledge \u2013 the union of which European empiricism has always insisted \u2013 cultivated distinct forms of research results in China. The divide between metaphysics and use was marked in the traditional Chinese culture by universal binary constructs based on the idea of \u2018essence\u2019 and \u2018use\u2019, such as dao/qi ( ), ben/mo ( ) and ti/yong ( ). The fundamental importance of essence (dao, ben, ti) was often cited at crucial moments when useful knowledge (and outside influences) threatened to distablize established institutional and moral foundations. These notions of research, like many other aspects of the Chinese culture, have undergone tremendous changes since the late 19th century, but they continue to exert an important force in the Chinese reformulation of research under the new global influences.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "3df477fc85b34f67f6b50ca08221ba7f16f2cffc",
    "url": "https://www.semanticscholar.org/paper/3df477fc85b34f67f6b50ca08221ba7f16f2cffc",
    "title": "Query expansion based on a semantic graph model",
    "abstract": "Query expansion is a classical topic in the field of information retrieval, which is proposed to bridge the gap between searchers' information intents and their queries. Previous researches usually expand queries based on document collections, or some external resources such as WordNet and Wikipedia [1, 2, 3, 4, 5]. However, it seems that independently using one of these resources has some defects, document collections lack semantic information of words, while WordNet and Wikipedia may not include domain-specific knowledge in certain document collection. Our work aims to combine these two kinds of resources to establish an expansion model which represents not only domain-specific information but also semantic information. In our preliminary experiments, we construct a two-layer word graph and use Random-Walk algorithm to calculate the weights of each term in pseudo-relevance feedback documents, then select the highest weighted term to expand original query. The first layer of the word graph contains terms in related documents, while the second layer contains semantic senses corresponding to these terms. These terms and semantic senses are treated as vertices of the graph and connected with each other by all possible relationships, such as mutual information and semantic similarities. We utilized mutual information, semantic similarity and uniform distribution as the weight of term-term relation, sense-sense relation and word-sense relation respectively. Though these experiments show that our expansion outperform original queries, we are troubled with some difficult problems. Given the framework of semantic graph model, we need more effort to find out an optimal graph to represent the relationships between terms and their semantic senses. We utilized a two-layer graph model in our preliminary research, where terms from different documents are treated equally. Maybe we can introduce the document as a third layer in future work, where we can differ the same terms in different documents according to document relevance and context. Then we need appropriately represent initial weights of this words, senses and relationships. Various measures for weights of terms and term relations have been proved effective in other information retrieval tasks, such as TFIDF, mutual information (MI), but there is little research on weights for semantic senses and their relations. For polysemous words, we add all of their semantic senses to the graph and assume that these senses are uniformly distributed. Actually, it is not precise for a word in a special document and query. As we know, a polysemous word may have only one or two senses in a document, and they are not uniformly distributed. Give a word, what we should do is to determine its word senses in a relevant document and estimate the distribution of these senses. Word sense disambiguation may help us in this problem. Then, there are many methods to compute word similarity according to WordNet, which we use to represent the weights of relationships between word senses. Varelas et al implemented some popular methods to compute semantic similarity by mapping terms to an ontology and examining their relationships in that ontology [4]. We also need to know which algorithm for semantic similarity is most suitable for our model. Additional, WordNet is suitable to calculate word similarity but not suitable to measure word relevance. The inner hyperlinks of Wikipedia could help us to calculate word relevance. We wish to find an effective way to combine the similarity measure from WordNet and relevance measure from Wikipedia, which may completely reflect word relationships.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1c5f881649b7cd7754d0945d5e4c358d9639c1dd",
    "url": "https://www.semanticscholar.org/paper/1c5f881649b7cd7754d0945d5e4c358d9639c1dd",
    "title": "Natural Language Processing and Information Systems, 12th International Conference on Applications of Natural Language to Information Systems, NLDB 2007, Paris, France, June 27-29, 2007, Proceedings",
    "abstract": null,
    "venue": "International Conference on Applications of Natural Language to Data Bases",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1b088cc71bbeb3a3d321c3a70451dee504f4a431",
    "url": "https://www.semanticscholar.org/paper/1b088cc71bbeb3a3d321c3a70451dee504f4a431",
    "title": "Unsupervised Clustering of Morphologically Related Chinese Words",
    "abstract": "Unsupervised Clustering of Morphologically Related Chinese Words Chia-Ling Lee (r00922072@ntu.edu.tw) Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan Ya-Ning Chang (yaningchang@gate.sinica.edu.tw) Institute of Linguistics, Academia Sinica, Taipei, Taiwan Chao-Lin Liu (chaolin@nccu.edu.tw) Department of Computer Science, National Chengchi University, Taipei, Taiwan Chia-Ying Lee (chiaying@gate.sinica.edu.tw) Institute of Linguistics, Academia Sinica, Taipei, Taiwan Jane Yung-jen Hsu (yjhsu@csie.ntu.edu.tw) Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan Abstract Many linguists consider morphological awareness a major factor that affects children\u2019s reading development. A Chi- nese character embedded in different compound words may carry related but different meanings. For example, \u201c\u5546 \u5e97(store)\u201d, \u201c\u5546\u54c1(commodity)\u201d, \u201c\u5546\u4ee3(Shang Dynasty)\u201d, and \u201c\u5546\u671d(Shang Dynasty)\u201d can form two clusters: {\u201c\u5546\u5e97\u201d, \u201c\u5546 \u54c1\u201d} and {\u201c\u5546\u4ee3\u201d, \u201c\u5546\u671d\u201d}. In this paper, we aim at unsuper- vised clustering of a given family of morphologically related Chinese words. Successfully differentiating these words can contribute to both computer assisted Chinese learning and nat- ural language understanding. In Experiment 1, we employed linguistic factors at the word, syntactic, semantic, and contex- tual levels in aggregated computational linguistics methods to handle the clustering task. In Experiment 2, we recruited adults and children to perform the clustering task. Experimental re- sults indicate that our computational model achieved the same level of performance as children. Keywords: morphological awareness; human cognition; com- putational linguistics; Chinese character meaning Introduction Morphological awareness, defined as \u201cchildren\u2019s conscious awareness of the morphemic structure of words and their abil- ity to reflect on and manipulate that structure\u201d, is associated with children\u2019s reading ability and comprehension (Liu & McBride-Chang, 2010; Kirby et al., 2012; Ku & Anderson, 2003). It is thought by many linguists to strongly affect read- ing development in children (Liu & McBride-Chang, 2010). A Chinese character embedded in different compound words may carry related but different meanings. For exam- ple, the meaning of the character \u201c\u5546/shang1/\u201d in words\u201c\u5546 \u5e97(store)\u201d and \u201c\u5546\u54c1(commodity)\u201d is commerce. In contrast, in \u201c\u5546\u4ee3(Shang Dynasty)\u201d, \u201c\u5546\u201d refers to a Chinese dynasty. Successful clustering of related Chinese words would make a contribution to Chinese learning. In addition, differentiat- ing the character\u2019s meanings in such morphologically related words can facilitate Chinese word sense disambiguation and help improve Chinese word segmentation (Navigli, 2009). In this research, we employ natural language processing and computational linguistics techniques to differentiate the meanings of a particular character that is embedded in differ- ent Chinese words. We apply different methods which take diverse factors into account, such as grammar, syntax, seman- tics, and context. We also aggregate all methods and build a better ensemble model. Furthermore, we conduct another experiment in which we asked adults and children to do the same clustering task. Experimental results indicate that our model can achieve the same level of performance as children in the clustering task. There is previous work related to morphological aware- ness. Wang, Hsu, Tien, and Pomplun (2012) predicted raters\u2019 transparency judgments of Chinese morphological character based on latent semantic analysis (LSA) (Landauer, Foltz, & Laham, 1998). If a word is more similar to the primary mean- ing, it is more likely to be judged as semantically transparent, and opaque otherwise. Galmar and Chen (2010) tried to identify different mean- ings of a Chinese character using LSA and semantic pat- tern matching in augmented minimum spanning tree. Galmar (2011) built a term-by-document matrix, and used the batch version of self-organizing maps (Kohonen, 2001) to visualize the interplay between morphology and semantics in Chinese words. To discriminate Chinese character meanings, in addi- tion to LSA techniques, we consider diverse information from comprehensive aspects. There are numerous word-to- word semantic similarity or relatedness measures proposed in the past. In knowledge-based approaches, WordNet 1 was 1 http://wordnet.princeton.edu",
    "venue": "CogSci",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science",
      "Psychology"
    ]
  },
  {
    "paperId": "e050b06b2e4937b516d630043cb68b8bc77078df",
    "url": "https://www.semanticscholar.org/paper/e050b06b2e4937b516d630043cb68b8bc77078df",
    "title": "Advances in Natural Language Processing: 4th International Conference, EsTAL 2004, Alicante, Spain, October 20-22, 2004. Proceedings",
    "abstract": null,
    "venue": "Lecture Notes in Computer Science",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "240c0fb45f387208e8dac4b17649260c11e0818a",
    "url": "https://www.semanticscholar.org/paper/240c0fb45f387208e8dac4b17649260c11e0818a",
    "title": "A hybrid method to categorize HTML documents",
    "abstract": "In this paper we introduce a hybrid method for classifying HTML documents. In this method the statistical, semantic and writing style features of text are used to categorize documents. Categorization can be done in both supervised and unsupervised modes and categories may be predefined or be created dynamically (clustering). The classification system exploits an ontology of interesting topics. The ontology which contains categories and their hierarchical relations can be updated automatically during the system\u2019s lifetime. Newly defined categories can be added to the ontology and existing categories can be changed according to the documents received. The statistical part of the method is based on the Rocchio algorithm. The algorithm has been changed to cover the special conditions for dynamic category building, for categorizing with and without training data and for variable length feature vectors. The semantic part of the algorithm exploits Wordnet to substitute words with their corresponding concepts and does some word sense disambiguation tasks prior to clustering. This way documents will be clustered according to their concepts instead of words. The other part of the method considers writing style features of text such as writing in bold/italic style, writing with different (bigger) fonts or occurring words and concepts in special places of the document, such as the title, headers or hyperlinks. In this paper, after a brief overview on existing methods of document classification, the proposed method will be discussed and some experimental results of classifying documents will be shown. Experiments show that the hybrid method results in some improvements in performance (the accuracy).",
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": null
  },
  {
    "paperId": "e2909e97e05c657b0690098753ee5ec9e8ddd294",
    "url": "https://www.semanticscholar.org/paper/e2909e97e05c657b0690098753ee5ec9e8ddd294",
    "title": "Web Page Summarization by using Concept Hierarchies",
    "abstract": "To address the problem of information overload and to make effective use of information contained on the Web, we created a summarization system that can abstract key concepts and can extract key sentences to summarize text documents including Web pages. Our proposed system is the first summarization system that uses a knowledge base to generate new abstract concepts to summarize documents. To generate abstract concepts, our system first maps words contained in a document to concepts contained in the knowledge base called ResearchCyc, which organized concepts into hierarchies forming an ontology in the domain of human consensus reality. Then, it increases the weights of the mapped concepts to determine the importance, and propagates the weights upward in the concept hierarchies, which provides a method for generalization. To extract key sentences, our system weights each sentence in the document based on the concept weights associated with the sentence, and extracts the sentences with some of the highest weights to summarize the document. Moreover, we created a word sense disambiguation method based on the concept hierarchies to select the most appropriate concepts. Test results show that our approach is viable and applicable for knowledge discovery and semantic Web.",
    "venue": "ICAART",
    "citationCount": 2,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "be415f55bb624e4846cdec6137afdc26420537a2",
    "url": "https://www.semanticscholar.org/paper/be415f55bb624e4846cdec6137afdc26420537a2",
    "title": "Computational Linguistics and Intelligent Text Processing, 10th International Conference, CICLing 2009, Mexico City, Mexico, March 1-7, 2009. Proceedings",
    "abstract": null,
    "venue": "CICLing",
    "citationCount": 3,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1fed7d9203ccd0487e8b42cf3bec37c16dc74d0b",
    "url": "https://www.semanticscholar.org/paper/1fed7d9203ccd0487e8b42cf3bec37c16dc74d0b",
    "title": "R\u00e9sum\u00e9 automatique des commentaires de consommateurs",
    "abstract": "In this thesis, we present our work in the domain of opinion mining, in which we focus on automatic processing of customer reviews. We propose the architecture of an application that summarises automatically a collection of customer reviews based on a query using an ontology as its main resource. A prototype of this application is described and its functioning is explained using an example. This elucidates the need to summarise customer reviews and the real potential of our architecture to do so. The ontology, which describes products commented in the reviews as well as other relevant entities, was created semi-automatically. We present a novel method of automatic product feature extraction that uses a terminology extractor which requires only superficial syntactic analysis of the text. Evaluating this method on our corpus of reviews of electronic products (in English) gave a good precision of about 80%. This accuracy cannot be directly compared to those of previous methods because of differences in the task definition. We then present our attempts to group the extracted product features of which there were between 400 and 800 for the products in our corpus of electronics reviews. We found that co-occurrences of features are not useful in the search of semantic relations between the features. Possible groupings found using WordNet-based measures of semantic similarity look more promising. Experiments with this method included the design and evaluation of a domain-specific word sense disambiguation algorithm. The accuracy obtained was about 53%, which is comparable to that of other methods of domain-specific word sense disambiguation. To study the possibility of basing our summaries on natural language questions instead of queries, we also worked on the classification of questions in terms of their answer type. We present out experiments with classifying factual questions using various machine learning algorithms with n-grams as features. The best accuracies obtained were 80% for questions both in French and English; they were obtained using AdaBoost with decision trees as base learners.",
    "venue": "",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "c8257c6639b23080f1d94ea744ef88a9ae914c52",
    "url": "https://www.semanticscholar.org/paper/c8257c6639b23080f1d94ea744ef88a9ae914c52",
    "title": "Disease as an example of non-Aristotelian categories",
    "abstract": "This paper discusses the problem of definition of disease in formal ontologies. We argue that different conditions usually regarded as disease have no common exclusive intrinsic property, due to their ontologically heterogeneous nature. We maintain that being a disease depends on social circumstances. From a purely ontological view it is hard to accept. Still our proposed model can be used in formal systems and has the benefit to point to some aspects that are absolutely relevant for disease control and prevention. What is non-Aristotelian? The principles of defining categories (perhaps even the word 'category' as well) was first described by Aristotle. His way of definition was based on the following principles of logic: All statements must be either false or true No statement can be false and true at the same time and under the same circumstances. There is nothing in between true and false. An Aristotelian category is defined by two statements: one specifies the 'genus proximum' (an IsA statement) and the other a 'differentia specifica'. This way of definition is still valid today and widely used in ontology engineering. It is often referred to as intension. Another accepted way of definition is the extension, when we give an exhaustive list of things (or subcategories) that constitute a category. We have to stress, that via extension it is possible to define any set (in a mathematical sense) but such an exhaustive list forms a category or entity only if the listed things have a common exclusive property (a proposition that is true for all and only for the listed things). This common exclusive property corresponds to the 'differentia specifica', so extensions and intensions are alternative ways to achieve the same. Contemporary literature mentions fuzzy logic and fuzzy categories as non-Aristotelian [1]. Indeed in fuzzy logic there is a continuum of truth values between 0 and 1, so fuzzy logic violates the third law listed above. For instance the statement \"I am a talented man\" might have a truth value of something like 0.2 or 0.87, depending on how talented I am. \"Talented man\" is therefore a fuzzy concept. But in case this value is 0.2 for me than the question can be reformulated: is it true that my membership function value in the set \"talented man\" is 0.2? This question can be answered by 'Yes' or 'No'. Doing so, we can still satisfy all criteria of Aristotelian logic. So what I mean by 'non-Aristotelian' is something different. We demonstrated in a previous paper [2] that there are many biomedical categories that neither theoretically nor practically can be defined in terms of an Aristotelian definition. This is not because of their fuzzy nature, but due to the fact that things constituting the category apparently lack any common exclusive property. Notion of 'Disease' seems to be a typical example. E.g. some diseases are painful, but not all of them, while some painful conditions are not diseases, etc. Such categories can not be defined in Aristotelian sense either by using the two-valued or fuzzy logic. Defining 'disease' by extension would mean to create an exhaustive list of diseases, including all extinct ones and those that will occur in the future only. It is unnecessary to emphasise the hopelessness of such an approach. We may wonder, how humans are still can agree for the most part what constitutes a disease and what not. What is a 'Disease'? The main point in understanding what is a 'disease' lies in the sentence above: \"for the most part\". This means that we do not agree always. So there are some conditions considered as disease by certain people but not by others. The level of agreement depends on the similarity of the social/cultural background of people. L King stressed that classifying something as disease depends on social situations [3]. Homosexuality is a good example: depending on the culture of the society it can be regarded as sin, disease, or normal variation. This corresponds how a society reacts to homosexuality: by punishment, by treatment or by tolerance (no reaction). Whenever a condition is regarded as disease in a given society it provokes some specific (health) reaction against itself. The actual reaction can be different depending again on the culture and the development of the given society, but basically it can have three forms: 1) avoid situations where the disease may occur ('prevention'); 2) help somehow the patient to get rid of the disease ('treatment') or at least to learn how to live with it ('rehabilitation'); 3) exclude the patient from the community in order to avoid making more harm to population ('isolation'). The latter happened e.g. with people suffering from leprosy in ancient time, or with many psychiatric patients up to the modern age. Obviously some conditions are regarded as disease in every known and conceivable society. E.g. appendicitis, nose-bleeding, fracture of extremities, and cholera probably were considered as diseases in every culture, even in lack of recognition of their pathological nature or cause. But the definition of disease can not be restricted to those conditions. Serious diseases, e.g. epilepsy, chondrodystrophy would be excluded. Epilepsy, the 'morbus sacer' (sacred disease) could be seen as a transcendental phenomenon in certain cultures, while condrodystophic patients due to their short extremities and relatively oversized head were beloved court clowns of the middle age. We do not state however that all instances of all diseases necessarily provoke a health activity actually. Many of disease occurrences might remain unrecognised or ignorantly tolerated. What we state is that any disease has this potential or function. As all screw drivers have the ability to fix or loose screws even if some of them may be never used for that. So we rephrase our definition as follows: Anything is a disease that in a given society has the potential to provoke some specific (health) action against itself. Disease as hybrid class But what kinds of things have the ability to provoke a health action? Let us investigate the ontological nature of those things. Here another problem arises, that explains why many attempts failed to formally define disease. Top level ontologies distinguish endurants and perdurants (often the names occurrent/continuant or snap/span entities are used respectively). Whatever names are used, it concerns disjoint categories: Endurants exist entirely at any time they exist; while perdurants have temporal parts. It is very clear that nothing can be both endurant and perdurant. However, taking the definition of disease presented above, we realise that nothing forbids both endurants and perdurants to have the ability to provoke health actions. We do not say that a certain given disease (or any actual instance of it) could be endurant and perdurant as well. What we state is that disease is a category that may comprise both endurants and perdurants. That is why we call this category heterogeneous: it is a union of mutually disjoint subcategories. If we consider different diseases in detail we might see, that in most of the cases there is some pathological process (or there is a pathological alteration of or lack of some physiological process) that leads to some morphological alterations. Processes are perdurants, whereas morphological phenomena are endurants. Again looking at the details we may realise that a pathological process is a consequence of a structural state that is again a consequence of another process. So, most of the diseases are rather complex compositions of processes and morphological alterations at different organisational levels (molecular, cellular, organ level, etc.). E.g. a cancer (as a pathological mass in the body) is an endurant resulting from some pathological cell division process that is a perdurant. But this process probably is a result of some abnormal macromolecular structures that are endurants, and so on. Even if we do not go into the details we realize, that in case of many diseases the presence of a structural condition dominates the clinical picture (e.g. arterial aneurisms, fractures and ruptures, various cysts etc.) while in other cases (e.g. consumptive coagulopathy, autoimmune disorders) are process-dominated. Even if we know that all these conditions are complexes of processes and structural states, it is worthwhile to set up two categories for morphology and process dominated diseases. The former is subsumed by endurant the latter by perdurant. Then 'Disease' (as such) could be defined as the union of these two, and the ability to provoke health action should be added as a necessary condition for disease. Alignment to top level ontologies In order to maintain consistency and compatibility, it is often recommended that domain ontologies should be aligned to some top level ontology. Testing two frequently referenced top level ontologies, DOLCE (Descriptive Ontology for Linguistic and Cognitive Engineering created by Laboratory of Applied Ontology, CNR Rome, see http://www.loa-cnr.it/DOLCE.html) and BFO (Basic Formal Ontology created by IFOMIS see http://www.ifomis.uni-saarland.de/bfo/) we found some conflicts between them. In both top level ontologies processes are classified as perdurants (named occurents in BFO) Structural conditions in BFO are considered as \"quality\" (actually 'shape of a nose' is a given example of quality in BFO; think of nasal deformity as a good example of structurally defined diseases). In case of DOLCE quality is one of the top level categories, neither endurant nor perdurant. Structural conditions can be classified in DOCLE also as 'features' that are dependent essential wholes, classified as physical endurants, e.g. holes in a cheese (or similarly a cyst in the kidney), etc. Consequently, our notion of disease should be represented in BFO as a union of some processes (perdurants) and some qualities (endurants), while in case of DOLCE as union of some processes (perd",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "822c639628e4b48c28eb59da14b45df670966a99",
    "url": "https://www.semanticscholar.org/paper/822c639628e4b48c28eb59da14b45df670966a99",
    "title": "Natural language processing and information systems : 9th International Conference on Applications of Natural Language to Information Systems, NLDB 2004, Salford, UK, June 23-25, 2004 : proceedings",
    "abstract": "Regular Papers.- A Natural Language Model and a System for Managing TV-Anytime Information from Mobile Devices.- State- and Object Oriented Specification of Interactive VoiceXML Information Services.- Interpreting Semi-formal Utterances in Dialogs about Mathematical Proofs.- Event Ordering Using TERSEO System.- The Role of User Profiles in Context-Aware Query Processing for the Semantic Web.- Deriving FrameNet Representations: Towards Meaning-Oriented Question Answering.- Lightweight Natural Language Database Interfaces.- Ontology-Driven Question Answering in AquaLog.- Schema-Based Natural Language Semantic Mapping.- Avaya Interactive Dashboard (AID): An Interactive Tool for Mining the Avaya Problem Ticket Database.- Information Modeling: The Process and the Required Competencies of Its Participants.- Experimenting with Linguistic Tools for Conceptual Modelling: Quality of the Models and Critical Features.- Language Resources and Tools for Supporting the System Engineering Process.- A Linguistics-Based Approach for Use Case Driven Analysis Using Goal and Scenario Authoring.- Effectiveness of Index Expressions.- Concept Similarity Measures the Understanding Between Two Agents.- Concept Indexing for Automated Text Categorization.- Acquiring Selectional Preferences from Untagged Text for Prepositional Phrase Attachment Disambiguation.- Semantic Enrichment for Ontology Mapping.- Testing Word Similarity: Language Independent Approach with Examples from Romance.- Language Modeling for Effective Construction of Domain Specific Thesauri.- Populating a Database from Parallel Texts Using Ontology-Based Information Extraction.- A Generic Coordination Model for Pervasive Computing Based on Semantic Web Languages.- Improving Web Searching Using Descriptive Graphs.- An Unsupervised WSD Algorithm for a NLP System.- Enhanced Email Classification Based on Feature Space Enriching.- Synonymous Paraphrasing Using WordNet and Internet.- Automatic Report Generation from Ontologies: The MIAKT Approach.- A Flexible Workbench for Document Analysis and Text Mining.- Short Papers.- Towards Linguistic Foundations of Content Management.- Constructing Natural Knowledge Ontologies to Implement Semantic Organizational Memory.- Improving the Naming Process for Web Site Reverse Engineering.- On Embedding Machine-Processable Semantics into Documents.- Using IR Techniques to Improve Automated Text Classification.- Architecture of a Medical Information Extraction System.- Improving Information Retrieval in MEDLINE by Modulating MeSH Term Weights.- Identification of Composite Named Entities in a Spanish Textual Database.- ORAKEL: A Natural Language Interface to an F-Logic Knowledge Base.- Accessing an Information System by Chatting.- Ontology-Based Question Answering in a Federation of University Sites: The MOSES Case Study.- Semantic Tagging and Chunk-Parsing in Dynamic Modeling.- Semantic Filtering of Textual Requirements Descriptions.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "58df7c3280f9f7bd9d6fc751301f30a51ecf4037",
    "url": "https://www.semanticscholar.org/paper/58df7c3280f9f7bd9d6fc751301f30a51ecf4037",
    "title": "Computer Processing of Oriental Languages. Beyond the Orient: The Research Challenges Ahead, 21st International Conference, ICCPOL 2006, Singapore, December 17-19, 2006, Proceedings",
    "abstract": null,
    "venue": "International Conference on the Computer Processing of Oriental Languages",
    "citationCount": 4,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "081271cec2d40e1f379a4f30aa7b2e54a56c668e",
    "url": "https://www.semanticscholar.org/paper/081271cec2d40e1f379a4f30aa7b2e54a56c668e",
    "title": "Common Ground : For a Sociology of Code",
    "abstract": "Software development is a highly technical, but also exceedingly collective, process. A thorough understanding of this process might, therefore, benefit from a sociological analysis of actors, artifacts, and activities. This was the premise of a set of studies that we conducted on development processes of Free/Open Source Software (F/OSS). In this narrative essay, we provide a reflective account of those studies, focusing on the various ways in which social theory contributed to the inquiry. In particular, we illustrate how a range of ideas, theories, and insights from sociologies of art, science, and economics helped us, respectively, make sense of our vast amount of data, develop novel conceptualizations of software artifacts, and understand the character of collective activities involved in F/OSS. Based on these reflections, we draw lessons on how these ideas can be expanded to other computational artifacts and activities. Prelude \u201cThey don\u2019t listen,\u201d bemoaned a computer science colleague, who felt that his ideas went unnoticed, unappreciated, and simply ignored by the geographers who attended the two-day NSF workshop. Ironically, he is a computer scientist who works on social policy, and the geographers are Geographical Information Systems (GIS) experts with a knack for understanding technology and building computer models \u2014 perhaps why they all both invited to the same workshop. The workshop aimed to bridge knowledge across several quite different communities \u2013 GIS modelers and social scientists, geographers and computer scientists, academics and businesspeople, Europeans and North Americans, and so forth. With this context in mind, I expressed some sympathy with my colleague\u2019s lament, I responded, \u201cYou\u2019re right,\" but added, \"and neither do we.\u201d In a sense, this essay is a reflection on such exchanges. In it, we report on what can happen if and when computer scientists, IS researchers, and IT professionals \u2014 we \u2014 converse deeply with social scientists and take social theory seriously. We write about important gaps we found in techno-centric accounts of hard technology problems, specifically large software systems: how they come about, and how they are managed over long periods of time. We expose our routes to discovering, adopting, integrating, and eventually extending these social theories as rich and, we think, essential underpinnings, for understanding these complex technical artifacts, complementary to purely technical theory and practice. Over decades, computer science has developed a particular disciplinary understanding of its core concepts, which in the realm of software system development (the subject matter of the present inquiry) include notions such as \u201csoftware,\u201d \u201crequirement,\u201d \u201cdesign,\u201d \u201ctest\u201d \u201cmodule,\u201d \u201cspecification,\u201d \u201cbug,\u201d \u201cpatch,\u201d and so on. As constructs, concepts like these manifest a high degree of persistence, having been sustained and linked together over years of mainly unquestioned application in the ontological machinery of the computing world. As Smith (1996) has argued, however, these concepts and theories do not do justice to computation in the wild: \u201cthat eruptive body of practices, techniques, networks, machines, and behavior that has so palpably revolutionized late-twentieth-century life\u201d (and after!) In this essay, we want to demonstrate how social theory has helped us develop concepts and understandings that may do greater justice to computational practice. By \"do greater justice\" here we mean that in the end we expect to provide added structure to several observable, examined, data-founded paradoxes central to some existing conceptual machinery of the theory and practice of large-scale systems development. We will try to place a more solid social theory-derived armature inside some floating islands of abstraction circulating commonly in the computing world. By exploring the uncharted territory where computing and social theory interact, we hope to map out a wider area of common ground. An Engineering Question Sticks its Head Out Computer software development is a multi-billion-dollar industry that is central to both the basic functioning and the international economy of the modern world. Yet software development is a tedious, costly, and complex process. Surprisingly, as a whole, current development practice still results in expensive, untimely, and unreliable products. For nearly half a century, software engineers have been trying to improve these circumstances by developing tools, techniques, models, and procedures that streamline development processes, optimize resources, and reduce expenses. Compared to the magnitude of these improvement efforts, however, the relative gains have been minimal. According to several sources of statistics circa 2006, roughly 25% of software projects run over budget, another 25% do not finish on time, and almost 45% get cancelled altogether. This leaves us with a basic question: Why does software development is so inefficient, expensive, and unreliable? A Sociological Question Emerges Beyond the cognitive, skill-based, and tool-supported practices of individual designers and programmers, software development is principally a collective activity. Software systems both large and small are designed, built, used, and maintained in a distributed way by groups and communities involving small numbers to thousands of contributors and tens of millions of users. Nowhere has this become more visible than through the rapid rise of \"free/open-source software\" (F/OSS) development communities. F/OSS projects range from simple programs (single function, hundreds of lines of code, rarely changing) to the extremely complex suites whose components change constantly and asynchronously (full operating systems with suites of affiliated applications and system libraries; complete office-processing, web-management/browsing, and program-development tool suites; high-performance analysis tools for physics, astronomy, and biology; enterprise resource management (ERP) systems; and more). These efforts represent active distributed collectives of designers, developers, maintainers, and \"users\", both organizational and individual. As one indication of some of the social complexity involved, the New York Times recently reported that the Mozilla Firefox web browser project has 75 to 100 million users, while \"Some 1,000 to 2,000 people have contributed code to Firefox, according to the Mozilla Foundation. An estimated 10,000 people act as testers for the program, and an estimated 80,000 help spread the word. Similarly, by 2007, some 1,682 people have reported bugs against the Fedora Core 6 Linux operating system project. These observations suggest that one big part of the answer to the engineering question posed above may indeed lie in the social organization of software development. This point is taken quite seriously by many F/OSS proponents. Based on credos and principles distinct from those of the commercial software, F/OSS proponents attribute its success to the social processes of software development that allow the production of artifacts which are purportedly faster, cheaper, and more reliable. In regards to reliability, for instance, the proponents of F/OSS claim that the active participation of multitudes of volunteers brings about a high degree of quality assurance that is not attainable in commercial software. This claim is best expressed in what has come to be known as Linus\u2019s Theorem: \"Given enough eyeballs, all bugs are shallow.\" Whether we agree with claims like this or not, the F/OSS model has provided a viable alternative to the rival proprietary model by the sheer fact that it \u201cworks.\u201d In so doing, F/OSS shifts the original engineering question towards a sociological one: How can large distributed collectives of experts and nonexperts produce artifacts that are comparable in quality to those created by conventionally-organized teams of experts? Recognizing the sociological nature of this question, several years ago we began a study of the social processes of problem management in software developed by an open source community centered around what has now become the Mozilla Foundation. This software, the original \"Mozilla application suite,\" comprised the Mozilla web browser, an associated email/news reader, and a web page composer. Work on it was supported by several development tools, most notably for our studies the problem-reporting and tracking database tool called Bugzilla (see Figure 1).1 Since the 1980s large software projects have used a widening array of tools to ease the management of knowledge for software development and maintenance. Modern tools such as mailing lists, wikis, blogs, bug reporting/analysis databases, and workflow tools have been integrated into software processes in both F/OSS and commercial development practices for many years now. These systems serve as media for representing ideas, designing, discussing, planning, developing strategy, coordinating, fighting, storytelling, and humor. They embody fullfledged social and organizational environments. Our goal in studying Bugzilla was to understand the social processes involved in fixing bugs and, more generally, in the development of software. We were working with hypotheses that linked variation in social processes (such as seeking/providing information, negotiating, resolving conflicts, establishing priorities, selecting lines of attack on problems, and making evaluations) to variation in outcomes (such as time to repair problems, quality of problem solutions, long-term robustness of repairs, and participant reputations). As we worked toward a clear, empiricallygrounded model of social processes in software maintenance, our first task was naturally to examine the Bugzilla data to find instances of bugs and of social processes, map their associations, and establish correspondences, and find correlations. But how? The \u201cdata\u201d involved here consists of hundreds of thousands of ",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "a3f24a875afa21e0af3a11ff74087ba44bb8555c",
    "url": "https://www.semanticscholar.org/paper/a3f24a875afa21e0af3a11ff74087ba44bb8555c",
    "title": "Inducing Terminology for Lexical Acquisition",
    "abstract": "Few attention has been paid to terminology extraction for what concerns the possibilities it offers to corpus linguistics and lexical acquisition. The problem of detecting terms in textual corpora has been approached in a complex framework. Terminology is seen as the acquisition of domain specific knowledge (i.e. semantic features, selectional restrictions) for complex terms and /or unknown words. This has useful implications on more complex text processing tasks (e.g. information extraction). An hybrid symbolic and probabilistic approach to terminology extraction has been defined. The proposed inductive method puts a specific attention to the linguistic description of what terms are as well as to the statistical characterization of terms as complex units of information typical of domain sub-. languages. Experimental evidence of the proposed method are discussed. 1 I n t r o d u c t i o n Nowadays corpus processing techniques are widely adopted to approach the well-known lexical bottleneck problems in language engineering. Lexical acquisition methods rely on collocational analysis (pure statistics), robust parsing (syntax-driven acquisition) or semantic annotations as they are found in large thesaura or on-line dictionaries. The lexical information that trigger induction varies from simple word/tokens to syntactically annotated or semantically typed collocations (e.g. power fu l vs. s trong tea (Smadj a, 1989)), syntactic disambiguation rules (e.g. (Hindle and Rooths,1993), (Brill and Resnik,1994)) or sense disalnbiguation rules are usually derived. Such information is lexical as it, encodes constraints (of different types) at the word level, to be thus inherited by morphologic variants of a given lemma. This strongly lexicalized knowledge, as it is extracted from corpus data, requires lexical entries to be known in advance in some morphologic database. POS taggers or temmatizers are generally used to suitably map tokens to lemmas. It should be noted that lemmas in a corpus depends on the underlying sublanguage and their nature and shape is not as general as it is usually encoded in a morphologic dictionary. As an example, let s tudio (i.e. s tudy as a noun) be an entry in an italian morphologic dictionary. Typical information in such a database is the following: studio pos=noun gen=mas aura=sing Tlle only legal morphologic variant of ,studio is studi (studies, with nura=plur). When searching for s tudio in a corpus of environment related texts 1, we found this kind of occurrences (e.g. short contexts): ... studi di base .... (basic studies) ... studi di impatto ambientale .... (*studies on the environmental impact) ... studi di fa t t ib i l i td .... (feasibility studies), ... studi di ri ferimento .... (reference studies) It is very common in a corpus (not balanced, thus focused to a limited domain) to find a set of specifications of nouns that have some specific properties: \u2022 they are not always compositional (e.g. s tudio",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "citationCount": 20,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "608f9006cfd3416d17a06966f244a023a296bc29",
    "url": "https://www.semanticscholar.org/paper/608f9006cfd3416d17a06966f244a023a296bc29",
    "title": "Advances in Knowledge Discovery and Data Mining, 9th Pacific-Asia Conference, PAKDD 2005, Hanoi, Vietnam, May 18-20, 2005, Proceedings",
    "abstract": null,
    "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1516129460ead6efa13d24cc518dbad918aaaffe",
    "url": "https://www.semanticscholar.org/paper/1516129460ead6efa13d24cc518dbad918aaaffe",
    "title": "Intelligent Information Access by Exploiting Lexical Knowledge and Machine Learning",
    "abstract": "Algorithms designed to support users in retrieving relevant information base their relevance computations on user profiles, in which representations of the users interests are maintained. This paper focuses on the use of supervised machine learning techniques to induce user profiles for Intelligent Information Access. The access must be personalized by profiles allowing users to retrieve information on the basis of conceptual content. To address this issue, we propose a method to learn sense-based user profiles based on WordNet, a lexical database. I. WORDNET-BASED DOCUMENT REPRESENTATION Personalization is an important method for digital libraries to take a more active role in dynamically tailoring its information and service offer to individuals. Novel solutions for personalized information access exploit machine learning algorithms to induce a structured model of a user\u2019s interests, referred to as user profile, from text documents. We propose a strategy to induce semantic user profiles in which keywords are replaced with their meanings, defined in lexicons or ontologies. We adopted WordNet [1] as a reference lexicon. As regards document representation, we propose a model called bag of synsets (BOS), in which the senses corresponding to the words in the documents are considered as features. The adopted sense repository is WordNet (version 1.7.1). In WordNet, nouns, verbs, adjectives and adverbs are organized into synsets (synonym sets), each representing one underlying lexical concept. The problem of determining which of the senses of an ambiguous word is invoked in a particular use of the word (word sense disambiguation, WSD) has to be solved [2]. We propose a WSD strategy based on the idea that semantic similarity between synsets a and b is inversely proportional to the distance between them in the WordNet IS-A hierarchy, measured by the number of nodes in the path from a to b. More details about the WSD procedure are reported in [3]. A document is mapped into a list of WordNet synsets according to the following three rules: a) each monosemous word w in a slot of a document d is mapped into the corresponding WordNet synset; b) for each pair of words \u3008noun, noun\u3009 or \u3008adjective, noun\u3009, a search in WordNet is made in order to verify if at least one synset exists for the bigram \u3008w1, w2\u3009. In the positive case, the WSD strategy is applied on the bigram, otherwise it is applied separately on w1 and w2; c) each polysemous unigram w is disambiguated using a set of words surrounding w as the context of w. The WSD strategy has been used to process documents in the EachMovie dataset, a collection made of 1,628 movie descriptions1. The BOS representation of a document consists of a list of synsets recognized from the words in the document. II. THE ITEM RECOMMENDER PROFILING SYSTEM ITem Recommender (ITR) is a content-based profiling system able to induce semantic user profiles as naive Bayesian classifiers by learning from documents represented according to the BOS model [4]. Given a set of documents in which each document is labeled by a specific user with a rating representing his degree of interest on that document, the system is able to learn the user profiles as a Bayesian text classifier able to classify a new document as interesting or uninteresting for that user. Naive Bayes is a probabilistic approach to inductive learning. The learned probabilistic model estimates the a posteriori probability, P (cj |di), of document di belonging to class cj . This estimation is based on the a priori probability, P (cj), i.e. the probability of observing a document in class cj , P (di|cj), that is the probability of observing document di given cj , and P (di), the probability of observing the instance di at all. Using these probabilities, Bayesian classifiers apply Bayes theorem to calculate P (cj |di). To classify a document di, the class with the highest probability is selected. As a working model for the naive Bayes classifier, we use the multinomial event model [5]: P (cj |di) = P (cj) \u220f",
    "venue": "IRCDL",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a1e11ba1aebbbe9aad62a3ac948c6d992dd75f54",
    "url": "https://www.semanticscholar.org/paper/a1e11ba1aebbbe9aad62a3ac948c6d992dd75f54",
    "title": "Getting at the Cognitive Complexity of Linguistic Metadata Annotation: A Pilot Study Using Eye-Tracking",
    "abstract": "Getting at the Cognitive Complexity of Linguistic Metadata Annotation \u2013 A Pilot Study Using Eye-Tracking Steffen Lohmann Dept. of Computer Science & Applied Cognitive Science Universit\u00a8at Duisburg-Essen Duisburg, Germany Katrin Tomanek Language & Information Engineering (JULIE) Lab Universit\u00a8at Jena Jena, Germany J\u00a8urgen Ziegler Dept. of Computer Science & Applied Cognitive Science Universit\u00a8at Duisburg-Essen Duisburg, Germany Udo Hahn Language & Information Engineering (JULIE) Lab Universit\u00a8at Jena Jena, Germany with each other, or with standard random sampling, in terms of annotation efficiency the AL community, up until now, as- sumed uniform annotation costs for each linguistic unit, e.g., words (Ringger et al., 2008 ; Settles, Craven, & Friedland, 2008 ; Arora, Nyberg, & Ros\u00b4e, 2009). This claim, however, has been shown to be invalid in several studies (Hachey, Alex, & Becker, 2005 ; Settles et al., 2008 ; Tomanek & Hahn, 2010). If uniformity does not hold and, hence, the number of annotated units does not indicate the true annotation efforts required for a specific sample, empirically more adequate cost models have to be developed. Accordingly, we here consider different classes of syntactic and semantic complexity that might affect the cognitive load during the annotation process, with the overall goal to find empirically more adequate vari- ables for cost modeling. Abstract We report on an experiment where the decision behavior of annotators issuing linguistic metadata is observed with an eye- tracking device. As experimental conditions we consider the role of textual context and linguistic complexity classes. Still preliminary in nature, our data suggests that semantic com- plexity is much harder to deal with than syntactic one, and that full-scale textual context is negligible for annotation, with the exception of semantic high-complexity cases. We claim that such observational data might lay the foundation for em- pirically grounded annotation cost models and the design of cognitively adequate annotation user interfaces. Keywords: Natural Language Metadata Annotation; Annota- tion Behavior; Eye-Tracking; Syntactic Complexity; Semantic Complexity; Cognitive Cost Modeling Introduction Supervised approaches to machine learning (ML) are cur- rently very popular in the natural language processing (NLP) community. While linguistic regularities are no longer hand- crafted by human experts in this paradigm, human interven- tion is still required to produce sufficient amounts of reliably annotated training material from which ML classifiers may learn or, considered as empirically valid ground truth, against which NLP systems can be evaluated. The assignment of linguistic metadata (e.g., related to parts of speech, syntactic parses, or semantic interpretations) to plain natural language corpus data, a process called annota- tion, is a complex cognitive task. It requires a sound compe- tence of the natural language in the corpus, as well as a decent level of domain and even text genre expertise. Meanwhile lots of annotated corpora have been built which contain these precious human judgments (e.g., PennTreeBank (Marcus, Santorini, & Marcinkiewicz, 1993), PennPropBank (Palmer, Gildea, & Kingsbury, 2005) or OntoNotes (Pradhan et al., 2007)). Almost all of these annotated corpora were assembled by collecting the documents to be annotated on a random sampling basis (once the original document set had been restricted thematically or chronologically). Only recently, more sophisticated approaches to select the annotation material are being investigated in the NLP com- munity. One of the most promising approaches is known as Active Learning (AL) (Cohn, Ghahramani, & Jordan, 1996 ; Tomanek, Wermter, & Hahn, 2007) where an intentional se- lection bias is enforced and only those linguistic samples are selected from the entire document collection which are con- sidered to be most informative to learn an effective classifica- tion model. When different approaches to AL are compared The complexity of linguistic utterances can be judged ei- ther by structural or by behavioral criteria. Structural com- plexity emerges, e.g., from the static topology of phrase structure trees and procedural graph traversals exploiting the topology of parse trees (see Szmrecs\u00b4anyi (2004) or Cheung et Kemper (1992) for a survey of metrics of this type). How- ever, structural complexity criteria do not translate directly into empirically justified cost measures. The behavioral approach accounts for this problem as it renders observational data of the annotators\u2019 eye movements. The technical vehicle to gather such data are eye-trackers which have already been used in psycholinguistics (Rayner, 1998). Eye-trackers were able to reveal, e.g., how subjects deal with ambiguities (Frazier & Rayner, 1987 ; Rayner, Cook, Juhas, & Frazier, 2006 ; Traxler & Frazier, 2008) or with sentences requiring re-analysis, so-called garden path sentences (Altmann, Garnham, & Dennis, 2007 ; Sturt, 2007). The rationale behind the use of eye-tracking devices for the observation of the annotation behavior is that the length of gaze durations and the behavioral patterns underlying gaze movements are considered to be indicative for the hardness of the linguistic analysis and the expenditures for the search of clarifying linguistic evidence (e.g., anchor words) to solve hard decision tasks such as phrasal attachments or word sense disambiguation. Gaze duration and search time are then taken as empirical correlates of processing complexity and, hence, unveil the real costs. We therefore consider eye-tracking as a promising means to get a better understanding of the nature of linguistic annotation processes with the ultimate goal of identifying predictive factors for annotation cost models.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f25d6d58408895a8b15df386c4489ab6feabdb70",
    "url": "https://www.semanticscholar.org/paper/f25d6d58408895a8b15df386c4489ab6feabdb70",
    "title": "Bridging the Gap Between Lexicon and Corpus: Convergence of Formalisms",
    "abstract": "I first consider the spectrum of lexical information from the semantic to the textual. A range of lexicons are classified according towhere they sit on this scale. Lexicographic tools and WSD programs are included in the classification, and this is justified. There is currently a lacuna between the most text-oriented of the lexicographic approaches, and the most sophisticated of the data-driven ones. Lexical tuning requires that the lacuna be filled, so corpus data can flow into the lexicon. Following an analysis of similarities and differences between lexicographic tools and data-driven approaches, a strategy for bridging the gap is proposed. 1 A Spectrum of Lexicons Lexicons look in two directions: towards the text, and towards semantics. When we look up a word or phrase encountered in a text to find the meaning, the textual orientation provides the input, the semantic one, the output. In a language generation system, the roles are reversed. In principle lexicons could be very highly developed in relation to both the textual and the semantic orientation. In practice, for computational lexicons in particular, the emphasis tends to be on one or the other. (The MicroKosmos project is interesting in this regard. It has two teams, one working on the text-oriented lexicon, the other on the ontology (Viegas and Nirenburg, 1995).) The work was undertaken under EPSRC grant GR/K/18931 Thus lexicons can be placed on a spectrum according to where the emphasis lies: how \u2018surfacey\u2019 they are, as in Fig. 1. At the semantic end lie AI ontologies. Traditional nativespeaker dictionaries such as the OED are somewhat closer to the text, and learner dictionaries, with their emphasis on grammatical and textual patterning, more so. (Since the advent of corpus lexicography in the early 1980s, the entire UK dictionary-publishing community has been steadily creeping in a text-ward direction, with the learners\u2019 dictionaries leading the way.) NLP lexicons which have been developed with parsing inmind, such asCOMLEX or the ANLT lexicon, are further along a textual direction. Further still down this road are \u2018lexicons\u2019 of patterns as used in Information Extraction. Then there is a gap; and then we move to data-driven artifacts like the decision lists Yarowsky uses for word sense disambiguation, Sch\u00fctze\u2019s sense clusters, and Grefenstette\u2019s thesauri (Yarowsky, 1995; Sch\u00fctze, 1998; Grefenstette, 1994). At the end point of the scale are sets of corpus citations for the word. Underlying the analysis is the thesis that the lexical entry for a word is an abstraction from the occurrences of the word in the language. A higher level of abstraction takes us further away from the linguistic data, towards the semantic end of the spectrum.1 2 Correlated distinctions There are several related but distinct dimensions along which lexicons can be analysed: 1Investigators who view lexicons as primarily approximations to mental lexicons might find this perspective odd: here lurks an issue regarding the primacy of linguistic as against psychological data in the study of the lexicon. For discussion see (Kilgarriff, 1992, section 1.4.1). AI Knowledge rep KL-ONE and similar COMLEX, ANLT (lexicons for parsing) CYC, MicroKosmos ontology MT & MicroKosmos lexicons (learners\u2019, bilingual) PENMAN Upper Model Wilks\u2019s Preference semantics",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6abb2a4869084e8283d1b870442677bdb210c1f0",
    "url": "https://www.semanticscholar.org/paper/6abb2a4869084e8283d1b870442677bdb210c1f0",
    "title": "International Standards for System Integration",
    "abstract": "Missing from the repertoire of mechanisms for systems integration presented at past INCOSE symposia are the International Standards for process component integration being developed in the context of industrial automation. To improve the efficiency of beneficially constructed interactions between systems and system components, the international community is adopting a wide range of standards through formal development and review processes. For this presentation, the focus is on the standards efforts for industrial automation conducted by ISO TC184 SC5 working groups. The work products range from shop floor communication structures through enterprise level system concept management \u2013 all with a process centric orientation. Of particular interest are the enablers of interoperable supply chain components and support for systems throughout their lifetime. IMPACT OF STANDARDS ON SYSYEMS ENGINEERING While many aspects of systems engineering can benefit from the use of International Standards, three benefits immediately come to mind. First, is the role these standards play in tool use. From electrical connectors to CAD files, standards support the creation of reusable parts throughout the product life-cycle. Second, standards generally codify existing practice reinforced by substantial research. And third, standards often represent the best practices a discipline has to offer. A standard emerges only after its subject matter has been carefully crafted to promote a practice that many agree works well in international commerce. INCOSE has participated in standards development but has played a more tentative role in the promotion of standards as a means to accomplish the objective of systems engineering. Knowledge of International Standards and their sphere of use will enable the benefits of use to accrue to the broader system engineering community. THE INTEROPERATION GOAL Are we there yet? The success of our industrial age and our emerging information age is critically dependent upon meaningful interactions among elemental system components. While human elements of behaviour will continue to serve central roles in strategic guidance, we are progressing toward systemic component and system interactions across layers of enterprise structure for which human mediation is no longer essential. The use of adopted International Standards enables the uniform selection of interaction mechanisms to drive more efficient and effective system performance. Central to TC184 SC5, and many other ISO subcommittees, is the effort to bring forward standardization that supports integration and interoperability in manufacturing enterprises. INCOSE member involvement with ISO 10303 AP233 is one such effort. However, (IDEAS 2003) reports that we are far from achieving the levels of interoperability among manufacturing systems and components that many believe are essential for significant improvement in manufacturing productivity. We continue the exchange of capital and labor to reduce cost and increase output per unit of expense, and we improve the communication channels that are now essential to production systems. However, as reported in (National 2001) our dynamic response to changes in strategy, tactics, and operational needs continues to be limited by the paucity of interoperability between systems, and between components within systems. The extent to which we are successful in providing useful component and system interaction is expressed in the current International Standards and de-facto industry standards that define information exchange. Having emerged from the automation of tasks and the adoption of information management as a key factor in modern manufacturing, the need for interoperability of the kind we seek is rather new. Reliance upon human mediated interoperation is no longer sufficient. Yet, enabling sophisticated adaptive component and system interoperation is proving to be very difficult. Interaction Levels. Systems and components thereof interact in different ways ranging along a continuum from isolated action to full interaction. Some interactions are the result of execution or resource dependencies. Others are simply consequential. To guide the development of international standards, we find it helpful to consider three kinds of system interaction. When all connections between components are direct, almost in a physical sense, we can say that the components of a system are unified. A model of this system is a unified model and model components have essentially the same conceptual representation although distinctions in levels of detail resulting from constituent separation or decomposition, and of properties emerging from aggregation or composition, remain. A user oriented view of a unified system is always consistent although it may be incomplete. Unified systems are the conceptual ideal most easily realized on a small scale, and perhaps only on a small scale. However, for the purpose of conceptual modeling, unified systems represented as subsystems or components are often the norm, e.g., a \u201cblack box\u201d. When connections between components and systems become indirect, i.e., when a transformation from one representational form or view to another occurs, and system behaviour results from specific knowledge about the means to transfer information, products, and processes, then we can say that the system is integrated. The models of this system, often with distinct conceptual representations, form an integrated system model wherein individual components interact using fixed representations known by other components a-priori. Integrated systems and components retain unique identity and interact through static messaging protocols. Because the models involved may have distinct representations, a consistent view of the system may not be possible. When connections between components and systems become malleable or ad-hoc in their manifestation, then system behaviour must move from static descriptions to incorporate dynamic features that enable interoperable interaction. In this case one component, or agent as it is often called, acts as if it were another component while maintaining its own distinct features. Interoperable systems, subsystems and components interact successfully because they are effective communicators and interpreters of system knowledge representation. These components can move beyond reaction to situational adaptation by using capabilities for context awareness and thus enhance opportunities for successful interaction. Systems integration is now the standard of practice and the area of interest to most practitioners. In fact, the vast majority of our international and de-facto standards effort to date target integration enablement. But interoperability, especially in a heterogeneous setting like a supply chain, goes beyond our methodologies for integration and offers new challenges for system and enterprise understanding. WG1 of SC5 is pursuing the codification of that understanding into new international standards. Along the way we find ourselves addressing the standards for process integration that must serve as the foundation for process interoperation. Ontology Issues. The three characterizations of interaction given above probably do not resonate with all readers. In addition, the terms system, subsystem and component are only loosely distinguished because the primary consideration is for the terms unified, integrated and interoperable. The emphasis is on the interaction rather than on the things interacting. However, some may feel that the characterization of interaction can only be articulated correctly when the nature of the things interacting is determined. Each community of practice evolves its own sense of appropriate terminology to describe its domain. Since standards, both international and defacto, are developed by working groups, each standard bears a perspective on word choice and meaning that represents an agreement among those approving adoption of the standard. And even then, we tend to allow wide latitude in word use. For example, (Kosanke 2004) reports on variation in the use of the term \u2018resource\u2019 that is commonly found in our manufacturing standards. Within TC184 SC5 some groups consider \u2018resource\u2019 to include anything consumed by manufacturing processes, e.g. electrical energy and lubricants, as well as the capital and human resources required to conduct those processes. Other groups, like our WG1, restrict \u2018resource\u2019 to non-consumables, e.g., machines and physical facilities. Some individual participants advocate including processes as a deployable resource. All are valid uses of the term but one must be aware of the usage context. WG1 does not ignore consumables; it just considers them to be a part of the resource rather than distinct from it. Such consideration is consistent with the granularity of its modelling and architectural context. To be interoperable, components and systems must correctly interpret words and other symbols used as labels and data in an appropriate context. With (ISO10303-1 1994), SC4 of ISO TC184 made significant strides in correct interpretation of product descriptions in the standard more commonly referred to as STEP (STandard for Exchange of Product model data). This standard, a data centric approach to enable integration, is a precursor to process interoperability in its restricted, albeit expanding, domain context. The (ISO18629-1 2004) standard described below uses an ontological approach to bridge the data \u2013 process gap with formal rigor. Unfortunately, as the subject matter of standards becomes less concrete, i.e., further removed from physical phenomena, consistency in interpretation becomes more difficult. Language, idiom, and culture often have profound impact on concept expression and interpretation. While resolving this aspect of interoperability is beyond the charge of SC5 WG1, we are constantly reminded of its importance to our efforts. One mech",
    "venue": "",
    "citationCount": 8,
    "fieldsOfStudy": null
  },
  {
    "paperId": "2792f3a8058c7a019b2a18fb3b7e43c828086b81",
    "url": "https://www.semanticscholar.org/paper/2792f3a8058c7a019b2a18fb3b7e43c828086b81",
    "title": "Deep Learning in Natural Language Processing",
    "abstract": null,
    "venue": "Springer Singapore",
    "citationCount": 75,
    "fieldsOfStudy": null
  },
  {
    "paperId": "522c99183c1dde215db8a12d281e58c399db4939",
    "url": "https://www.semanticscholar.org/paper/522c99183c1dde215db8a12d281e58c399db4939",
    "title": "HIR: A Hybrid IR Ranking Model",
    "abstract": "The aim of information retrieval (IR) process is to respond effectively to user queries by retrieving information that are better meets their expectations. A gap could exist between user information needs and his/her defined context as the level of the user's expertise in the search domain is directly influencing the query richness. The information retrieval system (IRS) must be intelligent enough to identify information needs and respond effectively to meet the expected needs, regardless the level of user's expertise level. This process is difficult and it remains a major and open challenge in the domain of IR. IR models integrate many sources to achieve an effective retrieval system. Semantic IR is an environment in which semantic techniques were applied to sort the documents according to their degree of relevance to the query. The present work proposes a hybrid model to rank documents. The proposed model is based on a query likelihood language model and the semantic similarity between concepts to assess the relevance between query-document pairs. Concepts were extracted by projection on WordNet ontology then word sense disambiguation was conducted. A semantic index was built to validate the proposed model. The conducted empirical experiments show that the proposed model is outperformed the compared benchmarks in the measured IR metrics.",
    "venue": "2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1dd19753c01476612db6f668967ded7b859deb83",
    "url": "https://www.semanticscholar.org/paper/1dd19753c01476612db6f668967ded7b859deb83",
    "title": "A Practical Guide to Hybrid Natural Language Processing: Combining Neural Models and Knowledge Graphs for NLP",
    "abstract": null,
    "venue": "",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "19016ebcd2dfbed15fd0f6a535ec52fca4db4442",
    "url": "https://www.semanticscholar.org/paper/19016ebcd2dfbed15fd0f6a535ec52fca4db4442",
    "title": "XML Topic Maps: Creating and Using Topic Maps for the Web",
    "abstract": "From the Book: \nA human being is part of a whole, called by us the \"Universe,\" a part limited in time and space. He experiences himself, his thoughts and feelings, as something separated from the rest\u0097a kind of optical delusion of his consciousness. This delusion is a kind of prison for us, restricting us to our personal desires and to affection for a few persons nearest us. Our task must be to free ourselves from this prison by widening our circles of compassion to embrace all living creatures and the whole of nature in its beauty. Albert Einstein, What I Believe, 1930 \n \nIn a former life, I built microprocessor-based data acquisition systems, originally for locating and monitoring wind and solar energy systems. I suppose it is fair to say that I have long been involved in roaming solution space. Along the way, farmers, on whose land the energy systems were often situated, discovered that my monitoring tools would help them form better predictions of fruit frost, irrigation needs, and pesticide needs. My program, which ran on an Apple II that had telephone access to the distributed monitoring stations, printed out large piles of data. Epiphany happened on the day that a manager of one of those monitoring systems came to me and asked \"What else is this data good for?\" That was the day I entered the field of artificial intelligence, looking for ways to organize all that data and mine it for new knowledge. \n \nA recent issue of a National Public Radio discussion focused on the nature and future of literature. Listening to that discussion while navigating the perils of Palo Alto traffic, I heard two comments that I shall paraphrase, with emphasis placed according to my ownwhims, as follows: \n \nIn the past, we turned to the great works of literature to ponder what is life. Today, we turn to the great works of science to ponder the same issues. \n \nIn some sense, the message I pulled out of that is that we (thatUs the really big we) tend to appeal to science and technology to find comfort and solutions to our daily needs. In that same sense, I found justification for this book and the vision I had when the book was conceived. Make no mistake here, I already had plenty of justification for the vision and the book; as is often pontificated by many, we are engulfed in a kind of information overload that threatens to choke off our ability to solve major problems that face all of humanity. \n \nNo, the vision is not an expression of doom and gloom. Rather, it is an expression of my own deep and optimistic belief that it is through education, through an enriched human intellect that solutions will be found, or at least, the solution space will become a more productive environment in which to operate. The vision expressed here is well grounded in the need to organize and mine data, all part of the solution space. \n \nWhile walking along a corridor at an XML conference in San Jose early in the year 2000, I noticed a sign that said Topic Maps, with an arrow pointing to the right. I proceeded immediately to execute a personal \"column right\" command, entered a room, and met Steve Newcomb. The rest all makes sense; while in Paris later that year, I saw the need to take the XTM technology to the public. This book was then conceived at XML2000 in Paris, and several authors signed on immediately. This book came with a larger vision than simply taking XTM to the public. I saw topic maps as an important tool in solution space. The vision included much more; topic maps are just one of many tools in that space. I wanted to start a book series, one that is thematically associated with my view of solution space. \n \nThis book is the first in a book series, flying under the moniker Open Knowledge Systems. By using the word open, I am saying that the series is about making the tools and information required to operate in solution space completely open and available to all who would participate. \"Open\" implies that each book in the series intends to include an Open Source Software project, one that enables all readers to immediately \"play in the sandbox\" and, hopefully, go beyond by extending the software and contributing that new experience to solution space. \n \nEach contribution to the Open Knowledge Systems series is intended to be a living document, meaning that each work will be available at a web site, the entire content of which will be browsable and supported with an online forum such that topics discussed in the books can be further discussed online. \n \nThis book is about Topic Maps, particularly Topic Maps implemented in the XTM Version 1.0 Standard format, as conceived by the XTM Authoring Group, which was started by an experienced group of individuals along with the vision and guidance of Steven Newcomb and Michel Biezunski, both contributing authors in this book. As with many new technologies, the XTM standard is, in most regards, not yet complete. In fact, a standard like XTM can never be complete simply because such standards must co-evolve with the environment in which they are applied. In the same vein, a book such as this cannot be a coherent work, simply because much of what is evolving now is subject to differing opinions, views, and so forth. \n \nBecause of my view that solution space, itself, is co-evolving along with the participants in that space, I have adopted an editorial management style that I suspect should be explained. My style is based on the understanding that I am combining contributions from many different individuals, each with a potentially different worldview, and each with a different writing style. The content focus of this book is, of course, on Topic Maps, but I believe that it is not necessary to force a coherent worldview on the different authors; it is my hope that readers, and, indeed, solution space will profit by way of exposure to differing views and opinions. There will, by the very nature of this policy, be controversy. Indeed, we are exploring the vast universe of discourse on the topic of knowledge, and there exists plenty of controversy just in that sand box alone. \n \nThere is also the possibility of overlap. Some chapters are likely to offer the same or similar, or even differing points of view on the same point. Case in point: knowledge representation. We have several chapters, one on Ontological Engineering, one on Knowledge Representation, and one on Knowledge Organization. Two talk in some detail about semantic networks, and others go heavily into how people learn. ItUs awfully easy to see just how these can overlap, and they do. My management style has been that which falls out of research in Chaos: use the least amount of central management; let the authors sort it out for themselves. History will tell us if this approach works.",
    "venue": "",
    "citationCount": 252,
    "fieldsOfStudy": [
      "Engineering"
    ]
  },
  {
    "paperId": "eee9ce1954703813c2d77fd8ec95404a755a3c6f",
    "url": "https://www.semanticscholar.org/paper/eee9ce1954703813c2d77fd8ec95404a755a3c6f",
    "title": "AI 2007: Advances in Artificial Intelligence, 20th Australian Joint Conference on Artificial Intelligence, Gold Coast, Australia, December 2-6, 2007, Proceedings",
    "abstract": null,
    "venue": "Australian Conference on Artificial Intelligence",
    "citationCount": 101,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ]
  },
  {
    "paperId": "d97c4539dcb3f4793b5b2d81d3caf92206c6346d",
    "url": "https://www.semanticscholar.org/paper/d97c4539dcb3f4793b5b2d81d3caf92206c6346d",
    "title": "Applications of Fuzzy Sets Theory, 7th International Workshop on Fuzzy Logic and Applications, WILF 2007, Camogli, Italy, July 7-10, 2007, Proceedings",
    "abstract": null,
    "venue": "International Workshop on Fuzzy Logic and Applications",
    "citationCount": 88,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "dddf3dd8975472413dbde6dad46801839aac1d66",
    "url": "https://www.semanticscholar.org/paper/dddf3dd8975472413dbde6dad46801839aac1d66",
    "title": "Natural Language Processing - IJCNLP 2005, Second International Joint Conference, Jeju Island, Korea, October 11-13, 2005, Proceedings",
    "abstract": null,
    "venue": "International Joint Conference on Natural Language Processing",
    "citationCount": 78,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "cff4f0d61fc0d3199841f424eaa83d745fd82bef",
    "url": "https://www.semanticscholar.org/paper/cff4f0d61fc0d3199841f424eaa83d745fd82bef",
    "title": "Wiktionary: the metalexicographic and the natural language processing perspective",
    "abstract": "Dictionaries are the main reference works for our understanding of language. They are used by humans and likewise by computational methods. So far, the compilation of dictionaries has almost exclusively been the profession of expert lexicographers. The ease of collaboration on the Web and the rising initiatives of collecting open-licensed knowledge, such as in Wikipedia, caused a new type of dictionary that is voluntarily created by large communities of Web users. This collaborative construction approach presents a new paradigm for lexicography that poses new research questions to dictionary research on the one hand and provides a very valuable knowledge source for natural language processing applications on the other hand. The subject of our research is Wiktionary, which is currently the largest collaboratively constructed dictionary project. \n \nIn the first part of this thesis, we study Wiktionary from the metalexicographic perspective. Metalexicography is the scientific study of lexicography including the analysis and criticism of dictionaries and lexicographic processes. To this end, we discuss three contributions related to this area of research: (i) We first provide a detailed analysis of Wiktionary and its various language editions and dictionary structures. (ii) We then analyze the collaborative construction process of Wiktionary. Our results show that the traditional phases of the lexicographic process do not apply well to Wiktionary, which is why we propose a novel process description that is based on the frequent and continual revision and discussion of the dictionary articles and the lexicographic instructions. (iii) We perform a large-scale quantitative comparison of Wiktionary and a number of other dictionaries regarding the covered languages, lexical entries, word senses, pragmatic labels, lexical relations, and translations. We conclude the metalexicographic perspective by finding that the collaborative Wiktionary is not an appropriate replacement for expert-built dictionaries due to its inconsistencies, quality flaws, one-fits-all-approach, and strong dependence on expert-built dictionaries. However, Wiktionary's rapid and continual growth, its high coverage of languages, newly coined words, domain-specific vocabulary and non-standard language varieties, as well as the kind of evidence based on the authors' intuition provide promising opportunities for both lexicography and natural language processing. In particular, we find that Wiktionary and expert-built wordnets and thesauri contain largely complementary entries. \n \nIn the second part of the thesis, we study Wiktionary from the natural language processing perspective with the aim of making available its linguistic knowledge for computational applications. Such applications require vast amounts of structured data with high quality. Expert-built resources have been found to suffer from insufficient coverage and high construction and maintenance cost, whereas fully automatic extraction from corpora or the Web often yields resources of limited quality. Collaboratively built encyclopedias present a viable solution, but do not cover well linguistically oriented knowledge as it is found in dictionaries. That is why we propose extracting linguistic knowledge from Wiktionary, which we achieve by the following three main contributions: (i) We propose the novel multilingual ontology OntoWiktionary that is created by extracting and harmonizing the weakly structured dictionary articles in Wiktionary. A particular challenge in this process is the ambiguity of semantic relations and translations, which we resolve by automatic word sense disambiguation methods. (ii) We automatically align Wiktionary with WordNet 3.0 at the word sense level. The largely complementary information from the two dictionaries yields an aligned resource with higher coverage and an enriched representation of word senses. (iii) We represent Wiktionary according to the ISO standard Lexical Markup Framework, which we adapt to the peculiarities of collaborative dictionaries. This standardized representation is of great importance for fostering the interoperability of resources and hence the dissemination of Wiktionary-based research. To this end, our work presents a foundational step towards the large-scale integrated resource UBY, which facilitates a unified access to a number of standardized dictionaries by means of a shared web interface for human users and an application programming interface for natural language processing applications. A user can, in particular, switch between and combine information from Wiktionary and other dictionaries without completely changing the software. \n \nOur final resource and the accompanying datasets and software are publicly available and can be employed for multiple different natural language processing applications. It particularly fills the gap between the small expert-built wordnets and the large amount of encyclopedic knowledge from Wikipedia. We provide a survey of previous works utilizing Wiktionary, and we exemplify the usefulness of our work in two case studies on measuring verb similarity and detecting cross-lingual marketing blunders, which make use of our Wiktionary-based resource and the results of our metalexicographic study. We conclude the thesis by emphasizing the usefulness of collaborative dictionaries when being combined with expert-built resources, which bears much unused potential.",
    "venue": "",
    "citationCount": 11,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "df73772445b59d7a53b4f1d22d0fec16c1b14430",
    "url": "https://www.semanticscholar.org/paper/df73772445b59d7a53b4f1d22d0fec16c1b14430",
    "title": "Semantic Analysis using Wikipedia Graph Structure",
    "abstract": "Wikipedia is becoming an important knowledge source in various domain specific applications based on concept representation. While lexical resources like WordNet cover generic English well, they are weak in their coverage of domain-specific terms and named entities, which is one of the strengths of Wikipedia. Furthermore, semantic relatedness methods that rely on the hierarchical structure of a lexical resource are not directly applicable to the Wikipedia link structure, which is not hierarchical and whose links do not capture well defined semantic relationships like hyponymy. We introduce a vector space representation of concepts using Wikipedia graph structure to calculate semantic relatedness. The proposed method starts from the neighbourhood graph of a concept as the primary form and transfers this graph into a vector space to obtain the final representation. The proposed method achieves state-of-the-art results on various relatedness datasets. We evaluate Wikipedia in a domain-specific semantic relatedness task and are able to demonstrate that Wikipediabased methods can be competitive with state of the art ontology-based methods and distributional methods in the biomedical domain. The comparison includes a wide range of structure and corpus-based methods, such as our proposed word2vec-based embeddings: a hybrid distributional/knowledge-based word2vec and node-embedding, a word2vec application on graph structure. Our representations have also been reported to achieve the highest results in a query expansion task. We also use a standard coherence model to show that the proposed relatedness method performs successfully in Word Sense Disambiguation (WSD). We then suggest a different formulation for coherence to demonstrate that, in a short enough sentence, there is one key entity that can help disambiguate every other entity. Using this finding, we provide a vector space based method that can outperform the standard coherence model in a significantly shorter computation time. We use our findings in WSD to create a complete wikifier, a supervised approach based on learning to rank that combines our new coherence measure with other sources of information, such as textual context. The final product is an open source project that is available through direct API or web service.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "47ba9f3c8fba4cf82e48cdc05b8dfd5fd6636f87",
    "url": "https://www.semanticscholar.org/paper/47ba9f3c8fba4cf82e48cdc05b8dfd5fd6636f87",
    "title": "The role of informatics in promoting patient safety",
    "abstract": "This issue of JAMIA is focused on informatics applications to enhance patient safety. This is one of the most important, yet underemphasized, aspects of the informatics curriculum across the country. Although media attention occasionally concentrates on what can go wrong when information systems are employed in practice, there is also much to say on what might go wrong if information systems were not employed. Additionally, a proper amount of standardization of clinical practices can elevate sub-optimal care to an acceptable level, often reducing cost and patient suffering as a result. Decision support for medication prescribing and dispensing has always been one of the most direct ways for information systems to promote patient safety. Whalen (p. 849) reports on lessons learned in a pediatrics hospital from a transition to a new electronic health record (EHR) system, and Walsh (p. 911) studies the accuracy of the medication list in the EHR and its implications for care, research, and improvement. Cheng (p. 873) shows how using drug knowledgebase information to distinguish between commonly confused drugs can prevent errors, and Vajravelu (p. 780) proposes a new algorithm to analyze multiple pharmacologic exposures using EHR data. Additionally, Samwald (p. 895) shares the experience of implementing pharmacogenomics decision support across seven European countries. The domains in which information systems can improve patient safety are numerous. Waters (p. 901) studies current use, interest, and perceived usability of clinical pathways for primary care, while Sittig (p. 915) describes the levels of adherence to recommended EHR safety practices across eight healthcare organizations. EHRbased intervention and reports on several safety topics are also presented in this issue of JAMIA: Ray (p. 863) uses statistical anomaly detection models for decision support system malfunctions, Chen (p. 790) analyzes interaction patterns of trauma providers that are associated with increased patients\u2019 lengths of stay in the hospital, while Vahdat (p. 827) reports on a simulation study of the effects of EHR implementation on timeliness of care in a dermatology clinic. Berger (p. 833) integrates physical abuse measures into a pediatric clinical decision support system, while Meyer (p. 841) evaluates a mobile application to improve clinical laboratory test ordering. The applications and algorithms described in this issue of JAMIA would be hard to implement without standardization of terminologies, ontologies, and foundational research in natural language processing and information retrieval. Examples of advances in these areas are also featured: Cuzzola (p. 819) links UMLS to DBpedia to promote knowledge discovery, Wang (p. 809) describes efforts involving RxNorm that are leading to a normalized clinical drug knowledge base in China, Vreeman (p. 886) presents a unified terminology for radiology procedures (the \u201cLOINC RSNA Radiology Playbook\u201d), and Blosnich (p. 907) shows how it is possible to use EHR-based clinician text notes to validate transgender-related ICD codes. Additional articles describe approaches that enable a variety of information systems: Mei (p. 800) describes an interactive medical word sense disambiguation method, Kilicoglu (p. 856) reports on the results of automatic recognition of self-acknowledged limitations in the clinical research literature, and Baladron (p. 774) proposes a tool for filtering PubMed search results by sample size. JAMIA continues to publish a combination of application and foundational articles that allows our readers to stay abreast with the best developments in the field. Patient safety is an important topic that has been in the informatics portfolio since its start and for which novel solutions continue to emerge. New topics are also continuously enlarging the informatics portfolio. Stay tuned for the August issue that highlights articles on informatics applications focused on patients, their family and friends, and the expanding scope of our field.",
    "venue": "J. Am. Medical Informatics Assoc.",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ]
  },
  {
    "paperId": "b3f0e09df9d9ff8d4376fe3c2dc765b089b107da",
    "url": "https://www.semanticscholar.org/paper/b3f0e09df9d9ff8d4376fe3c2dc765b089b107da",
    "title": "OBJECTION BASED KNOWLEDGE RECOVERY FRAMEWORK BY UTILIZING DEEP LEARNING",
    "abstract": ": The Information Revolution is becoming a difficult task humans were not designed to process massive quantities of information cult task humans were not designed to process massive quantities of information, when clients research with new fields would get precise outcomes for a profound hunt, Smart Crawler positions sites to organize exceptionally pertinent ones it is done based on deep search. Deep search analysis involves precisely targeted and sometimes complex queries on data sets that may be measured in petabytes and exabytes, often with requirements for real-time or near-real-time responses. Because real-time analysis of such large data sets can require distribution more datas. The queries is submitted to the application will be preprocessed, after that lone root words will be taken and discover Synonym, Hypernym and Hyponym encourages the client to get their pertinent pursuit. Our dynamic administration of substance incorporates rot and support technique can imitate clients' recovery system. On the off chance that any words in the showed list is been chosen then the site connections, pictures and news nourishes will be given as definite yield to the client. The existing paper present a synonym based data mining approach where our proposed would deals with Hypernym and Hyponym encourages the client to makes the task easier way and of improving the ranking of the website much easier way. Likewise it allows user to get answer to their query easily through any of search engine available in market IndexTerms -. synonym,hypernym,hyponym,wordnet,wedsitelinks,images I.INTRODUCTION To bunch the content records over the web page in light of the client wrote key term. To enhance profound web look (ontology) and overcome gathering of irrelevant archives into the same group. Plans to help Web clients find the best scan instruments for their pursuit needs, resulting in quicker and more exact query items. We display work accept that all client nearby instance storehouses have contentbased descriptors alluding to the subjects, be that as it may, a large volume of records existing on the web may not have such substance based descriptors. For this issue we procedures like philosophy mapping and content grouping/bunching were proposed.These methodologies will be explored in future work to take care of this issue. The examination will extend the appropriateness of the metaphysics model to the larger part of the current web archives and increase the commitment and criticalness of the present work. Psychological studies show that humans rely on both episodic memory and semantic memory to recall information or events from the past. Human\u2019s episodic memory receives and stores temporally dated episodes or events, together with their spatial-temporal relations, while human\u2019s semantic memory, on the other hand, is a structured record of facts, meanings, concepts and skills that one has acquired from the external world. Semantic information is derived from accumulated episodic memory[1]. WordNet is in some cases called a ontology, a determined claim that its makers don't make. the hypernym/hyponym connections among the noun synsets can be translated \u200eas specialization relations among reasonable classifications. In different words, WordNet can be deciphered and utilized as a lexical philosophy in the software engineering sense. However, such a cosmology ought to be rectified before being utilized, on the grounds that it contains many basic semantic irregularities; for instance there are, (I) basic specializations for select categories and (ii) redundancies in the specialization pecking order. Besides, transforming WordNet into a lexical metaphysics usable for learning portrayal ought to normally additionally include (I)recognizing the specialization relations into subtypeOf and instanceOf relations, and (ii) partner natural unique identifiers to every class. Albeit such corrections and changes have been performed and archived as a major aspect of the integration of WordNet 1.7 into the agreeably updatable learning base of WebKB-2, most projects guaranteeing to re-utilize WordNet for knowledge-based applications (ordinarily, knowledge-arranged data recovery) essentially re-utilize it directly. In the literature, a number of techniques and tools like bookmarks, history tools, search engines, metadata annotation and exploitation, and contextual recall systems have been developed to support personal web revisitation[8]. Web browsers and search engine sites are the primary tools people use to access the vast quantities of information available online. These tools typically treat informationseeking tasks as a trans k[5]. While many searches are for new information, a significant use of search engines is to find information that was found before. For example, a query or keyword is often used to bookmark a Web page[3] . Fig:1 Process of hyponym &hypernym \u00a9 2018 JETIR March 2018, Volume 5, Issue 3 www.jetir.org (ISSN-2349-5162) JETIR1803017 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 86 II.BACK END PROCESS: Wordnet mysql is an easy-to-use cross-platform browser application for the WordNet SQL database (MySql, PostgreSql, SQlite,...) that displays results as trees. Standard query capabilities extend beyond words to other WordNet entities such as senses, synsets, semlinks, lexlinks, etc. Behind the scenes, the Hibernate framework maps relational data to objects. provides a ready-to-use WordNet SQL database in MySQL is a library of Plain Old Java Objects to encapsulate WordNet entitites (word, sense, synset, etc.) and handle them transparently without having to manage SQL queries, using the Hibernate framework. Comes with a programming sample WordNet::Similarity requires a local copy of WordNet as well as the WordNet::QueryData Perl module, which provides a direct Perl interface to the WordNet database III.ARCHITECTURE DIAGRAM: Fig:2The process of deep web search engine IV.EXISTING SYSTEM: The searching for centre pages with the help of search engines, is to avoiding visiting a large number of pages. A matching query style is unsuitable to investigate information in unfamiliar fields and to learn new associations and knowledge with relevance to a query. When users cannot specify accurate search words, the search results are often useless. Furthermore, as information becomes more ubiquitous and demands for various searches grow, there is an increasing need to support search behaviours beyond simple lookup. The second stage, In-site searching by excavating most relevant links in hidden web directories; we design a link tree data structure to achieve wider coverage or a website. Here the more accurate results for a focused crawl, Smart Crawler ranks websites to prioritize highly relevant ones for a given topic.[1]. V.OUR WORK: The multi-keyword ranked search and synonym based search supports a practically efficient and flexible searchable scheme in our proposed work.To address multi-keyword search and result ranking, Vector Space Model (VSM) is used to build document index, that is to say, each document is expressed as a vector where each value of dimensions is the Term Frequency (TF) weight of its corresponding keyword.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "3b16ca4646471afc06fdaec1e1b8b98a024ebed6",
    "url": "https://www.semanticscholar.org/paper/3b16ca4646471afc06fdaec1e1b8b98a024ebed6",
    "title": "Search Engines to QAS: Explorative Analysis",
    "abstract": "Search engine is pedestal machine on two-dimensional logic and possibility hypothesis which are short of concept based i.e. fact based existent human understanding and advanced way of thinking and precision to formulate answers.QA (Question answering) system is hybrid machine with analysis ability to fetch data from diverse knowledge source assorted format and formulate a answer to user search query. Research domain in advanced Information retrieval is machine to prospect a information seeking engine (search engine advancement).formulate precise answer to user search question is aspire of QA system .current search engines are keyword base with sort of semantic incorporation (semantic search in Google) Enhancement in search engines is three tier, user question understanding (user model, context, intent), text/data-mining analytics with machine learning (Support vector machine(SVM),Artificial neural networks(ANN)),and adha Rank (bings algorithm) to formulate trainable machine to reduce search space and time.QA system is a semantic concept based framework which identifies the nucleus keywords and phrases incorporating meaning i.e. sense of word to phrase ,sense of phrase to sentence ,to corpus which abolish the disambiguation of NL(natural language) in search engines. A prototype design of Answer engine is proposed that performs concept map of user question .the semantic concept framework enhances the ability of reckoning in Answering system. GUI interface performs mapping of user question in concept group (ontology class i.e. Spatial ontology, domain ontology, task ontology, universal ontology).text analytics formulates extraction of Answer term, phrases from corpus or knowledge dataset (unstructured & structured) using n-gram. Machine learning algorithms facilitate intelligence in search corpus. Ranking algorithm presents information on relevance precision mapped to user search intent. AdaRank repetitively build \u2018frail rankers\u2019 on the foundation of re-weighted training data and lastly linearly join the feeble rankers for making ranking forecast. The training process of AdaRank is exactly enhancing the performance measure used. AdaRank significantly optimizes the base technologies BM25, Ranking SVM, and Rank Boost.QA system are future of search engine, expert systems, and rule based systems. Current paradigm information processing machines to intelligent information machine which perform time variant search on large dataset with higher precision to question. This paper a search analysis has been carried out on 20 research article on Advance IR technologies. The search analysis present in detailed study on three methodologies (question understanding, text analytics, machine learning and ranking algorithm) when optimized invent enhanced search machine, QA system. Search paper concludes that intelligence in information processing has evolved search engines, rule based machines, expert systems to hybrid reasoning machines-QA Systems.",
    "venue": "",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "68db4e9007bf8061b91939621e2e3bfcc246fc99",
    "url": "https://www.semanticscholar.org/paper/68db4e9007bf8061b91939621e2e3bfcc246fc99",
    "title": "Natural Language Processing - IJCNLP 2004, First International Joint Conference, Hainan Island, China, March 22-24, 2004, Revised Selected Papers",
    "abstract": null,
    "venue": "International Joint Conference on Natural Language Processing",
    "citationCount": 50,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1a8d53272a64d44fe1204224245eb0103a04778f",
    "url": "https://www.semanticscholar.org/paper/1a8d53272a64d44fe1204224245eb0103a04778f",
    "title": "Collaborative construction of a good quality, broad coverage and copyright free Japanese-French dictionary",
    "abstract": "This research project is located in the field of natural language processing (NLP), at the intersection of computer science and linguistics, specifically multilingual lexicography and lexicology. \nConcerning the Web, although French and Japanese are two well resourced languages (Berment, 2004), is not the case of the French-Japanese couple: \n- Electronic French-Japanese bilingual dictionaries (denshi jisho) can not be copied to a computer or reused; \n- There is a French-Japanese dictionary on the Web1, but it only contains 40 000 entries, no examples and is not available for download. \nThere are collaborative Web dictionaries such as the Japanese-English JMdict project led by Jim Breen (2004) that contains over 173,000 items. These resources are freely downloadable. It is therefore possible to carry out such projects. \nDuring a first stay in Japan from November 2001 to March 2004, we had already noticed the lack of French-Japanese bilingual resources on the Web. Which gave rise to the Papillon project about the construction of a multilingual lexical database with a pivot structure (Serasset et al., 2001). Since then, progress has been made in several areas (technical, theoretical, social) (Mangeot, 2006) but the actual production of data has made very little progress. On the other hand, there is a new trend in reusing existing lexical resources (word sense disambiguation, using open source resources (Wiktionary, dbpedia) merging with ontologies, etc.). Although they allow to consolidate and expand the coverage of existing resources, these experiences still use data created by hand by professional lexicographers. There are printed French-Japanese dictionaries of good quality and sufficiently old to be royalty free. It should be possible to reuse these resources as part of our project to build a good quality dictionary and broad coverage available on the Web. \nBased on this observation, we defined the following project to build a rich multilingual lexical system with priority over French-Japanese languages. The construction will be done first by reusing existing resources (printed Japanese-French dictionaries, Japanese-other language dictionaries, \n\ufffc1http://www.dictionnaire-japonais.com \n\ufffc\ufffc\ufffc \nWikipedia) and automatic operations (scanning and corrections, calculating translation links) and then by volunteer contributors working as a community on the Web. They will have to contribute to dictionary articles according to their level of expertise and knowledge in the field of lexicography or bilingual translation. \nThe resulting resources will be royalty-free and intended for use by both humans via conventional bilingual dictionaries and by machines for automatic language processing tools (analysis, machine translation, etc.).",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "75b24899d1c25a165a40ec575377dfa7f9aea5d8",
    "url": "https://www.semanticscholar.org/paper/75b24899d1c25a165a40ec575377dfa7f9aea5d8",
    "title": "Semantic indexing via knowledge organization systems : applying the CIDOC-CRM to archaeological grey literature",
    "abstract": "The volume of archaeological reports being produced since the introduction of PG161 has significantly increased, as a result of the increased volume of archaeological investigations conducted by academic and commercial archaeology. It is highly desirable to be able to search effectively within and across such reports in order to find information that promotes quality research. A potential dissemination of information via semantic technologies offers the opportunity to improve archaeological practice, not only by enabling access to information but also by changing how information is structured and the way research is conducted. This thesis presents a method for automatic semantic indexing of archaeological grey-literature reports using rule-based Information Extraction techniques in combination with domain-specific ontological and terminological resources. This semantic annotation of contextual abstractions from archaeological grey-literature is driven by Natural Language Processing (NLP) techniques which are used to identify \u201crich\u201d meaningful pieces of text, thus overcoming barriers in document indexing and retrieval imposed by the use of natural language. The semantic annotation system (OPTIMA) performs the NLP tasks of Named Entity Recognition, Relation Extraction, Negation Detection and Word Sense disambiguation using hand-crafted rules and terminological resources for associating contextual abstractions with classes of the ISO Standard (ISO 21127:2006) CIDOC Conceptual Reference Model (CRM) for cultural heritage and its archaeological extension, CRM-EH, together with concepts from English Heritage thesauri and glossaries. The results demonstrate that the techniques can deliver semantic annotations of archaeological grey literature documents with respect to the domain conceptual models. Such semantic annotations have proven capable of supporting semantic query, document study and cross-searching via web based applications. The research outcomes have provided semantic annotations for the Semantic Technologies for Archaeological Resources (STAR) project, which explored the potential of semantic technologies in the integration of archaeological digital resources. The thesis represents the first discussion on the employment of CIDOC CRM and CRM-EH in semantic annotation of grey-literature documents using rule-based Information Extraction techniques driven by a supplementary exploitation of domain-specific ontological and terminological resources. It is anticipated that the methods can be generalised in the future to the broader field of Digital Humanities.",
    "venue": "",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "97d6a6ca8e33f442c334488f8a8af907bd833628",
    "url": "https://www.semanticscholar.org/paper/97d6a6ca8e33f442c334488f8a8af907bd833628",
    "title": "A semantic framework for discovering casual relationships",
    "abstract": "The explosive growth of information at a mind-boggling scale has become an emerging \nphenomenon of our times. Discovering knowledge from a vast pool of resources is expected to \nremain a major challenge. In this respect, the extraction of semantic relations then becomes an \nimportant research area. While the extraction of ontological relations has been widely explored, \nthe discovery of non-taxonomic relations is still a major bottleneck. Current approaches tend to \npredominantly employ syntactic approaches and rely largely on extensive manual efforts in the \nconstruction of linguistic resources. Our literature review has revealed major gaps in terms of the \nextraction of non-taxonomic relationships, particularly when it comes to implicit relationships. \nAs a response to this problem, our research then explores a semantic approach for addressing the \ndiscovery of non-taxonomic relations such as causal relationships. \nBased on an empirical study of causality theory and related works, we have formulated a \nsemantic approach for extracting causal patterns in text. The proposed framework incorporates a \nnovel causality sense extraction method, \u201cPurpose Based Word Sense Disambiguation\u201d, together \nwith a context-specific approach, \u201cGraph based Semantics\u201d, for uncovering causality structural \npatterns. Our approach has produced a set of causality features that is even able to highlight \nimplicit causality patterns. We have employed benchmark data sets of SemEval 2007 and \nSemEval 2010 data sets together with standard linguistic resources such as WordNet, SemCore \nand XWNGloss in producing a series of intermediary linguistic resources as building blocks of \nthe framework. \nA new qualitative measure for determining causal patterns has been formulated and used in \nconjunction with a gold standard for validating the significance of the findings. We have \nemployed the C5.0 classifier to evaluate the effectiveness of the causality patterns as derived \nv \nfrom the framework. We have demonstrated via the realization of the framework, a purely \nsemantic approach is possible without the need for extensive manual efforts. This research will \nserve as a key milestone and basis for ensuing discovery of non-taxonomic semantic relations \nsuch as causality.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "71980183d4adfdc42cffdbd12ef80acf41208b42",
    "url": "https://www.semanticscholar.org/paper/71980183d4adfdc42cffdbd12ef80acf41208b42",
    "title": "Combinatoria l\u00e9xica, polisemia y polisemia regular en una base de conocimiento l\u00e9xico conceptual : el caso de \"Redes diccionario combinatorio del espa\u00f1ol contempor\u00e1neo\" y functional grammar knowledge base",
    "abstract": "This dissertation attempts to link the linguistic information contained in REDES Diccionario combinatorio del espanol contemporaneo (Bosque, 2004), or REDES, to the \nontological framework of Functional Grammar Knowledge Base, or FunGramKB. REDES is a dictionary that gathers systematic restrictions imposed by some 4,000 Spanish predicates to their selection of lexical arguments, while FunGramKB is a multilingual and multipurpose lexical conceptual knowledge base (KB) designed for Natural Language Processing (NLP). This work is thus part of the field known as electronic lexicography for the XXIst century or the third millennium \n(Fuertes and Tarp, 2011).This new lexicography comprises electronic lexicographical resources which are much \nmore complex than the commonly used digitalized versions of traditional dictionaries. It is composed of lexical databases or lexical knowledge bases, of variable complexity and depth, built on electronic platforms. Even though electronic lexicography can also serve the typical dictionary queries posed by humans, the best-known NLP applications include machine translation (MT), \nquestion and answer systems, information extraction, and voice recognition programs. However, the main problem facing NLP continues to be Word Sense Disambiguation (WSD). Most words in a language are polysemic and a successful NLP application will depend on its ability to assign the \ncorrect sense to each word in a given context. A great deal of the work that is taking place in NLP is therefore focused on finding strategies to effectively disambiguate words in their context.In working with REDES and FunGramKB, it could seem at first glance that we are dealing with two distant fields \u2013print lexicography and knowledge engineering, respectively\u2013, but this thesis assumes that linking these resources would yield significant benefits for both. Furthermore, it would combine two sources of valuable linguistic and theoretical data: on the one hand, REDES \ncontributes thousands of patterns of systematic predicate-argument word combinations, taken from real use in Spanish-language corpora. These combinations are not presented as \u201ccollocations\u201d, that is, combinations between a single predicate and a single argument, or vice versa, but rather, \nbetween predicates and \u201clexical classes\u201d, groups of arguments that share a common semantic basis. On the other hand, FunGramKB contributes a multilevel electronic platform designed for NLP, which includes a conceptual, a lexical and a grammatical model, and has been built around a hierarchical and taxonomical ontology of universal cognitive concepts.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "05f125be96a70b7cda245339430d2324002c9abc",
    "url": "https://www.semanticscholar.org/paper/05f125be96a70b7cda245339430d2324002c9abc",
    "title": "\u0391\u03bb\u03b3\u03cc\u03c1\u03b9\u03b8\u03bc\u03bf\u03b9 \u03ba\u03b1\u03b9 \u03c4\u03b5\u03c7\u03bd\u03b9\u03ba\u03ad\u03c2 \u03b5\u03be\u03b1\u03c4\u03bf\u03bc\u03b9\u03ba\u03b5\u03c5\u03bc\u03ad\u03bd\u03b7\u03c2 \u03b1\u03bd\u03b1\u03b6\u03ae\u03c4\u03b7\u03c3\u03b7\u03c2 \u03c3\u03b5 \u03b4\u03b9\u03b1\u03b4\u03b9\u03ba\u03c4\u03c5\u03b1\u03ba\u03ac \u03c0\u03b5\u03c1\u03b9\u03b2\u03ac\u03bb\u03bb\u03bf\u03bd\u03c4\u03b1 \u03bc\u03b5 \u03c7\u03c1\u03ae\u03c3\u03b7 \u03c5\u03c0\u03bf\u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03c9\u03bd \u03c3\u03b7\u03bc\u03b1\u03c3\u03b9\u03bf\u03bb\u03bf\u03b3\u03b9\u03ce\u03bd",
    "abstract": "The tremendous growth of the Web in the recent decades has made the searching for information as one of the most important issues in research in Computer Technologies. \nToday, modern search engines respond quite well to the user queries, but the results are not always relative to the data the user is looking for. Therefore, search engines are making significant efforts to rank the most relevant query results to the user in the top results of the ranking list. This work mainly deals with this problem, the ranking of the relevant results to the user in the top of the ranking list even when the queries contain multiple meanings. In the context of this research, algorithms and techniques were constructed based on the technique of relevance feedback which improves the results returned by a search engine. Main source of feedback are the results which the users selects during the navigation process. The user extends the original information (search keywords) with new information derived from the results that chooses. Having a new set of information concerning to the user's preferences, the relevancy of this information is compared with the other results (those returned before choosing this effect) and change the order of the results by promoting and suggesting the results that are more relevant to the new set of information. \nAnother problem that must be addressed when the users submit queries to the search engines is that the queries are usually small in number of words and ambiguous. Therefore, there must be ways to disambiguate the different concepts/senses and ways to find the concept/sense that interests the user. Disambiguation of the search terms is a process that has been studied in the literature in several different ways. This work proposes new strategies to disambiguate the senses/concepts of the search terms and explore their efficiency in search engines. Their innovation is the use of PageRank as an indicator of the importance of a sense/concept for a query term. \nAnother technique that exploits semantics in our work is the use of text annotation. The use of text annotation is a technique that assigns to the words of the text extra information such as the meaning assigned to each word based on the semantic content of the text. Assigning additional semantic information in a text helps users and search engines to seek or describe better the text information. In my thesis, techniques for improving the automatic annotation of small texts with entities from Wikipedia are presented, a process that referred in the literature as Wikification. \nIt is widely known that the Web contain documents with the same information and documents with almost identical information. Despite the efforts of the search engine\u2019s algorithms to find the results that contain repeated information; there are still cases where the results retrieved by a search engine contain repeated information. In this work effective techniques are presented that find and cut the repeated information from the results of the search engines. Specifically, the results that contain the same information are removed, and the results that contain repeated information are merged into new texts (SuperTexts) that contain the information of the initial results without the repeated information. \nAnother part of this work tries to exploit the semantic information of search engine\u2019s results using tools of the Semantic Web. The goal of the Semantic Web is to make the resources of the Web understandable to humans and machines. The Semantic Web in their first steps functioned as a detailed description of the body of the Web documents. The development of tools for querying Semantic Web is still in its infancy. The current search techniques are not adapted to the indexing and retrieval of semantic information with a few exceptions. In our research we have created efficient techniques and tools for using the Semantic Web. Specifically an algorithm was constructed that converts to ontology the search engine\u2019s results integrating semantic and syntactic information in order to answer natural language questions. \nAlso this paper contains XML filtering techniques that use semantic information. Specifically, an efficient distributed system is proposed for the semantic filtering of XML documents that gives better results than the existing approaches. \nFinally as part of this thesis is additional research that improves the performance of the search engines from a different angle. It is presented a technique for cutting the inverted lists of the inverted files. Specifically a combination of the proposed technique with existing compression techniques is achieved, leading to better compression results than the existing ones.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "e3d729656e2e1069a9badcccd7339f816209a7ba",
    "url": "https://www.semanticscholar.org/paper/e3d729656e2e1069a9badcccd7339f816209a7ba",
    "title": "Advances in Artificial Intelligence - IBERAMIA-SBIA 2006, 2nd International Joint Conference, 10th Ibero-American Conference on AI, 18th Brazilian AI Symposium, Ribeir\u00e3o Preto, Brazil, October 23-27, 2006, Proceedings",
    "abstract": null,
    "venue": "IBERAMIA-SBIA",
    "citationCount": 19,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "aabba76c7d45ce6dc5222ff36b6371ee56806ac1",
    "url": "https://www.semanticscholar.org/paper/aabba76c7d45ce6dc5222ff36b6371ee56806ac1",
    "title": "Meaning is its use : towards the use of Distributional Semantics for Content-based Recommender Systems",
    "abstract": "The recent spread of collaborative platforms and social networks made the authoring of content easier and easier. However, due to this uncontrolled phenomenon, the amount of data continuously grows without a proper control in terms of quality and reliability of produced content. This makes the problem of Information Overload much more felt than the past. As already demonstrated by many successful use cases such as Amazon , Netflix 2 and Pandora , Recommender Systems (RSs) are the tools that can cope the best with this issue, since they can help sifting this flow of information by providing users with personalized access to textual and multimedia information sources. Even if the most popular recommendation algorithms follow the collaborative approach, content-based recommender systems (CBRS) have proved to be effective in real-world scenarios, since they can face with some limitations of collaborative algorithms such as scalability and the new item problem. Generally speaking, techniques for CBRS are based on the assumption that the relevance of an unseen item for a target user is usually predicted by matching the features stored in a user profile (inferred from the items previously considered as relevant) with those describing the new item. From this insight it immediately follows that CBRS pipelines need to model both items and user profiles with richer semantic representations, since the approaches based on simple string matching can not handle all the facets typical of natural languages and suffer from the typical issues of polysemy and synonymy. Consequently, the research in both natural language processing and computational linguistics area is gaining more and more attention: beside the classical techniques based on the use of ontologies or the exploitation of word sense disambiguation algorithms, Distributional Models (DM) are recently emerging . These approaches got their name from distributional semantics and describe a set of techniques originally introduced in computational linguistics and cognitive sciences. These approaches are based on the assumption that the semantics of a term can be inferred by 1 http://www.amazon.com 2 http://www.netflix.com 3 http://www.pandora.com analyzing its use in large corpus of textual data. Specifically, these techniques rely on the distributional hypothesis, which states that \u201dWords that occur in the same contexts tend to have similar meanings\u201d. By following the famous Wittgenstein\u2019s sentence (\u201dMeaning is its use\u201d) it is possible, as already demonstrated by Rubenstein and Goodenough in the mid-1960s [4], to infer the meaning of a term (such as leash) by analyzing the meaning of the other terms it co-occurs with (dog, animal, etc.). Similarly, the correlation between different terms (e.g., leash and muzzle) can be inferred by analyzing how similar are the contexts in which they are used. The use of this methodology provides a clear advantage, since a model to represent terms and documents in a lightweight semantic vector space, called Word Space [1], can be built in a totally unsupervised way according to the use of the terms in a corpus of textual data. The goal of this talk is to show how DM can be effective exploited to provide CBRS with a lightweight semantic representation of both items and use profiles. Specifically, a novel content-based recommendation framework that adopts DM as main building block, called eVSM (enhanced Vector Space Model), is introduced [2]. In this framework the original VSM is extended by means of distributional models as well as a negation operator based on Quantum Logic and an incremental technique for dimensionality reduction, called Random Indexing. In the experimental sessions the effectiveness of eVSM is evaluated in many offline and online settings, and it emerged that eVSM overcomes several state-of-the-art models in terms of goodness of recommendations and accuracy of the proposed ranking [3]. Specifically, it outperforms in a significant way LSI, the classical VSM and a Bayes text classifier in the task of providing users with recommendations about movies. The general outcomes of the offline evaluations are confirmed in the online ones as well, since eVSM showed its capability of providing good recommendations in two user studies carried out in the area of music recommendation.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "de8528415a758c1d68b0b2eedf2409fcb079c1b0",
    "url": "https://www.semanticscholar.org/paper/de8528415a758c1d68b0b2eedf2409fcb079c1b0",
    "title": "Integrating Generic and Specialized Wordnets",
    "abstract": "Although generic (i.e. domain independent) and specialized (i.e. domain specific) lexical resources are usually developed with different aims, an integrated consultation seems to be necessary for many NLP based applications. We describe an integration procedure based on the definition of plug-in relations that are established to manage overlaps and inconsistencies between the two resources. The approach has been experimented connecting ItalWordNet, a generic lexical database for Italian, and Economic-WordNet, a specialized wordnet for the economic and financial domain. In this paper we address the issue of integrating the information included in a generic lexical database with the information included in a specialized (i.e. domain specific) lexical database. We restrict our investigation to wordnet-like lexical resources, that is lexical databases whose model is derived from WordNet (Miller, 1990) (Fellbaum, 1998). The aim is to define a set of procedures to allow an of the two resources, such that overlapping senses are merged and conflicting situations are properly managed. Our starting point are two existing wordnets for Italian: ItalWordnet (IWN)1, a generic wordnet deriving from EuroWordNet (Vossen, 1999), and Economic-wordnet (ECOWN), a wordnet for the economic and financial domain. The current application scenario is a recommender system (Magnini and Strapparava, 2001), which includes a 1 This research has been supported by SI-TAL (Integrated System for the Automatic Treatment of Language), a National Project devoted to the creation of large linguistic resources and software for Italian written and spoken language processing. document processing module based on a light form of word sense disambiguation. The experimental domain is that of financial news. As a first attempt to use the two resources in conjunction, a \u201cspecific-first\u201d strategy was implemented, which, given a lemma in the document, first looks up in the specific wordnet and just in case of failure resorts to the generic wordnet. An evident drawback of this approach is that it does not cover cases of words belonging to both of the databases when the correct interpretation of the word is placed in the generic wordnet. For instance, in the phrase \u201cbuilding its share market\u201d (taken from a financial news), the word \u201cshare\u201d is used with the generic meaning of \u201cportion\u201d, while in the specialized wordnet we would have the economic sense of \u201cshare\u201d as part of the capital stock. To overcome this limit we tried with a \u201cunion\u201d strategy which, for a given lemma, considers the sum of the senses for that lemma in the two resources. The problem here is that senses for words belonging to both the databases tend to proliferate, making disambiguation harder. What seems necessary is a deeper integration of the respective word senses, such that overlapping senses are merged and conflicting situations are detected and solved. Our scenario allows some significant simplifications with respect to the general problem of merging two distinct ontologies (Hovy, 1998). On the one side we have a specialized database, whose content is supposed to be more accurate and precise as far as specialized information is concerned; on the other side we can assume that the generic resource guarantees a more uniform coverage as far as high level senses are concerned. These two assumptions provide us with a powerful precedence criterion to be used for managing inheritance in the integration procedure. The contribution of our work consists in a !#\" $ % &(' ) approach which allows the connecting of the generic and the specialized wordnets in a flexible and modular way. This is realized by means of a semi-automatic procedure with four main steps: (i) first, a minimum set of specialized \u201cbasic synsets\u201d is identified; (ii) basic synsets are aligned to corresponding generic synsets and a particular plug-in configuration is selected; (iii) for each plug-in configuration a merging algorithm reconstructs the corresponding portion of the integrated wordnet; (iv) possible inconsistencies are solved. There are two main benefits of this approach. First, already existing specialized resources can be connected to a generic resource without any change in the resource being necessary, a part from the data conversion into a wordnet-like format. Second, the inheritance of linguistic oriented information makes the specialized resources usable in existing wordnet-based applications. The paper is structured as follows. Section 2 presents the lexical resources we have used. Section 3 introduces the basic notions of the plug-in approach with some technical details. Section 4 describes the plug-in procedure and reports the results of an application of the approach. Section 5 places our proposal in the context of related works. * +-, . , / 0 132 .5476 8 ,91 0 2;: , 4@?BA / 45. , C D In this work we assume a taxonomic wordnetlike structure of the lexicon, where nodes in the hierarchy are synsets (i.e. synonym sets), and a rather large set of conceptual relations (e.g. Part-of, Cause, Hypernymy, Pertains-to, etc.) are available to build a semantic net among synsets. The EuroWordNet model has been adopted, which is rich enough to encompass most of the relations used in existing non wordnet-like terminological databases. We focus on the integration of already existing generic and specialized wordnets; both the data acquisition modalities and the evaluation of the quality of the resources do not affect our approach. As for generic lexical databases, typically they contain knowledge with no specific coverage and much attention is placed on coding linguistic-oriented information, such as subcategorization relations and fine-grained sense distinctions. There are several examples of existing generic lexical databases, including the English WordNet (Miller, 1990), monolingual wordnets for several European languages (e.g. Dutch, Spanish, Italian, Basque, etc.), the SENSUS (Hovy, 1998) and the Mikrokosmos (Mahesh, 1996) ontologies. Specialized databases focus on a certain domain, providing sub-hierarchies of highly specialized concepts with a limited use of lexical and linguistic relations. Synset variants tend to assume the shape of complex terms (i.e. multiwords) and the role of the domain expert is crucial for establishing correct relations. In addition high level knowlegde (i.e. the top ontology) tend to be simplified and domain oriented. Many specialized lexical databases have been developed, particularly for concrete applications, including, for example, a taxonomy for the medical domain (Gangemi et E F ., 1999), the Art and Architecture Getty Thesaurus and the Getty Thesaurus of Geographical Names. The plug-in model we present has been applied within the SI-TAL project to connect a generic wordnet and a specialized wordnet that have been created independently. GIH J K LNM O P Q R H (Roventini et S T ., 2000) created as part of the EuroWordNet project and further developed through the introduction of adjectives and adverbs, is the lexical database involved in the plug-in as a generic resource and consists of about 45,000 lemmas. U5V W X W Y@Z V []\\^W _ ` acb d is a specialized wordnet for the economic domain and consists of about 5,000 lemmas distributed in about 4,700 synsets. Table 1 summarizes the quantitative data of the two resources considered.",
    "venue": "",
    "citationCount": 16,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "abbc5ce292b72565a65bf2f79b276eda6de3c1a5",
    "url": "https://www.semanticscholar.org/paper/abbc5ce292b72565a65bf2f79b276eda6de3c1a5",
    "title": "Lexipedia : A Multilingual Digital Linguistic Database",
    "abstract": "Lexipedia, a multilingual digital linguistic database aims to provide all types and kinds of information that a linguistic item carries in a language, and its cross-linguistic morphemic equivalent in other languages. It provides a wide range of information from graphemic to idiomatic expressions and beyond. In this paper, Lexipedia is conceptualised as a model of human knowledge of language, and its description and architecture is an effort towards modelling such linguistic knowledge. I. LEXICAL DATABASE: ISSUES AND LIMITATIONS For more than 2000 years, paper dictionaries are co mpiled with a view to provide specific information that it aims to provide. Hence, there are several types of dictiona r es providing specific information depending upon the t ype of dictionary. Similarly, electronic/digital dictionar y does the same by replacing the format. An electronic diction ary, though primarily designed to provide basic informat ion such as grammatical category, meaning, usage, frequency, et ., has also got its usage in various other ancillary t sks in the newer domains of language use. Such electronic dict ionary, however, has a major shortcoming as it provides spe cific information considering the scope, usage, and stora ge for which it is developed. In other words, other differ ent kinds of information that the language users require are oft n not featured but are readily available in another dicti onary specifically created for it. In another aspect, suc h dictionary is a mere list of lexical items with its specific i nformation, and does not reflect how human beings store and pro cess such lexical items. With the advent of newer domains of language use, however, different kinds of resources are conceptua lised and designed to store information which serve as databa se for different kinds of applications and processes. One such electronic lexical database is WordNet, which organ ises words into sets of cognitively synonymous sets (oft en called synsets [1] and [2].) It stores lexical items of a language hierarchically and the conceptual-semantic and lexi cal semantic relationships between these items are dete rmined cognitively. In other words, it is a hybrid of dict ionary and thesaurus providing information of the both. Howeve r, the major concern for which Princeton cognitive psychol ogist George A. Miller developed WordNet is to model a da tab se that is consistent with the knowledge acquired abou t how human beings process language. In addition to it, W ordNet is interpreted and used as ontology. Despite its wi der use in several applications like Word Sense Disambiguation (WSD), Information Retrieval (IR), automatic text classification, automatic text summarization, etc., WordNet like other lexical databases too has its own limita tions. These databases are designed with certain specific objectives, hence, to access the detailed informati on about a particular linguistic item one has to access severa l different kinds of databases specifically meant to provide th e required specific information. For example, to access detail d information about a word ' \u0915\u0924\u093e\u092c' in Nepali, one has to access WordNet for conceptual-semantic and lexicalsemantic relations, pronunciation dictionary, or ev en separate databases for usage, idioms, proverbial us age, etc. Similarly, if one has to find its equivalent in oth er languages, one has to scan bi/multilingual dictiona ry. As it is known, accessing different databases often lead to inconsistency since each database is constructed to fulfill certain objective. Moreover, such databases are pri marily not designed to provide different kinds of informat ion that a Natural Language Process system requires. In other words, it is imperative to build a consistent, uniform, de dicated database which serves NLP applications. In section 2, the paper explores conceptual design and organisation of different fields, which are modular ised with respect to specific information. A principled basis of comparing various linguistic phenomenon across lang u ges and to achieve such an objective to avoid miss-comp arison, and in creating typological databases are the subje ct matter of the following section. Section 4 deals with the computational aspect along with the design of the b ack-end and algorithms to execute various information. One of the input interfaces is also highlighted in building su ch database. The final section is a summary. II. LEXIPEDIA: CONCEPT AND ORGANISATION In view of the above shortcomings of the lexical databases, Lexipedia is conceptualised to provide a ll and every kind of information that a particular linguis tic item in a particular language embeds, and its cross-linguis tic morphemic equivalent in other languages. Here, it i s imperative to mention that linguistic item includes free forms as well as bound forms. The latter is the res ult of grammaticalisation, a historical processes resultin g various forms, functions and constructions (see [3] and [4] ). Language in India www.languageinindia.com 11:5 May 2011 Special Volume:Problems of Parsing in Indian Languages Vijayanand Kommaluri and L. Ramamoorthy, Editors Problems of Parsing in Indian Languages 52 Lexipedia is designed to model how humans organise these linguistic items, and in turn how these items are related with each other as well as with its linguis tic usage in various other forms, functions and constructions in a language. In other words, it is designed to reflect all kinds of information that a user of a language carries overt ly/covertly over the synchronic/diachronic dimension about a pa rticular item in a language, and its morphemic equivalent ac ross languages. Lexipedia, hence, provides wide ranging information on a linguistic item which is organised in modules. Since, information that Lexipedia provides is wide and vast, it is organised into different modules, where ach module provides specific information regarding an i tem. Having such a modular architecture for information organisation has an advantage as each module can be customised according to the need of the application /users as well as for resource building. These modules are de sign d as follows: A. Graphemic An item's scriptal graphemic information is provide d following the script used for a particular language like Devanagari for Hindi, Nepali, Marathi, Bodo, etc.; Srijanga script for Lepcha, etc. It also provides spelling v ariations if an item has in a particular language. Along with it , transliteration of the item following the LDCIL transliteration scheme and the (broad) IPA transcri ption are also provided.",
    "venue": "",
    "citationCount": 2,
    "fieldsOfStudy": null
  },
  {
    "paperId": "bfa51cf7fa3e0b10893908d909cd8932d76bb7ae",
    "url": "https://www.semanticscholar.org/paper/bfa51cf7fa3e0b10893908d909cd8932d76bb7ae",
    "title": "Bootstrapping Relation Extraction Using Parallel News Articles",
    "abstract": "Relation extraction is the task of finding entities in text connected by semantic relations. Bootstrapping approaches to relation extraction have gained considerable attention in recent years. These approaches are built with an underlying assumption, that when a pair of words is known to be related in a specific way, sentences containing those words are likely to express that relationship. Therefore, sentences containing the pair of words may be used as training data for the relation extractor. We test this assumption for various relations drawn from two domains, using parallel and non-parallel corpora of news articles. We find that the assumption holds with substantially greater probability for parallel corpora. 1 Background and Related Work Due to the enormous variety of expression in natural language, semantic structures can surface in many ways. Finding each of these ways manually, either by annotating or constructing interpretation rules by hand is impractical. One goal of DARPA\u2019s Machine Reading Project is to minimize this human effort when moving to new domains. Semi-supervised bootstrapping methods in relation extraction are a natural choice because of their ability to benefit from a small amount of manual effort and a large, unannotated corpus. The Machine Reading Project\u2019s goals are much deeper than just relation extraction. Our approach is componentwise, and we consider relation extraction to be one component of the machine reading task. Other components include word-to-concept mapping including word sense disambiguation and named entity recognition, and co-reference resolution including event co-reference and cross-document coreference. These components are not totally separable, each one constrains and informs the others. We use Markov Logic Networks [Domingos et al., 2006] (MLNs) to perform joint inference over the outputs of the natural language processing components and the background knowledge. Algorithm 1 gives a general outline of relation extraction bootstrapping. This algorithm bootstraps an extractor E for a single relation, although in some cases it may be beneficial to bootstrap multiple relations together [Carlson et al., 2009]. It begins with a corpus, C, and a seed set of related pairs of instances, R, for example {(Cavaliers, 109), (Suns, 91)...} for the teamScore relation. Alternatively, we may obtain seeds from a high precision (but low recall) relation extractor, in which case we first run the relation extractor over the corpus to obtain R. The first step of the bootstrapping loop is to label all occurrences of the pairs from R in C. Then an extractor is trained to identify the labeled sentences as examples of the target relation. The process repeats with the trained extractor providing an expanded set R. Algorithm 1 A general relation bootstrapping algorithm Input: R: A set of instance pairs for a target relation. C: An unlabeled corpus. Output: E: The trained relation extractor. Procedure: repeat O \u2190 LABELOCCURRENCES(R,C) E \u2190 TRAINEXTRACTOR(O) Rprev \u2190 R R \u2190 RUNEXTRACTOR(E,C) until R = Rprev return E In a seminal work, Hearst [1992] demonstrated a means of finding hyponyms from text using an initial set of simple syntactic patterns and a semi-automatically discovered set of additional syntactic patterns. Many researchers have also used parallel (or comparable) corpora to find paraphrases [Barzilay and Lee, 2003]. Shinyama et al. [2002] used parallel news stories to extract dependency paths with similar meanings based on shared named entity fillers. A related idea, distant supervision [Mintz et al., 2009], uses a database of known facts to label a separate corpus of natural language. Rather than beginning with a small number of seed instances, distant supervision uses a large database such as Freebase [Bollacker et al., 2008] containing hundreds of thousands of entities. This approach operates not only on named entities but also nominals. For example, the /people/person/profession relation relates a (named) person to a profession. Bunescu and Mooney [2007] used a method of multiple instance learning to train a relation extractor using a similar idea. Though there was no bootstrapping, the system relied on the fact that a large enough bag of sentences containing two named entities known to be in relation will contain a sentence stating that relation. Their system began with sets of positive and negative pairs for each relation. Many sentences relating the positive pairs do not actually state the relation and none of the sentences relating the negative pairs state the relation. Therefore the penalty for misclassifying positive examples was set to be one ninth the penalty for misclassifying negative examples. This parameter was set by trial and error. They did not employ a parallel corpus. Unlike previous work we consider not just entity-entity relations but also entity-event relations such as teamInGame. In these cases the event is almost never represented by a named entity and is often not represented by a noun phrase at all. Often it is a verb that refers to an event, such as \u201cmeet\u201d, \u201cplay\u201d or \u201cwin\u201d for a game event. While previous work used either a parallel corpus or a non-parallel corpus, we use both with a common set of relations. In the context of pure relation extraction, extracting a instance pair for teamInGame such as (Packers, played) has little value. In isolation all we can conclude is that the Green Bay Packers have played at least one game. However, in the context of Machine Reading, labeling two instances in a sentence as related by a specific relation can have much higher value. A sentence such as \u201cThe Packers played the Steelers\u201d may yield the the two extractions teamInGame(Packers, played) and teamInGame(Steelers, played) where the two extractions are connected by the single instance mention of \u201cplayed\u201d. Event co-reference may also enable the system to determine the date of the game referenced by \u201cplayed\u201d if it is stated in another sentence. 1.1 The Assumption of Bootstrapping Algorithm 1 relies on the implicit assumption that when we encounter a sentence containing a pair of words known to be in a specific semantic relation, that sentence is likely to express that relation. This assumption has been at least partially validated by the success of the NLP bootstrapping methods that depend on it. The focus of this paper is more narrow than bootstrapping relation extraction. We evaluate only a single function in the general bootstrapping algorithm: LABELOCCURRENCES. By determining its precision, we measure the degree to which the implicit bootstrapping assumption holds for various relations in parallel and non-parallel corpora. 2 Automatically Constructed Parallel Corpora The Machine Reading Project motivated relation extraction on two domains: National Football League (NFL) and Intelligence Community (IC). The relations in the NFL ontology were general enough to apply to any team sport, so we will often refer to this as the Sports domain. The abundance of news articles reporting on the same event makes parallel or comparable newswire corpora an attractive resource. Dolan et al. built an aligned corpus [2004], useful in paraphrase acquisition research, from news articles. We built a large parallel corpus for both the Sports domain and the IC domain from clustered newswire. We used Google News1 to locate and cluster articles describing the same story. Each document cluster covers a single news story, often a single event such as a game or terrorist attack. By searching Google News with domain relevant keywords, a set of results similar to Figure 1 is retrieved. By following the link to \u201call 285 articles\u201d a set of documents all describing the same event can be gathered. Because both the relevance to the cluster and the quality (pagerank) of the articles decline with the number of search results retrieved, we limited the number of articles to the top ranked one third, or at most 100. Figure 1: Example of a Google News article cluster Each corpus is approximately half a gigabyte of text, gathered over about four months. On each day the corpus gathering software downloaded the clusters for the top ranked stories for each search term. The sports corpus was constructed by querying for news articles with the keywords: \u201cNFL\u201d, \u201cNBA\u201d, \u201cNCAA football\u201d, and \u201cMLB\u201d. The IC corpus was constructed from the keywords: \u201cintelligence community\u201d, \u201cconflict region\u201d, \u201cAl Qaeda\u201d, \u201cTaliban\u201d, \u201cforeign election\u201d, and \u201cinsurgent\u201d. The sports corpus contains 145,000 documents across 3861 clusters, with an average of 37.5 documents per cluster. The IC corpus is similar, with 130,000 documents, 3114 clusters and 41.5 documents per cluster. The news articles were automatically cleaned of the boilerplate typical of web articles and stripped of all HTML. The resulting text documents were segmented into sentences using Stanford\u2019s [Klein and Manning, 2003] sentence segmenter. Because of the way the corpus was gathered, we know only that the documents inside each cluster were considered to be about the same story by Google News. We do not know that documents in other clusters are not about the same story. Often, multiple consecutive days will have articles about the same event. Given our architecture, these would appear in different clusters. 3 Experimental Setup To determine the potential value of a parallel corpus for bootstrapping relation extractors, the experiment measures how consistently a relation holds between two seed instances within related news stories and across unrelated stories. Table http://news.google.com/ Relation Gloss Search Pattern NFL (Sports) Relations gameDate The game referred to as \u2019x\u2019 was played on y \u201c on \u201d gameWinner The team, x, won the game referred to as \u2019y\u2019 \u201c won \u201d gameLoser The team, x, lost the game referred to as \u2019y\u2019 \u201c lost \u201d teamInGame The team, x, played in the game referred to as \u2019y\u2019 \u201c between \u201d teamScore The team, x, scored y points \u201c scored \u201d Intelligence Commun",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": null
  },
  {
    "paperId": "c1aa337c618cae08a5ee39422bb030bf38dfa8af",
    "url": "https://www.semanticscholar.org/paper/c1aa337c618cae08a5ee39422bb030bf38dfa8af",
    "title": "Enhancing Information Services Using Machine-to- Machine Terminology Services",
    "abstract": "This paper describes the basic concepts of terminology services and their role in information retrieval interfaces. Terminology services are consumed by other software applications using machine-to-machine protocols, rather than directly by end-users. An example of a terminology service is the pilot developed by the High Level Thesaurus (HILT) project which has successfully demonstrated its potential for enhancing subject retrieval in operational services. Examples of enhancements in three such services are given. The paper discusses the future development of terminology services in relation to the Semantic Web. Terminology services A terminology server is defined in Wikipedia as \u201c... software providing a range of terminology-related software services through an Applications Programming Interface to its client applications.\u201d The services are not intended for end-users. Instead, they are to be used by computer programmers to improve client applications; that is, specific end-user services such as subject-based information retrieval interfaces. A client application will typically submit data to the terminology server along with a request for them to be processed in a specified way and the results returned to the application. The application may then further process the results before displaying them to, or otherwise interacting with, an end-user. The application software is run on the client computer, which is not the same machine as the terminology server computer. The interaction between the two sets of hardware and software is known as machine-to-machine (m2m) processing. Terminology services have been defined as \u201cWeb services involving various types of knowledge organization resources [vocabularies], including authority files, subject heading systems, thesauri, Web taxonomies, and classification schemes ... Web services are modular, Web-based, machine-to-machine applications that can be combined in various ways.\u201d An example service is given as mapping from a i From the Wikipedia definition of 21 November 2010, available at: http://en.wikipedia.org/wiki/Terminology_Server term in one vocabulary to one or more terms in another vocabulary. The OCLC Terminology Services project has developed a set of simple services involving various English subject heading systems including Library of Congress Subject Headings (LCSH), Medical Subject Headings (MeSH), and Thesaurus for Graphic Materials (TGM), although it does not currently include any mappings between these vocabularies. The services accept a client term (or its identifier) and return data about matching terms in a vocabulary specified by the client. Related terms from the vocabulary are included in the process. The client also specifies the format of the returned data, chosen to suit the needs of the client software; one of the available formats is Simple Knowledge Organization System (SKOS), a component of the Semantic Web. The previously cited Wikipedia article suggests several categories of m2m terminology service which can be expressed in terms of application functions as: \u2022 Matching user-defined text with lexical resources, including dictionaries, authority files, and thesauri. \u2022 Translations from one language to another, using controlled vocabularies and semantic mappings. \u2022 Semantic relationships within specific vocabularies used in Knowledge organization systems (KOS). \u2022 Semantic relationships between specific vocabularies using ontology mappings. These functions can be used in client applications to improve subject information retrieval interfaces for end-users. Work with the Scottish Collections Network (SCONE) and CAIRNS has suggested examples of enhancements that would benefit users. One is spell-checking user input, to trap typing errors or match spelling variants. This might be done transparently, or with feedback to the user as a \u201cDid you mean ... ?\u201d message. Another example is clarifying a user\u2019s search term when it is ambiguous relative to the one or more KOS involved: does the user intend \u201ctree\u201d to refer to forest or family? This process has been referred to as \u201cdisambiguation\u201d; it perhaps comes as no surprise to see that Wikipedia has to disambiguate it with the entry \u201cDisambiguation (disambiguation)\u201d although the default definition of \u201cWord sense disambiguation\u201d is the basis of its usage in relation to KOS. A further example is switching an uncontrolled user term to a controlled vocabulary term; as before, this may be achieved automatically, without reference to the user, or with intervention by means of a \u201cUse: ... \u201c message display. An important enhancement for union catalogues such as CAIRNS is the ability to match a user-supplied subject term to the equivalent term in each of the different vocabularies used for subject access in the different library catalogues in scope. This helps control the precision of the subsequent \u201cone-stop\u201d search across multiple heterogeneous subject headings. ii From the Wikipedia definition of 21 November 2010, available at: http://en.wikipedia.org/wiki/Disambiguation_%28disambiguation%29 HILT: High-Level Thesaurus project The High-Level Thesaurus(HILT) project started in 2000; its fourth phase was completed in May 2009. The project was funded by the UK\u2019s Joint Information Systems Committee and supported by OCLC. Its overall scope was to provide subject interoperability in a multi-scheme environment via inter-scheme mapping, with an additional goal of identifying a generic approach that could be developed through distributed collaborative action. The main objectives of the fourth phase were to research and develop pilot solutions for problems in cross-searching multi-subject schemes. A terminologies server was implemented using the Dewey Decimal Classification (DDC) as a \u201cswitching language\u201d between different Anglophone subject schemes and other vocabularies, including the DDC captions and relative index, Art and Architecture Thesaurus, UNESCO thesaurus, LCSH, MeSH, and several others. Most of the mappings are partial, created for test purposes. Some non-English terms are also mapped for similar purposes. Several m2m protocols are used by the server; in particular, its output is made available in SKOS format. The project also developed pilot embedding of some the terminology services in the user interfaces of several operational information services. These were SCONE, Intute and The Depot.",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "6f383c4795694a525bf70c878593a2e10bd01673",
    "url": "https://www.semanticscholar.org/paper/6f383c4795694a525bf70c878593a2e10bd01673",
    "title": "Informationist Science Fiction Theory and Informationist Science Fiction",
    "abstract": "ion and the symbolic conveyance of material facts. Broderick is simultaneously perspicacious on two key ontologically grounded points: that the knowledge of scientists is not just linguistic or textual, and correspondingly, that scientific language and discourse are informed by the material natural world empirically through interactions other than the linguistic and textual alone. He recognises that when scientists use narrative, metaphor and discourse in ideating about and linguistically expressing theories of the material world, they are using such literary devices fundamentally differently to fiction writers: with common veridical referents in mind and on the basis of the assumption of shared professional scientific understanding of those referents and pursuant to the development of systematic definitions de rigueur. SF texts mimic this in their references to the SF mega-text, and gravitate towards scientific writing in that they tend to exhibit heterogeneous information synthesis with a greater proportion of sources in the contributing source set supplying scientific veridical information. Broderick also sees that Snow\u2018s conception of two cultures is a journalistic simplification of reality. Of all modes of fiction writing, competent SF texts best engage with that reality, as evidenced by these same informational properties. Science and engineering engage the physical through semi-linguistic and super-linguistic tools like mathematics, spatial and diagrammatic nomological representations, special notations, nuanced technologically and experimentally-driven methodological expression and exposition. Such tools and techniques, and the super-lexical or mathematical notations associated with them, are hallmarks of theoretical and applied science, where they arise out of the need to represent as rigorously and accurately as possible \u2013 with high informational fidelity \u2013 the ontology and facts of material and natural states of affairs and processes. Diagrammatic depictions, mathematical notations, and hybridisations of the two, are an indication in a text of its mandate to engage with scientific veridical information sources. Such source types \u2013 which include the kind of biological and/or physical natural objects and processes which are the focus of hard scientific study are frequently complex and not easily accessible to human intuition, perception or common understanding. Natural language alone is not adequate to their analysis and interpretation. Hence the need for complex mathematical and visual abstractions in scientific and mathematical theorising. Nobel Laureate and physicist Richard P. Feynman is famous for promoting the use of diagrams as an alternative to pure mathematics in explicating quantum physics and relativity theory, and his Feynman diagrams revolutionised the field in the late twentieth century. One sometimes encounters directed graphs or tables in literary theoretic treatises. Certainly node graphs and special ontologically informed lexical notations are plentiful in linguistics, where phonemes must be symbolised, anthropology accounted for, and physical mouth parts depicted, but then linguistics, like psychology, often aspires to the status of a hard science. When veridical information must be captured and represented for scientific purposes, natural language is either heavily augmented by other notational techniques, or becomes adjunct to the latter, as a labelling system or for augmentative natural language explication. The discourse in most genres of adult fictional texts hardly ever admits mathematical or diagrammatic notations, although there are historical and contemporary exceptions. Exceptions are mostly found in SF. However, an approximation of the practice, and perhaps its fictive origins, are evident in fantasy writing and other older literary forms. Novels from realist, romantic and historical generic traditions sometimes incorporate diagrammatic or diagram-like objects within their texts, although they are generally closer to artistic illustration than scientific or mathematical explication. It is hard to see how Tolkien\u2018s albeit rather more artistic than cartographic fantastical maps of Middle Earth, are not diagrammatic. Such are not lexical, although typically annotated. Robert Louis Stevenson\u2018s Treasure Island includes a 393 Broderick, D. The Architecture of Babel: Discourses of Literature and Science. Carlton: Melbourne University Press, 1994. 394 Ibid., 101, 131. 395 Ibid., 125. 396 Cheng, P. C.-H, \u2015Scientific discovery with law encoding diagrams\u2016 in Creativity Research Journal, Vol. 9, Nos. 2 & 3, 145-162 Routledge Informaworld 1996 , accessed 5th November 2008., 145, 148. 397 Davies, P. \u2015Introduction\u2016 in Six Easy Pieces Massachusetts: Addison Wesley, 1996 (California Institute of Technology, 1963.), xi. 398 Examples exist in Suvin, D. Metamorphoses of Science Fiction. Yale: Yale University Press, 1980, 155, 236, 237. 399 Children\u2018s literature is another exception. CHAPTER 3 THE FICTION OF VERIDICAL, COUNTERFACTUAL AND HETEROGENEOUS INFORMATION 87 map, although Defoe\u2018s 1779 Robinson Crusoe had no map added until its fourth edition. A much later Ursula K. Le Guin, following Tolkien in the fantasy mode, includes five maps, four interlinear, in A Wizard of Earthsea (1968.) Henry Rider Haggard included a stylised fictive map intra-text in King Solomon\u201fs Mines (1885.) John Rieder sees Haggard\u2018s map as inspired by Stevenson\u2018s example and expounds at length on its political, cultural, sexual, colonial and social signification. These greater fictive signifieds, rooted in the context of the author\u2018s episteme and habitus, are exemplary of alternative semantic and informational encodings. Rieder cites the illustration of a central fictive object in Haggard\u2018s She (1887), a pottery fragment depicted with Uncial Greek inscriptions called the \u2015sherd of Amenartas\u2016, as exemplary of the text\u2018s fictive engagement with the sciences of anthropology and linguistics. These early examples of novels which engage with alternative non-lexical artefactual information types, to the extent that some small part of their informational content is non-lexical, and yet not manifest as simply artistic illustration, but formed and encoded according to nomologically attuned depictive and representational rules drawn from the sciences associated with navigation and geography. They are exemplary of the aesthetic appeal of information heterogeneity over and above discursive genre insertion. To the extent that they are not required to obey grammatical rules, nor reflect linguistic dynamics, maps and diagrams are very uncertain representations. In terms of Shannon-entropy and the ontology of artefactual information types, as symbolic depictions they are very high information sources. They are more uncertain if fictive \u2013 because they need not reflect the veridical information of any real world information source configuration. Such pseudo-informational representations have no intended direct real-world reference, and thus the possibilities for their configuration are not restricted by real contingencies or nomological limitations. Included in a fictive text, they constitute very high counterfactual or pure pseudo-information. This same technique of employing maps for world construction and narrative authenticity is rarely employed in information age and informationist SF. Of the map-bearing texts cited in the previous paragraph, only Le Guin\u2018s is information age. A notable exception is Vernor Vinge\u2018s A Fire Upon the Deep, which includes a stylised fictive cosmological map of the galaxy complete with light year scale, exploded region and two perspective views by faux orthographic projection. The inclusion of actual mathematical notation or formulas in SF is not common, even in information age SF, but texts of the SF mode arguably include this type of information more than those of any non-SF mode. Mathematics has a natural language discourse associated with it, but discourse on Bakhtin\u2018s model of speech genres is centrally something social and uttered using natural language, or derived from the same. Mathematical notations are perhaps as discursive as they are utterable. Neal Stephenson and Vernor Vinge often insert technological and especially information-technological language or \u0333computerese\u2018 within their SF texts as inserted discursive genres, for narrative aesthetic effect and fictive authenticity. A pioneer of this approach is Fred Hoyle: The Black Cloud includes numerous examples of both artefactual veridical information and artefactual counterfactual pseudo-information in the form of mathematical notation and diagrams. Vinge\u2018s A Fire Upon the Deep includes numerous examples of inserted techno-epistolary. Stephenson\u2018s novels exhibit a predilection for pedagogical explication in narrative which invests his Snow Crash with frequent intertextual forays into topics as diverse as ancient Sumerian mythology and contemporary computing programming practice. Cryptonomicon is perhaps the most striking example of well written informationist SF to date. As such, it engages heavily with scientific 400 Stevenson, R.L. Treasure Island. London: Puffin Books, 2008. (orig 1883.) 401 Le Guin, U.K. A Wizard of Earthsea. London: Penguin, 1968, 8, 9, 19, 41, 99, 171. 402 Rieder, J. Colonialism and the Emergence of Science Fiction. Middletown: Wesleyan, 2008, 23. 403 Ibid., 54-5. 404 In the sense that mathematical formulae are not designed for discursive utterance on either the Bakhtinian or traditional definition, which generally implies the use of what is termed natural language. Mathematical expressions may be more readily regarded as discourse on the archaic sense of the word, the definition of which is the process or power of reasoning. 405 Laurence Sterne\u2018s The Life and Opinions of Tristram Shandy contains numerous meta-fictional graphical line-drawing depictions of the story line itself, which are largely whimsical, but non-l",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "History"
    ]
  },
  {
    "paperId": "b24f14f99db3721230a92c888983c9009bc63b09",
    "url": "https://www.semanticscholar.org/paper/b24f14f99db3721230a92c888983c9009bc63b09",
    "title": "Knowledge-based methods for automatic extraction of domain-specific ontologies",
    "abstract": "Semantic web technology aims at developing methodologies for representing large amount of knowledge in web accessible form. The semantics of knowledge should be easy to interpret and understand by computer programs, so that sharing and utilizing knowledge across the Web would be possible. Domain specific ontologies form the basis for knowledge representation in the semantic web. Research on automated development of ontologies from texts has become increasingly important because manual construction of ontologies is labor intensive and costly, and, at the same time, large amount of texts for individual domains is already available in electronic form. However, automatic extraction of domain specific ontologies is challenging due to the unstructured nature of texts and inherent semantic ambiguities in natural language. Moreover, the large size of texts to be processed renders full-fledged natural language processing methods infeasible. \nIn this dissertation, we develop a set of knowledge-based techniques for automatic extraction of ontological components (concepts, taxonomic and non-taxonomic relations) from domain texts. The proposed methods combine information retrieval metrics, lexical knowledge-base (like WordNet), machine learning techniques, heuristics, and statistical approaches to meet the challenge of the task. These methods are domain-independent and automatic approaches. \nFor extraction of concepts, the proposed WNSCA+{PE, POP} method utilizes the lexical knowledge base WordNet to improve precision and recall over the traditional information retrieval metrics. A WordNet-based approach, the compound term heuristic, and a supervised learning approach are developed for taxonomy extraction. We also developed a weighted word-sense disambiguation method for use with the WordNet-based approach. An unsupervised approach using log-likelihood ratios is proposed for extracting non-taxonomic relations. Further more, a supervised approach is investigated to learn the semantic constraints for identifying relations from prepositional phrases. The proposed methods are validated by experiments with the Electronic Voting and the Tender Offers, Mergers, and Acquisitions domain corpus. Experimental results and comparisons with some existing approaches clearly indicate the superiority of our methods. \nIn summary, a good combination of information retrieval, lexical knowledge base, statistics and machine learning methods in this study has led to the techniques efficient and effective for extracting ontological components automatically.",
    "venue": "",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "a4dc7596b7f7e4cf21e3d8a069a48ae175545fca",
    "url": "https://www.semanticscholar.org/paper/a4dc7596b7f7e4cf21e3d8a069a48ae175545fca",
    "title": "Special Track on Applied Natural Language Processing",
    "abstract": "The rapid pace of development in natural language processing fields such as textual studies, speech recognition, speech production, text mining, and data mining (to name but a few) has led to an ever growing interest in tools able to understand, assess, organize, categorize, and extract information from natural language sources. These sources include materials gathered from libraries, the internet, natural language conversation, human-computer interaction, corpora, and any other source from which language can be gathered and analyzed. However, while excellent research continues to develop tools capable of making such analysis possible, some research must be dedicated to the applications of this technology, often applications above and beyond the original intent of the research. The FLAIRS special track on Applied Natural Language Processing (ANLP) is a forum for such research where those working in natural language processing, computational linguistics, applied linguistics, and related areas can distribute, disseminate, and discuss their findings, feelings, and future directions. Some of the many areas emphasized by the ANLP track to include for contributions include (but are not limited to) multilingual processing, learning environments, multimodal communication, bioNLP, spam filtering, language acquisition (first and second), textual assessment, language varieties, materials development, generic classification, educational applications, information retrieval, speech processing, machine learning, knowledge representations, English for specific purposes, textual assessment indices, coreference resolution, word sense disambiguation, dialogue management and systems, language generation, language models, ontologies, and reasoning.",
    "venue": "FLAIRS",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "59c38671a0bef771d40683fc138b7750a016df0c",
    "url": "https://www.semanticscholar.org/paper/59c38671a0bef771d40683fc138b7750a016df0c",
    "title": "Computational Linguistics and Intelligent Text Processing, 9th International Conference, CICLing 2008, Haifa, Israel, February 17-23, 2008, Proceedings",
    "abstract": null,
    "venue": "CICLing",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "34673a10b8b1081570c9c4b737e68c8e14ccd898",
    "url": "https://www.semanticscholar.org/paper/34673a10b8b1081570c9c4b737e68c8e14ccd898",
    "title": "Inductive Logic Programming, 16th International Conference, ILP 2006, Santiago de Compostela, Spain, August 24-27, 2006, Revised Selected Papers",
    "abstract": null,
    "venue": "International Conference on Inductive Logic Programming",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "f92a6ddd6c452c31021cc45c2fea5b88f0e48e12",
    "url": "https://www.semanticscholar.org/paper/f92a6ddd6c452c31021cc45c2fea5b88f0e48e12",
    "title": "Incorporation of Contextual Retrieval and Data Fusion Approach Towards Improving The Retrieval Precision.",
    "abstract": "Generally, the functionality of information retrieval (IR) could be divided into two \ncategories where one section deals with search and retrieval while the other \ncomponent concerns with the subject or content analysis. In the search and retrieval \npart, the IR systems present a ranked list of relevant documents depending on the \nuser submitted query as the representation of the user's information need. The ranked \nlist given indicates the probability of the document is relevant to the query by \nordering the highest relevant document at the top position and so forth. However, \nqueries are often formulated with simplified short words, such as \"Java\". These \nwords are unable to summarise precisely the user's information need and its context, \ni.e. \"java, programming language\" or \"java, the island\". Consequently, the user's \ninformation need is not satisfied as the highest relevant document was not positioned \naccordingly or too much relevant document was presented in the ranked list. \nBesides, by using the simplified query made the context is not easily extractable, and \nin recent years there has been much research interest in contextual retrieval. Likewise IR, contextual retrieval retrieved the relevant document by using the combination of \nquery, user context and search technology into a single framework. Furthermore, in \ncontextual retrieval, the user's context is exploited to differentiate the relevant \ndocument that is useful at that time the requests occur. \nOn the other hand, in order to match the queries and the document representation, \ndifferent IR schemes were applied to calculate the probability. As a result, often \nretrieval precision is different for differing IR schemes, where dissimilar lists of \nrelevant documents for the same query submitted are presented. Thus, data fusion \napproach is implemented in the IR to overcome this complication where multiple \nsources of results are combined. The implementation of data fusion approach in IR \ninvolves the merging of retrieval result from different IR schemes into a single \nunified ranked list that supposedly presents a list of high precisely relevant \ndocument. \nThis study presents an approach to incorporate contextual retrieval and data hsion \nby using a one-keyword query towards improving retrieval precision. The methods to \nidentify user context are categorised into four approaches; relevance feedback, user \nprofiles, word-sense disambiguation and knowledge engineering. In order to extract \nuser context and to model contextual retrieval, term-weighting scheme based on user \nprofiles and knowledge engineering approaches for Watson scheme and word-sense \ndisambiguation approach for Wordsieve scheme are implemented in this study. Five \nrandomly selected documents are selected and submitted to these schemes and the \nuser's context extracted is used to expand the initial query for retrieval process.In addition, the feasibility of adopting a data fusion approach was assessed in this \nstudy by testing two preconditions; --the efficacy and dissimilarity tests for the IR \nscheme candidates, as there is a possibility that the precision improvement may not \nbe accomplished. Two queries which are Java and Jaguar, expanded by using user's \ncontext extracted by Watson and WordSieve are submitted and more than ten \nthousand documents are collected as the data collection for conducting the \nexperiment. The performance of the experiment is evaluated by using three \nassessments; precision recall graph, precision evaluation based on document ranked \nand mean average precision. The data fusion experiment based on contextual \nretrieval results has reveals significant improvement on retrieval precision where the \nlowest percentage gained compared to the basic IR scheme is approximate to thirty \nseven percent, ten percent improvement compared to Watson and fifthteen percent \nimprovement compared to WordSieve based on mean average precision calculation",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1368ce3af5b5b0b8f3978df8c0c666ca3104e493",
    "url": "https://www.semanticscholar.org/paper/1368ce3af5b5b0b8f3978df8c0c666ca3104e493",
    "title": "Modeling and Using Context, 5th International and Interdisciplinary Conference, CONTEXT 2005, Paris, France, July 5-8, 2005, Proceedings",
    "abstract": null,
    "venue": "CONTEXT",
    "citationCount": 8,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "73649864035e29c9d3441f7abc9c9a47870959cb",
    "url": "https://www.semanticscholar.org/paper/73649864035e29c9d3441f7abc9c9a47870959cb",
    "title": "Text, Speech and Dialogue, 9th International Conference, TSD 2006, Brno, Czech Republic, September 11-15, 2006, Proceedings",
    "abstract": null,
    "venue": "TSD",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science",
      "History"
    ]
  },
  {
    "paperId": "9b1260912253113d9d38b41a4a7137b702801bd4",
    "url": "https://www.semanticscholar.org/paper/9b1260912253113d9d38b41a4a7137b702801bd4",
    "title": "Artificial intelligence in medicine : 8th Conference on Artificial Intelligence in Medicine in Europe, AIME 2001, Cascais, Portugal, July 1-4, 2001 : proceedings",
    "abstract": "Mediated Agent Interaction.- On Articulation and Localization - Some Sociotechnical Issues of Design, Implementation, and Evaluation of Knowledge Based Systems.- Prototype Selection and Feature Subset Selection by Estimation of Distribution Algorithms. A Case Study in the Survival of Cirrhotic Patients Treated with TIPS.- Detection of Infectious Outbreaks in Hospitals through Incremental Clustering.- Mining Data from a Knowledge Management Perspective: An Application to Outcome Prediction in Patients with Resectable Hepatocellular Carcinoma.- Discovering Associations in Clinical Data: Application to Search for Prognostic Factors in Hodgkin's Disease.- Visualisation of Multidimensional Data for Medical Decision Support.- A clustering-based constructive induction method and its application to rheumatoid arthritis.- Improving Identification of Difficult Small Classes by Balancing Class Distribution.- Credal Classification for Dementia Screening.- Evaluation of Prognostic Factors and Prediction of Chronic Wound Healing Rate by Machine Learning Tools.- Making Reliable Diagnoses with Machine Learning: A Case Study.- A Classification-Tree Hybrid Method for Studying Prognostic Models in Intensive Care.- Combining Unsupervised and Supervised Machine Learning in Analysis of the CHD Patient Database.- Coronary Heart Disease Patient Models Based on Inductive Machine Learning.- Prediction of Protein Secondary Structures of All Types Using New Hypersphere Machine Learning Method.- Integrating Different Methodologies for Insulin Therapy Support in Type 1 Diabetic Patients.- Diagnosing Patient State in Intensive Care Patients Using the Intelligent Ventilator (INVENT) System.- A User Interface for Executing Asbru Plans.- Improving HISYS1 with a Decision Support System.- Diagnosis of Iron-Deficiency Anemia in Hemodialyzed Patients through Support Vector Machines Technique.- A Visual Tool for a User-Friendly Artificial Neural Network Based Decision Support System in Medicine.- Modeling of Ventricular Repolarisation Time Series by Multi-Layer Perceptrons.- Expert Knowledge and Its Role in Learning Bayesian Networks in Medicine: An Appraisal.- Improving the Diagnostic Performance of MUNIN by Remodelling of the Diseases.- Extended Bayesian Regression Models: A Symbiotic Application of Belief Networks and Multilayer Perceptrons for the Classification of Ovarian Tumors.- The Effects of Disregarding Test Characteristics in Probabilistic Networks.- Knowledge Acquisition and Automated Generation of Bayesian Networks for a Medical Dialogue and Advisory System.- Educational Tool for Diabetic Patients Based on Causal Probabilistic Networks.- NasoNet, Joining Bayesian Networks, and Time to Model Nasopharyngeal Cancer Spread.- Using time-oriented data abstraction methods to optimize oxygen supply for neonates.- Visual Definition of Temporal Clinical Abstractions: A User Interface Based on Novel Metaphors.- Using Temporal Probabilistic Knowledge for Medical Decision Making.- Temporal Issues in the Intelligent Interpretation of the Sleep Apnea Syndrome.- Generating Symbolic and Natural Language Partial Solutions for Inclusion in Medical Plans.- Using Part-of-Speech and Word-Sense Disambiguation for Boosting String-Edit Distance Spelling Correction.- Semantic Interpretation of Medical Language - Quantitative Analysis and Qualitative Yield.- Medical Knowledge Acquisition from the Electronic Encyclopedia of China.- Quantitative and Qualitative Approaches to Reasoning Under Uncertainty in Medical Decision Making.- Comparison of Rule-Based and Bayesian Network Approaches in Medical Diagnostic Systems.- Parts, Locations, and Holes - Formal Reasoning about Anatomical Structures.- Abductive Inference of Genetic Networks.- Interface of Inference Models with Concept and Medical Record Models.- Integrating Ripple Down Rules with Ontologies in an Oncology Domain.- Towards a Simple Ontology Definition Language (SOntoDL) for a Semantic Web of Evidence-Based Medical Information.- Knowledge Acquisition System to Support Low Vision Consultation.- Systematic Construction of Texture Features for Hashimoto's Lymphocytic Thyroiditis Recognition from Sonographic Images.- Dynamic Adaptation of Cooperative Agents for MRI Brain Scans Segmentation.- Numeric and Symbolic Knowledge Representation of Cortex Anatomy Using Web Technologies.- Depth-Four Threshold Circuits for Computer-Assisted X-ray Diagnosis.- Management of Hospital Teams for Organ Transplants Using Multi-Agent Systems.- An AI-Based Approach to Support Communication in Health Care Organizations.- TAME Time Resourcing in Academic Medical Environments.- A Conceptual Framework to Model Chronic and Long-Term Diseases.- Modelling of Radiological Examinations with POKMAT, a Process Oriented Knowledge Management Tool.- A Multi-Agent System for Organ Transplant Co-ordination.- A Platform Integrating Knowledge and Data Management for EMG Studies.- Using ONCODOC as a Computer-Based Eligibility Screening System to Improve Accrual onto Breast Cancer Clinical Trials.- Using Critiquing for Improving Medical Protocols: Harder than It Seems.- Evaluating the Impact of Clinical Practice Guidelines on Stroke Outcomes.- Challenges in Evaluating Complex Decision Support Systems: Lessons from Design-a-Trial.- On the Evaluation of Probabilistic Networks.- Evaluation of a Case-Based Antibiotics Therapy Adviser.",
    "venue": "",
    "citationCount": 5,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "7e10088427e5c7c4234db64f3283e63a1a7d2042",
    "url": "https://www.semanticscholar.org/paper/7e10088427e5c7c4234db64f3283e63a1a7d2042",
    "title": "Designing Online Experiences: Beyond the Tyranny of Usability",
    "abstract": "This paper critically examines the widely accepted principles of usability in web design. In particular, it investigates the work of Jakob Nielsen, whose name has become synonymous with 'user-friendliness' as a result of popular applications of his 'ten usability heuristics' and argument that Flash is '99% bad'. It interrogates the assumptions embedded in his research, especially the notion that web consumption is predominantly a utilitarian activity. In an infant discipline such as web design, what are the implications for students and practitioners' creative output when such ideas become its theoretical canon? What are the foreseeable consequences when knowledge in the field is disseminated by industry practitioners, such as Nielsen and other usability experts, to fellow multimedia designers? The paper explores the possible outcomes of the dot.com industry preaching its own practice, particularly when this in turn is fed back into the education of prospective web professionals. What can be borrowed from other design disciplines that can contribute to a rethinking of online interactions as more than task-driven? For example, in terms of industrial design, a web site can be considered a product; while architecturally, it can be perceived as a virtual space. It has only been recently that the concept of \u2018experience design\u2019 has emerged in web design, which suggests an approach which both contests and extends 'rules' of usability. This seems indicative of broader disciplinary differences between the abovementioned fields and the design of information systems, from which the area of usability has developed. The paper attempts to chart the historical trajectory of ontologies of web design and reconcile these with approaches from other design disciplines as a way of establishing alternatives to the taken-for-granted mantra that 'usability is king'. Introduction Within the dot.com industry, Dr Jakob Nielsen has attained guru status, having been variously described as \u2018the world\u2019s leading expert on web usability\u2019, \u2018the smartest person on the web\u2019, and someone who \u2018knows more about what makes websites work than anyone else on the planet\u2019 (Goto and Cotler 2002: 209). This paper critically reviews his ideas and work, particularly his representation of web design and usability as oppositional forces. In such a contest, when usability has won the battle with design, the result is \u2018relentlessly sensible\u2019 design. The paper attempts to interrogate the assumptions embedded in the concept of usability as defined by Nielsen\u2019s research, especially the notion that web consumption is predominantly a utilitarian activity, and that a simple formulaic approach will produce a \u2018common sensibility\u2019 in the design of online experiences. The intention is not to criticise usability per se: that is, I am not commenting on the usability movement or the musings of other usability experts. Nor am I advocating \u2018unusability\u2019 in web design. However, what I do want to explore is the elevation of Nielsen\u2019s ideas to the canon in the new discipline of web design and development to the extent that it does not allow for innovation or other ways of thinking about online experiences. What can be borrowed from other design disciplines that can contribute to a rethinking of online interactions as more than taskdriven? The paper attempts to chart the historical trajectory of ontologies of web design and reconcile these with approaches from other design disciplines as a way of establishing alternatives to the taken-for-granted mantra that \u2018usability is king\u2019. Finally, Nielsen\u2019s ideas are contextualised politically, so that they are understood in relation to the audiences of his work and the conditions under which that work was created. What are the consequences when knowledge in the field of web design is predominantly disseminated by a single/few experts to industry practitioners and used in the education of prospective web professionals? Moreover, what happens when this occurs across disciplinary boundaries, from engineers to designers, but without any actual interdisciplinarity? \u2018I am not a visual designer\u2019: usability as the binary opposite of design Nielsen refers to \u2018usability engineering\u2019 (1994), which suggests that usability is a quantifiable attribute that can be constructed using specific methods. Moreover, it is indicative of the position from which he critiques design: he defines and interrogates design from a self-proclaimed engineer\u2019s perspective. \u2018In past years, the greatest usability barrier was the preponderance of cool design. Most projects were ruled by usability opponents who preferred complexity over simplicity... Happily, glamourbased design has lost and usability advocates have won the first and hardest vistory.\u2019 (Nielsen 5 August 2001) In other words, according to Nielsen, designers are \u2018usability opponents\u2019 who are at war with engineers of usability. Designers prefer complexity, while usability experts embrace simplicity. This relationship is not only mutually exclusive, but hierarchical in that usability is given the higher ground. There is no room for a balanced or even integrated relationship between design and usability, as seen in Nielsen\u2019s (25 July 2000) call for the \u2018end of web design\u2019. If \u2018usability is king\u2019 (Grabham 2003; Goldberg 2003; Guldman 2002) as is often claimed, then the king is a tyrant. It is this privileging of usability to the extent that there is little need for design that I refer to as a tyranny. For Nielsen (29 October 2000), where the work of designers dominate, \u2018usability disease\u2019 ensues, implying that the superiority of usability has been abused through excessive or over-indulgent design. An otherwise healthy web site has been infected with design: \u2018Websites must tone down their individual appearance and distinct design in all ways...\u2019 (Nielsen 23 July 2000) The simplicity with which Nielsen characterises usability issues is also applied in his reductive definition of web design as that which makes web sites attractive or visually outstanding. To demonstrate that he practices the simplicity that he preaches, Nielsen\u2019s own web site, www.useit.com, has practically no graphics. He justifies this by asserting that in order for web pages to download within 10 seconds on a dialup modem, they need to be no more than 30Kb thereby ruling out the inclusion of most graphics. Nielsen advocates a removal of anything that makes a site differentiable, but is not useful. He recommends making a site memorable through its usability, but not visually. This \u2018practice of simplicity\u2019 pervades Nielsen\u2019s work, from the ways in which he proposes to measure usability through ten usability heuristics through to his over-simplification of design to merely \u2018making things pretty\u2019. In what has been described as his \u2018relentlessly sensible\u2019 approach, creating good online experiences is distilled into ten rules, one of which is aesthetic and minimalist design. Complexity is sacrificed to the extreme pragmatism of usability, such that everything is easy, both for the designer and the user. As in the book title of another usability expert, Steve Krug (2000), \u2018don\u2019t make me think!\u2019: where the intention of usability is to make the achievement of online tasks easy and efficient for users, the design process is depicted as having little use and requiring minimal thought. Usability as ease-of-use \u2018Usability is a quality attribute that assesses how easy user interfaces are to use. The word \u201cusability\u201d also refers to methods for improving ease-of-use during the design process.\u2019 (Nielsen 25 August 2003) This notion of usability as ease-of-use contains a number of assumptions, the main one being that web interactions are essentially utilitarian activities. If the design of online experiences is seen only in utilitarian terms, then Nielsen\u2019s ideas pertain only to particular types of web experiences, not all. Nielsen himself implies this by referring to big name sites such as Amazon.com as case studies. \u2018On the Internet, it\u2019s survival of the easiest: if customers can\u2019t find a product, they can\u2019t buy it.\u2019 (Nielsen and Norman 14 February 2000) Therefore, ease of use is the key issue where e-commerce is concerned. However, Nielsen does note that it is more than Amazon.com\u2019s usability which makes it a successful web site, namely incentives to return and get discounts. There are also other genres of sites, such as online games, which are not constructed for purely utilitarian purposes in the sense that finding out how to use the controls is part of the objective: making usability a challenge is a deliberate aspect of the design. The need for speed in online interactions, as proposed in one of Nielsen\u2019s ten usability heuristics, often accompanies ease of use in recommendations about usability. That is, it is argued that ease of use and expediency go hand-in-hand in enabling the user to achieve their online goals as quickly and smoothly as possible. Again, this immediately restricts the types of web sites and interactions which relate to Nielsen\u2019s arguments about usability. For example, browsing connotes an activity which allows the user to work at their own pace, to muse upon their findings before making a decision. Therefore, Nielsen\u2019s assertion that web sites must have \u2018zero learning time or die\u2019 (Nielsen 25 July 2000) does not seem relevant to an online situation which involves browsing, learning and reflecting in a way which is not hurried or pressing. This is supported by Soloway and Prior (1996) who assert that, whether a design is good or bad, users will persist, particularly if they have a goal in mind. Nielsen and Tognazzini do acknowledge that the web browser is inappropriate for the easy and speedy qualities which constitute good usability: \u2018At the risk of repeating an old saw, when you only have a hammer, everything looks like a nail... Our hammer has been the Web browser... The browser is a useful tool. It needs to cease being the only tool, and ",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "7847fdf04aa682f67b79a52b9fad4d3d5739bfb6",
    "url": "https://www.semanticscholar.org/paper/7847fdf04aa682f67b79a52b9fad4d3d5739bfb6",
    "title": "Content-driven Knowledge management Tools through Evolving Ontologies Deliverable 5-1-Information Extraction : State-ofthe-Art Report",
    "abstract": "ing and Summarising: DRE can summarise documents and augment them with hyperlinks that point to related texts. Visualisation: A graphical interface is available that allows a unified view on different types of 18 Cf. Chomsky (1966) and Chomsky (1968). IST Project IST-1999-10132 On-To-Knowledge On-To-Knowledge: Content-driven Knowledge management Tools through Evolving Ontologies Deliverable 5 74 documents (emails, reports, news groups, etc.). Comparison and Search: Autonomy provides User Profiling, Document Retrieval, Cross Referencing. All these applications are based on the DRE Concept extraction and comparison engine. Indexing and classification: Categorisation is also provided by Autonomy. In such a scenario a specific agent maintaining an interest model each represent its own category. Documents satisfying the similarity requirements are categorised according to the most similar agent. Virtues and shortcomings of Autonomy\u2019s approach The merit of Autonomy\u2019s approach probably is its departure from traditional linguistic approaches, and thereby implementing an approach that is general and pragmatic enough to perform a relatively good job in real world practical settings. In all its principles the approach is rather unconventional, seen from a computer linguistics point of view, which leads to the effect that the approach has some unique opportunities for dealing with scenario\u2019s that are more problematic when using traditional IR approaches. Most prominent of these unique opportunities is its (relative) language independence, due to the fact that no grammatical knowledge is required for analysis. On the other hand there are some disadvantages with the approach Autonomy takes: the narrative to compare to has to have a certain size in order for statistical approaches to work, the effects of small \u201cinterest statement\u201d in the size of a single sentence can be that irrelevant parts of the \u201cinterest statement\u201d are selected for comparison with target documents. related to that is the fact that there are many mismatches and irrelevant documents found. there is no explicit knowledge built up. Therefore it is hard to see how Autonomy\u2019s DRE can be used for knowledge induction and generation of knowledge bases, The main problem with Autonomy\u2019s lack of \u201cdeep\u201d understanding of texts and documents seems to be the fact that especially \u201csingle line question\u201d scenarios (cf. AskJeeves) seems to be in need of such \u201cdeeper\u201d understanding, since a deeper understanding can be compared to a kind of query extension where available background knowledge is used for annotating a single line query. With only probabilistic and information theoretical information available, an analysis of such a short text will easily by confused by irregular sentences and incidental formulations (cf. Appendix B: test of Autonomy). 19 Tests showed that in case of small, single sentence questions led to the selection of single words or phrases like \u201csome\u201d or \u201csomething like\u201d where selected as pertinent to the stated interest, which in the current case led to the effect that selected documents where also mentioning these utterances, thereby completely missing the context of the question (cf. Appendix B: test of Autonomy). IST Project IST-1999-10132 On-To-Knowledge On-To-Knowledge: Content-driven Knowledge management Tools through Evolving Ontologies Deliverable 5 75 2.2.7 CORPORUM and Mimir CORPORUM supports both personal and enterprise wide document and information management that is management by content. The CORPORUM system is founded on CognIT's M\u00edm\u00edr technology developed in Norwegian research labs. This technology focuses on meaningful content rather than odd data or standardised document parameters. CognIT\u2019s mission is to capture the content with respect to the interest of the individual rather than address the document itself. There are three essential aspects of this. \u2022 CORPORUM interprets text in the sense that it builds ontologies that reflect world concepts as the user of the system sees and expresses this. This ontology constitutes a model of a person\u2019s interest or concern. \u2022 The interest model is applied as a knowledge base in order to determine contextual and thematic correspondence with documents presented before it. The interest model and the text interpretation process drive an information extraction process that characterises the hit in terms of relevance and in terms of content. This information can be stored in a persistent database for future reference. Figure 30: Corporum core competence areas (cf. Figure 18). The CORPORUM software consists of a linguistic component, taking care of tasks as lexical analysis (tokenisation, part-of-speech tagging and possibly lexicon application), morphological analysis (collation, inflection) and analysis at the syntactical level (grammar application). At the semantic level (cf. Figure 11), CORPORUM performs word sense disambiguation by describing the context in which a particular word is being used. Doing so is naturally closely related to knowledge representation issues. CORPORUM is able to augment \u201cmeaning\u201d structures with concepts that are invented from the text. The core of the CORPORUM system (the MiMir engine) is also able to extract the information most pertinent to a specific text for summary creation, extract the so called Core Concept Area from a text and represents results according to ranking which is based on its interestingness towards a specific contextual theme set by a user. On top of that, the CORPORUM system is able to generate explanations, which will allow a user to make an informed guess on which documents to look at and which to ignore. CORPORUM can point to exactly those parts of the targeted documents that are most pertinent to a specific user\u2019s interest. Figure 30 shows the competence area\u2019s of the CORPORUM software. Input Text Text Corpus Comparisons Semantic Analysis IST Project IST-1999-10132 On-To-Knowledge On-To-Knowledge: Content-driven Knowledge management Tools through Evolving Ontologies Deliverable 5 76 2.2.7.1 Overall architecture Figure 31: The overall CORPORUM architecture The overall architecture of the CORPORUM system is depicted in Figure 31. It consists of 4 basic software components each coming with a well-defined API. This implies that CORPORUM can be reconfigured compared to the depiction given here. It also implies that each of the components can be reused in a different setting. This is especially important for the part that contains the MIMIR based engine. This is the heart of the CORPORUM system, but it may operate on an independent basis as long as its API is observed. Because of this it is possible to include a subset of the components shown in various other configurations. One example could be an ERP system that needs contextual indexing of different types of documents. Figure 31 shows the several components that the architecture consists of. The user may have access to CORPORUM from any web-browser hooked up on the net. Access to CORPORUM is given through a regular ASP functionality that communicates with a Web Data Server. The Web Data Server handles the interface to the database. CORPORUM is suited with a standard relational database. However, any database can be applied. A change to this part will not have any effect on 2 INTERN",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": null
  },
  {
    "paperId": "ae1a38c8b9056ad953ed9fe977e35f6bff2d2eeb",
    "url": "https://www.semanticscholar.org/paper/ae1a38c8b9056ad953ed9fe977e35f6bff2d2eeb",
    "title": "Semantic tagging of a corpus using the Topic Navigation Map standard",
    "abstract": "Our work carried out as part of the Scriptorium project has confronted us with a variety of problems which shed light on important issues related to corpus architectural design, such as the definition of fine-grained textual units, extraction of relevant subsections of the corpus, and in particular linking techniques enabling , text annotation with arbitrary and at times conflicting meta-data. The need for greater flexibility and expressive power for our annotation schemes has led us to apply linking techniques as defined in HyTime (ISO 10744:1992) and Topic Maps (ISO13250). We have used these techniques in particular to construct a semantic map of the corpus which enables hypertext navigation in terms of the topics inductively acquired through text-mining software. Navigation is aided by a 3D geometric representation of the semantic space of the corpus. 1. What is Scriptorium ? Scriptorium (Lahlou et al., 1995a), is a project developed in the Research & Development Division of EDF (Electricit\u00e9 de France) in collaboration with ENS (Ecole Normale Sup\u00e9rieure). The aim of this project is to identify prominent and emerging topics from the automatic analysis of the discourse of the company's (EDF) different social players (managers, trade-unions, employees, etc. by way of textual data analysis methods. The corpus under study in this project has 8 million words and is very heterogeneous (it contains book extracts, corporate press, union press, summaries of corporate meetings, transcriptions of taped trade union messages, etc. All documents are SGML tagged following the TEI (Dunlop, 1995) recommendations. Each document is provided with a descriptive header and is segmented into minimal textual units or chunks (which correspond to paragraphs). Scriptorium is a modular architecture which provides an open framework where different text-mining software can be plugged in. At present the following software has been used : ALCESTE (Reinert, 1987) and ZELLIG (Habert et al. 1999). The results of these text mining tools are integrated into the architecture as structured annotation layers pointing at the relevant locations in the corpus. 2. Current approaches in semantic annotation Increasing availability of textual data in electronic form has made semantic access to document collections a crucial issue. Semantic annotation of corpora is mainly performed today on the basis of pre-existing categories which are defined in external resources and then projected onto the corpus. These resources include lexical resources such as dictionaries, thesauri or domain-specific terminologies and conceptual resources such as generalpurpose or application-specific ontologies. Work in the field of Word Sense Disambiguation (WSD) for instance, exploits lexical resources with the aim of selecting the relevant meaning of a word in a given context from a pre-established list of all the word's potential meanings. Reducing ambiguity by tagging each word of a corpus with its contextual meaning is then exploited in Information Retrieval (IR) to increase the precision of the results generated in response to a query. Semantic ontologies and networks are also exploited in view of enhancing navigation or IR performance. To date, the most widespread lexical semantic network is WordNet (Miller et al., 1990), developed at Princeton University since 1985. The nodes of this network are lists of synonyms or synsets. Each synset represents a meaning which can be lexicalized through a set of synonymous words. A meaning is defined differentially in terms of the relations a given synset has with other synsets of the network. The network is partitioned into different areas which correspond to parts of speech (POS) categories (nouns, adjectives, verbs ). Different types of relations structure the network which depend on the POS category. For instance, hyponomy relations are defined for nouns, antonomy for adjectives, implication for verbs, etc. The network is essentially conceptual as the relations are established between meanings and not between lexical items. Corpus annotation with WordNet categories has been used to optimize navigation and IR, in particular to expand queries by generating synonyms that can be inferred through the relations of the network. This is aimed at improving recall in response to a query. Semantic annotation can also be performed on the basis of an application-specific conceptual model. Given the growing merging between XML and database technologies and the increasing use of XML as a standard format for exchanging data among heterogeneous computer systems, document markup is used for annotating metadata whose structure is described in an external model comparable to a database schema. This model specifies the entities and relations of a given application domain. In this approach too, semantic annotation is based on a pre-defined categorization scheme, which is in this case similar to the fields and records of a database table. In contrast to the approaches described above, the focus is different within the context of document-based systems aimed at technology watch (or social watch as is the case for Scriptorium), given that the aim is not to access the corpus by means of a pre-defined list of topic categories, but to identify emerging topics in the corpus. Furthermore, the nature of these topics is also different, as they are defined inductively by text mining tools based on data-driven methods. They are in consequence contextual and endogenous to a given corpus. It is essential for text mining software to run on homogeneous corpora in order to yield relevant results. As we have pointed out above, our document collection is extremely heterogeneous. The concepts underlying the documents cannot be represented in one single semantic space. Therefore we do not attempt to build a semantic representation of the entire corpus through one only analysis. Our approach therefore consists in building subcorpora of exploitable size which are homogeneous with respect to a given parameter. To this aim, we use an extractor developed using the XML Python libraries which dynamically assembles subsets of text chunks in response to a query. This extractor runs queries concerning the descriptive parameters stored in each document's header as well as fulltext searching constraints. These sub-corpora are then analyzed by the text mining tools, which at present include ALCESTE and ZELLIG. In the following two sections we show how we construct semantic classes with ALCESTE and exploit them to construct a navigable topic map overlaid on the corpus. 3. Generating semantic classes with ALCESTE ALCESTE is a textual data analysis software developed by M. Reinert at CNRS. This program performs a classification of the text chunks of a given corpus in order to produce classes of statistically related chunks. The notion of a chunk corresponds to that of an elementary textual sequence of variable size. In our case we have chosen the paragraph, as that is our basic segmentation unit. Similarity of text chunks is based on a distributional criteria. It depends on the number of word stems that chunks have in common, taking into account the frequency of occurrence of the words in the corpus. Classification is based on a hierarchical descending clustering algorithm in which successive dichotomies are carried out using the first axis of a factor analysis. Details on the algorithm can be found in (Reinert, 1985). The important point is that the method is contrastive, in other words a class of chunks is formed in opposition to the rest, not in terms of an absolute criteria. After these classes have been determined, the program seeks the list of words most characteristic or specific to each class according to a chi2 metric. This also produces a geometric representation of classes and words in the space, based on the axis yielded by the factor analysis. We use the coordinates of the classes and words on the first three axis to obtain a 3D representation of the semantic space of a given ALCESTE analysis. The resulting ALCESTE classes reveal underlying representations or concepts which are lexicalized through a set of related chunks and a characteristic vocabulary. These classes cannot be interpreted as absolute semantic categories but rather as points of view or semantic contexts relative to the sub-corpus under study. More on the interpretation of ALCESTE classes in (Lahlou, 1995b). 4. The Topic Navigation Map Standard Topic Navigation Maps (TNM) is an international standard (ISO/IEC 13250) providing a language expressed in SGML and HyTime to construct a layer of topics and relations aimed at classifying and semantically annotating a collection of documents. A topic map is implemented as a set of independent links. A topic map can be compared to a semantic network overlaid on a pool of textual data. The important point is that there is a separation between the textual data and the topic map structure. As an independent structure, a topic map can be associated to different textual resources; likewise, and more relevant to our aims, different topic maps can be layered over the same document collection providing thus multiple views on the same corpus. A topic map structure is optimized for navigation, but in contrast to HTML provides the mechanisms for explicitly defining a semantic model for this structure, aimed at for instance, describing the relation expressed by a link. The main element of a Topic Map is a topic which is an independent multi-headed link pointing to a set of occurrences. A topic link groups all textual occurrences (of any granularity) which are related to the topic. A topic has a topic type (or multiple topic types), which correspond to link types in HyTime. A topic occurrence is any addressable textual unit in the document collection. Topic occurrences can be described by the role they play. This corresponds to the facility provided in HyTime for describing the role",
    "venue": "",
    "citationCount": 1,
    "fieldsOfStudy": null
  },
  {
    "paperId": "b43d73ff5c96a4cd022df3d0aaa78aff58cda739",
    "url": "https://www.semanticscholar.org/paper/b43d73ff5c96a4cd022df3d0aaa78aff58cda739",
    "title": "Logic programming : 19th International Conference, ICLP 2003, Mumbai, India, December 9-13, 2003 : proceedings",
    "abstract": "Invited Talks.- Achieving Type Safety for Low-Level Code.- Logic Information Systems for Logic Programmers.- A Logic Programming View of Authorization in Distributed Systems.- Compositional Verification of Infinite State Systems.- A Constraint-Based Approach to Structure Prediction for Simplified Protein Models That Outperforms Other Existing Methods.- Invited Tutorials.- Concurrency, Time, and Constraints.- Symbolic Model-Checking for Biochemical Systems.- Component-Based Software Development and Logic Programming.- A Tutorial on Proof Theoretic Foundations of Logic Programming.- Regular Papers.- Objective: In Minimum Context.- Handling Existential Derived Predicates in View Updating.- Efficient Evaluation of Logic Programs for Querying Data Integration Systems.- Argumentation Databases.- Order and Negation as Failure.- Computing Minimal Models, Stable Models, and Answer Sets.- Uniform Equivalence of Logic Programs under the Stable Model Semantics.- Answer Set Programming Phase Transition: A Study on Randomly Generated Programs.- Termination Analysis with Types Is More Accurate.- A Propagation Tracer for GNU-Prolog: From Formal Definition to Efficient Implementation.- Intensional Sets in CLP.- Implementing Constraint Propagation by Composition of Reductions.- Forward versus Backward Verification of Logic Programs.- Native Preemptive Threads in SWI-Prolog.- Flow Java: Declarative Concurrency for Java.- On the Complexity of Dependent And-Parallelism in Logic Programming.- Higher-Order Substitution Tree Indexing.- Incremental Evaluation of Tabled Logic Programs.- On Deterministic Computations in the Extended Andorra Model.- Timed Concurrent Constraint Programming: Decidability Results and Their Application to LTL.- Is There an Optimal Generic Semantics for First-Order Equations?.- Loop Formulas for Disjunctive Logic Programs.- Default Knowledge in Logic Programs with Uncertainty.- Posters.- A Generic Persistence Model for (C)LP Systems.- Definitions in Answer Set Programming.- A New Mode Declaration for Tabled Predicates.- Adding the Temporal Relations in Semantic Web Ontologies.- Polynomial-Time Learnability from Entailment.- Integration of Semantic Networks for Corpus-Based Word Sense Disambiguation.- Development and Application of Logical Actors Mathematical Apparatus for Logic Programming of Web Agents.- A Real Implementation for Constructive Negation.- Simulating Security Systems Based on Logigrams.- Online Justification for Tabled Logic Programs.- Inducing Musical Rules with ILP.- A Distinct-Head Folding Rule.- Termination Analysis of Logic Programs.- Refactoring Logic Programs.- Termination of Logic Programs for Various Dynamic Selection Rules.- Adding Preferences to Answer Set Planning.- Controlling Semi-automatic Systems with FLUX.- The Language Model LMNtal.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "632f3ffb49d22dec24e823ea67b6d0d6f26000a3",
    "url": "https://www.semanticscholar.org/paper/632f3ffb49d22dec24e823ea67b6d0d6f26000a3",
    "title": "Advances in artificial intelligence - IBERAMIA 2002 : 8th Ibero-American Conference on AI, Seville, Spain, November 12-15, 2002 : proceedings",
    "abstract": "Knowledge Representation and Reasoning.- Improving Naive Bayes Using Class-Conditional ICA.- Detecting Events and Topics by Using Temporal References.- Asymmetric Neighbourhood Selection and Support Aggregation for Effective Classification.- Incremental Learning of Tree Augmented Naive Bayes Classifiers.- A Comparison of PCA and GA Selected Features for Cloud Field Classification.- Filtering Noisy Continuous Labeled Examples.- Designing Adaptive Hypermedia for Internet Portals: A Personalization Strategy Featuring Case Base Reasoning with Compositional Adaptation.- Improving Classification Accuracy of Large Test Sets Using the Ordered Classification Algorithm.- A Comparative Study of Some Issues Concerning Algorithm Recommendation Using Ranking Methods.- Properties and Complexity in Feasible Logic-Based Argumentation for Electronic Commerce.- A Hybrid CBR Model for Forecasting in Complex Domains.- Generalized Modifiers as an Interval Scale: Towards Adaptive Colorimetric Alterations.- A Conceptual Graph and RDF(S) Approach for Representing and Querying Document Content.- Automatic Optimization of Multi-paradigm Declarative Programs.- SLFD Logic: Elimination of Data Redundancy in Knowledge Representation.- Indeed: Interactive Deduction on Horn Clause Theories.- Restricted ?-Trees and Reduction Theorems in Multiple-Valued Logics.- Max-CSP Approach for Software Diagnosis.- Machine Learning.- Local Search Methods for Learning Bayesian Networks Using a Modified Neighborhood in the Space of DAGs.- A Quasi-Metric for Machine Learning.- Shared Ensemble Learning Using Multi-trees.- A GRASP Algorithm for Clustering.- An Analysis of the Pheromone Q-Learning Algorithm.- SOAP: Efficient Feature Selection of Numeric Attributes.- Uncertainty and Fuzzy Systems.- Designing Fuzzy Relations in Orthogonal Persistence Object-Oriented Database Engines.- An Interactive Framework for Open Queries in Decision Support Systems.- Integration of Fault Detection and Diagnosis in a Probabilistic Logic Framework.- Series-Parallel and Tree-Decomposition Approaches for Fuzzy Constraint Networks.- Change-Detection Using Contextual Information and Fuzzy Entropy Principle.- Improving Simple Linguistic Fuzzy Models by Means of the Weighted COR Methodology.- A Semiquantitative Approach to Study Semiqualitative Systems.- Genetic Algorithms.- Multi-objective Optimization Evolutionary Algorithms Applied to Paroxysmal Atrial Fibrillation Diagnosis Based on the k-Nearest Neighbours Classifier.- Population Studies for the Gate Matrix Layout Problem.- New Generic Hybrids Based upon Genetic Algorithms.- Genetic Algorithms and Biological Images Restoration: Preliminary Report.- Evolution of Multi-adaptive Discretization Intervals for a Rule-Based Genetic Learning System.- An Immunological Approach to Combinatorial Optimization Problems.- A Genetic Algorithm for Solving a Production and Delivery Scheduling Problem with Time Windows.- A Prediction System for Cardiovascularity Diseases Using Genetic Fuzzy Rule-Based Systems.- Multiple Crossover per Couple with Selection of the Two Best Offspring: An Experimental study with the BLX-? Crossover Operator for Real-Coded Genetic Algorithms.- Neural Nets.- Adaptive Random Fuzzy Cognitive Maps.- Convex Hull in Feature Space for Support Vector Machines.- Applying Neural Networks and Genetic Algorithms to the Separation of Sources.- A Neural Associative Pattern Classifier.- Rule Extraction from Radial Basis Function Networks by Using Support Vectors.- Empirical Performance Assessment of Nonlinear Model Selection Techniques.- An Efficient Neural Network Algorithm for the p-Median Problem.- Improving Cellular Nonlinear Network Computational Capabilities.- Learning to Assess from Pair-Wise Comparisons.- Forecasting Time Series Combining Machine Learning and Box-Jenkins Time Series.- Gaussian Synapse Networks for Hyperspectral Image Segmentation.- An Associative Multivalued Recurrent Network.- Machine Learning Models for Online Dynamic Security Assessment of Electric Power Systems.- On Endmember Detection in Hyperspectral Images with Morphological Associative Memories.- Application of Learning Machine Methods to 3D Object Modeling.- Distributed Artificial Intelligence and Multi-agent Systems.- Enriching Information Agents' Knowledge by Ontology Comparison: A Case Study.- Negotiation among Autonomous Computational Agents.- Interface Agents Development in MASA for Human Integration in Multiagent Systems.- Comparing Distributed Reinforcement Learning Approaches to Learn Agent Coordination.- Empowered Situations of Autonomous Agents.- Distributed Agenda Management through Decentralised Vowels Co-ordination Approach.- Meta-modelling in Agent Oriented Software Engineering.- Multi-agent Systems and Network Management- A Positive Experience on Unix Environments.- Formal Specification of Opinions Applied to the Consensus Problem.- Natural Language Processing.- Practical NLP-Based Text Indexing.- Definite Description Resolution Enrichment with WordNet Domain Labels.- A Hidden Markov Model Approach to Word Sense Disambiguation.- A Simple Connectionist Approach to Language Understanding in a Dialogue System.- Wide-Coverage Spanish Named Entity Extraction.- Terminology Retrieval: Towards a Synergy between Thesaurus and Free Text Searching.- Mixed Parsing of Tree Insertion and Tree Adjoining Grammars.- Task Oriented Dialogue Processing Using Multiagents Theory.- Automatic Adaptation of a Natural Language Interface to a Robotic System.- Intelligent Tutoring Systems.- Semantic Comparison of Texts for Learning Environments.- I-PETER: Modelling Personalised Diagnosis and Material Selection for an Online English Course.- An Agent-Based System for Supporting Learning from Case Studies.- Emergent Diagnosis via Coalition Formation.- Adaptive Bayes.- Control and Real Time.- A Dynamic Scheduling Algorithm for Real-Time Expert Systems.- A Process Knowledge-Based Controller for Maneuverability Improvement of a Nonlinear Industrial Process.- An Architecture for Online Diagnosis of Gas Turbines.- STeLLa v2.0: Planning with Intermediate Goals.- Scheduling as Heuristic Search with State Space Reduction.- Domain-Independent Online Planning for STRIPS Domains.- A Pomset-Based Model for Estimating Workcells' Setups in Assembly Sequence Planning.- New Methodology for Structure Identification of Fuzzy Corollers in Real Time.- Robotics.- Comparing a Voting-Based Policy with Winner-Takes-All to Perform Action Selection in Motivational Agents.- Bayesian Approach Based on Geometrical Features for Validation and Tuning of Solution in Deformable Models.- Recognition of Continuous Activities.- Kinematic Control System for Car-Like Vehicles.- Habituation Based on Spectrogram Analysis.- Dynamic Schema Hierarchies for an Autonomous Robot.- Computer Vision.- Monte Carlo Localization in 3D Maps Using Stereo Vision.- 3D Complex Scenes Segmentation from a Single Range Image Using Virtual Exploration.- Recognizing Indoor Images with Unsupervised Segmentation and Graph Matching.- Vision-Based System for the Safe Operation of a Solar Power Tower Plan.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Engineering"
    ]
  },
  {
    "paperId": "49ef2659994befcf4bb8360a171210d3be3dca6e",
    "url": "https://www.semanticscholar.org/paper/49ef2659994befcf4bb8360a171210d3be3dca6e",
    "title": "Computational Linguistics and Intelligent Text Processing: Second International Conference, CICLing 2001 Mexico City, Mexico, February 18\u201324, 2001 Proceedings",
    "abstract": null,
    "venue": "Lecture Notes in Computer Science",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  },
  {
    "paperId": "1fb91f6d15b2fe45147d4f12dd590de11e6d8938",
    "url": "https://www.semanticscholar.org/paper/1fb91f6d15b2fe45147d4f12dd590de11e6d8938",
    "title": "Abstraction Considered Harmful: Lazy Learning of Language Processing 1 Empirical Learning of Natural Language",
    "abstract": "ION CONSIDERED HARMFUL: LAZY LEARNING OF LANGUAGE PROCESSING Walter Daelemans Computational Linguistics Tilburg University, The Netherlands, and Center for Dutch Language and Speech, University of Antwerp, Belgium walter.daelemans@kub.nl 1 Empirical Learning of Natural Language Browsing through the Machine Learning literature, we nd that language learning is not a hot topic in machine learning, and that most work in the area addresses either the computational modeling of child language acquisition or the extraction of domain knowledge from text. The algorithms used are also predominantly analytic (symbol-level learning) rather than empirical (knowledge-level learning). While these are obviously interesting research areas and approaches, the absence of more research on the empirical learning of language knowledge and behaviour from text and speech data strikes me as strange. After all, the main problem of the AI discipline of Natural Language Processing (NLP) is a knowledge acquisition bottleneck: for each new language, domain, theoretical framework, and application, linguistic knowledge bases (lexicons, rule sets, grammars) have to be built basically from scratch. This problem is not surprising, given the complexity of NLP, and the di culty of nding 'hard and fast' rules governing language processing. There are at least three reasons why Machine Learning researchers should become more interested in NLP as an application area. Availability of Large Datasets. NLP problems provide realistically sized training sets for inductive algorithms. Datasets of tens or hundreds of thousands of instances are readily available. Traditional \\benchmark datasets\" usually contain far less instances. Experimenting with linguistic datasets will provide a more realistic evaluation of the practical usefulness of algorithms, and will force algorithm designers to work on performance issues. Real-World Application. \\Hand-crafting\" NLP knowledge bases has proven to be infeasible or una ordable for most practical applications. The market pull for applications in NLP (especially Text Analysis and Machine Translation) is enormous, but has not been matched by current language technology due to the knowledge acquisition bottleneck. ML techniques may help in realising the enormous market potential for NLP applications. Complexity of Tasks. Data sets describing language problems at all levels of description (beneath, at, and above the word level) exhibit a complex interaction of regularities, sub-regularities, pockets of exceptions, idiosyncratic exceptions, and noise. As 1 such, they are a perfect model for a large class of other poorly-understood real world problems (e.g. medical diagnosis) for which it is less easy to nd large amounts of data. A better understanding of which algorithms work best for this class of problems will transfer to many other problem classes. In the remainder of the paper, I will describe an approach to empirical learning of language knowledge which we have been investigating the last ve years in Tilburg and Antwerp in the context of the Atila project. At this point I would like to acknowledge the contributions of my current and former colleagues to this e ort, especially those of Antal van den Bosch (Tilburg, now Maastricht), Steven Gillis, Gert Durieux, and Peter Berck (Antwerp), and Jakub Zavrel (Tilburg). 2 NLP as a Cascade of Classi cation Tasks To solve NLP knowledge acquisition bottlenecks we need ML methods, but in order to achieve any results, we must show that the important NLP tasks can indeed be formulated in a ML framework. Tasks in NLP are context-sensitive mappings between representations (e.g., from text to speech, from spelling to parse tree, from parse tree to logical form, from source language to target language, etc.). These mappings tend to be many-to-many and complex because they can typically only be described by means of the con icting regularities, sub-regularities, and exceptions alluded to earlier. E.g., in a hyphenation programme for a word processor (the simplest and most prosaic NLP problem imaginable), possible positions for hyphens have to be found in a spelling representation; the task is to nd a mapping from a spelling representation to a syllable representation. In Dutch, even this simple task is not trivial because the phonological regularities governing syllable structure are sometimes overruled by more speci c constraints from morphology (the morphological structure of the word and the nature of its a xes). On top of that, there are constraints which are conventional or which derive from the foreign origin of words. Linguistic engineering (handcrafting) of a rule set and exception lists for this type of problem is time-consuming, costly, and does not necessarily lead to accurate and robust systems. Empirical Learning (inductive learning from examples) is fundamentally a classi cation paradigm. Given a description in terms of feature-value pairs of an input, a category label is produced. This category should normally be taken from a nite inventory of possibilities, known beforehand. It is our claim that all useful linguistic tasks can be rede ned this way and can thus be taken on in a ML context. All linguistic problems can be described as mappings of two kinds: disambiguation (or identi cation) and segmentation (identi cation of boundaries) (see Daelemans, 1995). Disambiguation. Given a set of possible categories and a relevant context in terms of attribute values, determine the correct category for this context. Instances of disambiguation include part of speech tagging (disambiguating the syntactic category of a word), grapheme-to-phoneme conversion, lexical selection in generation, morphological synthesis, word sense disambiguation, term translation, and stress assignment. Segmentation. Given a target and a context, determine whether a boundary is associated with this target, and if so which one. Examples include syllabi cation, hyphenation, morphological analysis, and constituent boundary detection. 2 In such a perspective, complex NLP tasks like parsing can be de ned as a cascade of segmentation tasks ( nding constituent boundaries) and disambiguation tasks (deciding the morphosyntactic category of words, and the label of constituents, and resolving attachment ambiguities). An approach often used to arrive at the classi cation representation needed is the windowing method (as used in Sejnowski & Rosenberg, 1986 for text to speech), in which an imaginary window is moved one item at a time over an input string where one item in the window (usually the middle item or the last item) acts as a target item, and the rest as the context. 3 Generalization without Abstraction Especially in Language Engineering applications (building commercially useful NLP applications), the main evaluation criteria for systems are (i) e ciency in terms of space and time requirements, and (ii) accuracy in terms of behaviour of the system on previously unseen input. In a learning framework, this criterion is de ned in terms of generalization accuracy {the percentage of correct outputs associated with inputs the system was not trained on{ and is usually estimated using cross-validation. Many empirical learning methods (e.g. top down induction of decision trees, rule induction methods, and supervised connectionist learning algorithms) are eager learning methods. They achieve generalization by means of abstraction. The original training data is abstracted into a representational structure (condition-action rules, trees, weight matrices) that models the regularities governing the input-output associations in the training data. However, abstraction is not a prerequisite for generalization. There is a class of lazy learning algorithms that is based on the fundamental idea that a system can generalize what it has learned by using its experiences directly, instead of using abstractions extracted from them. From the research we did on inductive learning for NLP tasks, it becomes clear that this lazy learning approach achieves better generalization accuracy than eager learning algorithms. A possible explanation for this is the structure of NLP tasks discussed earlier: apart from a number of clear generalizations, a lot of subregularities and exceptions exist in the data. Exceptions tend to come in `families'. It is therefore advantageous to keep exceptions (some family members may turn up during testing) rather than abstracting away from them: being there is better than being probable. In the remainder of this section I will shortly present the basics of the lazy learning approach. Later sections will describe applications of this idea to NLP tasks. Lazy Learning is a form of supervised, inductive learning from examples. Examples are represented as a vector of feature values with an associated category label. During training, a set of examples (the training set) is presented in an incremental fashion to the classi er, and added to memory. During testing, a set of previously unseen feature-value patterns (the test set) is presented to the system. For each test pattern, its distance to all examples in memory is computed, and the category of the least distant instance(s) is used as the predicted category for the test pattern. In AI, the concept has appeared in several disciplines (from computer vision to robotics), using terminology such as similarity-based, example-based, memory-based, exemplar-based, case-based, analogical, nearest-neighbour, and instance-based (Stan ll andWaltz, 1986; Kolodner, 1993; Aha et al. 1991; Salzberg, 1990). Ideas about this type of analogical reasoning can 3 be found also in non-mainstream linguistics and pyscholinguistics (Skousen, 1989; Derwing & Skousen, 1989; Chandler, 1992; Scha, 1992). In computational linguistics (apart from incidental computational work of the linguists referred to earlier), the general approach has only recently gained some popularity: e.g., Cardie (1994, syn",
    "venue": "",
    "citationCount": 3,
    "fieldsOfStudy": null
  },
  {
    "paperId": "96a4511cb6c70ee81b7d6803b69e61bce9cdd29a",
    "url": "https://www.semanticscholar.org/paper/96a4511cb6c70ee81b7d6803b69e61bce9cdd29a",
    "title": "GWAI-89 : 13th German Workshop on Artificial Intelligence, Eringeferld, 19-22 September 1989 : proceedings",
    "abstract": "1. Nicht - Klassische Deduktive Systeme.- Propagation of Temporally Indexed Values in Multiple Contexts.- Nicht-monotone Beweiser fur Autoepistemische Logik und Defaultlogik.- Problem Solver Control Over the ATMS.- Context Logic - An Introduction.- Proof Transformation Towards Human Reasoning Style.- Revising Domain Theories in Model-Based Reasoning Systems.- 2. Deduktive Systeme.- Plan Generation by Linear Proofs: On Semantics.- A New Deductive Approach to Planning.- Detecting Redundancy Caused by Congruent Links in Clause Graphs.- An Approach to Parallel Unification Using Transputers.- EQTHEOPOGLES - A Completion Theorem Prover for PLIEQ.- A Resolution Calculus Extended by Equivalence.- Linear Paramodulation modulo Equality.- 3. Neuronale Netze.- Explanation and Connectionism.- Optimierung des Lernverhaltens neuronaler Netze durch Berucksichtigung verschiedener Abstraktionsklassen bei der Netzwerktopologie.- NetSim: Ein Simulator fur Neuronale Netze.- 4. Bildverarbeitung.- Modellgestutztes Bildverstehen von Dokumenten.- Lernen von Strukturbeschreibungen fur ein wissensbasiertes Bildanalysesystem.- Zur Konstmktion einer geometrischen Szenenbeschreibung aus Stereobildfolgen unter Verwendung eines ATMS.- 5. Mensch-Maschine Kommunikation.- SPICOS II - Linguistische Analyse im Datenbankdialog.- Diskursreprasentation im Datenbank-Abfrage Dialog SPICOS II.- Das System ZORA - Wissenbasierte Generierung von Zeigegesten.- 6. Naturlich-Sprachliche Systeme.- Processing Contrast Relations.- Reasoning for Text Understanding - Knowledge Processing in the 1st LILOG-Prototype.- Prototypical Disambiguation of Word Meaning Exemplified by Adjective-Noun Combinations.- Zielgerichtete Wortschatzerweiterungen in naturlichsprachlichen Systemen.- OSKAR- Ein PROLOG-Programm zur Modellierung der Struktur und der Verarbeitung raumlichen Wissens.- Komplexe Individuen in Referentiellen Netzen.- Towards a Framework for Knowledge-Based Machine Translation.- Transfer in Machine Translation by Non-Confluent Term-Rewrite Systems.- Determining Consistency of Feature Terms with Distributed Disjunctions.- A Parsing System Based on a Deductive Database.- Kategoriales Parsing mit definiten Klauseln.- 7. Kognitive und tu torieile Systeme.- Ein erster Blick auf ANTLIMA: Visualisierung statischer raumlicher Relationen.- Two Views of Motion: On Representing Move Events in a Language-Vision System.- DEPIC-2D: Eine Komponente zur depiktionalen Reprasentation und Verarbeitung raumlichen Wissens.- Towards Principles of Ontology.- Automatische kognitive Diagnose in einem Programmier-Tutor.- 8. Programmsynthese.- Machine-Assisted Program Construction and Modification.- XPRTS - An Implementation Tool for Program Synthesis.- 9. Expertensysteme.- Modulare Expertensystemarchitekturen.- A Layered Algebraic Specification Technique for Expert Systems.- OFFICE-PLAN: Tackling the Synthesis Frontier.- Modifying the Model Set During Diagnosis.- Towards Structured Production Systems - Efficient Implementation of Meta-Level Architectures.- Dialogstrukturen in Gruppendiskussionen - Ein Modell fur argumentative Verhandlungen mehrerer Agenten.- Bayesian Integration of Uncertain and Conflicting Evidence.- Inferenzen bei Ungewissheit in Expertensystemen.- An Integrative Model of Learning by Being Told, from Examples and by Exploration.- Toward a Rapid Prototyping Environment for Expert Systems.- Computational Architectures for Computer-Integrated Engineering and Manufacturing: An Artificial Intelligence Perspective.- 10. Spezielle Seminare zu Grundlagen und Anwendungen der K.- Einfuhrungsseminar: Maschinelles Lernen.- Tutorial: High Level Tools for Knowledge Systems Design.- Sektion: Expertensystemlabor.- Fachseminar: Formale und kognitive Grundlagen von Wissensreprasentationen.- Feature-Logik.- Zur Implementierbarkeit Analogischer Reprasentationen.- Kognitive Kategorien.- KL-One-basierte, hybride Reprasentationssysteme.",
    "venue": "",
    "citationCount": 0,
    "fieldsOfStudy": [
      "Computer Science"
    ]
  }
]