[{"paperId": "d3712c115fd077ef0789f33ba80c6e4904ca85d5", "url": "https://www.semanticscholar.org/paper/d3712c115fd077ef0789f33ba80c6e4904ca85d5", "title": "A cooperative crowdsourcing framework for knowledge extraction in digital humanities - cases on Tang poetry", "abstract": "The purpose of this paper is to propose a knowledge extraction framework to extract knowledge, including entities and relationships between them, from unstructured texts in digital humanities (DH).,The proposed cooperative crowdsourcing framework (CCF) uses both human\u2013computer cooperation and crowdsourcing to achieve high-quality and scalable knowledge extraction. CCF integrates active learning with a novel category-based crowdsourcing mechanism to facilitate domain experts labeling and verifying extracted knowledge.,The case study shows that CCF can effectively and efficiently extract knowledge from multi-sourced heterogeneous data in the field of Tang poetry. Specifically, CCF achieves higher accuracy of knowledge extraction than the state-of-the-art methods, the contribution of feedbacks to the training model can be maximized by the active learning mechanism and the proposed category-based crowdsourcing mechanism can scale up the effective human\u2013computer collaboration by considering the specialization of workers in different categories of tasks.,This research proposes CCF to enable high-quality and scalable knowledge extraction in the field of Tang poetry. CCF can be generalized to other fields of DH by introducing domain knowledge and experts.,The extracted knowledge is machine-understandable and can support the research of Tang poetry and knowledge-driven intelligent applications in DH.,CCF is the first human-in-the-loop knowledge extraction framework that integrates active learning and crowdsourcing mechanisms; he human\u2013computer cooperation method uses the feedback of domain experts through the active learning mechanism; the category-based crowdsourcing mechanism considers the matching of categories of DH data and especially of domain experts.", "venue": "Aslib Journal of Information Management", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 10}, "nlp_mention_counts": {"ke": 10}, "ld_mention_counts": {"ke": 10}, "relevance_score": 0.9996646498695336}, {"paperId": "192d084cd6d0b63dbbe56c3813b08e83cdaec5d9", "url": "https://www.semanticscholar.org/paper/192d084cd6d0b63dbbe56c3813b08e83cdaec5d9", "title": "Using Spreading Activation to Evaluate and Improve Ontologies", "abstract": "In this paper, we explore the relationship between the human-encoded semantics of ontologies and their application to natural language processing (NLP) tasks, such as word-sense disambiguation (WSD), for which such ontologies may not have been originally designed. We present a method for assessing the semantic content of an ontology with respect to a target domain, by spreading activation over a graph that represents instances of ontology concepts and relationships, in domain text. Our proposed method has several advantages beyond existing ontology metrics. By identifying bias or imbalance in the ontology, we can suggest target areas for improvement, and simultaneously facilitate the automated optimisation of the graph for use in the chosen NLP task. On applying this method to the Unified Medical Language System (UMLS) ontology, we significantly outperformed existing graph-based methods for WSD in biomedical NLP (0.82 accuracy). The subsequent introduction of a fall-back mechanism, using word-sense probability, achieved state of the art for unsupervised biomedical WSD (0.89 accuracy).", "venue": "International Conference on Computational Linguistics", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "wsd", "onto", "nlp", "onto", "nlp", "wsd", "onto", "nlp", "onto", "onto", "nlp", "wsd", "onto", "wsd"], "mention_counts": {"nlp": 4, "wsd": 4, "onto": 8}, "nlp_mention_counts": {"nlp": 4, "wsd": 4}, "ld_mention_counts": {"onto": 8}, "relevance_score": 0.9975273768433653}, {"paperId": "69aa5126098f39ada0ac4855d0f186b741bc7ed5", "url": "https://www.semanticscholar.org/paper/69aa5126098f39ada0ac4855d0f186b741bc7ed5", "title": "Towards the relationship between Semantic Web and NLP", "abstract": "With the development of Semantic the Web technology, the NLP technology has much broader prospects. This article analyses the fusion degrees between the two technologies based on the survey of relations of them. We explain the relationship between Semantic Web and NLP in two aspects. One is NLP how to support Semantic Web development in Ontology Learning, Ontology Query and Multilingual Ontology Mapping. The other is Semantic Web technologies how to improve NLP results in Information Extraction and Word Sense Disambiguation. We also propose some research challenges for the cooperation between Semantic Web and NLP.", "venue": "International Conference on Natural Language Processing and Knowledge Engineering", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "ie", "onto", "sw", "onto", "onto", "sw", "nlp", "nlp", "sw", "sw", "nlp", "nlp", "nlp", "wsd"], "mention_counts": {"onto": 3, "nlp": 6, "sw": 5, "wsd": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 6, "wsd": 1, "ie": 1}, "ld_mention_counts": {"sw": 5, "onto": 3}, "relevance_score": 0.9975273768433653}, {"paperId": "545322f1eef5a3e467a160f2d4037ea139165b4b", "url": "https://www.semanticscholar.org/paper/545322f1eef5a3e467a160f2d4037ea139165b4b", "title": "Towards Learning from User Feedback for Ontology-based Information Extraction", "abstract": "Many engineering projects involve the integration of various hardware parts from different suppliers. In preparation, parts that are best suited for the project requirements have to be selected. Information on these parts\u2019 characteristics is published in so called data sheets usually only available in textual form, e.g. as PDF files. To realize the automated processing, these characteristics have to be extracted into a machine-interpretable format. Such a process requires a lot of manual intervention and is prone to errors. Domain ontologies, among other approaches, can be used to implement the automated information extraction from the data sheets. However, ontologies rely solely on the experiences and perspectives of their creators at the time of creation. To automate the evolution of ontologies, we developed ConTrOn Continuously Trained Ontology that automatically extracts information from data sheets to augment an ontology created by domain experts. The evaluation results of ConTrOn show that the enriched ontology can help improve the information extraction from technical documents. Nonetheless, the extracted information should be reviewed by experts before using it in the integration process. We want to provide an intuitive way of reviewing, in which the extracted information will be highlighted on the data sheets. The experts will be able to accept, reject, or correct the extracted data via a graphical interface. This process of revision and correction can be leveraged by the system to improve itself: learning from its own mistakes and identifying common patterns to adapt in the next extraction iteration. This paper presents ideas how to use machine learning based on user feedback to improve the information extraction process.", "venue": "DI2KG@KDD", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "ie", "ie", "onto", "onto", "ie", "ie", "ie", "onto", "onto", "onto"], "mention_counts": {"onto": 7, "ie": 7}, "nlp_mention_counts": {"ie": 7}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.9933071490757153}, {"paperId": "1dcee66494852538a554091f41e87740a6fd32d8", "url": "https://www.semanticscholar.org/paper/1dcee66494852538a554091f41e87740a6fd32d8", "title": "Language Independent and Practical Ontology in Korean-Japanese Machine Translation Systems", "abstract": "This paper presents a semi-automatic ontology construction method using various resources, and an ontology-based word sense disambiguation method in machine translation. To acquire a reasonably practical ontology in limited time and with less manpower, we extend the Kadokawa thesaurus by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. The former can be obtained by converting valency information and case frames from previously built computational dictionaries used in machine translation. The latter can be acquired from concept co-occurrence information, which is extracted automatically from large corpora. The ontology stores rich semantic constraints among 1110 concepts, and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology. In practical machine translation systems, our word sense disambiguation method achieved a 6.0 per cent and 7.9 per cent improvement over methods that do not use an ontology for each Japanese and Korean translation.", "venue": "Lit. Linguistic Comput.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "wsd", "onto", "nlp", "onto", "mt", "onto", "wsd", "onto", "onto", "mt", "onto", "mt"], "mention_counts": {"nlp": 1, "wsd": 2, "onto": 7, "mt": 4}, "nlp_mention_counts": {"nlp": 1, "wsd": 2, "mt": 4}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.9933071490757153}, {"paperId": "a6851635364a0ea708958601def8d8a4d562375c", "url": "https://www.semanticscholar.org/paper/a6851635364a0ea708958601def8d8a4d562375c", "title": "RDF2PT: Generating Brazilian Portuguese Texts from RDF Data", "abstract": "The generation of natural language from Resource Description Framework (RDF) data has recently gained significant attention due to the continuous growth of Linked Data. A number of these approaches generate natural language in languages other than English, however, no work has been proposed to generate Brazilian Portuguese texts out of RDF. We address this research gap by presenting RDF2PT, an approach that verbalizes RDF data to Brazilian Portuguese language. We evaluated RDF2PT in an open questionnaire with 44 native speakers divided into experts and non-experts. Our results suggest that RDF2PT is able to generate text which is similar to that generated by humans and can hence be easily understood.", "venue": "LREC", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlg", "rdf", "rdf", "nle", "nlg", "rdf", "nll", "ld", "rdf", "rdf", "nlu"], "mention_counts": {"ld": 1, "nll": 1, "nlu": 1, "nlp": 1, "nlg": 2, "nle": 1, "rdf": 5}, "nlp_mention_counts": {"nlu": 1, "nll": 1, "nlp": 1, "nlg": 2, "nle": 1}, "ld_mention_counts": {"ld": 1, "rdf": 5}, "relevance_score": 0.9820137900379085}, {"paperId": "a36bf3c74af0fa2393c31db45f09c03fc2cee004", "url": "https://www.semanticscholar.org/paper/a36bf3c74af0fa2393c31db45f09c03fc2cee004", "title": "Knowledge extraction from time series and its application to surface roughness simulation", "abstract": "Knowledge extracted from time series data influences decision-making in business, medicine, manufacturing, science and in other fields. Various knowledge extraction methods have so far been proposed wherein it is typically assumed that a piece of time series data possesses a set of trends that deterministically or stochastically repeat in time. However, for noisy time series data (data having no trend) the delay maps (return maps) (x(t) ,x (t + \u03b4)) ,t =0 ,1 ,... , \u03b4 =1 ,2 ,... , N(N is a small integer), are more informative than the time series itself. This paper shows a knowledge extraction method that extracts a small set of \"if ... then ... \" rules from the return maps of a given set of time series data. A JAVA TM based tool is developed to automate the rule extraction process. This tool is also able to use the extracted rules recursively to simulate the qualitatively similar time series. The performance of the proposed knowledge extraction method (as well as the tool) is demonstrated by using an example time series (surface roughness profile of a machined surface). This exemplification demonstrates that the proposed knowledge extraction method can be used to enhance the performance of computer integrated manufacturing systems by giving those systems a means to exchange the information of nonlinear behaviors among the subsystems (process planning, quality control, and so on).", "venue": "Information, Knowledge, Systems Management", "citationCount": 20, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 6}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"ke": 6}, "relevance_score": 0.9820137900379085}, {"paperId": "513331cf8380815acc931e29346fe60e2c437c75", "url": "https://www.semanticscholar.org/paper/513331cf8380815acc931e29346fe60e2c437c75", "title": "Ontology-based Information Extraction with SOBA", "abstract": "In this paper we describe SOBA, a sub-component of the SmartWeb multi-modal dialog system. SOBA is a component for ontologybased information extraction from soccer web pages for automatic population of a knowledge base that can be used for domainspecific question answering. SOBA realizes a tight connection between the ontology, knowledge base and the information extraction component. The originality of SOBA is in the fact that it extracts information from heterogeneous sources such as tabular structures, text and image captions in a semantically integrated way. In particular, it stores extracted information in a knowledge base, and in turn uses the knowledge base to interpret and link newly extracted information with respect to already existing entities.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 99, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "kg", "ie", "ie", "ie", "kg", "ie", "kg", "kg", "ie", "ie"], "mention_counts": {"kg": 4, "onto": 2, "ie": 6}, "nlp_mention_counts": {"ie": 6}, "ld_mention_counts": {"kg": 4, "onto": 2}, "relevance_score": 0.9820137900379085}, {"paperId": "f2786451bc0a02835e6890a76f71e04e75ce55e8", "url": "https://www.semanticscholar.org/paper/f2786451bc0a02835e6890a76f71e04e75ce55e8", "title": "Ontology-based information extraction: An introduction and a survey of current approaches", "abstract": "Information extraction (IE) aims to retrieve certain types of information from natural language text by processing them automatically. For example, an IE system might retrieve information about geopolitical indicators of countries from a set of web pages while ignoring other types of information. Ontology-based information extraction (OBIE) has recently emerged as a subfield of information extraction. Here, ontologies - which provide formal and explicit specifications of conceptualizations - play a crucial role in the IE process. Because of the use of ontologies, this field is related to knowledge representation and has the potential to assist the development of the Semantic Web. In this paper, we provide an introduction to ontology-based information extraction and review the details of different OBIE systems developed so far. We attempt to identify a common architecture among these systems and classify them based on different factors, which leads to a better understanding on their operation. We also discuss the implementation details of these systems including the tools used by them and the metrics used to measure their performance. In addition, we attempt to identify the possible future directions for this field.", "venue": "Journal of information science", "citationCount": 426, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto", "tp", "ie", "ie", "sw", "onto", "onto", "ie", "onto"], "mention_counts": {"sw": 1, "onto": 5, "tp": 1, "ie": 5}, "nlp_mention_counts": {"tp": 1, "ie": 5}, "ld_mention_counts": {"sw": 1, "onto": 5}, "relevance_score": 0.9820137900379085}, {"paperId": "fd1cb28fdde85869e1e9d6c5eee5488356ee3a2b", "url": "https://www.semanticscholar.org/paper/fd1cb28fdde85869e1e9d6c5eee5488356ee3a2b", "title": "Extending the Finnish Linked Data Infrastructure with Natural Language Processing Services in FIN-CLARIAH", "abstract": "The DARIAH-EU infrastructure for Digital Humanities (DH) is often focusing on using structured data for quantitative studies, while the EU-CLARIN infrastructure deals primarily with unstructured natural language texts. However, in DH research both texts and structured data are often needed. It therefore makes sense to develop and use both infrastructures together, as suggested in the Dutch CLARIAH pro-gramme and the corresponding FIN-CLARIAH initiative in Finland, a new part of the Finnish research infrastructure road map of the Academy of Finland. This poster paper introduces work in FIN-CLARIAH relating to the idea of integrating natural language processing (NLP) tools with the Linked Open Data (LOD) Infrastructure for Digital Humanities in Finland (LODI4DH). We present a plan for NLP services to be opened as part of the Linked Data Finland (LDF.fi) platform. The new services are used on the other hand for knowledge extraction from Finnish texts for weaving LOD, and on the other hand for language DH data analyses of the published datasets in applications in many domains, such as political culture. The extended LDF.fi platform will provide users with documented APIs for NLP services using unified output formats as well as software delivery as Docker containers, to lower the bar for deployment.", "venue": "DHNB", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "lod", "ld", "lod", "ld", "nlp", "lod", "ke", "nlp", "nlp", "nlp"], "mention_counts": {"ld": 2, "nlp": 5, "lod": 3, "ke": 1}, "nlp_mention_counts": {"nlp": 5, "ke": 1}, "ld_mention_counts": {"ld": 2, "ke": 1, "lod": 3}, "relevance_score": 0.9820137900379085}, {"paperId": "4ee4e49c48d592269284aeff7eaf4f83692f7b06", "url": "https://www.semanticscholar.org/paper/4ee4e49c48d592269284aeff7eaf4f83692f7b06", "title": "Text to Insight: Accelerating Organic Materials Knowledge Extraction via Deep Learning", "abstract": "Scientific literature is one of the most significant resources for sharing knowledge. Researchers turn to scientific literature as a first step in designing an experiment. Given the extensive and growing volume of literature, the common approach of reading and manually extracting knowledge is too time consuming, creating a bottleneck in the research cycle. This challenge spans nearly every scientific domain. For the materials science, experimental data distributed across millions of publications are extremely helpful for predicting materials properties and the design of novel materials. However, only recently researchers have explored computational approaches for knowledge extraction primarily for inorganic materials. This study aims to explore knowledge extraction for organic materials. We built a research dataset composed of 855 annotated and 708,376 unannotated sentences drawn from 92,667 abstracts. We used named\u2010entity\u2010recognition (NER) with BiLSTM\u2010CNN\u2010CRF deep learning model to automatically extract key knowledge from literature. Early\u2010phase results show a high potential for automated knowledge extraction. The paper presents our findings and a framework for supervised knowledge extraction that can be adapted to other scientific domains.", "venue": "Proceedings of the Association for Information Science and Technology", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 6}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"ke": 6}, "relevance_score": 0.9820137900379085}, {"paperId": "8ddcbb5084b73097b2d605fdc23a0d38dba4a942", "url": "https://www.semanticscholar.org/paper/8ddcbb5084b73097b2d605fdc23a0d38dba4a942", "title": "Knowledge Discovery Web Service for Spatial Data Infrastructures", "abstract": "The size, volume, variety, and velocity of geospatial data collected by geo-sensors, people, and organizations are increasing rapidly. Spatial Data Infrastructures (SDIs) are ongoing to facilitate the sharing of stored data in a distributed and homogeneous environment. Extracting high-level information and knowledge from such datasets to support decision making undoubtedly requires a relatively sophisticated methodology to achieve the desired results. A variety of spatial data mining techniques have been developed to extract knowledge from spatial data, which work well on centralized systems. However, applying them to distributed data in SDI to extract knowledge has remained a challenge. This paper proposes a creative solution, based on distributed computing and geospatial web service technologies for knowledge extraction in an SDI environment. The proposed approach is called Knowledge Discovery Web Service (KDWS), which can be used as a layer on top of SDIs to provide spatial data users and decision makers with the possibility of extracting knowledge from massive heterogeneous spatial data in SDIs. By proposing and testing a system architecture for KDWS, this study contributes to perform spatial data mining techniques as a service-oriented framework on top of SDIs for knowledge discovery. We implemented and tested spatial clustering, classification, and association rule mining in an interoperable environment. In addition to interface implementation, a prototype web-based system was designed for extracting knowledge from real geodemographic data in the city of Tehran. The proposed solution allows a dynamic, easier, and much faster procedure to extract knowledge from spatial data.", "venue": "ISPRS Int. J. Geo Inf.", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 6}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"ke": 6}, "relevance_score": 0.9820137900379085}, {"paperId": "13c93d79c11df684a717f7ad046167dba2a0c34d", "url": "https://www.semanticscholar.org/paper/13c93d79c11df684a717f7ad046167dba2a0c34d", "title": "Construction of biological networks from unstructured information based on a semi-automated curation workflow", "abstract": "Abstract Capture and representation of scientific knowledge in a structured format are essential to improve the understanding of biological mechanisms involved in complex diseases. Biological knowledge and knowledge about standardized terminologies are difficult to capture from literature in a usable form. A semi-automated knowledge extraction workflow is presented that was developed to allow users to extract causal and correlative relationships from scientific literature and to transcribe them into the computable and human readable Biological Expression Language (BEL). The workflow combines state-of-the-art linguistic tools for recognition of various entities and extraction of knowledge from literature sources. Unlike most other approaches, the workflow outputs the results to a curation interface for manual curation and converts them into BEL documents that can be compiled to form biological networks. We developed a new semi-automated knowledge extraction workflow that was designed to capture and organize scientific knowledge and reduce the required curation skills and effort for this task. The workflow was used to build a network that represents the cellular and molecular mechanisms implicated in atherosclerotic plaque destabilization in an apolipoprotein-E-deficient (ApoE \u2212/\u2212 ) mouse model. The network was generated using knowledge extracted from the primary literature. The resultant atherosclerotic plaque destabilization network contains 304 nodes and 743 edges supported by 33 PubMed referenced articles. A comparison between the semi-automated and conventional curation processes showed similar results, but significantly reduced curation effort for the semi-automated process. Creating structured knowledge from unstructured text is an important step for the mechanistic interpretation and reusability of knowledge. Our new semi-automated knowledge extraction workflow reduced the curation skills and effort required to capture and organize scientific knowledge. The atherosclerotic plaque destabilization network that was generated is a causal network model for vascular disease demonstrating the usefulness of the workflow for knowledge extraction and construction of mechanistically meaningful biological networks.", "venue": "Database J. Biol. Databases Curation", "citationCount": 28, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 6}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"ke": 6}, "relevance_score": 0.9820137900379085}, {"paperId": "4c3a789655282c0d4a6e3a9f5a82f79d69a51f54", "url": "https://www.semanticscholar.org/paper/4c3a789655282c0d4a6e3a9f5a82f79d69a51f54", "title": "Template construction and Tibetan knowledge extraction", "abstract": "Entity knowledge extraction is the foundation of the Tibetan knowledge graph construction, which provides support for Tibetan question answering system, information retrieval, information extraction and other researches, and promotes national unity and social stability. This paper proposes a SVM and template-based approach to Tibetan person knowledge extraction. Through constructing the training corpus, we build the templates based the shallow parsing analysis of Tibetan syntactic, semantic features and verbs. Using the training corpus, we design a hierarchical SVM classifier to realize the entity knowledge extraction. Finally, experimental results prove the method has greater improvement in Tibetan person knowledge extraction.", "venue": "2018 International Conference on Artificial Intelligence and Big Data (ICAIBD)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "ke", "ie", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 5, "ie": 1}, "nlp_mention_counts": {"ke": 5, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 5}, "relevance_score": 0.9820137900379085}, {"paperId": "07be3fd820419251590de8e8357902d1073d06fb", "url": "https://www.semanticscholar.org/paper/07be3fd820419251590de8e8357902d1073d06fb", "title": "Method of Tibetan Person Knowledge Extraction", "abstract": "Person knowledge extraction is the foundation of the Tibetan knowledge graph construction, which provides support for Tibetan question answering system, information retrieval, information extraction and other researches, and promotes national unity and social stability. This paper proposes a SVM and template based approach to Tibetan person knowledge extraction. Through constructing the training corpus, we build the templates based the shallow parsing analysis of Tibetan syntactic, semantic features and verbs. Using the training corpus, we design a hierarchical SVM classifier to realize the entity knowledge extraction. Finally, experimental results prove the method has greater improvement in Tibetan person knowledge extraction.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke", "ke", "kg", "ke", "ke"], "mention_counts": {"ke": 5, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 5, "ie": 1}, "ld_mention_counts": {"ke": 5, "kg": 1}, "relevance_score": 0.9820137900379085}, {"paperId": "b75b467ba586e6c5c3206576ea54fc06344688ed", "url": "https://www.semanticscholar.org/paper/b75b467ba586e6c5c3206576ea54fc06344688ed", "title": "Implementation of the Multiagent-based Product Knowledge Extraction System", "abstract": "Building upon an analysis of traditional product knowledge extraction methods, a new prototype of Product Knowledge Extraction System (PKES) based on multiagent theory is developed. In this system, a Client/Server(C/S) architecture based on a local computer network is adopted for knowledge extraction. The knowledge extraction task is divided into different parts of sub-tasks and distributed to different clients. By making full use of the capability of client computers and coordination of the clients and the server, the product knowledge extraction task could be implemented efficiently. To achieve better use of the extracted product knowledge, the knowledge management functions are also integrated with the PKES system. Finally, a demonstration of the effectiveness and viability of the system is given by a simulation of the PKES.", "venue": "2009 WRI Global Congress on Intelligent Systems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 6}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"ke": 6}, "relevance_score": 0.9820137900379085}, {"paperId": "690559ba750a109252522b4150465904f97844d3", "url": "https://www.semanticscholar.org/paper/690559ba750a109252522b4150465904f97844d3", "title": "Natural language based constraints for web services", "abstract": "Natural Language Processing (NLP) is an important and beneficial field of knowledge in the modern age of software development and playing an essential role in problem solving, researchers used it to evaluate and interpret problems. Constraints perform significant role in Natural Language Processing (NLP), these have been already used in natural language generation to solve generation and ambiguity issues. OWL Syntax is challenging, writing constraints in OWL is time consuming process and understanding of the domain is also required, specifically to new users. The proposed methodologies will affluence the application process by operating natural language constraints over web to ease the commercial business. In this paper our basic aim is to write NL constraints in simple English language and purposed model automatically convert them to OWL. The purposed model contains simple NL constraints which are analyzed using NLP techniques and afterward by using RDF and RDFS, OWL is produced. The design system produces profligate and rapid NL constraints, instead of OWL in which writing constraints is very problematic and elucidate many complex complications of web application.", "venue": "2016 Sixth International Conference on Innovative Computing Technology (INTECH)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "rdf", "onto", "nlp", "onto", "nlp", "onto", "onto", "onto", "nlg"], "mention_counts": {"nlp": 5, "nlg": 1, "onto": 5, "rdf": 1}, "nlp_mention_counts": {"nlp": 5, "nlg": 1}, "ld_mention_counts": {"onto": 5, "rdf": 1}, "relevance_score": 0.9820137900379085}, {"paperId": "8c300bf6090c427631d772b875fdabf57af4257a", "url": "https://www.semanticscholar.org/paper/8c300bf6090c427631d772b875fdabf57af4257a", "title": "Semantic Similarity from Natural Language and Ontology Analysis", "abstract": "Automatic text understanding requires knowledge and, so far, machines know only what we give or teach them. As a consequence, most natural language processing (NLP) tasks crucially rely on the existence of linguistic resources that encode information about language, be it of morphological, syntactic, or semantic nature. Such resources are typically acquired via two main approaches: the knowledge-based approach, or top\u2013down, where information is manually curated by humans, and the corpus-based approach, or bottom\u2013up, where information is automatically learned from corpora. Although the latter has gained ground during the last decade\u2014benefiting from the availability of large amounts of text and from increased computing capacities\u2014the former remains fundamental for it allows us to collect reliable, fine-grained, and explicit information. Lexical knowledge bases (LKBs), also known as lexico-semantic resources, provide information about words and potentially entities, and are at the core of knowledgebased approaches. They are widely used in a variety of NLP tasks (e.g., word sense disambiguation, information retrieval, and question answering), all the more so since their traditional limitations (i.e., lack of language and domain-specific coverage) have recently started to fall. Indeed, beside the long-established process of expert-based resource creation (e.g., WordNet), Web technologies have enabled the collaborative, crowd-based construction of resources (e.g., Wikipedia and Wiktionary). This contributed to significantly widen the scope of the available machine-readable knowledge and, in the context of an already diverse landscape of LKBs, it encouraged and motivated even more the need to integrate different resources so as to make the best of them all. This book introduces linked lexical knowledge bases by giving an account of their foundations and presenting their main applications. Its target audience includes NLP practitioners or students who wish to better understand linked lexical knowledge bases, how they are built, and their typical usages and added value. The book is organized into eight chapters plus a preface, and is additionally put into perspective by a foreword (by Ido Dagan) that considers the history, recent evolution, and probable future directions of knowledge acquisition.", "venue": "Computational Linguistics", "citationCount": 39, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "nlp", "onto", "kg", "kg", "nlp", "nlp", "nlp", "wsd"], "mention_counts": {"nlp": 4, "kg": 4, "wsd": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 4, "wsd": 1}, "ld_mention_counts": {"kg": 4, "onto": 1}, "relevance_score": 0.9525741268224334}, {"paperId": "82a663d3b21a85d2b4632ffa9600662f0f147b81", "url": "https://www.semanticscholar.org/paper/82a663d3b21a85d2b4632ffa9600662f0f147b81", "title": "The MEANING Project", "abstract": "Resumen: A pesar del progreso que se realiza en el Procesamiento del Lenguaje Natural (PLN) a un estamos lejos de la Comprensii on del Lenguaje Natural. Un paso importante hacia este objetivo es el desarrollo de t ecnicas y recursos que traten conceptos en lugar de palabras. Sin embargo, si queremos construir la prr oxima generacii on de sistemas inteligentes que traten Tecnolog a de Lenguaje Humano en dominios abiertos necesitamos resolver dos tareas intermedias y complementarias: Resolucii on de la ambiguedad l exica de las palabras y enriquecimeinto automm atico y a gran escala de bases de conocimiento l exico Abstract: Progress is being made in Natural Language Processing (NLP) but there is still a long way towards Natural Language Understanding. An important step towards this goal is the development of technologies and resources that deal with concepts rather than words. However, to be able to build the next generation of intelligent open domain Human Language Technology (HLT) application systems we need to solve two complementary and intermediate tasks: Word Sense Disambi-guation (WSD) and automatic large-scale enrichment of Lexical Knowledge Bases. Knowledge Technologies aim to provide meaning to petabytes of information content our societies will generate in a near future. Information and knowledge management systems need to evolve accordingly, to help release next generation of intelligent open domain HLT to deal with the growing potential of the knowledge-rich and multilingual society. To develop a trustable semantic web infrastructure and a multilingual ontology framework to support knowledge management it is required a wide range of techniques to progressively automate the knowledge lifecycle. In particular this include extracting high level meaning from the large collections of content data and its representation and management in a common knowledge bases. Even now, building large and rich knowledge bases takes a great deal of expensive manual eeort; this has severely hampered HLT application development. For example, dozens of person-years have been invest into the development of wordnets, but the data in these resources is still not suuciently rich to support advanced concept-based HLT applications directly. Furthermore, resources produced by introspection usually fail to register what really occurs in texts. Applications will not scale up to working in the open domain without more detailed and rich general-purpose and also domain-speciic linguistic knowledge. To be able to build the next generation of intelligent open domain HLT application systems we need to solve two complementary intermediate tasks: \u2026", "venue": "Proces. del Leng. Natural", "citationCount": 6, "fieldsOfStudy": ["Sociology", "Computer Science"], "mentions": ["wsd", "kg", "sw", "kg", "nlp", "onto", "nlu", "hlt", "kg", "nlp"], "mention_counts": {"onto": 1, "nlu": 1, "nlp": 2, "sw": 1, "kg": 3, "wsd": 1, "hlt": 1}, "nlp_mention_counts": {"nlp": 2, "wsd": 1, "hlt": 1, "nlu": 1}, "ld_mention_counts": {"kg": 3, "sw": 1, "onto": 1}, "relevance_score": 0.9525741268224334}, {"paperId": "67c409686c11782e0d2385c84b67c0f00915814b", "url": "https://www.semanticscholar.org/paper/67c409686c11782e0d2385c84b67c0f00915814b", "title": "Auto Generation of Gold Standard, Class Labeled Data Set and Ontology Extension Tool [QuadW]", "abstract": "Automatic Knowledge Extraction (AKE) from domain independent, unstructured text sources is a challenging task in Natural Language Processing and Text analytics. Though, supervised learning mechanisms are very much result promising, application is painful due to the mandatory requirement of a class labeled training data set, as it involves expensive manual effort which is more time consuming. As a solution for this problem, this paper introduces a novel mechanism to build a self-learned classifier model that can automatically generate class labeled training data set for Knowledge/Information Extraction from domain independent unstructured text. Sri Lankan English newspapers (which comprise unstructured text in unconstrained domains) are the main data source for this study and a prototype was built to Professional Information Extraction with the semantic pattern Who holds/held What position, Where and When (Four words start with \u2018W\u2019, hence named \u2018QuadW\u2019). Methodology uses advanced machine learning techniques such as, a Random Forest with Adaboost ensemble algorithm to build a composite classification model. This classifier is called as self-learned since, it generates its own training data set automatically. This composite model has improved accuracy and avoided over fitting to data as well. The rule-based feature extraction algorithm and the hand-craft ontology developed, can also be considered as novel components of this study. Self-learned classifier has been extensively improved and tested to show higher accuracy with precision and recall close to one. Therefore, the classified output from the self-learned classifier can be used as a gold-standard data set for future research in Professional Information Extraction. The constructed ontology with approximately 400 facts, also can be effectively used in future researches. Further, introduced classifier can be used as a tool to extend the existing ontology as well. A novel usage of machine learning algorithms to text classification demonstrates that, this study goes with the state-of-the-art technologies.", "venue": "2019 Second International Conference on Advanced Computational and Communication Paradigms (ICACCP)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "onto", "onto", "ie", "ie", "ie", "onto", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "onto": 4, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 3}, "ld_mention_counts": {"ke": 1, "onto": 4}, "relevance_score": 0.9525741268224334}, {"paperId": "f6a36a324ee1c98b40bd430436a58e66c44cab18", "url": "https://www.semanticscholar.org/paper/f6a36a324ee1c98b40bd430436a58e66c44cab18", "title": "Toponym Disambiguation in an English-Lithuanian SMT System with Spatial Knowledge", "abstract": "This paper presents an innovative research resulting in the English-Lithuanian statistical factored phrase-based machine translation system with a spatial ontology. The system is based on the Moses toolkit and is enriched with semantic knowledge inferred from the spatial ontology. The ontology was developed on the basis of the GeoNames database (more than 15 000 toponyms), implemented in the web ontology language (OWL), and integrated into the machine translation process. Spatial knowledge was added as an additional factor in the statistical translation model and used for toponym disambiguation during machine translation. The implemented machine translation approach was evaluated against the baseline system without spatial knowledge. A multifaceted evaluation strategy including automatic metrics, human evaluation and linguistic analysis, was implemented to perform evaluation experiments. The results of the evaluation have shown a slight improvement in the output quality of machine translation with spatial knowledge.", "venue": "NODALIDA", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "onto", "onto", "onto", "mt", "mt", "mt", "mt", "onto"], "mention_counts": {"onto": 5, "mt": 5}, "nlp_mention_counts": {"mt": 5}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "71d51bcdc0a20d3142a6184990d31e8406b71e9e", "url": "https://www.semanticscholar.org/paper/71d51bcdc0a20d3142a6184990d31e8406b71e9e", "title": "Semantic Web for Machine Translation: Challenges and Directions", "abstract": "A large number of machine translation approaches have recently been developed to facilitate the fluid migration of content across languages. However, the literature suggests that many obstacles must still be dealt with to achieve better automatic translations. One of these obstacles is lexical and syntactic ambiguity. A promising way of overcoming this problem is using Semantic Web technologies. This article is an extended abstract of our systematic review on machine translation approaches that rely on Semantic Web technologies for improving the translation of texts. Overall, we present the challenges and opportunities in the use of Semantic Web technologies in Machine Translation. Moreover, our research suggests that while Semantic Web technologies can enhance the quality of machine translation outputs for various problems, the combination of both is still in its infancy.", "venue": "JT@ISWC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "sw", "sw", "mt", "sw", "sw", "mt", "mt", "mt", "sw"], "mention_counts": {"sw": 5, "mt": 5}, "nlp_mention_counts": {"mt": 5}, "ld_mention_counts": {"sw": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "35eb714fe2f21662b1446544e3262b2ae376dc6f", "url": "https://www.semanticscholar.org/paper/35eb714fe2f21662b1446544e3262b2ae376dc6f", "title": "Embedding Methods for Natural Language Processing", "abstract": "Embedding-based models are popular tools in Natural Language Processing these days. In this tutorial, our goal is to provide an overview of the main advances in this domain. These methods learn latent representations of words, as well as database entries that can then be used to do semantic search, automatic knowledge base construction, natural language understanding, etc. Our current plan is to split the tutorial into 2 sessions of 90 minutes, with a 30 minutes coffee break in the middle, so that we can cover in a first session the basics of learning embeddings and advanced models in the second session. This is detailed in the following.Part 1: Unsupervised and Supervised EmbeddingsWe introduce models that embed tokens (words, database entries) by representing them as low dimensional embedding vectors. Unsupervised and supervised methods will be discussed, including SVD, Word2Vec, Paragraph Vectors, SSI, Wsabie and others. A comparison between methods will be made in terms of applicability, type of loss function (ranking loss, reconstruction loss, classification loss), regularization, etc. The use of these models in several NLP tasks will be discussed, including question answering, frame identification, knowledge extraction and document retrieval.Part 2: Embeddings for Multi-relational DataThis second part will focus mostly on the construction of embeddings for multi-relational data, that is when tokens can be interconnected in different ways in the data such as in knowledge bases for instance. Several methods based on tensor factorization, collective matrix factorization, stochastic block models or energy-based learning will be presented. The task of link prediction in a knowledge base will be used as an application example. Multiple empirical results on the use of embedding models to align textual information to knowledge bases will also be presented, together with some demos if time permits.", "venue": "EMNLP 2014", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "nlp", "kg", "ke", "kg", "nlu", "nlp", "nlp"], "mention_counts": {"nlp": 3, "kg": 4, "ke": 1, "nlu": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1, "nlu": 1}, "ld_mention_counts": {"kg": 4, "ke": 1}, "relevance_score": 0.9525741268224334}, {"paperId": "637d726173933fdfb0729dca0e75064b005c791a", "url": "https://www.semanticscholar.org/paper/637d726173933fdfb0729dca0e75064b005c791a", "title": "Text Information Extraction Based on OWL Ontologies", "abstract": "This paper presents an information extraction system which can precisely extract semantic elements from texts by using OWL-based domain ontologies. In the system, each inputted text is viewed as a non-formalized instance of a domain ontology. And the outputted semantic elements extracted from the text are formalized elements composed of classes, individuals and properties. To perform the extraction, two extracting algorithms are proposed and used in the system. They are \"semantic information extracting algorithm\" and \"semantic information re-recognizing algorithmrdquo. When the extraction begins, ldquosemantic information extracting algorithm\" will extract classes, individuals and explicit properties from the text and store them in the database. After that, the text will be scanned by the \"semantic information re-recognizing algorithm\" and some useful information that is not found by \"semantic information extracting algorithm\" will be extracted. Experiments show that the results are accurate with two algorithms working cooperatively.", "venue": "2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "onto", "ie", "ie", "ie", "ie", "ie"], "mention_counts": {"onto": 5, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "ce718d281f21a25da6fe878dd8af5c2921271f9f", "url": "https://www.semanticscholar.org/paper/ce718d281f21a25da6fe878dd8af5c2921271f9f", "title": "Lexical Units in Ontological Semantics", "abstract": "In this paper we try to focus on the ontological semantic lexicons, one of the static knowledge sources that are used in Ontological Semantic theory for natural language processing. Ontological semantic (OS) is an approach to developing an exhaustive and detailed linguistic theory of meaning that is sufficient for NLP (natural language processing) by computers. It is interested in developing all the necessary processing and knowledge modules and combining them in a comprehensive system for a class of real-life NLP applications, such as MT, information extraction, text summarization, question answering, etc. It is a knowledge based system that required a vast amount of information regarding the world around a specific domain of application. Although we review the fundamentals of the approach, the focus is on the knowledge sources structures especially the lexicon. The paper Concentrate on some specific aspects that are key to the development of this OS approach, such as the acquisition of lexical units information, and the structure of the lexicon which is one of the central resources used in it. And some modern issues like the possibility of automation of static knowledge acquisition.", "venue": "2007 International Symposium on Computational Intelligence and Intelligent Informatics", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "nlp", "ie", "nlp", "nlp", "nlp", "kg"], "mention_counts": {"nlp": 4, "onto": 4, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 4, "ie": 1}, "ld_mention_counts": {"kg": 1, "onto": 4}, "relevance_score": 0.9525741268224334}, {"paperId": "38e399ab2ee9043bd7fa2b161f7a73c512f72f6b", "url": "https://www.semanticscholar.org/paper/38e399ab2ee9043bd7fa2b161f7a73c512f72f6b", "title": "Knowledge extraction by Internet monitoring to enhance crisis management", "abstract": "This paper presents our work on developing a system for Internet monitoring and knowledge extraction from different web documents which contain information about disasters. This system is based on ontology of the disasters domain for the knowledge extraction and it presents all the information extracted according to the kind of the disaster defined in the ontology. The system disseminates the information extracted (as a synthesis of the web documents) to the users after a filtering based on their profiles. The profile of a user is updated automatically by interactively taking into account his feedback.", "venue": "ISCRAM", "citationCount": 2, "fieldsOfStudy": ["Engineering", "Computer Science"], "mentions": ["ke", "ie", "onto", "ke", "onto", "ie", "ke"], "mention_counts": {"ke": 3, "onto": 2, "ie": 2}, "nlp_mention_counts": {"ke": 3, "ie": 2}, "ld_mention_counts": {"ke": 3, "onto": 2}, "relevance_score": 0.9525741268224334}, {"paperId": "6ff7021ad6a66f5a9f4f3e755f5d6d367e656ff7", "url": "https://www.semanticscholar.org/paper/6ff7021ad6a66f5a9f4f3e755f5d6d367e656ff7", "title": "KEEP: An Industrial Pre-Training Framework for Online Recommendation via Knowledge Extraction and Plugging", "abstract": "An industrial recommender system generally presents a hybrid list that contains results from multiple subsystems. In practice, each subsystem is optimized with its own feedback data to avoid the disturbance among different subsystems. However, we argue that such data usage may lead to sub-optimal online performance because of thedata sparsity. To alleviate this issue, we propose to extract knowledge from thesuper-domain that contains web-scale and long-time impression data, and further assist the online recommendation task (downstream task). To this end, we propose a novel industrial KnowlEdge Extraction and Plugging (KEEP) framework, which is a two-stage framework that consists of 1) a supervised pre-training knowledge extraction module on super-domain, and 2) a plug-in network that incorporates the extracted knowledge into the downstream model. This makes it friendly for incremental training of online recommendation. Moreover, we design an efficient empirical approach for KEEP and introduce our hands-on experience during the implementation of KEEP in a large-scale industrial system. Experiments conducted on two real-world datasets demonstrate that KEEP can achieve promising results. It is notable that KEEP has also been deployed on the display advertising system in Alibaba, bringing a lift of +5.4% CTR and +4.7% RPM.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "6b25e437556bc8e810c81bb8d90896a5a051c18c", "url": "https://www.semanticscholar.org/paper/6b25e437556bc8e810c81bb8d90896a5a051c18c", "title": "Multidirectional knowledge extraction process for creating behavioral personas", "abstract": "The consideration of user aspects in the design of computer systems is vital to maintaining the quality of these products. The search for user information as to their interests, needs, and behaviors, requires actions that can be costly. However, the use of such information in an inefficient, inaccurate manner, without the formation of knowledge discourages the process of user-centered development. Interface designers need to collect information and consume it efficiently. One option for documenting and efficiently consuming the information from user research is to apply the technique of personas. The extraction of knowledge in databases is the selection and processing of data with the purpose of identifying new patterns, provide greater accuracy on known patterns, and model the real world. Thus, this work presents a multidirectional process of extracting knowledge for creating behavioral personas. The processes of data mining are performed and compared with variations allowing a real knowledge extraction in the creation of personas. The process is multidirectional being able to be applied in both directions for the composition of any kind of personas. The results of the work demonstrate the extraction of knowledge originating from demographic information to the construction of personas focusing on behavioral aspects.", "venue": "IHC+CLIHC", "citationCount": 25, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "d21301e58137ee396be1ead8c9eb97784ba1083f", "url": "https://www.semanticscholar.org/paper/d21301e58137ee396be1ead8c9eb97784ba1083f", "title": "Converting unstructured and semi-structured data into knowledge", "abstract": "With the rapid growth in number and dimension of databases and database applications in business, administrative, industrial and other fields, it is necessary to examine the automatic extraction of knowledge from these large databases. Due to knowledge extraction from databases, these have become rich and safe sources for generating and verification of knowledge, and the knowledge discovery can be applied in software management, querying process, making decisions, process control and many other fields of interest. At the same time, there is a challenge in managing unstructured data. Among organizations with large concentration of unstructured information, there is a greater tendency to devote more resources to this kind of data. The acquisition of knowledge from unstructured data is often difficult and expensive. Some possible solutions on extracting useful information (knowledge) from unstructured data are provided. Knowledge extraction is the process of creation of knowledge from structured, unstructured and semi-structured data. The objective of this paper is to present the possibilities of extracting knowledge from unstructured and semi-structured data particularly. The theories and tools for knowledge extraction are the subject of the emerging field of knowledge discovery in databases (KDD). Definitions of KDD are provided and the general multistep KDD process is outlined. A brief summary of recent KDD real-world applications is also provided. Finally, the article enumerates challenges for future research and development in KDD systems.", "venue": "2013 11th RoEduNet International Conference", "citationCount": 38, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "a9b4b1ec648c6fb41855c1cfc53df2b691dbfa59", "url": "https://www.semanticscholar.org/paper/a9b4b1ec648c6fb41855c1cfc53df2b691dbfa59", "title": "Knowledge extraction and the integration by artificial life approach", "abstract": "Artificial life (A-Life) is a new paradigm to realize a phenomena of life and to extract the hidden principles. One of the most attractive features in the A-Life approach is emergence: simple elements interact with each other based on lower level rules, and then the higher level complex phenomena can emerge by interaction. The paper proposes a method for knowledge extraction and integration based on an A-Life approach. The proposed system has two parts: the knowledge extraction network and the A-Life environment. The simple elements interact in the A-Life environment and the data is transferred to the knowledge extraction network. The knowledge is extracted in the form of rules in the rule layer and then they are fed back to the simple elements in the A-Life environment. We dealt with a path planning problem as an example of A-Life environment. In the simulation, we assumed a severe condition: the position of the goal was unknown to the robots. Since the robots did not know the goal in the initial condition, the trajectory by the first robot that reached the goal is very complicated. The trajectory data which the robots had taken were inputted to the knowledge extraction network to extract rules. The trajectories become smooth step by step because of the extracted rules. We extracted various kinds of the rules using several different simple environments. By using the rules extracted from the simpler environments, the robot could reach the goal in a more complex environment.", "venue": "SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "1e34306c8f812fd3568fe0bea09449d200d438aa", "url": "https://www.semanticscholar.org/paper/1e34306c8f812fd3568fe0bea09449d200d438aa", "title": "Online Knowledge Extraction and Preference Guided Multi-Objective Optimization in Manufacturing", "abstract": "The integration of simulation-based optimization and data mining is an emerging approach to support decision-making in the design and improvement of manufacturing systems. In such an approach, knowledge extracted from the optimal solutions generated by the simulation-based optimization process can provide important information to decision makers, such as the importance of the decision variables and their influence on the design objectives, which cannot easily be obtained by other means. However, can the extracted knowledge be directly used during the optimization process to further enhance the quality of the solutions? This paper proposes such an online knowledge extraction approach that is used together with a preference-guided multi-objective optimization algorithm on simulation models of manufacturing systems. Specifically, it introduces a combination of the multi-objective evolutionary optimization algorithm, NSGA-II, and a customized data mining algorithm, called Flexible Pattern Mining (FPM), which can extract knowledge in the form of rules in an online and automatic manner, in order to guide the optimization to converge towards a decision maker\u2019s preferred region in the objective space. Through a set of application problems, this paper demonstrates how the proposed FPM-NSGA-II can be used to support higher quality decision-making in manufacturing.", "venue": "IEEE Access", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "42f239276eb4ed42f393ca58b207676cad107d70", "url": "https://www.semanticscholar.org/paper/42f239276eb4ed42f393ca58b207676cad107d70", "title": "Intelligent Methods to Extract Knowledge from Process Data in the Industrial Applications", "abstract": "Data are an expression of the language or numerical values that show some features. And the information is extracted from data for the specific purposes. The knowledge is utilized as information to construct rules that recognize patterns or make a decision. Today, knowledge extraction and application of that are broadly accomplished for the easy comprehension and the performance improvement of systems in the several industrial fields. The knowledge extraction can be achieved by some steps that include the knowledge acquisition, expression, and implementation. Such extracted knowledge is drawn by rules with data mining techniques. Clustering (CL), input space partition (ISP), neuro-fuzzy (NF), neural network (NN), extension matrix (EM), etc. are employed for the knowledge expression based upon rules. In this paper, the various approaches of the knowledge extraction are surveyed and categorized by methodologies and applied industrial fields. Also, the trend and examples of each approaches are shown in the tables and graphes using the categories such as CL, ISP, NF, NN, EM, and so on.", "venue": "Int. J. Fuzzy Log. Intell. Syst.", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "95d2ceca2e4303e73d62e5d22a7cd8e669e78a8a", "url": "https://www.semanticscholar.org/paper/95d2ceca2e4303e73d62e5d22a7cd8e669e78a8a", "title": "Knowledge Extraction for Identification of Chinese Organization Names", "abstract": "In this paper, a knowledge extraction process was proposed to extract the knowledge for identifying Chinese organization names. The knowledge extraction process utilizes the structure property, statistical property as well as partial linguistic knowledge of the organization names to extract new organizations from domain texts. The knowledge extraction processes were experimented on large amount of texts retrieved from WWW. With high standard of threshold values, new organization names can be identified with very high precision. Therefore the knowledge extraction processes can be carried out automatically to self improve the performance in the future.", "venue": "ACL 2000", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "c5f238b3f1aa123a04036373fe41e1759b704d2d", "url": "https://www.semanticscholar.org/paper/c5f238b3f1aa123a04036373fe41e1759b704d2d", "title": "Possibilistic Pertinence Feedback and Semantic Networks for Goal's Extraction", "abstract": "Pertinence Feedback is a technique that enables a u ser to interactively express his information requir ement by modifying his original query formulation with furth er information. This information is provided by exp licitly confirming the pertinent of some indicating objects and/or goals extracted by the system. Obviously th e user cannot mark objects and/or goals as pertinent until some a re extracted, so the first search has to be initiat ed by a query and the initial query specification has to be good enou gh to pick out some pertinent objects and/or goals from the Semantic Network. In this paper we present a short survey of fuzzy an d Semantic approaches to Knowledge Extraction. The goal of such approaches is to defin e flexible Knowledge Extraction Systems able to dea l with the inherent vagueness and uncertainty of the Extractio n process. It has long been recognised that interac tivity improves the effectiveness of Knowledge Extraction systems. Novice user\u2019s queries is the most natural and inter active medium of communication and recent progress in recognitio n is making it possible to build systems that inter act with the user. However, given the typical novice user\u2019s queries su bmitted to Knowledge Extraction Systems, it is easy to imagine that the effects of goal recognition errors in novi ce user\u2019s queries must be severely destructive on t he system\u2019s effectiveness. The experimental work reported in th is paper shows that the use of possibility theory i n classical Knowledge Extraction techniques for novice user\u2019s q uery processing is more robust than the use of the probability theory. Moreover, both possibilistic and probabilis tic pertinence feedback can be effectively employed to improve the effectiveness of novice user\u2019s query processing.", "venue": "ArXiv", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "24d6ee78d957192d38793b97326f9cfc4cfd73d9", "url": "https://www.semanticscholar.org/paper/24d6ee78d957192d38793b97326f9cfc4cfd73d9", "title": "On the Design of PSyKE: A Platform for Symbolic Knowledge Extraction", "abstract": "A common practice in modern explainable AI is to post-hoc explain black-box machine learning (ML) predictors \u2013 such as neural networks \u2013 by extracting symbolic knowledge out of them, in the form of either rule lists or decision trees. By acting as a surrogate model, the extracted knowledge aims at revealing the inner working of the black box, thus enabling its inspection, representation, and explanation. Various knowledge-extraction algorithms have been presented in the literature so far. Unfortunately, running implementations of most of them are currently either proof of concepts or unavailable. In any case, a unified, coherent software framework supporting them all \u2013 as well as their interchange, comparison, and exploitation in arbitrary ML workflows \u2013 is currently missing. Accordingly, in this paper we present the design of PSyKE, a platform providing general-purpose support to symbolic knowledge extraction from different sorts of black-box predictors via many extraction algorithms. Notably, PSyKE targets the extraction of symbolic knowledge in logic form , making it possible to extract first-order logic clauses as output. The extracted knowledge is thus both machine- and human-interpretable, and it can be used as a starting point for further symbolic processing\u2014e.g. automated reasoning.", "venue": "WOA", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "bb7393b8423bc49b3f63463a69ce8f9858075652", "url": "https://www.semanticscholar.org/paper/bb7393b8423bc49b3f63463a69ce8f9858075652", "title": "Workflow for Knowledge Extraction from Neural Network Classifiers", "abstract": "Artificial neural network classifiers are widespread models used by many machine learning engineers. Although due to fact they are black box models, in mission critical areas (like healthcare, finance, atomic power) when explainability is required they cannot be used even when they show higher classification performance in comparison to explainable models like decision trees. To mitigate this problem knowledge extraction algorithms have been proposed allowing to extract knowledge in different forms. Current paper gives a review of three knowledge extraction algorithms, presents their strengths and weaknesses. Finally knowledge extraction workflow utilizing abovementioned algorithms is described.", "venue": "2018 59th International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "e40c16249946936a3a68cfd705c24957a53d9751", "url": "https://www.semanticscholar.org/paper/e40c16249946936a3a68cfd705c24957a53d9751", "title": "Symbolic knowledge extraction from opaque ML predictors in PSyKE: Platform design & experiments", "abstract": "A common practice in modern explainable AI is to post-hoc explain black-box machine learning (ML) predictors \u2013 such as neural networks \u2013 by extracting symbolic knowledge out of them, in the form of either rule lists or decision trees. By acting as a surrogate model, the extracted knowledge aims at revealing the inner working of the black box, thus enabling its inspection, representation, and explanation. Various knowledge-extraction algorithms have been presented in the literature so far. Unfortunately, running implementations of most of them are currently either proofs of concept or unavailable. In any case, a unified, coherent software framework supporting them all \u2013 as well as their interchange, comparison, and exploitation in arbitrary ML workflows \u2013 is currently missing. Accordingly, in this paper we discuss the design of PSyKE, a platform providing general-purpose support to symbolic knowledge extraction from different sorts of black-box predictors via many extraction algorithms. Notably, PSyKE targets symbolic knowledge in logic form, allowing the extraction of first-order logic clauses. The extracted knowledge is thus both machine- and human-interpretable, and can be used as a starting point for further symbolic processing\u2014e.g. automated reasoning.", "venue": "Intelligenza Artificiale", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "79fe6fe1810f7dec043fc5ebb6b3ce59da90c3cf", "url": "https://www.semanticscholar.org/paper/79fe6fe1810f7dec043fc5ebb6b3ce59da90c3cf", "title": "Learning by Reading by Learning to Read", "abstract": "Knowledge-based natural language processing systems learn by reading, i.e., they process texts to extract knowledge. The performance of these systems crucially depends on knowledge about the domain of language itself, such as lexicons and ontologies to ground the semantics of the texts. In this paper we describe the architecture of the GIBRALTAR system, which is based on the OntoSem semantic analyzer, which learns by reading by learning to read. That is, while processing texts GIBRALTAR extracts both knowledge about the topics of the texts and knowledge about language (e.g., new ontological concepts and semantic mappings from previously unknown words to ontological concepts) that enables improved text processing. We present the results of initial experiments with GIBRALTAR and directions for future research.", "venue": "International Conference on Semantic Computing (ICSC 2007)", "citationCount": 26, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "tp", "tp", "nlp", "onto", "onto", "tp", "ke"], "mention_counts": {"onto": 3, "nlp": 1, "ke": 1, "kg": 1, "tp": 3}, "nlp_mention_counts": {"nlp": 1, "tp": 3, "ke": 1}, "ld_mention_counts": {"kg": 1, "onto": 3, "ke": 1}, "relevance_score": 0.9525741268224334}, {"paperId": "65f80e9ab05e251c8bf57d0a619f14fef1cd63db", "url": "https://www.semanticscholar.org/paper/65f80e9ab05e251c8bf57d0a619f14fef1cd63db", "title": "Concept recognition as a machine translation problem", "abstract": "Automated assignment of specific ontology concepts to mentions in text is a critical task in biomedical natural language processing, and the subject of many open shared tasks. Although the current state of the art involves the use of neural network language models as a post-processing step, the very large number of ontology classes to be recognized and the limited amount of gold-standard training data has impeded the creation of end-to-end systems based entirely on machine learning. Recently, Hailu et al. recast the concept recognition problem as a type of machine translation and demonstrated that sequence-to-sequence machine learning models have the potential to outperform multi-class classification approaches. We systematically characterize the factors that contribute to the accuracy and efficiency of several approaches to sequence-to-sequence machine learning through extensive studies of alternative methods and hyperparameter selections. We not only identify the best-performing systems and parameters across a wide variety of ontologies but also provide insights into the widely varying resource requirements and hyperparameter robustness of alternative approaches. Analysis of the strengths and weaknesses of such systems suggest promising avenues for future improvements as well as design choices that can increase computational efficiency with small costs in performance. Bidirectional encoder representations from transformers for biomedical text mining (BioBERT) for span detection along with the open-source toolkit for neural machine translation (OpenNMT) for concept normalization achieve state-of-the-art performance for most ontologies annotated in the CRAFT Corpus. This approach uses substantially fewer computational resources, including hardware, memory, and time than several alternative approaches. Machine translation is a promising avenue for fully machine-learning-based concept recognition that achieves state-of-the-art results on the CRAFT Corpus, evaluated via a direct comparison to previous results from the 2019 CRAFT shared task. Experiments illuminating the reasons for the surprisingly good performance of sequence-to-sequence methods targeting ontology identifiers suggest that further progress may be possible by mapping to alternative target concept representations. All code and models can be found at: https://github.com/UCDenver-ccp/Concept-Recognition-as-Translation.", "venue": "BMC Bioinformatics", "citationCount": 6, "fieldsOfStudy": ["Computer Science", "Medicine", "Biology"], "mentions": ["nlp", "onto", "mt", "mt", "onto", "onto", "mt", "mt", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 5, "mt": 4}, "nlp_mention_counts": {"nlp": 1, "mt": 4}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.9525741268224334}, {"paperId": "51910432af30042f69afa60542a87c056b129c52", "url": "https://www.semanticscholar.org/paper/51910432af30042f69afa60542a87c056b129c52", "title": "Using multiple ontologies in information extraction", "abstract": "Ontology-Based Information Extraction (OBIE) has recently emerged as a subfield of Information Extraction (IE). Here, ontologies - which provide formal and explicit specifications of conceptualizations - play a crucial role in the information extraction process. Several OBIE systems have been implemented previously but all of them use a single ontology although multiple ontologies have been designed for many domains. We have studied the theoretical basis for using multiple ontologies in information extraction and have developed information extraction systems that use them. These systems investigate the two major scenarios for having multiple ontologies for the same domain: specializing in sub-domains and providing different perspectives. The domain of universities has been used for the former scenario through a corpus collected from university websites. For the latter, the domain of terrorist attacks and a corpus used by a previous Message Understanding Conference (MUC) have been used. The results from these two case studies indicate that using multiple ontologies in information extraction has led to a clear improvement in performance measures.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 36, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "ie", "onto", "ie", "onto", "onto", "onto", "ie", "onto", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 8, "ie": 7}, "nlp_mention_counts": {"ie": 7}, "ld_mention_counts": {"onto": 8}, "relevance_score": 0.9295345381318304}, {"paperId": "48d553d84f8844b6d86dbff55567f5fd7e9303d5", "url": "https://www.semanticscholar.org/paper/48d553d84f8844b6d86dbff55567f5fd7e9303d5", "title": "An approach for natural language understanding in GIS based on ontology", "abstract": "A natural language interface can make a geographic information system (GIS) easy to use. It allows one to use the natural language quickly and conveniently to operate in such as digital city management system or traffic guidance system. This paper discusses the method of nature language understanding in GIS based on ontology. Natural language understanding is general apply in computer or artificial intelligence research area, yet in GIS the natural language understanding is mainly concerned about spatial information. In order to implement the natural language understanding for spatial information perfectly we use the ontology model. First we put forward a generally process of natural language understanding in GIS, defined the conception of the ontology, next set up the ontology structure, ontology-based understanding model, also indicate the mechanism of natural language understanding based on ontology. Finally are a case study and a prototype, a discussion about the research deficiency and the development forecast of my research.", "venue": "Geoinformatics", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Engineering"], "mentions": ["nlu", "nlu", "nlu", "nlu", "nlp", "nlu", "onto", "onto", "onto", "nlu", "onto", "onto", "onto", "onto", "nlu"], "mention_counts": {"nlp": 1, "onto": 7, "nlu": 7}, "nlp_mention_counts": {"nlp": 1, "nlu": 7}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.9295345381318304}, {"paperId": "179957fd648e0af875bc8f3797904b7cf7955199", "url": "https://www.semanticscholar.org/paper/179957fd648e0af875bc8f3797904b7cf7955199", "title": "Word Sense Disambiguation for Ontology Learning", "abstract": "Ontology learning aims to automatically extract ontological concepts and relationships from related text repositories and is expected to be more efficient and scalable than manual ontology development. One of the challenging issues associated with ontology learning is word sense disambiguation (WSD). Most WSD research employs resources such as WordNet, text corpora, or a hybrid approach. Motivated by the large volume and richness of user-generated content in social media, this research explores the role of social media in ontology learning. Specifically, our approach exploits social media as a dynamic context rich data source for WSD. This paper presents a method and preliminary evidence for the efficacy of our proposed method for WSD. The research is in progress toward conducting a formal evaluation of the social media based method for WSD, and plans to incorporate the WSD routine into an ontology learning system in the future.", "venue": "Americas Conference on Information Systems", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "wsd", "wsd", "onto", "wsd", "wsd", "wsd", "onto", "onto", "onto", "onto", "wsd", "wsd", "wsd"], "mention_counts": {"wsd": 8, "onto": 7}, "nlp_mention_counts": {"wsd": 8}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.9295345381318304}, {"paperId": "d9ce9e2615601c0e3dbcaa1813f7020db0615226", "url": "https://www.semanticscholar.org/paper/d9ce9e2615601c0e3dbcaa1813f7020db0615226", "title": "Ontology-Based Drug Product Information Extraction System", "abstract": "Along with Internet and Infobahn is springing up, it has become an important problem that how to catch the interesting information exactly and rapidly in drug ocean. Information Extraction (IE) is a new information management technology: basing on pre-definition template, it extracts special information from semi-structured data and unstructured data. A new approach to extracting drug product information from normal document based on application ontology is presented in this paper. In the paper, we analyze the system architecture, the taxonomy of information extraction, the key technology and weighing measure of information extraction, and introduce the main frame of the system and describe how to design and implement main modules. The results of IE experiment indicate ontology-based drug information extraction system can improve the F-measure which is reflection of recall and precision. Keywords-information extraction; ontology; semantic; inquiry; domain ontology I. INFORMATION EXTRACTION TECHNOLOGY Information Extraction (IE) [1] is a new information processing technology, whose purpose is extracting specific information from semi-structured text and unstructured text according to pre-definition template, and the main function of information extraction system [2] is extracting specific factual information from text. Usually, the information extracted is described in the structured form and can be stored in database directly for being queried and further analysis. Information extraction system can not only help people find information needed easily, but also help do further information processing (e.g. data-mining, text generation) after gaining the interest information efficiently from the information content of reasonable analysis and organization. Compared with traditional information retrieval technology, information extraction has obvious advantages [3]. Information retrieval gains related documents only by match retrieval, and it can\u2019t understand the factual information of documents but equates them with insignificant materials packed with vocabulary. However, information extraction extracts effective information content by the process of text analysis, semantic analysis, structured generation and etc. Information extraction is the further step of information retrieval, because it can not only search information but also understand information for users and output information via the form appointed by users, and it is more advanced than information retrieval. II. ONTOLOGY-BASED EXTRACTION METHOD Ontology-based drug information extraction is an information extraction system which is based on ontology and can process semantic [4]. The ontology technique can make the information extracted accurate and comprehensive. The system extracts specific information from the body of the texts according to the relations, concepts and keys in ontology, and writes the extraction results into intermediate result file. The system improves the information extraction to a new level by the application of ontology in information extraction. The method does not depend on the structure of documents because it is based on ontology. In theory, if the domain ontology is powerful enough, the method can achieve high precision and recall rate in the information extraction of the domain. Because of the distributed nature of ontology, the new domain ontology can be used in different places and be created by experts in different domain. When new domain ontology has be created, if the URI of the system has been given, the information extraction based on new ontology can be done to implement the extension in new domain in further. Generally, ontology\u2013based information extraction is carried out by both knowledge engineering method and automatic training method [5]. The knowledge engineering method is that rules and templates are made artificially via analyzing and adjusting the ontology knowledge base by experts. The automatic training method is that templates and their automatic stuffed knowledge base and rules are derived via machine learning based on the given tagged example document set or the statistical method. 978-1-4244-4134-1/09/$25.00 \u00a92009 IEEE Figure 1. The general structure of information extraction system III. OVERALL DESIGN OF THE EXTRACTION SYSTEM The whole information extraction system is divided into Application Ontology, Ontology Parsing, Syntax Analysis, Dictionary Editing, Rule Generation, Information Extraction, Database, Inquiry and Statistics, and etc, showed in Fig. 1. The introductions of every module are as follows. 1) Application Ontology. Ontology is the foundation of the information extraction system and includes concepts and relations of described domain. In addition, ontology also includes the restriction of concepts and relations, domain and range of relations, hierarchical structure between concepts, and etc. Ontology is a description of domain. Information extraction based on ontology is information extraction of domain which is described by ontology. 2) Ontology Parsing. The all concepts, relations and hierarchical structure which have been described in ontology are parsed in this process. These concepts and relations are stored into database in this system. The relations between records in database reflect the hierarchical relations between concepts in ontology. 3) Dictionary Editing. In the part of Ontology Parsing, the system only records concepts and relations, but the keys which represent these concepts and relations are not recorded. However, the specific vocabulary must be extracted finally, and Dictionary Editing is assigned to manage the keys of these concepts and relations, with which the users can add, modify, and delete the keys. And so do the keys\u2019 domain and range. That is, the user can manage the domain and range of relations, and store the results into database. 4) Syntax Analysis. Syntax Analysis is a module of pretreatment. Generally, the document to be processed is document of natural language. In this system, Syntax Analysis is mainly deleting the useless components (e.g. adverb, quantifier, adjectives, and etc.) in complex sentence and reserving the available noun structures and verb structures. In this way, the precision of information extraction can be improved. 5) Rule Generation. After parsing Domain Ontology and Dictionary Editing, the Rule Generation module generates the information extraction rule according to the parsing results and Dictionary Editing results in database. The system can do information extraction with the input documents according to the information extraction rule. 6) Information Extraction. The main functions of this module are information extraction using the rules generated by Rule Generation and the output text after pretreatment of Syntax Analysis, and storing the results into database. 7) Inquiry and Statistics. The main functions of this module are inquiring the results extracted from data according to conditions users appointed, and making statistical analysis on the results. IV. BASIC PROCESS OF THE EXTRACTION SYSTEM 1) Building Domain Ontology. Generally, the ontology is built by domain expert, and it includes the concepts, relations, restrictions, and other information about domain. 2) Ontology Parser parses Domain Ontology. The domain information is extracted and in the system proposed this paper it is stored into database. 3) Running the module of Dictionary Editing. It manages the keys of ontology\u2019s concepts and relations, and the domain In qu iry a nd S ta tis tic s Resource Database Dictionary Editing Syntax Analysis Ontology Parsing Untreated Document Ontology", "venue": "International Conference on BioMedical Engineering and Informatics", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "ie", "onto", "onto", "ie", "onto", "onto", "ie", "onto", "ie", "ie", "onto", "onto", "onto", "ie", "onto", "ie", "onto", "ie", "onto", "ie", "ie", "onto", "onto", "onto", "onto", "onto", "onto", "onto", "onto", "ie", "ie", "ie", "ie", "onto", "onto", "ie", "ie", "onto", "ie", "onto", "ie", "onto", "onto", "kg", "ie", "ie", "onto", "ie", "ie", "ie", "onto", "ie", "onto", "tp", "ie", "ie", "onto", "onto", "ke", "ie", "ie", "onto", "onto", "onto", "onto", "ie", "onto", "onto", "kg", "ie", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 39, "ke": 1, "kg": 2, "tp": 1, "ie": 34}, "nlp_mention_counts": {"ke": 1, "tp": 1, "ie": 34}, "ld_mention_counts": {"kg": 2, "onto": 39, "ke": 1}, "relevance_score": 0.9230769230769231}, {"paperId": "7632bb1b34f5338cc6ddc677cfb37806a428b9ba", "url": "https://www.semanticscholar.org/paper/7632bb1b34f5338cc6ddc677cfb37806a428b9ba", "title": "The Rationale for Building an Ontology Expressly for NLP", "abstract": "In this paper we argue for the need of NLP-specific resources to support truly high level, semantically oriented applications. We describe what, in our experience, constitutes useful knowledge for such applications and why most extant resources are not sufficient for this purpose, leading our Ontological Semantics group to build its own. We suggest that extensive time and energy are being spent on resources for NLP, though not on developing ones of higher utility but, rather, on trying to discover ways of using less than ideal ones. We believe that a more useful long-term approach to the problem of knowledge acquisition for NLP would be to acquire what is needed from the outset, since it is likely that in the end such work will prove necessary anyway. Introduction. A frequent question asked of our Ontological Semantics (OntoSem) group is, what available knowledge resources do you use? WordNet? FrameNet? XTAG?, etc. The question is valid: a number of research groups are building resources that are claimed to have if not primary then secondary applicability to natural language processing (NLP). So, if one were to assume that knowledge is knowledge \u2013 with the implication that any and all knowledge is valuable \u2013 then one would expect the developers of a knowledge-based system like OntoSem to voraciously incorporate everything available. We, however, do not do this because past attempts to incorporate resources that were not built explicitly to support semantic-rich text processing were less time efficient than starting from scratch; and, in a practical, application-oriented environment like OntoSem, the potential theoretical insights from experiments in resource merging become secondary to the practical necessity of building systems. Thus, we have been developing a suite of interconnected static resources and processors that are specifically targeted at high-end applications. In this paper we present a brief overview of OntoSem, describe why a number of the most widely reported resources are less applicable to NLP than is widely believed and hoped, and present the opinion that, as a field, we should develop resources that are truly sufficient for high-end NLP rather than spend the same significant amount of time and effort attempting to utilize resources borrowed from other fields or developed for other purposes, with inevitably inferior results. A Snapshot of Ontological Semantics. OntoSem is a text processing environment that takes as input unrestricted raw text and carries out its tokenization, morphological analysis, syntactic analysis, and semantic analysis to yield formal text-meaning representations (TMRs). Text analysis relies on: \u2022 the OntoSem language-independent ontology, which is represented using its own metalanguage and currently contains around 5,500 concepts, each described by an average of 16 properties (\u201cfeatures\u201d), selected from the hundreds of properties defined in the ontology; the number of concepts is intentionally restricted, so that mappings from lexicons are many-to-one; \u2022 an OntoSem lexicon for each language processed, whose entries contain (among other information) syntactic and semantic zones (linked through special variables) as well as procedural-semantic attachments that we call \u201cmeaning procedures;\u201d the semantic zone most frequently invokes ontological concepts, either directly or with modifications, but can also describe word meaning extraontologically, for example, in terms of parameterized values of modality, aspect, time, etc., or combinations thereof; \u2022 a fact repository, which contains real-world facts represented as numbered \u201cremembered instances\u201d of ontological concepts (e.g., SPEECH-ACT-3186 is the 3186 instantiation of the concept SPEECH-ACT in the world model constructed during text processing as the embodiment of text meaning); \u2022 the OntoSem text analyzers, covering everything from tokenization to TMR creation; \u2022 the TMR language, which is the metalanguage for representing text meaning, compatible with the metalanguage of the ontology and the fact repository. Details of this approach to text processing can be found, e.g., in Nirenburg and Raskin forthcoming and Nirenburg et al. 2003. The ontology itself, a brief ontology tutorial, and an extensive lexicon tutorial can be viewed at http://ilit.umbc.edu. TMRs represent, to our knowledge, the most semantically rich, automatically generated expressions of text meaning of any extant system. They require detailed lexical and world knowledge, most of which must be manually acquired. Many believe that manual knowledge acquisition is too expensive to be feasible, so they work on circumventing this problem: some groups attempt to maximize the use of noisy knowledge in NLP applications, e.g., Krymolowski and Roth (1998); and numerous groups attempt to adapt WordNet for use in NLP (especially with respect to problems of ambiguity): e.g., Mihalcea and Moldovan (2001) automatically generate a more coarse-", "venue": "LREC", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "tp", "nlp", "nlp", "kg", "nlp", "nlp", "onto", "tp", "nlp", "onto", "onto", "nlp", "onto", "onto", "onto", "onto", "tp", "nlp", "tp", "onto", "onto", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 10, "kg": 1, "onto": 11, "tp": 4}, "nlp_mention_counts": {"nlp": 10, "tp": 4}, "ld_mention_counts": {"kg": 1, "onto": 11}, "relevance_score": 0.9230615063798328}, {"paperId": "80e3014094b189e06c05be159d7c0f76fce0fba8", "url": "https://www.semanticscholar.org/paper/80e3014094b189e06c05be159d7c0f76fce0fba8", "title": "A 2-phase frame-based knowledge extraction framework", "abstract": "We present an approach for extracting knowledge from natural language English texts where processing is decoupled in two phases. The first phase comprises several standard NLP tasks whose results are integrated in a single RDF graph of mentions. The second phase processes the mention graph with SPARQL-like mapping rules to produce a knowledge graph organized around semantic frames (i.e., prototypical descriptions of events and situations). The decoupling allows: (i) choosing different tools for the NLP tasks without affecting the remaining computation; (ii) combining the outputs of different NLP tasks in non-trivial ways, leveraging their integrated and coherent representation in a mention graph; and (iii) relating each piece of extracted knowledge to the mention(s) it comes from, leveraging the single RDF representation. We evaluate precision and recall of our approach on a gold standard, showing its competitiveness w.r.t. the state of the art. We also evaluate execution times and (sampled) accuracy on a corpus of 110K Wikipedia pages, showing the applicability of the approach on large corpora.", "venue": "ACM Symposium on Applied Computing", "citationCount": 30, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "kg", "nlp", "nlp", "ke", "kg", "ke", "rdf", "rdf"], "mention_counts": {"nlp": 3, "ke": 3, "kg": 2, "rdf": 2}, "nlp_mention_counts": {"nlp": 3, "ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 2, "rdf": 2}, "relevance_score": 0.9129351298794525}, {"paperId": "c9fce4a2ac8388ba601c6a664da7b5326d3cdf24", "url": "https://www.semanticscholar.org/paper/c9fce4a2ac8388ba601c6a664da7b5326d3cdf24", "title": "Knowledge Extraction from Texts: a method for extracting predicate-argument structures from texts", "abstract": "1. A i m s o f t h e p r o j e c t The general aim of our project is to improve the quality of existing systems extracting knowledge from texts by introducing refined lexical semantics data. The conlribution of lexical ~mantics to knowledge extraction is not new and has already been demonstrated in a few systems. Our more precise aims are to: propose and show feasability of more radical semantic classifications which facilitate lexical descriptions by factoring out as much information as possible, enhancing re-usability of linguistic ressources. We show how the different linguistic ressources can be org~mized and how they interact, investigate different levels of granularity in the semantic descriptions and their impact on the quality of the extracted knowledge. In our system, granularity is considered at two levels: (1) linguistic: linguistic knowledge representations may be more or less precise, (2) functional: most modules of our system can work independently and thus can be used ~pamtely, evaluate different algorithms for extracting knowledge, taking into account efficiency aspects, evaluate the costs of extending our system to larger sets of texts anti to differeut application domains. Our prqiect is applied to research projects descriptions (noted hereafter as RPD) where the annual work of researchers at the DER of EDF (Direction des Etudes et des Rechcrches, Electricit6 de France) is described in terms of research actions. The extracted knowledge must be sufficiently accurate to allow for the realization of the following Imrposes: (1) evaluation of the importance of the use of techniques, procedures anti equipments, (2) automatic distribution of documents in different services, (3) interrogation, e.g. who does what anti what kind of results are available, (4) identification of relations of various types between projects, (5) construction of synthesis of research activities on precise topics, and (6) creation of the 'history' of a project. About 2.000 RPD are produced each year, each of about 200 words hmg. The total vocabulary is about 50.000 different words. Texts include fairly complex linguistic constructs. We also use the EDF thesaurus (encoding for nouns: taxonomies, associative relations, and synonyms, in a broad sense). In this document, we first introduce the linguistic organization of our project, present the general form of texts and identify the type of information which mnst be extracted out of them. Next, we present a semantic representation for the extracted knowlexlge, and study in more depth the extraction of information under the form of predicate-argument and predicate-modifier structures (Jackendoff 87a, Ka~ and Fodor 63).", "venue": "COLING", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke", "ie"], "mention_counts": {"ke": 6, "ie": 1}, "nlp_mention_counts": {"ke": 6, "ie": 1}, "ld_mention_counts": {"ke": 6}, "relevance_score": 0.9129351298794525}, {"paperId": "e594246abe604262265597c547225bc89cd57413", "url": "https://www.semanticscholar.org/paper/e594246abe604262265597c547225bc89cd57413", "title": "Knowledge Extraction Based on Sentence Matching and Analyzing", "abstract": "Knowledge extraction extracts knowledge from documents by analyzing document contents, then markups knowledge attribute and stores them into knowledge base. This paper discusses definition of knowledge extraction at first, then presents goal, key technique, application and system architecture of knowledge extraction based on sentence matching and analyzing. The system identifies new sentences in a paper, and makes an analysis of the sentences from internal structure and subject semantics. Then justify knowledge metadata by analyzing association among sentences and pragmatic of the whole paper. At last, allocates knowledge attribute tag for extracted knowledge, such as definition, historic development, characteristics, key technique, classification, application and development trend in the future. Knowledge extraction based on sentence matching and analyzing can not only justify the academic copying or scientific citation automatically, as well as make a document reviewed automatically, but also achieve a conversion on analysis from paper and chapter to sentence and paragraph, which can induce a revolution in knowledge organization and management. Therefore, the study has far-reaching theoretical significance and wide application.", "venue": "2008 International Symposium on Knowledge Acquisition and Modeling", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "ke", "ke", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 6}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"kg": 1, "ke": 6}, "relevance_score": 0.9129351298794525}, {"paperId": "196a6ac3065c6a01db89395ffea501cd8042bfd6", "url": "https://www.semanticscholar.org/paper/196a6ac3065c6a01db89395ffea501cd8042bfd6", "title": "Ontology-based word sense disambiguation using semi-automatically constructed ontology", "abstract": "This paper describes a method for disambiguating word senses by using semi-automatically constructed ontology. The ontology stores rich semantic constraints among 1,110 concepts, and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology. In order to acquire a reasonably practical ontology in limited time and with less manpower, we extend the existing Kadokawa thesaurus by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. The former can be obtained by converting valency information and case frames from previously-built electronic dictionaries used in machine translation. The latter can be acquired from concept co-occurrence information, which is extracted automatically from large corpora. In our practical machine translation system, our word sense disambiguation method achieved a 9.2% improvement over methods which do not use an ontology for Korean translation.", "venue": "MTSUMMIT", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "mt", "wsd", "onto", "wsd", "onto", "onto", "nlp", "onto", "mt", "onto", "wsd"], "mention_counts": {"nlp": 1, "wsd": 3, "onto": 7, "mt": 2}, "nlp_mention_counts": {"nlp": 1, "wsd": 3, "mt": 2}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.9129351298794525}, {"paperId": "56ea86bc7fe656e0b512f9e3efd165ba2f78d3f3", "url": "https://www.semanticscholar.org/paper/56ea86bc7fe656e0b512f9e3efd165ba2f78d3f3", "title": "Semantic-based lightweight ontology learning framework: a case study of intrusion detection ontology", "abstract": "Building ontology for wireless network intrusion detection is an emerging method for the purpose of achieving high accuracy, comprehensive coverage, self-organization and flexibility for network security. In this paper, we leverage the power of Natural Language Processing (NLP) and Crowdsourcing for this purpose by constructing lightweight semi-automatic ontology learning framework which aims at developing a semantic-based solution-oriented intrusion detection knowledge map using documents from Scopus. Our proposed framework uses NLP as its automatic component and Crowdsourcing is applied for the semi part. The main intention of applying both NLP and Crowdsourcing is to develop a semi-automatic ontology learning method in which NLP is used to extract and connect useful concepts while in uncertain cases human power is leveraged for verification. This heuristic method shows a theoretical contribution in terms of lightweight and timesaving ontology learning model as well as practical value by providing solutions for detecting different types of intrusions.", "venue": "WI", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "onto", "nlp", "onto", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 5, "onto": 6}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.8824434265896761}, {"paperId": "5972b34e313a03dcdf3bd81589cd54069d8f439d", "url": "https://www.semanticscholar.org/paper/5972b34e313a03dcdf3bd81589cd54069d8f439d", "title": "Using Background Knowledge to Support Coreference Resolution", "abstract": "Systems based on statistical and machine learning methods have been shown to be extremely effective and scalable for the analysis of large amount of textual data. However, in the recent years, it becomes evident that one of the most important direction of improvement in natural language processing (NLP) tasks, like word sense disambiguation, coreference resolution, relation extraction, and other tasks related to knowledge extraction, is by exploiting semantics. While in the past, the unavailability of rich and complete semantic descriptions constituted a serious limitation of their applicability, nowadays, the Semantic Web made available a large amount of logically encoded information (e.g. ontologies, RDF(S)-data, linked data, etc.), which constitute a valuable source of semantics. However, web semantics cannot be easily plugged into machine learning systems. Therefore the objective of this paper is to define a reference methodology for combining semantics information available in the web under the form of logical theories, with statistical methods for NLP. The major problems that we have to solve to implement our methodology concern (i) the selection of the correct and minimal knowledge among the large amount available in the web, (ii) the representation of uncertain knowledge, and (iii) the resolution and the encoding of the rules that combine knowledge retrieved from Semantic Web sources with semantics in the text. In order to evaluate the appropriateness of our approach, we present an application of the methodology to the problem of intra-document coreference resolution, and we show by means of some experiments on the ACE 2005 dataset, how the injection of knowledge is correlated to the improvement of the performance of our approach on this tasks.", "venue": "European Conference on Artificial Intelligence", "citationCount": 34, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ke", "onto", "wsd", "sw", "ld", "nlp", "nlp", "nlp", "sw"], "mention_counts": {"onto": 1, "ld": 1, "sw": 3, "ke": 1, "nlp": 3, "wsd": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1, "wsd": 1}, "ld_mention_counts": {"ld": 1, "sw": 3, "onto": 1, "ke": 1}, "relevance_score": 0.8824434265896761}, {"paperId": "08e036080bac77c2a5f57f9cec78ce4eb4674d9b", "url": "https://www.semanticscholar.org/paper/08e036080bac77c2a5f57f9cec78ce4eb4674d9b", "title": "Knowledge Extraction by Using an Ontology Based Annotation Tool", "abstract": "This paper describes a Semantic Annotation Tool for extraction of knowledge structures from web pages through the use of simple user-defined knowledge extraction patterns. The semantic annotation tool contains: an ontology-based mark-up component which allows the user to browse and to mark-up relevant pieces of information; a learning component (Crystal from the University of Massachusetts at Amherst) which learns rules from examples and an information extraction component which extracts the objects and relation between these objects. Our final aim is to provide support for ontology population by using the information extraction component. Our system uses as domain of study \u201cKMi Planet\u201d, a Webbased news server that helps to communicate relevant information between members in our institute.", "venue": "Semannot@K-CAP 2001", "citationCount": 118, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ke", "ie", "ie", "onto", "ke", "ke"], "mention_counts": {"ke": 3, "onto": 3, "ie": 2}, "nlp_mention_counts": {"ke": 3, "ie": 2}, "ld_mention_counts": {"ke": 3, "onto": 3}, "relevance_score": 0.8824434265896761}, {"paperId": "3a61020c61a81af745da72e423d3ec9429f9e196", "url": "https://www.semanticscholar.org/paper/3a61020c61a81af745da72e423d3ec9429f9e196", "title": "Research on Ontology Construction and Information Extraction Technology Based on WordNet", "abstract": "Introducing ontology in the field of information extraction can effectively improve the performance of information extraction. Ontology of college teachers\u2019 resumes are constructed in the form three layers of ontology framework structure on the basis of exploring the related technologies and criterion for building ontology, the Racer inference engine is used for realizing the consistency and accuracy detection. WordNet similarity calculation method is improved on the basis of this, so that manual collection method can be integrated into WordNet semantic similarity, thus substantially improving this method. The test shows that the result precisions based on WordNet ontology construction and information extraction both see significant improvement, which sufficiently shows the feasibility and validity of the method. Subject Categories and Descriptors: I.2.10 [Vision and Scene Understanding]: Information Extraction; I.4.10 [Image Representation] General Terms: Ontotology Construction, Information Processing", "venue": "J. Digit. Inf. Manag.", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "onto", "onto", "onto", "ie", "ie", "ie", "ie"], "mention_counts": {"onto": 6, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.8824434265896761}, {"paperId": "6ddbf0dbb5e24fcf2d730440f974653bdaf0ee65", "url": "https://www.semanticscholar.org/paper/6ddbf0dbb5e24fcf2d730440f974653bdaf0ee65", "title": "Open domain knowledge extraction: inference on a web scale", "abstract": "Though ontologies are considered central to foster the Semantic Web effort, their practical application on a Web scale is limited by the difficulty of building and maintaining high-coverage ontologies, for any of the almost unbounded number of social and technical domains mirrored on the Web. In this paper we present Open Knowledge Extraction (Open KE), a novel paradigm that creates a bridge between information retrieval, taxonomy learning and automated reasoning. Open KE builds on recently published algorithms for Open Information Extraction (Open IE) and automated taxonomy learning, which were shown able to extract information on a Web scale basis in an unsupervised manner. The key idea of Open KE is to generalize Open IE's lexicalized extractions with an automatically learned taxonomy. Lexicalized extractions are transformed in logic predicates, and used to populate a Semantic Model. An inference engine is then used to perform inductions and deductions. In this paper we describe the knowledge extraction workflow in its entirety, and apply it to a large-scale experiment in the domains of Artificial Intelligence and Virology.", "venue": "Web Intelligence, Mining and Semantics", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ke", "ke", "onto", "ke", "sw", "ie", "ie"], "mention_counts": {"ke": 3, "sw": 1, "onto": 2, "ie": 2}, "nlp_mention_counts": {"ke": 3, "ie": 2}, "ld_mention_counts": {"ke": 3, "sw": 1, "onto": 2}, "relevance_score": 0.8824434265896761}, {"paperId": "5446dec6e0ae8283a333915d0da23f6b8a900291", "url": "https://www.semanticscholar.org/paper/5446dec6e0ae8283a333915d0da23f6b8a900291", "title": "Automatic Production of an Ontology with NLP: Comparison between a Prolog Based Approach and a Cloud Approach Based on Bluemix Watson Service", "abstract": "Nowadays, most of the information available on the web is in Natural Language. Extracting such knowledge from Natural Language text is an essential work and a very remarkable research topic in the Semantic Web field. The logic programming language Prolog, based on the definite-clause formalism, is a useful tool for implementing a Natural Language Processing (NLP) systems. However, web-based services for NLP have also been developed recently, and they represent an important alternative to be considered. In this paper we present the comparison between two different approaches in NLP, for the automatic creation of an OWL ontology supporting the semantic annotation of text. The first one is a pure Prolog approach, based on grammar and logic analysis rules. The second one is based on Watson Relationship Extraction service of IBM Cloud platform Bluemix. We evaluate the two approaches in terms of performance, the quality of NLP result, OWL completeness and richness.", "venue": "2016 10th International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "sw", "onto", "nlp", "onto", "nlp", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 6, "sw": 1, "onto": 4}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.8824434265896761}, {"paperId": "fc7df3544c4a2d2df3b76bac5f66865760316051", "url": "https://www.semanticscholar.org/paper/fc7df3544c4a2d2df3b76bac5f66865760316051", "title": "Legal Knowledge Extraction for Knowledge Graph Based Question-Answering", "abstract": "This paper presents the Open Knowledge Extraction (OKE) tools combined with natural language analysis of the sentence in order to enrich the semantic of the legal knowledge extracted from legal text. In particular the use case is on international private law with specific regard to the Rome I Regulation EC 593/2008, Rome II Regulation EC 864/2007, and Brussels I bis Regulation EU 1215/2012. A Knowledge Graph (KG) is built using OKE and Natural Language Processing (NLP) methods jointly with the main ontology design patterns defined for the legal domain (e.g., event, time, role, agent, right, obligations, jurisdiction). Using critical questions, underlined by legal experts in the domain, we have built a question answering tool capable to support the information retrieval and to answer to these queries. The system should help the legal expert to retrieve the relevant legal information connected with topics, concepts, entities, normative references in order to integrate his/her searching activities.", "venue": "International Conference on Legal Knowledge and Information Systems", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "kg", "ke", "kg", "ke", "nlp", "onto"], "mention_counts": {"nlp": 2, "kg": 2, "onto": 1, "ke": 3}, "nlp_mention_counts": {"nlp": 2, "ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 1, "kg": 2}, "relevance_score": 0.8824434265896761}, {"paperId": "ae2506d664044faf92d9b953db8a60b9c1917bc1", "url": "https://www.semanticscholar.org/paper/ae2506d664044faf92d9b953db8a60b9c1917bc1", "title": "A knowledge extraction process specification for today's non-semantic Web", "abstract": "The semantic Web shall enable Web agents an efficient, precise, and comprehensive extraction of knowledge. Nevertheless, this new Web is not likely to be adopted in the immediate future. We present a specification of a new framework in order to extract knowledge from today's dynamics nonsemantic Web. Our proposal is novel in that it associates semantics with the information extracted, which improves agent interoperability; it can also deal with changes to the structure of a Web page, which improves adaptability; furthermore, it achieves to delegate the knowledge extraction procedure to specialist agents, easing software development and promoting software reuse and maintainability.", "venue": "Proceedings IEEE/WIC International Conference on Web Intelligence (WI 2003)", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ke", "ke", "sw", "ke", "ie", "ke"], "mention_counts": {"sw": 2, "ke": 4, "ie": 1}, "nlp_mention_counts": {"ke": 4, "ie": 1}, "ld_mention_counts": {"sw": 2, "ke": 4}, "relevance_score": 0.8824434265896761}, {"paperId": "49bfdcee046bde9e981261874d82ecc224a4e8e1", "url": "https://www.semanticscholar.org/paper/49bfdcee046bde9e981261874d82ecc224a4e8e1", "title": "Arabic Ontology Learning from Un-structured Text", "abstract": "Ontology Learning (OL) from a text is a process that consists of text processing, knowledge extraction, and ontology construction. For Arabic language, text processing, and knowledge extraction tasks are not mature as for Latin languages. They have not been integrated into the full Arabic OL pipeline. Currently, there is very little automated support for using knowledge from Arabic literature in semantically-enabled systems. This paper demonstrates the feasibility of using some existing OL methods for Arabic text and elicits proposals for further work toward building open domain OL systems for Arabic. This is done by building an OL system based on some available NLP tools for Arabic text utilizing GATE text analysis system for corpus and annotation management. The prototype is evaluated similarly to other OL systems and its performance is promising and recommended to enable more effective research and application of Arabic ontology learning.", "venue": "2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "tp", "onto", "nlp", "tp", "ke", "onto", "onto", "onto"], "mention_counts": {"nlp": 1, "ke": 2, "onto": 4, "tp": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2, "tp": 2}, "ld_mention_counts": {"ke": 2, "onto": 4}, "relevance_score": 0.8824434265896761}, {"paperId": "a737ec98d397a146e5f694cd2f3531187ff2517d", "url": "https://www.semanticscholar.org/paper/a737ec98d397a146e5f694cd2f3531187ff2517d", "title": "A spatio-temporal emotional framework for knowledge extraction and mining in digital humanities", "abstract": "PurposeThis paper aims to construct a spatio-temporal emotional framework (STEF) for digital humanities from a quantitative perspective, applying knowledge extraction and mining technology to promote innovation of humanities research paradigm and method.Design/methodology/approachThe proposed STEF uses methods of information extraction, sentiment analysis and geographic information system to achieve knowledge extraction and mining. STEF integrates time, space and emotional elements to visualize the spatial and temporal evolution of emotions, which thus enriches the analytical paradigm in digital humanities.FindingsThe case study shows that STEF can effectively extract knowledge from unstructured texts in the field of Chinese Qing Dynasty novels. First, STEF introduces the knowledge extraction tools \u2013 MARKUS and DocuSky \u2013 to profile character entities and perform plots extraction. Second, STEF extracts the characters' emotional evolutionary trajectory from the temporal and spatial perspective. Finally, the study draws a spatio-temporal emotional path figure of the leading characters and integrates the corresponding plots to analyze the causes of emotion fluctuations.Originality/valueThe STEF is constructed based on the \u201cspatio-temporal narrative theory\u201d and \u201cemotional narrative theory\u201d. It is the first framework to integrate elements of time, space and emotion to analyze the emotional evolution trajectories of characters in novels. The execuability and operability of the framework is also verified with a case novel to suggest a new path for quantitative analysis of other novels.", "venue": "Aslib J. Inf. Manag.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ie", "ke"], "mention_counts": {"ke": 5, "ie": 1}, "nlp_mention_counts": {"ke": 5, "ie": 1}, "ld_mention_counts": {"ke": 5}, "relevance_score": 0.8824434265896761}, {"paperId": "1f8dcd54faaf7863ce300add1c373cca8f8df795", "url": "https://www.semanticscholar.org/paper/1f8dcd54faaf7863ce300add1c373cca8f8df795", "title": "Comparison of Natural Language Processing Tools for Automatic Gene Ontology Annotation of Scientific Literature", "abstract": "Manual curation of scientific literature for ontology-based knowledge representation has proven infeasible and unscalable to the large and growing volume of scientific literature. Automated annotation solutions that leverage text mining and Natural Language Processing (NLP) have been developed to ameliorate the problem of literature curation. These NLP approaches use parsing, syntactical, and lexical analysis of text to recognize and annotate pieces of text with ontology concepts. Here, we conduct a comparison of four state of the art NLP tools at the task of recognizing Gene Ontology concepts from biomedical literature using the Colorado Richly Annotated Full-Text (CRAFT) corpus as a gold standard reference. We demonstrate the use of semantic similarity metrics to compare NLP tool annotations to the gold standard.", "venue": "International Conference on Biomedical Ontology", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "onto", "onto", "nlp", "kg", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 6, "onto": 4, "kg": 1}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"kg": 1, "onto": 4}, "relevance_score": 0.8824434265896761}, {"paperId": "84a8f09c5d2c98655323df0aba242c7811e1debe", "url": "https://www.semanticscholar.org/paper/84a8f09c5d2c98655323df0aba242c7811e1debe", "title": "The Ontology of Natural Language Processing", "abstract": "In this paper, we describe our proposed methodology for constructing an ontology of natural language processing (NLP). We use a semi-automatic method; a combination of rule-based and machine learning techniques; to construct and populate an ontology with bilingual (English-Persian) concept labels (lexicon) and evaluate it manually. This methodology results in a complete ontology in the natural language processing domain with 887 concepts, 88 relations, and 71 features. The built ontology is populated with near 36000 NLP related papers and 32000 authors, and about 201000 \"is_Related_to\", 83500 \"is_Author_of\", and 29000 \"Presented_in\" relations. The instantiation is done to enable applications find experts, publications and institutions related to various topics in NLP field.", "venue": "2019 5th International Conference on Web Research (ICWR)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "nlp", "onto", "onto", "nlp", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 6, "onto": 5}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.8824434265896761}, {"paperId": "80ee6d1d28ca1f3f67a8f2a1889c14afda27623c", "url": "https://www.semanticscholar.org/paper/80ee6d1d28ca1f3f67a8f2a1889c14afda27623c", "title": "Study on Word Sense Disambiguation Knowledge Base Based on Multi-Sources", "abstract": "Although there are many resources used in Natural Language Process, a specialized knowledge base used in word sense disambiguation(WSD) is still a shortage. By extracting knowledge from different resources and using them, we can improve the accuracy rate of word sense disambiguation. In this article, we use different methods existed to extract properties from The Grammatical Knowledge-base of Contemporary Chinese(GKB), HowNet, The Word-Sense Tagging Corpus (STC) and The Semantic Knowledge-base of Contemporary Chinese(SKCC) to build a specific knowledge base, which can help us with Chinese word sense disambiguation.", "venue": "2011 3rd International Workshop on Intelligent Systems and Applications", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "kg", "ke", "wsd", "wsd", "kg", "nlp", "kg", "kg", "kg"], "mention_counts": {"nlp": 1, "kg": 5, "wsd": 3, "ke": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "wsd": 3}, "ld_mention_counts": {"kg": 5, "ke": 1}, "relevance_score": 0.8824434265896761}, {"paperId": "b8167b9aca9a8263d9ddd54843467d2c616be1ec", "url": "https://www.semanticscholar.org/paper/b8167b9aca9a8263d9ddd54843467d2c616be1ec", "title": "Emotion Detection for Social Robots Based on NLP Transformers and an Emotion Ontology", "abstract": "For social robots, knowledge regarding human emotional states is an essential part of adapting their behavior or associating emotions to other entities. Robots gather the information from which emotion detection is processed via different media, such as text, speech, images, or videos. The multimedia content is then properly processed to recognize emotions/sentiments, for example, by analyzing faces and postures in images/videos based on machine learning techniques or by converting speech into text to perform emotion detection with natural language processing (NLP) techniques. Keeping this information in semantic repositories offers a wide range of possibilities for implementing smart applications. We propose a framework to allow social robots to detect emotions and to store this information in a semantic repository, based on EMONTO (an EMotion ONTOlogy), and in the first figure or table caption. Please define if appropriate. an ontology to represent emotions. As a proof-of-concept, we develop a first version of this framework focused on emotion detection in text, which can be obtained directly as text or by converting speech to text. We tested the implementation with a case study of tour-guide robots for museums that rely on a speech-to-text converter based on the Google Application Programming Interface (API) and a Python library, a neural network to label the emotions in texts based on NLP transformers, and EMONTO integrated with an ontology for museums; thus, it is possible to register the emotions that artworks produce in visitors. We evaluate the classification model, obtaining equivalent results compared with a state-of-the-art transformer-based model and with a clear roadmap for improvement.", "venue": "Italian National Conference on Sensors", "citationCount": 20, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlp", "nlp", "onto", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 4}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "a3379e7827331de5b182674e49e62f675bfc0a55", "url": "https://www.semanticscholar.org/paper/a3379e7827331de5b182674e49e62f675bfc0a55", "title": "Identification and classification of relations for Indian languages using machine learning approaches for developing a domain specific ontology", "abstract": "Information extraction and classification using Natural Language Processing techniques of layered architecture such as pre-processing task, processing of semantic analysis etc., helps in implementing further deeper evaluation techniques for the accuracy of natural language based electronic database. This paper explores relational information extraction of multilingual IndoWordNet database matching with domain specific terms. Further, extracted information are processed through conventional statistical methods, Normalized Web Distance (NWD) similarity method and two other machine learning evaluation techniques such as Support Vector Machine (SVM), Neural Network (NN) to compare with their accuracy. Results of machine leaning based techniques outperform with significant improved accuracy over conventional methods. The objective of using these techniques along with semantic web technology is to initiate a proof of concept for ontology generation by identification and classification of extracted relational information from IndoWordNet. This paper also highlights domain specific challenges and issues in developing relational model of ontology.", "venue": "International Conference on Computational Techniques in Information and Communication Technologies", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "sw", "nlp", "ie", "onto", "ie", "onto", "onto"], "mention_counts": {"nlp": 1, "sw": 1, "onto": 3, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ie": 3}, "ld_mention_counts": {"sw": 1, "onto": 3}, "relevance_score": 0.8807970779778823}, {"paperId": "17034f59ed7224f923d3808c04403f4de92f430b", "url": "https://www.semanticscholar.org/paper/17034f59ed7224f923d3808c04403f4de92f430b", "title": "[Research on information extraction of electronic medical records in Chinese].", "abstract": "This is a research to enhance the application of natural language understanding and ontology in the Chinese medical text semantic annotation and content analysis, and so to provide technology support for the computer-readable electronic medical records (EMR). The Chinese EMR information extraction and statistical analysis of related subjects in accordance to the user's demands were performed through building the named entity rules, the classified word list and field ontology by using GATE platform on the basis of EMR text set's construction and pre-processing. The automatic and artificial semantic annotation of EMR text set was implemented. The situation of drugs used in medicinal treatment and the distribution of patients' age and sex were obtained. The ontology-based semantic information extraction can improve the function of computer for text understanding, and the discovery of knowledge in EMR through field ontology is feasible.", "venue": "Sheng wu yi xue gong cheng xue za zhi = Journal of biomedical engineering = Shengwu yixue gongchengxue zazhi", "citationCount": 5, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ie", "onto", "ie", "ie", "onto", "nlu", "onto", "onto"], "mention_counts": {"onto": 4, "nlu": 1, "ie": 3}, "nlp_mention_counts": {"nlu": 1, "ie": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "27664ed94886d6d9d6e3f129ff6aa0dbb203182b", "url": "https://www.semanticscholar.org/paper/27664ed94886d6d9d6e3f129ff6aa0dbb203182b", "title": "Extracting Occupational Therapy Concepts to Develop Domain Ontology", "abstract": "Recently, unstructured data on the World Wide Web has generated significant interest in the extraction of text, emails, web pages, reports and research papers in their raw form. Far more interestingly, extracting information from a specific domain using distributed corpora from the World Wide Web is a vital step towards creating corpus annotation. This paper describes a method of annotation, based on Occupational Therapy (OT) concepts, to build domain ontology using Natural Language Programming (NLP) technology. We used Java Annotation Patterns Engine (JAPE) grammar to support regular expression matching and thus annotate OT concepts using a GATE developer tool. This speeds up the time-consuming development of the ontology, which is important for experts in the domain facing time constraints and high workloads. The rules provide significant results: the pattern matching of OT concepts based on the lookup list produced 403 correct concepts and the accuracy was generally higher. Using NLP technique is a good approach to reducing the domain expert's work, and the results can be evaluated . Keywords-Ontology; Information extracting; Regular expression; Natural Language Programming.", "venue": "ICDS 2013", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 4, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "850f1698dd11cc6970ff1fdbe0241ca23b15e557", "url": "https://www.semanticscholar.org/paper/850f1698dd11cc6970ff1fdbe0241ca23b15e557", "title": "Web Information Extraction for the Creation of Metadata in Semantic Web", "abstract": "In this paper, we develop an automatic metadata creation system using the information extraction technology for the Semantic Web. The information extraction system consists of preparation part that takes written text as the input and produces the POS tags for the words in the sentences. Then we employ finite state machine technology to extract the units from the tagged sequences, including complex words, basic phrases and domain events. We use the components of an NLP software architecture, GATE, as the processing engine and support all required language resources for the engine. We have carried out an experiment on Chinese financial news. It shows promising precision rate while it need further investigation on the recall part. We describe the implementation of storing the extracted result in RDF to an RDF server and show the service interface for accessing the content.", "venue": "ROCLING/IJCLCLP", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "nlp", "sw", "ie", "sw", "rdf", "ie", "ie"], "mention_counts": {"nlp": 1, "sw": 2, "rdf": 2, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ie": 3}, "ld_mention_counts": {"sw": 2, "rdf": 2}, "relevance_score": 0.8807970779778823}, {"paperId": "049b334a318777610e22baa6536437a9100c1adc", "url": "https://www.semanticscholar.org/paper/049b334a318777610e22baa6536437a9100c1adc", "title": "frances: A Deep Learning NLP and Text Mining Web Tool to Unlock Historical Digital Collections: A Case Study on the Encyclopaedia Britannica", "abstract": "This work presents frances, an integrated text mining tool that combines information extraction, knowledge graphs, NLP, deep learning, parallel processing and Semantic Web techniques to unlock the full value of historical digital textual collections, offering new capabilities for researchers to use powerful analysis methods without being distracted by the technology and middleware details. To demonstrate these capabilities, we use the first eight editions of the Encyclopaedia Britannica offered by the National Library of Scotland (NLS) as an example digital collection to mine and analyse. We have developed novel parallel heuristics to extract terms from the original collection (alongside metadata), which provides a mix of unstructured and semi-structured input data, and populated a new knowledge graph with this information. Our Natural Language Processing models enable frances to perform advanced analyses that go significantly beyond simple search using the information stored in the knowledge graph. Furthermore, frances also allows for creating and running complex text mining analyses at scale. Our results show that the novel computational techniques developed within frances provide a vehicle for researchers to formalize and connect findings and insights derived from the analysis of large-scale digital corpora such as the Encyclopaedia Britannica.", "venue": "IEEE International Conference on e-Science", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "ie", "nlp", "kg", "kg", "kg", "sw"], "mention_counts": {"nlp": 3, "sw": 1, "kg": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "ie": 1}, "ld_mention_counts": {"kg": 3, "sw": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "b953c0e848cbf25ccef65e89566e8c04c464efb0", "url": "https://www.semanticscholar.org/paper/b953c0e848cbf25ccef65e89566e8c04c464efb0", "title": "Ontology-Based Interactive Information Extraction", "abstract": "Interactive Information Extraction brings together search and \ninformation extraction to provide fast, interactive text mining over \nlarge volumes of text such as Medline abstracts, full text scientific \narticles, patents etc. As well as covering the two ends of the spectrum: \nkeyword search over documents, and detailed linguistic patterns within \nsentences, the Interactive Information Extraction System, I2E, also \ncovers the points in between such as keywords within the same sentence, \nor co-occurrence of biological entities within sentences or documents. \nThis talk briefly introduces the idea of Interactive Information \nExtraction, and describes how terminologies/ontologies are incorporated. \nWe also show how I2E can be used to augment ontologies by finding \npotential synonyms or members of classes from the literature using \nlinguistic patterns. Finally we discuss issues concerning how best to \nuse ontologies for text mining.", "venue": "Ontologies and Text Mining for Life Sciences", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie", "onto", "ie", "ie", "onto"], "mention_counts": {"onto": 4, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "d7be0d0d30c8d2bdf08e755ab89dd9bc7c3d7b41", "url": "https://www.semanticscholar.org/paper/d7be0d0d30c8d2bdf08e755ab89dd9bc7c3d7b41", "title": "A semantic framework for extracting taxonomic relations from text corpus", "abstract": "Nowadays, ontologies have been exploited in many current applications due to the abilities in representing knowledge and inferring new knowledge. However, the manual construction of ontologies is tedious and time-consuming. Therefore, the automated ontology construction from text has been investigated. The extraction of taxonomic relations between concepts is a crucial step in constructing domain ontologies. To obtain taxonomic relations from a text corpus, especially when the data is deficient, the approach of using the web as a source of collective knowledge (a.k.a web-based approach) is usually applied. The important challenge of this approach is how to collect relevant knowledge from a large amount of web pages. To overcome this issue, we propose a framework that combines Word Sense Disambiguation (WSD) and web approach to extract taxonomic relations from a domain-text corpus. This framework consists of two main stages: concept extraction and taxonomic-relation extraction. Concepts acquired from the concept-extraction stage are disambiguated through WSD module and passed to stage of extraction taxonomic relations afterward. To evaluate the efficiency of the proposed framework, we conduct experiments on datasets about two domains of tourism and sport. The obtained results show that the proposed method is efficient in corpora which are insufficient or have no training data. Besides, the proposed method outperforms the state of the art method in corpora having high WSD results.", "venue": "\u02dcThe \u0153international Arab journal of information technology", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "onto", "wsd", "wsd", "onto", "onto", "wsd", "onto"], "mention_counts": {"wsd": 4, "onto": 4}, "nlp_mention_counts": {"wsd": 4}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "092312cb2a59502a2db7ffea1e31c19ad2eecc7c", "url": "https://www.semanticscholar.org/paper/092312cb2a59502a2db7ffea1e31c19ad2eecc7c", "title": "H2-Golden-Retriever: Methodology and Tool for an Evidence-Based Hydrogen Research Grantsmanship", "abstract": "Background of Study: Hydrogen is poised to play a major role in decarbonizing the economy. The need to discover, develop, and understand low-cost, high-performance, durable materials that can help maximize the cost of electrolysis as well as the need for an intelligent tool to make evidence-based Hydrogen research funding decisions relatively easier warranted this study. Aim: In this work, we developed H2 Golden Retriever (H2GR) system for Hydrogen knowledge discovery and representation using Natural Language Processing (NLP), Knowledge Graph and Decision Intelligence. This system represents a novel methodology encapsulating state-of-the-art technique for evidence-based research grantmanship. Methods: Relevant Hydrogen papers were scraped and indexed from the web and preprocessing was done using noise and stop-words removal, language and spell check, stemming and lemmatization. The NLP tasks included Named Entity Recognition using Stanford and Spacy NER, topic modeling using Latent Dirichlet Allocation and TF-IDF. The Knowledge Graph module was used for the generation of meaningful entities and their relationships, trends and patterns in relevant H2 papers, thanks to an ontology of the hydrogen production domain. The Decision Intelligence component provides stakeholders with a simulation environment for cost and quantity dependencies. the abstract. Technical articles of interest are then read to find relevant information and further insights to orient their keyword searches in a more promising direction. This makes keeping up with the torrential pace of publications very difficult, cumbersome, time-consuming and labor-intensive. Since Hydrogen production cost depends on multiple assumptions including materials, electrolyzer design, manufacturing practices, and soft costs, it is difficult to trace back the contribution of the price of a component to the overall system cost. This makes cost of a component to vary from one technical report to another; for instance, a study[12] reported that bipolar plates is the third contributor behind the Catalyst Coated Membrane (CCM) and Porous Transport Layer (PTL) to the stack cost while another article[13] reported it as the highest contributor. It is therefore hard to crystallize on which sub-component deserves the highest focus. Lastly, experts are on a race against time. Understanding how one component affects the overall cost is one thing but how much money must be invested in versus the time it will take to reduce the overall system cost must drive the decisions of the experts. In this work, we addressed the missing link between the slew of information available in literature and the decision to fund research and, by implication, propose an AI-augmented decision tool for knowledge acquisition, knowledge extraction and an evidence-based research funding decision support tool. techniques in include word tokenization, word stemming and lemmatization, topical modeling, named-entity recognition, summarization, word cloud and keyword extraction. These tasks use both linguistics and mathematics to connect the language of with the language of computers.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "nlp", "kg", "ke", "nlp", "onto"], "mention_counts": {"nlp": 3, "kg": 2, "onto": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1}, "ld_mention_counts": {"kg": 2, "onto": 1, "ke": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "1d118d42b61e11a230ee6ae5f5a50db585ff3ac8", "url": "https://www.semanticscholar.org/paper/1d118d42b61e11a230ee6ae5f5a50db585ff3ac8", "title": "Knowledge Extraction from Question and Answer Platforms on the Semantic Web: A Systematic Review of Technologies Available for Information Extraction", "abstract": "Knowledge Extraction is the process of getting structured data from unstructured or semi-structured sources. Much research has been conducted in this field and applying these technologies to the web has become a key effort in the past few years. This is due to changes from web 1.0 where the web was simply a set of static pages where user interaction was minimal. With the rise of web 2.0, the internet is no longer a medium to access static information. Users can now share their own thoughts easily thus increasing the amount of user generated content. This has made the web ripe with knowledge, however not all information can be easily accessed. This paper aims to bridge the gap between knowledge available and the knowledge accessed using knowledge extraction.", "venue": "2018 8th International Conference on Intelligent Systems, Modelling and Simulation (ISMS)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "ke", "ke", "sw"], "mention_counts": {"ke": 3, "sw": 1, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3, "sw": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "efe99ed29946cafdea6e328dc80538561fb35c12", "url": "https://www.semanticscholar.org/paper/efe99ed29946cafdea6e328dc80538561fb35c12", "title": "Knowledge Extraction: A Comparison between Symbolic and Connectionist Methods", "abstract": "The use of a linguistic representation for expressing knowledge acquired by learning systems is an important issue as regards to user understanding. Under this assumption, and to make sure that these systems will be welcome and used, several techniques have been developed by the artificial intelligence community, under both the symbolic and the connectionist approaches. This work discusses and investigates three knowledge extraction techniques based on these approaches. The first two techniques, the C4.5 and CN2 symbolic learning algorithms, extract knowledge directly from the data set. The last technique, the TREPAN algorithm extracts knowledge from a previously trained neural network. The CN2 algorithm induces if...then rules from a given data set. The C4.5 algorithm extracts decision trees, although it can also extract ordered rules, from the data set. Decision trees are also the knowledge representation used by the TREPAN algorithm.", "venue": "Int. J. Neural Syst.", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "ffdcc01ac23c1511060882d901ab78ae92e52e0a", "url": "https://www.semanticscholar.org/paper/ffdcc01ac23c1511060882d901ab78ae92e52e0a", "title": "Endless and Scalable Knowledge Table Extraction from Semi-structured Websites", "abstract": "The problem of scalable knowledge extraction from the Web has attracted much attention in the past decade. However, it is under explored how to extract the structured knowledge from semi-structured Websites in a fully automatic and scalable way. In this work, we define the table-formatted structured data with clear schema as Knowledge Tables and propose a scalable learning system, which is named as Kable to extract knowledge from semi-structured Websites automatically in a never ending and scalable way. Kable consists of two major components, which are auto wrapper induction and schema matching respectively. In contrast to the state of the art auto wrappers for semi-structured Web sites, our adopted approach can run around 1'000 times faster, which makes the Web scale knowledge extraction possible. On the other hand, we propose a novel schema matching solution which can work effectively on the auto-extracted structured data. With 3 months' continuous run using ten Web servers, we successfully extracted 427,105,009 knowledge facts. The manual labeling over sampled knowledge extracted show the up to 87% precision for supporting various Web applications.", "venue": "2012 IEEE 12th International Conference on Data Mining Workshops", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "dadeefebaba33607d90572c36b7b427b309fab6d", "url": "https://www.semanticscholar.org/paper/dadeefebaba33607d90572c36b7b427b309fab6d", "title": "Mining the disaster hotspots - situation-adaptive crowd knowledge extraction for crisis management", "abstract": "When disaster strikes, emergency professionals rapidly need to gain Situation Awareness (SAW) on the unfolding crisis situation, thus need to determine what has happened and where help and resources are needed. Nowadays, platforms like Twitter are used as real-time communication hub for sharing such information, like humans' on-site observations, advice and requests, and thus can serve as a network of \u201chuman sensors\u201d for retrieving information on crisis situations. Recently, so-called crowd-sensing systems for crisis management have started to utilize these networks for harvesting crisis-related social media content. However, up to now these mainly support their human operators in the visual analysis of retrieved messages only and do not aim at the automated extraction and fusion of semantically-grounded descriptions of the underlying real-world crisis events from these textual contents, such as providing structured descriptions of the types and locations of reported damage. This hampers further computational situation assessment, such as providing overall description of the on-going crisis situation, its associated consequences and required response actions. Consequently, this lack of semantically-grounded situational context does not allow to fully implement situation-adaptive crowd knowledge extraction, meaning the system can utilize already established (crowd) knowledge to correspondingly adapt its crowd-sensing and knowledge extraction process alongside the monitored situation, to keep pace with the underlying real-world incidents. In the light of this, in the present paper, we illustrate the realization of a situation-adaptive crowd-sensing and knowledge extraction system by introducing our crowdSA prototype, and examine its potential in a case study on a real-world Twitter crisis data set.", "venue": "Conference on Cognitive and Computational Aspects of Situation Management", "citationCount": 16, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "2e09d95fccfb870c5d92c2f27e135f327f93df30", "url": "https://www.semanticscholar.org/paper/2e09d95fccfb870c5d92c2f27e135f327f93df30", "title": "The Need for Knowledge Extraction: Understanding Harmful Gambling Behavior with Neural Networks", "abstract": "Responsible gambling is a field of study that involves supporting gamblers so as to reduce the harm that their gambling activity might cause. Recently in the literature, machine learning algorithms have been introduced as a way to predict potentially harmful gambling based on patterns of gambling behavior, such as trends in amounts wagered and the time spent gambling. In this paper, neural network models are analyzed to help predict the outcome of a partial proxy for harmful gambling behavior: when a gambler \u201cself-excludes\u201d, requesting a gambling operator to prevent them from accessing gambling opportunities. Drawing on survey and interview insights from industry and public officials as to the importance of interpretability, a variant of the knowledge extraction algorithm TREPAN is proposed which can produce compact, human-readable logic rules efficiently, given a neural network trained on gambling data. To the best of our knowledge, this paper reports the first industrial-strength application of knowledge extraction from neural networks, which otherwise are black-boxes unable to provide the explanatory insights which are crucially required in this area of application. We show that through knowledge extraction one can explore and validate the kinds of behavioral and demographic profiles that best predict self-exclusion, while developing a machine learning approach with greater potential for adoption by industry and treatment providers. Experimental results reported in this paper indicate that the rules extracted can achieve high fidelity to the trained neural network while maintaining competitive accuracy and providing useful insight to domain experts in responsible gambling.", "venue": "ECAI", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "77c6fb4681da3bf903167483710ae9dd46c76dc0", "url": "https://www.semanticscholar.org/paper/77c6fb4681da3bf903167483710ae9dd46c76dc0", "title": "Knowledge extraction from big data using MapReduce-based Parallel-Reduct algorithm", "abstract": "Extraction of knowledge and predictive analysis are the new challenges for the rapidly growing large volume of data to make the right decision at right time. It is difficult to store, analyze and visualize such large data volume with its diversities with standard data mining tools. Hence, in this paper, we develop a MapReduce approach of a Parallel-Reduct algorithm based on the rough set theory for knowledge extraction. It performs data and task parallelism with the help of Map and Reduce functions to find minimum reduct. An extensive experimental evaluation shows that the proposed MapReduce-based parallel approach effectively processes big data on the Hadoop platform and it is more efficient than the sequential approach to extract knowledge from large data sets under different coarseness.", "venue": "International Conference on Computer Science and Network Technology", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "5e87fc714e8da51415b9d1ae773076451f9c586a", "url": "https://www.semanticscholar.org/paper/5e87fc714e8da51415b9d1ae773076451f9c586a", "title": "Comparing Data-Driven Methods for Extracting Knowledge from User Generated Content", "abstract": "This study aimed to compare two techniques of business knowledge extraction for the identification of insights related to the improvement of digital marketing strategies on a sample of 15,731 tweets. The sample was extracted from user generated content (UGC) from Twitter using two methods based on knowledge extraction techniques for business. In Method 1, an algorithm to detect communities in complex networks was applied; this algorithm, in which we applied data visualization techniques for complex networks analysis, used the modularity of nodes to discover topics. In Method 2, a three-phase process was developed for knowledge extraction that included the application of a latent Dirichlet allocation (LDA) model, a sentiment analysis (SA) that works with machine learning, and a data text mining (DTM) analysis technique. Finally, we compared the results of each of the two techniques to see whether or not the results yielded by these two methods regarding the analysis of companies\u2019 digital marketing strategies were mutually complementary.", "venue": "Journal of Open Innovation: Technology, Market and Complexity", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "41a6bdb5eac4187707f4bfcd14ec6e7421a4f527", "url": "https://www.semanticscholar.org/paper/41a6bdb5eac4187707f4bfcd14ec6e7421a4f527", "title": "Entity Pair Recognition using Semantic Enrichment and Adversarial Training for Chinese Drug Knowledge Extraction", "abstract": "Existing knowledge extraction methods in pharmacy often use natural language processing tools and deep learning model to identify drug entities and extract their relationships from drug instructions, thus obtaining drug-drug or drug-disease knowledge. However, sentences in drug instructions may contain multiple drug-related entities, and existing methods lack the capability of identifying valid the \"drug-drug\" or \"drug-disease\" entity pairs. This will introduce significant noise data in the subsequent tasks such as entity relationship extraction and knowledge graph construction. Meanwhile, some mentions in the sentence can have hierarchical relations even if they do not form valid entity pairs, such information is also crucial to knowledge extraction. To solve these two problems, this paper proposes an entity pair verification model based on entity semantic enhancement and adversarial training. Through the experiment on more than 2000 kinds of drug instructions data, the experimental results show that the F1 value of the model for entity pair verification is up to 98.65%, which is up to 9.37% compared with the existing methods.", "venue": "International Symposium on Artificial Intelligence in Medical Sciences", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "ke", "ke", "kg"], "mention_counts": {"nlp": 1, "kg": 1, "ke": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "27b09d410aa1d911d95867d29a8c44aebf376b0e", "url": "https://www.semanticscholar.org/paper/27b09d410aa1d911d95867d29a8c44aebf376b0e", "title": "Knowledge extraction from Copernicus satellite data", "abstract": "We describe two alternative approaches of how to extract knowledge from high- and medium-resolution Synthetic Aperture Radar (SAR) images of the European Sentinel-1 satellites. To this end, we selected two basic types of images, namely images depicting arctic shipping routes with icebergs, and - in contrast - coastal areas with various types of land use and human-made facilities. In both cases, the extracted knowledge is delivered as (semantic) categories (i.e., local content labels) of adjacent image patches from big SAR images. Then, machine learning strategies helped us design and validate two automated knowledge extraction systems that can be extended for the understanding of multispectral satellite images.", "venue": "IOP Conference Series: Earth and Environmental Science", "citationCount": 0, "fieldsOfStudy": ["Physics", "Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "457d96d2e4570d720937f264d94179b3695d2fb3", "url": "https://www.semanticscholar.org/paper/457d96d2e4570d720937f264d94179b3695d2fb3", "title": "Scientific and Technological Text Knowledge Extraction Method of based on Word Mixing and GRU", "abstract": "The knowledge extraction task is to extract triple relations (head entity-relation-tail entity) from unstructured text data. The existing knowledge extraction methods are divided into \"pipeline\" method and joint extraction method. The \"pipeline\" method is to separate named entity recognition and entity relationship extraction and use their own modules to extract them. Although this method has better flexibility, the training speed is slow. The learning model of joint extraction is an end-to-end model implemented by neural network to realize entity recognition and relationship extraction at the same time, which can well preserve the association between entities and relationships, and convert the joint extraction of entities and relationships into a sequence annotation problem. In this paper, we propose a knowledge extraction method for scientific and technological resources based on word mixture and GRU, combined with word mixture vector mapping method and self-attention mechanism, to effectively improve the effect of text relationship extraction for Chinese scientific and technological resources.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "66eb4dab46a1557262c8f2a94075470a9390c03f", "url": "https://www.semanticscholar.org/paper/66eb4dab46a1557262c8f2a94075470a9390c03f", "title": "Harnessing spontaneous participation on social media: implementing the knowledge extraction component", "abstract": "Recent efforts to mainstream social media-based and citizen-led political deliberations to complement traditional government-led e-Participation, must among others address a number of technical challenges. One of these challenges is how to automatically filter and extract meaningful information or knowledge from streams of social media data contributed by citizens to inform agenda setting or serve as feedback for specific policy issues. A central resource in developing an effective information extraction system is the Lexicon of terms and relationship among terms in the domain of interest. This poster summarizes our work in developing a lexicon of public services names as the kernel of the knowledge extraction component of our Social Software Infrastructure for spontaneous participation. Our lexicon-based knowledge extraction process is scalable and could be easily extended to capture other information of interest from social media or related platforms.", "venue": "Digital Government Research", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "ie", "ke"], "mention_counts": {"kg": 1, "ke": 3, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 3}, "relevance_score": 0.8807970779778823}, {"paperId": "5643e2e1ad72f4f0b020b712b968adf2c11fe7ac", "url": "https://www.semanticscholar.org/paper/5643e2e1ad72f4f0b020b712b968adf2c11fe7ac", "title": "Model and Algorithms for Optimizing a Human Computing System Oriented to Knowledge Extraction by Use of Crowdsourcing", "abstract": "The paper is addressing the actual context of Data Deluge, where the need and also premises to extract more knowledge are increasing, along with the increase of our expectations about performances. Besides, improving artificial intelligence (AI), by machine learning (ML), deep learning (DL) or cognitive learning (CL) performance/potential, when adding human contributions where necessary, is an important and promising research area. Consequently, our model, algorithms (ALG1; ALG2) and soft programs provide useful new instruments for implementing and optimizing the workflow based on crowdsourcing, when using human potential in a human computing system. We aim to increase AI quality adding multiple human outputs for every AI task and leveraging learning rules to be then extended to larger sets of tasks. This way, such hybrid system could be oriented to more knowledge extraction, by the generalization of images/ captions/labels toward more complex tasks, like providing content essential or question answering. Our instruments include features of ranking workers and tasks profiles, which will support the main original process of knowledge extraction, but also the inference elements, by small amounts of learning data (regarding the workers skills and tasks efficiency) to be transferred to AI/ML/DL/CL, which then could be used for processing larger volumes of similar data. Among the results conclusions is that using progressive optimization, structuring the data/tasks in variable (progressive) sets and potential (skill/number) of workers, is both efficacious and efficient, allowing a flexible control of the system and workflow for matching a diversity of tasks complexity/ difficulty/volume and leveraging knowledge extraction.", "venue": "2020 13th International Conference on Communications (COMM)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "8cf257d8ed64feec890cc7b526ccbe70235f0108", "url": "https://www.semanticscholar.org/paper/8cf257d8ed64feec890cc7b526ccbe70235f0108", "title": "Generating Domain-Specific Knowledge Graphs: Challenges with Open Information Extraction", "abstract": "Knowledge Graphs (KGs) are a popular way to structure and represent knowledge in a machine-readable way. While KGs serve as the foundation for many applications, the automatic construction of these KGs from texts is a challenging task where Open Information Extraction techniques are prominently leveraged. In this paper, we focus on generating a domain-specific knowledge graph based on art-historic texts from a digitized text collection. We describe the combined use and adaption of existing open information extraction methods to build an art-historic KG that can facilitate data exploration for domain experts. We discuss the challenges that were faced at each step and present detailed error analysis to identify the limitations of existing methods when working with domain-specific corpora.", "venue": "TEXT2KG/MK@ESWC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "kg", "ie", "ie", "ke", "ie"], "mention_counts": {"kg": 3, "ke": 1, "ie": 3}, "nlp_mention_counts": {"ke": 1, "ie": 3}, "ld_mention_counts": {"kg": 3, "ke": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "dc63187a3742465f014a5ce8a46fce35d4d14e7a", "url": "https://www.semanticscholar.org/paper/dc63187a3742465f014a5ce8a46fce35d4d14e7a", "title": "A Proposal for a Two-Way Journey on Validating Locations in Unstructured and Structured Data", "abstract": "The Web of Data has grown explosively over the past few years, and as with any dataset, there are bound to be invalid statements in the data, as well as gaps. Natural Language Processing (NLP) is gaining interest to fill gaps in data by transforming (unstructured) text into structured data. However, there is currently a fundamental mismatch in approaches between Linked Data and NLP as the latter is often based on statistical methods, and the former on explicitly modelling knowledge. However, these fields can strengthen each other by joining forces. In this position paper, we argue that using linked data to validate the output of an NLP system, and using textual data to validate Linked Open Data (LOD) cloud statements is a promising research avenue. We illustrate our proposal with a proof of concept on a corpus of historical travel stories.", "venue": "LDK", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ld", "nlp", "lod", "nlp", "lod", "ld", "nlp"], "mention_counts": {"ld": 2, "nlp": 4, "lod": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"ld": 2, "lod": 2}, "relevance_score": 0.8807970779778823}, {"paperId": "a85432684ff8f1f945e92744ee9636ed66c78eef", "url": "https://www.semanticscholar.org/paper/a85432684ff8f1f945e92744ee9636ed66c78eef", "title": "Segmentation-based Knowledge Extraction from Chest X-ray Images", "abstract": "Computer-aided detection applications have been extensively used to assist physicians in clinical diagnoses. Extracted information from X-ray, positron emission tomography, and magnetic resonance images enables radiologists and other physicians to identify pathologies, correlate findings with the symptoms, and determine the treatment steps. In this study, we proposed an automatic knowledge extraction methodology from chest X-ray images. The extracted knowledge is obtained from the segmented sections of the images that include pathological findings. We evaluated these segmented images with a) classical machine learning and b) pretrained convolutional neural network (CNN) models. Evaluations were based on areas under the receiver operating characteristic (AUROC) with segmented images using the pretrained CNN and the traditional method models, and they produced the average AUROC scores of 0.96 and 0.52, respectively. Traditional methods yielded lower AUROC scores compared with pretrained CNN methods. However, traditional methods may still be considered as appropriate solutions for disease diagnoses primarily based on their advantages regarding running time and flexibility.", "venue": "2019 4th Asia-Pacific Conference on Intelligent Robot Systems (ACIRS)", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke", "kg", "ke"], "mention_counts": {"ke": 3, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "af54be9932acafcb39be1755caaa3ad67f3eeb84", "url": "https://www.semanticscholar.org/paper/af54be9932acafcb39be1755caaa3ad67f3eeb84", "title": "A new automatic knowledge extraction method for course documents applied in the web-based teaching system", "abstract": "With the development of web-based teaching system, automatic knowledge extraction from course documents becomes more and more important. This paper gives a new automatic knowledge extraction method for course documents. Based on TF strategy, this method uses frequency and location to measure the credit value of knowledge. Moreover, the penalty factor is defined to adjust the credit value of knowledge. In this paper, the naive Bayes method is improved and applied in automatic knowledge extraction from course documents. Finally, we compare the method proposed in this paper with improved naive Bayes method based on experiments results. The results show that the average performance of this method is better than that of the naive Bayes method.", "venue": "2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "6bcb7510f2541de1a5d50062c6ffae7d14606029", "url": "https://www.semanticscholar.org/paper/6bcb7510f2541de1a5d50062c6ffae7d14606029", "title": "Knowledge extraction from scenery images and the recognition using fuzzy inference neural networks", "abstract": "A new system of knowledge extraction and recognition from scenery images is proposed in this paper. The system can extract different levels of knowledge automatically using Fuzzy Inference Neural Network (FINN). The proposed system consists of several Knowledge Extraction Networks (KENs). Each one is composed of FINN, and it can extract fuzzy if-then rules automatically. The KEN has an input-output (I/O) layer and two different-sized rule layers. The I/O layer includes the input part and the output part. The input part receives information on a pixel such as the position, the Intensity, the Hue and the Saturation. The output part receives the label of the corresponding pixel such as sky, mountains and woods, etc. The larger rule layer extracts detailed knowledge and it uses for the image recognition. On the other hand, the smaller rule layer extracts global knowledge which can correct contradiction of detailed knowledge and can remove trivial knowledge. It can be seen that the proposed system can recognize the image almost correctly by computer experiments. Knowledge is obtained by integrating rules from each KEN and then translating them into linguistic form. The extracted knowledge is quite natural.", "venue": "SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)", "citationCount": 11, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "13c5765932a0fdf5bce9c350421626f3427093ae", "url": "https://www.semanticscholar.org/paper/13c5765932a0fdf5bce9c350421626f3427093ae", "title": "Heuristic constraints enforcement for training of and knowledge extraction from a fuzzy/neural architecture. I. Foundation", "abstract": "Using fuzzy/neural architectures to extract heuristic information from systems has received increasing attention. A number of fuzzy/neural architectures and knowledge extraction methods have been proposed. Knowledge extraction from systems where the existing knowledge limited is a difficult task. One of the reasons is that there is no ideal rulebase, which can be used to validate the extracted rules. In most of the cases, using output error measures to validate extracted rules is not sufficient as extracted knowledge may not make heuristic sense, even if the output error may meet the specified criteria. The paper proposes a novel method for enforcing heuristic constraints on membership functions for rule extraction from a fuzzy/neural architecture. The proposed method not only ensures that the final membership functions conform to a priori heuristic knowledge, but also reduces the domain of search of the training and improves convergence speed. Although the method is described on a specific fuzzy/neural architecture, it is applicable to other realizations, including adaptive or static fuzzy inference systems. The foundations of the proposed method are given in Part I. The techniques for implementation and integration into the training are given in Part II, together with applications.", "venue": "IEEE transactions on fuzzy systems", "citationCount": 32, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "157b8138726f6b25ddfcd6ad9c7eff644602fd13", "url": "https://www.semanticscholar.org/paper/157b8138726f6b25ddfcd6ad9c7eff644602fd13", "title": "Knowledge extraction from reinforcement learning", "abstract": "This paper deals with knowledge extraction from reinforcement learners. It addresses two approaches towards knowledge extraction: the extraction of explicit, symbolic rules front neural reinforcement learners; and the extraction of complete plans from such learners. The advantages of such knowledge extraction include: the improvement of learning (especially with the rule extraction approach); and the improvement of the usability of results of learning.", "venue": "IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "a6eaade8664ab0b08bd46a54ce661c403dc458ba", "url": "https://www.semanticscholar.org/paper/a6eaade8664ab0b08bd46a54ce661c403dc458ba", "title": "Assessing Wireless Network Dependability through Knowledge Extraction via Decision Trees", "abstract": "Critical infrastructures such as wireless network systems demand dependability. Dependability attributes reported here include availability, reliability, maintainability and survivability (ARMS). This research uses computer simulation and knowledge extraction to introduce a new approach to measure dependability of wireless networks. Earlier research has used computer simulation for estimating wireless network dependability. This work introduces a new methodology which uses discrete time event simulation in-put/output to train an artificial neural network and then extract knowledge via decision trees. A comparison of decision tree extraction technique results are discussed, including those from neural (TREPAN) and non neural networks (C4.5). Significant insights are gained into increasing wireless infrastructure dependability through such knowledge extraction techniques; however the neural approach is superior from a parsimonious and comprehensibility perspective.", "venue": "Third International Conference on Systems (icons 2008)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "29f0afb2914231af8e40b44619d9da3993adc4a3", "url": "https://www.semanticscholar.org/paper/29f0afb2914231af8e40b44619d9da3993adc4a3", "title": "A rough set multi-knowledge extraction algorithm and its formal concept analysis", "abstract": "Rough set theory provides an effective method to reduce attributes and extract knowledge. This paper represents a rough set multi-knowledge extraction algorithm and its formal concept analysis. The proposed algorithm can obtain multi-reducts by using rough set in decision table. The formal concept analysis is used to obtain rules from the main values of the attributes influencing the decision making and these rules build a multi-knowledge. Experimental results show that the proposed multi-knowledge extraction algorithm is efficient.", "venue": "International Conference on Intelligent Systems Design and Applications", "citationCount": 1, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "44242e0aa8b865096b958b1485c9a93540513f34", "url": "https://www.semanticscholar.org/paper/44242e0aa8b865096b958b1485c9a93540513f34", "title": "Applying a Semantic Interpreter to a Knowledge Extraction Task", "abstract": "A system that extracts knowledge from encyclopedic texts is presented. The knowledge extraction component is based on a semantic interpreter of English based on an enhanced WordNet. The input to the knowledge extraction component is the output of the semantic interpreter. The extraction task was chosen in order to test the semantic interpreter. The following aspects are described: the definition of verb predicates and semantic roles, the organization of the inferences, an evaluation of the system, and a session with the system.", "venue": "NLUCS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "614d673cc4ad8ff4444ca4f9c19a8c2080314aeb", "url": "https://www.semanticscholar.org/paper/614d673cc4ad8ff4444ca4f9c19a8c2080314aeb", "title": "Rule extraction from recurrent neural networks using a symbolic machine learning algorithm", "abstract": "Addresses the extraction of knowledge from recurrent neural networks trained to behave like deterministic finite-state automata (DFAs). To date, methods used to extract knowledge from such networks have relied on the hypothesis that network states tend to cluster and that clusters of network states correspond to DFA states. The computational complexity of such a cluster analysis has led to heuristics which either limit the number of clusters that may form during training or limit the exploration of the output space of hidden recurrent state neurons. These limitations, while necessary, may lead to reduced fidelity, i.e. the extracted knowledge may not model the true behavior of a trained network, perhaps not even for the training set. The method proposed uses a polynomial-time symbolic learning algorithm to infer DFAs solely from the observation of a trained network's input/output behavior. Thus, this method has the potential to increase the fidelity of the extracted knowledge.", "venue": "ICONIP'99. ANZIIS'99 & ANNES'99 & ACNN'99. 6th International Conference on Neural Information Processing. Proceedings (Cat. No.99EX378)", "citationCount": 22, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "50398333009b1c7cc29f6db9b37bff35131379e9", "url": "https://www.semanticscholar.org/paper/50398333009b1c7cc29f6db9b37bff35131379e9", "title": "NLP-based Ontology Learning from Legal Texts. A Case Study", "abstract": "The paper reports on the methodology and preliminary results of a case study in automatically extracting ontological knowledge from Italian legislative texts in the environmental domain. We use a fully\u2013implemented ontology learning system (T2K) that includes a battery of tools for Natural Language Processing (NLP), statistical text analysis and machine language learning. Tools are dynamically integrated to provide an incremental representation of the content of vast repositories of unstructured documents. Evaluated results, however preliminary, are very encouraging, showing the great potential of NLP\u2013powered incremental systems like T2K for accurate large\u2013scale semi-automatic extraction of legal ontologies.", "venue": "LOAIT", "citationCount": 27, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 4, "onto": 4}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "bc36834e62b1cd098a2be81628c16b9b41693997", "url": "https://www.semanticscholar.org/paper/bc36834e62b1cd098a2be81628c16b9b41693997", "title": "What kind of knowledge is in Wikipedia? Unsupervised extraction of properties for similar concepts", "abstract": "This article presents a novel method for extracting knowledge from Wikipedia and a classification schema for annotating the extracted knowledge. Unlike the majority of approaches in the literature, we use the raw Wikipedia text for knowledge acquisition. The main assumption made is that the concepts classified under the same node in a taxonomy are described in a comparable way in Wikipedia. The annotation of the extracted knowledge is done at two levels: ontological and logical. The extracted properties are evaluated in the traditional way, that is, by computing the precision of the extraction procedure and in a clustering task. The second method of evaluation is seldom used in the natural language processing community, but it is regularly employed in cognitive psychology.", "venue": "J. Assoc. Inf. Sci. Technol.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "ke", "nlp", "ke"], "mention_counts": {"nlp": 1, "ke": 3, "onto": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "9fbba3ac8b3a55657c52b5704f3884e9b1af413d", "url": "https://www.semanticscholar.org/paper/9fbba3ac8b3a55657c52b5704f3884e9b1af413d", "title": "Building Semantic/Ontological Knowledge by Text Mining", "abstract": "People have long talked about having NLP systems employ semantic knowledge on a large scale. However, no-one has yet built a large ontology that was indeed practically useful for tasks such as question answering, machine translation, and information retrieval. Work on WordNet, a major contender, shows that it requires more content to realize its full potential, while efforts to use CYC show how hard it is to build general-purpose ontologies that can support NLP applications. In this talk I outline some recent efforts to automatically acquire knowledge that may be placed into terminological ontologies and used by NLP systems, and mention some problems in evaluating the quality of the results.", "venue": "COLING 2002", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "onto", "onto", "mt", "nlp"], "mention_counts": {"nlp": 3, "onto": 4, "mt": 1}, "nlp_mention_counts": {"nlp": 3, "mt": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "2d7511fcf6e74576a656f5ecb16cef0a0366ac2d", "url": "https://www.semanticscholar.org/paper/2d7511fcf6e74576a656f5ecb16cef0a0366ac2d", "title": "Towards applying text mining and natural language processing for biomedical ontology acquisition", "abstract": "The use of text mining and natural language processing can extend into the realm of knowledge acquisition and management for biomedical applications. In this paper, we describe how we implemented natural language processing and text mining techniques on the transcribed verbal descriptions from retinal experts of biomedical disease features. The feature-attribute pairs generated were then incorporated within a user interface for a collaborative ontology development tool. This tool, IDOCS, is being used in the biomedical domain to help retinal specialists reach a consensus on a common ontology for describing age-related macular degeneration (AMD). We compare the use of traditional text mining and natural language processing techniques with that of a retinal specialist's analysis and discuss how we might integrate these techniques for future biomedical ontology and user interface development.", "venue": "TMBIO '06", "citationCount": 18, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "nlp", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 4, "onto": 4}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "205d5151172384cdea7c5611e06244d7010ab566", "url": "https://www.semanticscholar.org/paper/205d5151172384cdea7c5611e06244d7010ab566", "title": "Bio-Ontology and text: bridging the modeling gap", "abstract": "MOTIVATION\nNatural language processing (NLP) techniques are increasingly being used in biology to automate the capture of new biological discoveries in text, which are being reported at a rapid rate. Yet, information represented in NLP data structures is classically very different from information organized with ontologies as found in model organisms or genetic databases. To facilitate the computational reuse and integration of information buried in unstructured text with that of genetic databases, we propose and evaluate a translational schema that represents a comprehensive set of phenotypic and genetic entities, as well as their closely related biomedical entities and relations as expressed in natural language. In addition, the schema connects different scales of biological information, and provides mappings from the textual information to existing ontologies, which are essential in biology for integration, organization, dissemination and knowledge management of heterogeneous phenotypic information. A common comprehensive representation for otherwise heterogeneous phenotypic and genetic datasets, such as the one proposed, is critical for advancing systems biology because it enables acquisition and reuse of unprecedented volumes of diverse types of knowledge and information from text.\n\n\nRESULTS\nA novel representational schema, PGschema, was developed that enables translation of phenotypic, genetic and their closely related information found in textual narratives to a well-defined data structure comprising phenotypic and genetic concepts from established ontologies along with modifiers and relationships. Evaluation for coverage of a selected set of entities showed that 90% of the information could be represented (95% confidence interval: 86-93%; n = 268). Moreover, PGschema can be expressed automatically in an XML format using natural language techniques to process the text. To our knowledge, we are providing the first evaluation of a translational schema for NLP that contains declarative knowledge about genes and their associated biomedical data (e.g. phenotypes).\n\n\nAVAILABILITY\nhttp://zellig.cpmc.columbia.edu/PGschema", "venue": "Bioinform.", "citationCount": 28, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "onto", "onto", "onto", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 4}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8807970779778823}, {"paperId": "c1e3f57a789cc0631e7ffa0503ffe4e7fa01e949", "url": "https://www.semanticscholar.org/paper/c1e3f57a789cc0631e7ffa0503ffe4e7fa01e949", "title": "oWSD: A Tool for Word Sense Disambiguation in Its Ontology Context", "abstract": "Word sense disambiguation (abbr. WSD) is very important to the semantic web/web 2.0. However, there is still no easy-to-use tool available. As a remedy, here a simple and very efficient tool called oWSD is demonstrated. It disambiguates the senses of words in their ontological contexts, and obtains the right word senses from WordNet. It is very helpful to applications involving ontologies and natural language processing as well.", "venue": "SEMWEB", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "onto", "onto", "onto", "wsd", "wsd", "wsd"], "mention_counts": {"nlp": 1, "sw": 1, "onto": 3, "wsd": 3}, "nlp_mention_counts": {"nlp": 1, "wsd": 3}, "ld_mention_counts": {"sw": 1, "onto": 3}, "relevance_score": 0.8807970779778823}, {"paperId": "b3efb95224c3a48d627306684429b6a53f02c75a", "url": "https://www.semanticscholar.org/paper/b3efb95224c3a48d627306684429b6a53f02c75a", "title": "Natural Language and Knowledge Representation", "abstract": "The aim of this track is to bring researchers from the knowledge representation (KR) and the natural language processing (NLP) communities together to discuss common \" representational \" and \" reasoning \" issues. In addition to the two main challenging concerns , namely, expressivity and fast reasoning, representations should attempt to be transparent and friendly. The NLP community has made some progress in terms of processing and handling ambiguity and the KR community has realized that a lot of knowledge is already \" coded \" in NL. Researchers on both sides consider benefiting from each other's progress and taking on issues that were left to be solved by the \" other \" community. The accepted papers and posters in this track discuss issues relating to the semantic web, semantic annotation, NL semantics and NLP-based techniques, the bottleneck KR problem, ontologies and NL interpretation, the use of KR and knowledge bases to resolve NL ambiguity, the use of NL to \" disambiguate and strengthen \" single observations for learning tasks, the use of NL to support constructing DB/ontology query interface , the possibility of using (controlled) NL as a KR, underspecified representations and reasoning, mapping \" syntax and semantics \" to a KR, and the disadvantages of using KR that is remote from NL semantics.", "venue": "J. Log. Comput.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "sw", "onto", "nlp", "nlp", "onto", "kg"], "mention_counts": {"nlp": 4, "sw": 1, "onto": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"sw": 1, "onto": 2, "kg": 1}, "relevance_score": 0.8807970779778823}, {"paperId": "5f21507c873487d3c78dffd126cece5a1b8f8e32", "url": "https://www.semanticscholar.org/paper/5f21507c873487d3c78dffd126cece5a1b8f8e32", "title": "Commonsense knowledge, ontology and ordinary language", "abstract": "Over two decades ago a \"quite revolution\" overwhelmingly replaced knowledge- based approaches in natural language processing (NLP) by quantitative (e.g., statistical, corpus-based, machine learning) methods. Although it is our firm belief that purely quanti- tative approaches cannot be the only paradigm for NLP, dissatisfaction with purely engi- neering approaches to the construction of large knowledge bases for NLP are somewhat justified. In this paper we hope to demonstrate that both trends are partly misguided and that the time has come to enrich logical semantics with an ontological structure that reflects our commonsense view of the world and the way we talk about in ordinary language. In this paper it will be demonstrated that assuming such an ontological structure a number of challenges in the semantics of natural language (e.g., metonymy, intensionality, copredica- tion, nominal compounds, etc.) can be properly and uniformly addressed.", "venue": "Int. J. Reason. based Intell. Syst.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "kg", "onto", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 4, "onto": 3, "kg": 1}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 1, "onto": 3}, "relevance_score": 0.8807970779778823}, {"paperId": "1c816e8fdb826abde7dcd8b39307200eb54de839", "url": "https://www.semanticscholar.org/paper/1c816e8fdb826abde7dcd8b39307200eb54de839", "title": "Minimal training based semantic categorization in a voice activated question answering (VAQA) system", "abstract": "In this paper, we develop a knowledge based methodology that maps Automatic Speech Recognizer (ASR) transcriptions to prede\ufb01ned semantic categories in a Voice Activated Question Answering (VAQA) system. The proposed semantic categorization methodology, SemCat, uses a novel lexical chains/ontology based algorithm and relies heavily on customized but domain independent Natural Language Processing (NLP) tools and does not require any domain-speci\ufb01c utterance collections or manually annotated text data. SemCat requires minimal manual intervention during training, relying only on the semantics encoded in a brief, manually-created description for each prede\ufb01ned cat-egory/slot. SemCat uses these descriptions along with the eXtended WordNet Knowledge Base (XWN-KB) and several domain independent NLP tools including XWN lexical chains to accurately extract information and map user utterances to prede\ufb01ned categories. SemCat also uses the domain ontologies created automatically by the Jaguar knowledge acquisition tool to accurately extract domain/customer speci\ufb01c language/terms.", "venue": "Interspeech", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "kg", "onto", "nlp", "ie", "onto", "nlp"], "mention_counts": {"nlp": 3, "kg": 2, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "ie": 1}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.8807970779778823}, {"paperId": "111caf3b0a0028d9c92fd13a4a4a771fb5b30a9b", "url": "https://www.semanticscholar.org/paper/111caf3b0a0028d9c92fd13a4a4a771fb5b30a9b", "title": "Natural Language Processing for the Semantic Web", "abstract": "This book introduces core natural language processing (NLP) technologies to non-experts in an easily accessible way, as a series of building blocks that lead the user to understand key technologies, why they are required, and how to integrate them into Semantic Web applications. Natural language processing and Semantic Web technologies have different, but complementary roles in data management. Combining these two technologies enables structured and unstructured data to merge seamlessly. Semantic Web technologies aim to convert unstructured data to meaningful representations, which benefit enormously from the use of NLP technologies, thereby enabling applications such as connecting text to Linked Open Data, connecting texts to each other, semantic searching, information visualization, and modeling of user behavior in online networks. The first half of this book describes the basic NLP processing tools: tokenization, part-of-speech tagging, and morphological analysis, in addition to the main tools required for an information extraction system (named entity recognition and relation extraction) which build on these components. The second half of the book explains how Semantic Web and NLP technologies can enhance each other, for example via semantic annotation, ontology linking, and population. These chapters also discuss sentiment analysis, a key component in making sense of textual data, and the difficulties of performing NLP on social media, as well as some proposed solutions. The book finishes by investigating some applications of these tools, focusing on semantic search and visualization, modeling user behavior, and an outlook on the future.", "venue": "Natural Language Processing for the Semantic Web", "citationCount": 47, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "sw", "nlp", "ie", "nlp", "nlp", "nlp", "sw", "sw", "nlp", "sw", "nlp", "lod", "sw", "nlp"], "mention_counts": {"onto": 1, "nlp": 8, "sw": 5, "lod": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 8, "ie": 1}, "ld_mention_counts": {"sw": 5, "lod": 1, "onto": 1}, "relevance_score": 0.8728364547379447}, {"paperId": "b0dd9802dfb4ea0ee07289a2967e662d8fa15701", "url": "https://www.semanticscholar.org/paper/b0dd9802dfb4ea0ee07289a2967e662d8fa15701", "title": "NLP-driven ontology modeling for handling event semantics in NL constraints", "abstract": "To assist decision makers, there is need of providing an insight on current of scenario of market, considering really sensitive news involving economic events like acquisitions, stock splits, or dividend announcements. Similar to the work discussed above, to machine process natural language constraints, there is need of a mechanism that can automate events related information extraction and knowledge acquisition processes to facilitate software developers in fulfilling their cumbersome tasks, for quicker and accurate of software modeling. However, extraction of events and their related information from natural language constraints is a complex task considering the ambiguous nature of natural languages such as English. However, similar problems have been solved previously by other researchers with the help of using event semantic ontologies. However, there is need to plan a mechanism for developing an event semantic ontology for natural language constraints. It is discussed in the previous sections that the Ontology can provide assistance in identification of relations existing in various annotations. It is discussed in this thesis that it is significant to use ontology for identifying events from text. With respect to the scope of this thesis, a framework is developed to generate an event semantic ontology.", "venue": "2016 Sixth International Conference on Innovative Computing Technology (INTECH)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlg", "nll", "nle", "onto", "onto", "onto", "ie", "onto", "nlp", "nlu", "nlp"], "mention_counts": {"onto": 6, "nll": 1, "nlu": 1, "nlp": 3, "nlg": 1, "nle": 1, "ie": 1}, "nlp_mention_counts": {"nll": 1, "nlu": 1, "nlp": 3, "nlg": 1, "nle": 1, "ie": 1}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.8514061277791846}, {"paperId": "e35c8b4090bf5573359f15b6a5576b6908602e09", "url": "https://www.semanticscholar.org/paper/e35c8b4090bf5573359f15b6a5576b6908602e09", "title": "Yet another Platform for Extracting Knowledge from Corpora", "abstract": "The research field of \u0093extracting knowledge bases from text collections\u0094 seems to be mature: its target and its working hypotheses are clear. In this paper we propose a platform, YAPEK, i.e., Yet Another Platform for Extracting Knowledge from corpora, that wants to be the base to collect the majority of algorithms for extracting knowledge bases from corpora. The idea is that, when many knowledge extraction algorithms are collected under the same platform, relative comparisons are clearer and many algorithms can be leveraged to extract more valuable knowledge for final tasks such as Textual Entailment Recognition. As we want to collect many knowledge extraction algorithms, YAPEK is based on the three working hypotheses of the area: the basic hypothesis, the distributional hypothesis, and the point-wise assertion patterns. In YAPEK, these three hypotheses define two spaces: the space of the target textual forms and the space of the contexts. This platform guarantees the possibility of rapidly implementing many models for extracting knowledge from corpora as the platform gives clear entry points to model what is really different in the different algorithms: the feature spaces, the distances in these spaces, and the actual algorithm.", "venue": "LREC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ke", "ke", "ke", "kg", "kg"], "mention_counts": {"ke": 6, "kg": 2}, "nlp_mention_counts": {"ke": 6}, "ld_mention_counts": {"ke": 6, "kg": 2}, "relevance_score": 0.8514061277791846}, {"paperId": "c7bae7db1dbf0e73de04c0c1d5777c84cb5d5f47", "url": "https://www.semanticscholar.org/paper/c7bae7db1dbf0e73de04c0c1d5777c84cb5d5f47", "title": "Evaluating Ontologies with NLP-Based Terminologies - A Case Study on ACGT and Its Master Ontology", "abstract": "Natural language processing (NLP) plays a major role in knowledge engineering. However, NLP's usage is traditionally being seen as a means of extracting knowledge required for building knowledge resources, i.e. ontologies, knowledge bases. When it comes to the evaluation of these knowledge artefacts, then general trends are: expert reviewing, evaluating against existing ontologies and democratic ranking. We propose a new approach for evaluating domain coverage of application ontologies which is based on NLP techniques. The latter can be seen as one way of bridging the gap between terminologies and ontologies in order to create user-understandable expert systems.", "venue": "Formal Ontology in Information Systems", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "nlp", "nlp", "ke", "nlp", "onto", "onto", "kg", "onto", "nlp"], "mention_counts": {"nlp": 5, "kg": 1, "onto": 6, "ke": 1}, "nlp_mention_counts": {"nlp": 5, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 6, "kg": 1}, "relevance_score": 0.8514061277791846}, {"paperId": "61c1e8e41c731635ca5a8bf57f83e53574a1e80a", "url": "https://www.semanticscholar.org/paper/61c1e8e41c731635ca5a8bf57f83e53574a1e80a", "title": "Research on Sino-Tibetan Machine Translation Based on the Reusing of Domain Ontology", "abstract": "There are some problems on traditional machine translation and these problems have affected its application. The combination of ontology and machine translation can bring about change to machine translation. Machine translation based on the reusing of domain ontology can effectively solve the problems that exist in traditional machine translation. The domain ontology knowledge base of Tibetan folk culture is tried to be built and the Sino-Tibetan machine translation system based on the reusing of Tibetan folk culture domain ontology is tried to be constructed in our research. In this machine translation system, ambiguity problem which exits in traditional machine translation can be solved and the domain ontology can be constructed automatically. By that intelligent machine translation between Tibetan and Chinese can be achieved.", "venue": "Int. J. Online Biomed. Eng.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "mt", "kg", "mt", "mt", "mt", "onto", "mt", "onto", "mt", "onto", "onto", "mt", "mt", "mt", "mt"], "mention_counts": {"kg": 1, "onto": 6, "mt": 10}, "nlp_mention_counts": {"mt": 10}, "ld_mention_counts": {"kg": 1, "onto": 6}, "relevance_score": 0.8222931440238636}, {"paperId": "4bc3674b18d40d90dc922f293775cbac1fae7894", "url": "https://www.semanticscholar.org/paper/4bc3674b18d40d90dc922f293775cbac1fae7894", "title": "LinKBase, a Philosophically-Inspired Ontology for NLP/NLU Applications", "abstract": "LinKBase\u00ae is a biomedical ontology. Its hierarchical structure, coverage, use of operational, formal and linguistic relationships, combined with its underlying language technology, make it an excellent ontology to support Natural Language Processing and Understanding (NLP/NLU) and data integration applications. In this paper we will describe the structure and coverage of LinKBase\u00ae. In addition, we will discuss the editing of LinKBase\u00ae and how domain experts are guided by specific editing rules to ensure modeling quality and consistency. Finally, we compare the structure of LinKBase\u00ae to the structure of third party terminologies and ontologies and discuss the integration of these data sources into LinKBase\u00ae.", "venue": "KR-MED", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlu", "nlp", "onto", "nlp", "onto", "nlu"], "mention_counts": {"nlp": 3, "onto": 4, "nlu": 2}, "nlp_mention_counts": {"nlp": 3, "nlu": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "ee463f1f72a7e007bae274d2d42cd2e5d817e751", "url": "https://www.semanticscholar.org/paper/ee463f1f72a7e007bae274d2d42cd2e5d817e751", "title": "Automatically Extracting Qualia Relations for the Rich Event Ontology", "abstract": "Commonsense, real-world knowledge about the events that entities or \u201cthings in the world\u201d are typically involved in, as well as part-whole relationships, is valuable for allowing computational systems to draw everyday inferences about the world. Here, we focus on automatically extracting information about (1) the events that typically bring about certain entities (origins), (2) the events that are the typical functions of entities, and (3) part-whole relationships in entities. These correspond to the agentive, telic and constitutive qualia central to the Generative Lexicon. We describe our motivations and methods for extracting these qualia relations from the Suggested Upper Merged Ontology (SUMO) and show that human annotators overwhelmingly find the information extracted to be reasonable. Because ontologies provide a way of structuring this information and making it accessible to agents and computational systems generally, efforts are underway to incorporate the extracted information to an ontology hub of Natural Language Processing semantic role labeling resources, the Rich Event Ontology.", "venue": "COLING", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "onto", "onto", "ie", "onto", "nlp", "onto"], "mention_counts": {"nlp": 1, "onto": 5, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ie": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.8214593955366725}, {"paperId": "bb075b13e6c5b2f0290e24acf41f4a4e3538d19b", "url": "https://www.semanticscholar.org/paper/bb075b13e6c5b2f0290e24acf41f4a4e3538d19b", "title": "Spatial Ontology in Factored Statistical Machine Translation", "abstract": "This paper presents a statistical phrase-based machine translation system which is enriched with semantic data coming from a spatial ontology. Paper presents the spatial ontology, how it is integrated in statistical machine translation system using factored models and how it is being evaluated using both automatic and human evaluation. Spatial information is added as a factor in both translation and language models. SOLIM spatial ontology language is used to implement ontology and to infer necessary knowledge for training statistical machine translation system. The machine translation system is based on Moses toolkit.", "venue": "DB&IS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "mt", "mt", "mt", "mt", "mt"], "mention_counts": {"onto": 4, "mt": 5}, "nlp_mention_counts": {"mt": 5}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "aeaf24779e5c9ba6b28e21d9f768f37e84d2c46b", "url": "https://www.semanticscholar.org/paper/aeaf24779e5c9ba6b28e21d9f768f37e84d2c46b", "title": "An Nlp-Based Approach for Improving Human-Robot Interaction", "abstract": "Abstract This study aims to explore the possibility of improving human-robot interaction (HRI) by exploiting natural language resources and using natural language processing (NLP) methods. The theoretical basis of the study rests on the claim that effective and efficient human robot interaction requires linguistic and ontological agreement. A further claim is that the required ontology is implicitly present in the lexical and grammatical structure of natural language. The paper offers some NLP techniques to uncover (fragments of) the ontology hidden in natural language and to generate semantic representations of natural language sentences using that ontology. The paper also presents the implementation details of an NLP module capable of parsing English and Turkish along with an overview of the architecture of a robotic interface that makes use of this module for expressing the spatial motions of objects observed by a robot", "venue": "J. Artif. Intell. Soft Comput. Res.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp", "onto", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 5, "onto": 4}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "ff9018ae0fc2e254a055fb920f7f9eb65649d50d", "url": "https://www.semanticscholar.org/paper/ff9018ae0fc2e254a055fb920f7f9eb65649d50d", "title": "Ten Ways of Leveraging Ontologies for Rapid Natural Language Processing Customization for Multiple Use Cases in Disjoint Domains", "abstract": "With the ever-growing adoption of AI technologies by large enterprises, purely data-driven approaches have dominated the field in the recent years. For a single use case, a development process looks simple: agreeing on an annotation schema, labeling the data, and training the models. As the number of use cases and their complexity increases, the development teams face issues with collective governance of the models, scalability and reusablity of data and models. These issues are widely addressed on the engineering side, but not so much on the knowledge side. Ontologies have been a well-researched approach for capturing knowledge and can be used to augment a data-driven methodology. In this paper, we discuss 10 ways of leveraging ontologies for Natural Language Processing (NLP) and its applications. We use ontologies for rapid customization of a NLP pipeline, ontologyrelated standards to power a rule engine and provide standard output format. We also discuss various use cases for medical, enterprise, financial, legal, and security domains, centered around three NLP-based applications: semantic search, question answering and natural language querying.", "venue": "Open J. Semantic Web", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "nlp", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 5, "onto": 4}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "95178e23945ccc5000599ea6786f33870cc41e00", "url": "https://www.semanticscholar.org/paper/95178e23945ccc5000599ea6786f33870cc41e00", "title": "SPARQL/T A query language with SPARQL's syntax for semantic mining of textual complaints", "abstract": "Extracting information from complaints, either scraped from the Web or received directly from the client, is a necessity of many companies nowadays. The aim is to find inside them some actionable knowledge. To this purpose, verbal phrases must be analyzed, as many complaints refer to actions improperly performed. The Semantic Roles of the actions (who did what to whom) and the Named Entities involved need to be extracted. Moreover, for the correct interpretation of the claims, the software should be able to deal with some background knowledge (for example, a product\u2019s ontology). Although there are already many libraries and out of the shelf tools that allow tackling these problems singularly, it may be hard to find one that includes all the needed tasks. We propose here a query language that adopts the syntax of SPARQL to extracts information from natural language documents, pre-annotated with NLP information. The language provides the user with a simple and uniform interface to the most useful NLP tasks, isolating him or her from the details of the specific implementation. We argue that a query language is much easier and intuitive (from a laymen point of view) than an imperative one. Moreover, the adoption of the SPARQL syntax allows to seamlessly mix, inside the same query, NLP patterns with traditional RDF/OWL ones, simplifying the integration with Semantic Web technologies.", "venue": "IIR", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "rdf", "onto", "ie", "nlp", "ie", "onto", "nlp", "nlp"], "mention_counts": {"onto": 2, "nlp": 3, "sw": 1, "rdf": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 3, "ie": 2}, "ld_mention_counts": {"sw": 1, "onto": 2, "rdf": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "a9de628768529e509379b596919d06ab060ebcf8", "url": "https://www.semanticscholar.org/paper/a9de628768529e509379b596919d06ab060ebcf8", "title": "Experimenting with semantic web services to understand the role of NLP technologies in Healthcare", "abstract": "NLP technologies can play a significant role in healthcare where a predominant segment of the clinical documentation is in text form. In a graduate course focused on understanding semantic web services at West Virginia University, a class project was designed with the purpose of exploring potential use for NLP-based abstraction of clinical documentation. The role of NLP-technology was simulated using human abstractors and various workflows were investigated using public domain workflow and semantic web service technologies. This poster explores the potential use of NLP and the role of workflow and semantic web technologies in developing healthcare IT environments.", "venue": "AMIA", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "nlp", "sw", "nlp", "sw", "nlp", "sw", "sw", "nlp"], "mention_counts": {"nlp": 5, "sw": 4}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"sw": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "7413523e7c51ca070799b2795a1cd4373aaf8226", "url": "https://www.semanticscholar.org/paper/7413523e7c51ca070799b2795a1cd4373aaf8226", "title": "Parallelizing natural language techniques for knowledge extraction from cloud service level agreements", "abstract": "To efficiently utilize their cloud based services, consumers have to continuously monitor and manage the Service Level Agreements (SLA) that define the service performance measures. Currently this is still a time and labor intensive process since the SLAs are primarily stored as text documents. We have significantly automated the process of extracting, managing and monitoring cloud SLAs using natural language processing techniques and Semantic Web technologies. In this paper we describe our prototype system that uses a Hadoop cluster to extract knowledge from unstructured legal text documents. For this prototype we have considered publicly available SLA/terms of service documents of various cloud providers. We use established natural language processing techniques in parallel to speed up cloud legal knowledge base creation. Our system considerably speeds up knowledge base creation and can also be used in other domains that have unstructured data.", "venue": "2015 IEEE International Conference on Big Data (Big Data)", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "nlp", "sw", "ke", "nlp", "ke"], "mention_counts": {"nlp": 2, "kg": 2, "sw": 1, "ke": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 2}, "ld_mention_counts": {"kg": 2, "sw": 1, "ke": 2}, "relevance_score": 0.8214593955366725}, {"paperId": "3719cf1d86b93718d50886834ceaecd1c64eb1bd", "url": "https://www.semanticscholar.org/paper/3719cf1d86b93718d50886834ceaecd1c64eb1bd", "title": "State of the art in knowledge extraction from online polls: a survey of current technologies", "abstract": "The ongoing research and development in the field of Natural Language Processing has lead to a great number of technologies in its context. There have been major benefits when it comes to bringing together the worlds of natural language and semantic technologies, so more and more potential areas of application emerge. One of these is the subject of this paper, in particular the possible ways of knowledge extraction from single-question online polls. With concepts of the Social Web, internet users want to contribute and express their opinion. As a consequence, the popularity of online polls is rapidly increasing; they can be found in news articles of media sites, on blogs etc. It would be desirable to bring intelligence to the application of polls by using technologies of the SemanticWeb and Natural Language Processing as this would allow to build a great knowledge base and to draw conclusions from it. This paper surveys the current landscape of tools and state-of-the-art technologies and analyses them with regard to pre-defined requirements that need to be accomplished, in order to be useful for extracting knowledge from the results generated by online polls.", "venue": "ACSW", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "ke", "kg", "ke", "nlp"], "mention_counts": {"nlp": 2, "ke": 3, "kg": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "310126004335765babbb50c88504e7574edd1d92", "url": "https://www.semanticscholar.org/paper/310126004335765babbb50c88504e7574edd1d92", "title": "Mining User Queries with Information Extraction Methods and Linked Data", "abstract": "Purpose: Advanced usage of Web Analytics tools allows to capture the content of user queries. Despite their relevant nature, the manual analysis of large volumes of user queries is problematic. This paper demonstrates the potential of using information extraction techniques and Linked Data to gather a better understanding of the nature of user queries in an automated manner. \nDesign/methodology/approach: The paper presents a large-scale case-study conducted at the Royal Library of Belgium consisting of a data set of 83 854 queries resulting from 29 812 visits over a 12 month period of the historical newspapers platform BelgicaPress. By making use of information extraction methods, knowledge bases and various authority files, this paper presents the possibilities and limits to identify what percentage of end users are looking for person and place names. \nFindings: Based on a quantitative assessment, our method can successfully identify the majority of person and place names from user queries. Due to the specific character of user queries and the nature of the knowledge bases used, a limited amount of queries remained too ambiguous to be treated in an automated manner. \nOriginality/value: This paper demonstrates in an empirical manner both the possibilities and limits of gaining more insights from user queries extracted from a Web Analytics tool and analysed with the help of information extraction tools and knowledge bases. Methods and tools used are generalisable and can be reused by other collection holders.", "venue": "J. Documentation", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "kg", "kg", "ld", "ie", "ld", "ie", "kg", "ie"], "mention_counts": {"ld": 2, "kg": 3, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"ld": 2, "kg": 3}, "relevance_score": 0.8214593955366725}, {"paperId": "2c8dde814803d54844975cec6e543ee7ac7f6c3d", "url": "https://www.semanticscholar.org/paper/2c8dde814803d54844975cec6e543ee7ac7f6c3d", "title": "LODIE: Linked Open Data for Web-scale Information Extraction", "abstract": "This work analyzes research gaps and challenges for Web-scale Information Extraction and foresees the usage of Linked Open Data as a groundbreaking solution for the field. The paper presents a novel methodology for Web scale Information Extraction which will be the core of the LODIE project (Linked Open Data Information Extraction). LODIE aims to develop Information Extraction techniques able to (i) scale at web level and (ii) adapt to user information need. We argument that for the first time in the history of IE this will be possible given the availability of Linked Data, a very large-scale information resource, providing annotated data on a growing number of domains.", "venue": "SWAIE", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "ie", "lod", "ld", "ie", "lod", "lod", "ie"], "mention_counts": {"ld": 1, "lod": 3, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"ld": 1, "lod": 3}, "relevance_score": 0.8214593955366725}, {"paperId": "f965c0447ce22c06e8bb95bf4429d9c93be03d1d", "url": "https://www.semanticscholar.org/paper/f965c0447ce22c06e8bb95bf4429d9c93be03d1d", "title": "Distantly supervised Web relation extraction for knowledge base population", "abstract": "Extracting information from Web pages for populating large, cross-domain knowledge bases requires methods which are suitable across domains, do not require manual effort to adapt to new domains, are able to deal with noise, and integrate information extracted from different Web pages. Recent approaches have used existing knowledge bases to learn to extract information with promising results, one of those approaches being distant supervision. Distant supervision is an unsupervised method which uses background information from the Linking Open Data cloud to automatically label sentences with relations to create training data for relation classifiers. In this paper we propose the use of distant supervision for relation extraction from the Web. Although the method is promising, existing approaches are still not suitable for Web extraction as they suffer from three main issues: data sparsity, noise and lexical ambiguity. Our approach reduces the impact of data sparsity by making entity recognition tools more robust across domains and extracting relations across sentence boundaries using unsupervised co- reference resolution methods. We reduce the noise caused by lexical ambiguity by employing statistical methods to strategically select training data. To combine information extracted from multiple sources for populating knowledge bases we present and evaluate several information integration strategies and show that those benefit immensely from additional relation mentions extracted using co-reference resolution, increasing precision by 8%. We further show that strategically selecting training data can increase precision by a further 3%.", "venue": "Semantic Web", "citationCount": 34, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "lod", "kg", "kg", "ie", "ie", "ie", "ie"], "mention_counts": {"kg": 4, "lod": 1, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"kg": 4, "lod": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "4f73c1256423898d1eebafbb0daf87d63b80c5bd", "url": "https://www.semanticscholar.org/paper/4f73c1256423898d1eebafbb0daf87d63b80c5bd", "title": "From Natural Language to Ontology Population in the Cultural Heritage Domain. A Computational Linguistics-based approach", "abstract": "This paper presents an on-going Natural Language Processing (NLP) research based on Lexicon-Grammar (LG) and aimed at improving knowledge management of Cultural Heritage (CH) domain. We intend to demonstrate how our language formalization technique can be applied for both processing and populating a domain ontology. We also use NLP techniques for text extraction and mining to fill information gaps and improve access to cultural resources. The Linguistic Resources (LRs, i.e. electronic dictionaries) we built can be used in the structuring of effective Knowledge Management Systems (KMSs). In order to apply to Parts of Speech (POS) the classes and properties defined by the Conseil Interational des Musees (CIDOC) Conceptual Reference Model (CRM), we use Finite State Transducers/Automata (FSTs/FSA) and their variables built in the form of graphs. FSTs/FSA are also used for analysing corpora in order to retrieve recursive sentence structures, in which combinatorial and semantic constraints identify properties and denote relationship. Besides, FSTs/FSA are also used to match our electronic dictionary entries (ALUs, or Atomic Linguistic Units) to RDF subject, object and predicate (SKOS Core Vocabulary). This matching of linguistic data to RDF and their translation into SPARQL/SERQL path expressions allows the use ALUs to process natural-language queries.", "venue": "LREC", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "skos", "nlp", "rdf", "rdf", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 4, "skos": 1, "onto": 2, "rdf": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"skos": 1, "onto": 2, "rdf": 2}, "relevance_score": 0.8214593955366725}, {"paperId": "30596da17f14943eb98f428844e100724e28456d", "url": "https://www.semanticscholar.org/paper/30596da17f14943eb98f428844e100724e28456d", "title": "DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction Corpus", "abstract": "In the past decade, the DBpedia community has put significant amount of effort on developing technical infrastructure and methods for efficient extraction of structured information from Wikipedia. These efforts have been primarily focused on harvesting, refinement and publishing semi-structured information found in Wikipedia articles, such as information from infoboxes, categorization information, images, wikilinks and citations. Nevertheless, still vast amount of valuable information is contained in the unstructured Wikipedia article texts. In this paper, we present DBpedia NIF - a large-scale and multilingual knowledge extraction corpus. The aim of the dataset is two-fold: to dramatically broaden and deepen the amount of structured information in DBpedia, and to provide large-scale and multilingual language resource for development of various NLP and IR task. The dataset provides the content of all articles for 128 Wikipedia languages. We describe the dataset creation process and the NLP Interchange Format (NIF) used to model the content, links and the structure the information of the Wikipedia articles. The dataset has been further enriched with about 25% more links and selected partitions published as Linked Data. Finally, we describe the maintenance and sustainability plans, and selected use cases of the dataset from the TextExt knowledge extraction challenge.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "ke", "nlp", "ke", "ke", "nlp"], "mention_counts": {"ld": 1, "ke": 3, "nlp": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 3}, "ld_mention_counts": {"ld": 1, "ke": 3}, "relevance_score": 0.8214593955366725}, {"paperId": "f9ce3ccfe7bd1c6d400a378941bd584612510410", "url": "https://www.semanticscholar.org/paper/f9ce3ccfe7bd1c6d400a378941bd584612510410", "title": "Vision and Knowledge Bimodal Fine-grained Recognition Based on Hierarchical Structured Knowledge Extraction", "abstract": "In the field of fine-grained recognition, the most important method is to use visual information as the most important modality to complete the task. In order to prevent the model from being greatly affected by the data of a single modality, and to improve the accuracy and robustness and security, existing methods fused information such as text, voice or knowledge graphs on a single modality basis. In view of the limitations of the previous methods in the way of integrating knowledge modalities, this paper optimizes the existing knowledge extraction methods for bird datasets, and proposes a hierarchical and structured knowledge extraction method, so that the model can be classified when only knowledge modalities are used. The accuracy reaches 48.69%, which improves the accuracy by about 10% compared with previous methods. After multimodal fusion of knowledge extracted by this method and visual modalities, about 2% improvement was achieved on the knowledge-visual fusion framework KGRF, and the results proved the effectiveness of this method.", "venue": "International Conference on Data Intelligence and Security", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "kg", "ke"], "mention_counts": {"ke": 4, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "797b89a225ad3c2d88c0bc12d76d606181b371ac", "url": "https://www.semanticscholar.org/paper/797b89a225ad3c2d88c0bc12d76d606181b371ac", "title": "Information Extraction from the Web: An Ontology-Based Method Using Inductive Logic Programming", "abstract": "Relevant information extraction from text and web pages in particular is an intensive and time-consuming task that needs important semantic resources. Thus, to be efficient, automatic information extraction systems have to exploit semantic resources (or ontologies) and employ machine-learning techniques to make them more adaptive. This paper presents an Ontology-based Information Extraction method using Inductive Logic Programming that allows inducing symbolic predicates expressed in Horn clausal logic that subsume information extraction rules. Such rules allow the system to extract class and relation instances from English corpora for ontology population purposes. Several experiments were conducted and preliminary experimental results are promising, showing that the proposed approach improves previous work over extracting instances of classes and relations, either separately or altogether.", "venue": "IEEE International Conference on Tools with Artificial Intelligence", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "ie", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 4, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "84feacad2733b423b39fda9119c878971cee4fae", "url": "https://www.semanticscholar.org/paper/84feacad2733b423b39fda9119c878971cee4fae", "title": "Document Summarization and Information Extraction for Generation of Presentation Slides", "abstract": "In this paper, a semi automated technique to generate slide presentations from english text documents is proposed. The technique discussed in this paper is considered to be a pioneering attempt in the field of NLP (Natural Language Processing). The technique involves an information extractor and a slide generator, which combines certain NLP methods such as segmentation, chunking, summarization etc.., with certain special linguistic features of the text such as the ontology of the words, noun phrases found, semantic links, sentence centrality etc., In order to aid the language processing task, two tools can be utilized namely, MontyLingua which helps in chunking and Doddle helps in creating an ontology for the input text represented as an OWL (Ontology Web Language) file. The process of the technique comprises of extracting text, creating an ontology, identifying important phrases for bullets and generating slides.", "venue": "2009 International Conference on Advances in Recent Technologies in Communication and Computing", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "onto", "onto", "ie", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 5, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "ie": 1}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.8214593955366725}, {"paperId": "8139b57e5542e7b7c742270ea8cbaecccc58cf4e", "url": "https://www.semanticscholar.org/paper/8139b57e5542e7b7c742270ea8cbaecccc58cf4e", "title": "Ontology-Based Information Extraction from PDF Documents with Xonto", "abstract": "Information extraction is of paramount importance in several real world applications in the areas of business, competitive and military intelligence because it enables to acquire information contained in unstructured documents and store them in structured forms. Unstructured documents have different internal encodings, one of the most diffused encoding is the visualization-oriented Adobe portable document format (PDF). Although several sophisticated and indeed complex approaches were proposed, they are still limited in many aspects. In particular, existing information extraction systems cannot be applied to PDF documents because of their completely unstructured nature that pose many issues in defining IE approaches. In this paper the novel ontology-based system named XONTO, that allows the semantic extraction of information from PDF documents, is presented. The XONTO system is founded on the idea of self-describing ontologies in which objects and classes can be equipped by a set of rules named descriptors. These rules represent patterns that allow to automatically recognize and extract ontology objects contained in PDF documents also when information is arranged in tabular form. This way a self-describing ontology expresses the semantic of the information to extract and the rules that, in turn, populate itself. In the paper XONTO system behaviors and structure are sketched by means of a running example.", "venue": "Int. J. Artif. Intell. Tools", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ie", "ie", "onto", "ie", "ie", "onto"], "mention_counts": {"onto": 5, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.8214593955366725}, {"paperId": "5ad44275fc5e81189bab1aa7a8a7847c50d514d7", "url": "https://www.semanticscholar.org/paper/5ad44275fc5e81189bab1aa7a8a7847c50d514d7", "title": "OWL as a Target for Information Extraction Systems (Statement of Interest)", "abstract": "Current information extraction systems can do a good job of discovering entities, relations and events in natural language text. The traditional output of such systems is XML, with the ACE Pilot Format (APF) schema as a common target. We are developing a system that will take the output of an information extraction system as APF documents and directly populate a knowledge base with the information extracted. We report on an initial OWL ontology that covers the APF schema, a simple program to convert a set of APF documents to RDF data and a demonstration system build with Exhibit to view the results.", "venue": "OWLED", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "ie", "onto", "kg", "ie", "rdf", "ie"], "mention_counts": {"kg": 1, "onto": 3, "rdf": 1, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"kg": 1, "onto": 3, "rdf": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "2ff13a0e606277b182c9810dcc81f850b577c077", "url": "https://www.semanticscholar.org/paper/2ff13a0e606277b182c9810dcc81f850b577c077", "title": "Ontology-guided Extraction of Complex Nested Relationships", "abstract": "Many applications call for methods to enable automatic extraction of structured information from unstructured natural language text. Due to inherent challenges of natural language processing, most of the existing methods for information extraction from text tend to be domain specific. We explore a modular ontology-based approach to information extraction that decouples domain-specific knowledge from the rules used for information extraction. We describe a framework for extraction of a subset of complex nested relationships (e.g., Joe reports that Jim is a reliable employee). The extracted relationships are output in the form of sets of RDF (resource description framework) triples, which can be queried using query languages for RDF and mined for knowledge acquisition.", "venue": "IEEE International Conference on Tools with Artificial Intelligence", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "rdf", "nlp", "onto", "ie", "rdf", "ie", "rdf"], "mention_counts": {"nlp": 1, "onto": 2, "rdf": 3, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ie": 3}, "ld_mention_counts": {"onto": 2, "rdf": 3}, "relevance_score": 0.8214593955366725}, {"paperId": "db328685d00ec35fe35f9350f884c7b4b8db3f4c", "url": "https://www.semanticscholar.org/paper/db328685d00ec35fe35f9350f884c7b4b8db3f4c", "title": "Unsupervised Ontology Induction from Text", "abstract": "Extracting knowledge from unstructured text is a long-standing goal of NLP. Although learning approaches to many of its subtasks have been developed (e.g., parsing, taxonomy induction, information extraction), all end-to-end solutions to date require heavy supervision and/or manual engineering, limiting their scope and scalability. We present OntoUSP, a system that induces and populates a probabilistic ontology using only dependency-parsed text as input. OntoUSP builds on the USP unsupervised semantic parser by jointly forming ISA and IS-PART hierarchies of lambda-form clusters. The ISA hierarchy allows more general knowledge to be learned, and the use of smoothing for parameter estimation. We evaluate OntoUSP by using it to extract a knowledge base from biomedical abstracts and answer questions. OntoUSP improves on the recall of USP by 47% and greatly outperforms previous state-of-the-art approaches.", "venue": "Annual Meeting of the Association for Computational Linguistics", "citationCount": 145, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "kg", "onto", "ke", "ie", "ke"], "mention_counts": {"onto": 2, "nlp": 1, "ke": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 2, "ie": 1}, "ld_mention_counts": {"kg": 1, "onto": 2, "ke": 2}, "relevance_score": 0.8214593955366725}, {"paperId": "6339ababdcda13b19979b314d5afeb53a83a6a4a", "url": "https://www.semanticscholar.org/paper/6339ababdcda13b19979b314d5afeb53a83a6a4a", "title": "DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population", "abstract": "We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a uni\ufb01ed framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Speci\ufb01cally, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain suf\ufb01cient modularity and extensibility. We release the source code at GitHub 1 with Google Colab tutorials and comprehensive documents 2 for be-ginners. Besides, we present an online system 3 for real-time extraction of various tasks, and a demo video 4 .", "venue": "ArXiv", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ie", "kg", "kg", "ie", "ke"], "mention_counts": {"kg": 3, "ke": 2, "ie": 2}, "nlp_mention_counts": {"ke": 2, "ie": 2}, "ld_mention_counts": {"kg": 3, "ke": 2}, "relevance_score": 0.8214593955366725}, {"paperId": "b15ec477c6b94c8490b47bcdba74e7f942d8af2c", "url": "https://www.semanticscholar.org/paper/b15ec477c6b94c8490b47bcdba74e7f942d8af2c", "title": "OMIR: Ontology-Based Multimedia Information Retrieval System for Web Usage Mining", "abstract": "ABSTRACT Information extraction relevant to the user queries is the challenging task in the ontology environment due to data varieties such as image, video, and text. The utilization of appropriate semantic entities enables the content-based search on annotated text. Recently, the automatic extraction of textual content in the audio-visual content is an advanced research area in a multimedia (MM) environment. The annotation of the video includes several tags and comments. This paper proposes the Collaborative Tagging (CT) model based on the Block Acquiring Page Segmentation (BAPS) method to retrieve the tag-based information. The information extraction in this model includes the Ontology-Based Information Extraction (OBIE) based on the single ontology utilization. The semantic annotation phase in the proposed work inserts the metadata with limited machine-readable terms. The insertion process is split into two major processes such as database uploading to server and extraction of images/web pages based on the results of semantic phase. Novel weight-based novel clustering algorithms are introduced to extract knowledge from MM contents. The ranking based on the weight value in the semantic annotation phase supports the image/web page retrieval process effectively. The comparative analysis of the proposed BAPS-CT with the existing information retrieval (IR) models regarding the average precision rate, time cost, and storage space rate assures the effectiveness of BAPS-CT in OMIR.", "venue": "Cybernetics and systems", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "onto", "onto", "ie", "onto", "onto", "ie"], "mention_counts": {"ke": 1, "onto": 4, "ie": 3}, "nlp_mention_counts": {"ke": 1, "ie": 3}, "ld_mention_counts": {"ke": 1, "onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "cf1c284c7870edc237affd09ecf0e2e01831f8f9", "url": "https://www.semanticscholar.org/paper/cf1c284c7870edc237affd09ecf0e2e01831f8f9", "title": "Automatic Extraction of Information about the Molecular Interactions in Biological Pathways from Texts Based on Ontology and Semantic Processing", "abstract": "We develop a framework using ontology inference and semantic processing techniques to help biologists to extract knowledge directly from a large scale of biological literature in NCBI PubMed. The system integrated various sharable thesauri of WordNet, MeSH (Medical Subject Heading), and GO (Gene ontology) to support the automatic semantic annotation and analysis. The natural language processing and semantic processing are facilitated by the ontological inference, and the system could automatically extract the correct molecular interactions from the complex sentences in an abstract automatically. It facilitates the biologists not only to save time and efforts to construct and analyze biological pathways, but also to discover the novel molecular interactions by comparing the information extracted from the literature with that in such existing pathway database as KEGG. We evaluated the system performance based on the pathways in Apoptosis domain.", "venue": "2006 IEEE International Conference on Systems, Man and Cybernetics", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ke", "onto", "nlp", "ie", "onto"], "mention_counts": {"nlp": 1, "ke": 1, "onto": 4, "ie": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "37f09b047152ba75f4bb3fa6aa387d7eb6c8988d", "url": "https://www.semanticscholar.org/paper/37f09b047152ba75f4bb3fa6aa387d7eb6c8988d", "title": "SeseiOnto: Interfacing NLP and Ontology Extraction", "abstract": "For many years, information retrieval tools have been used to try to solve the information overload problem which was accentuated by the coming of age of the World Wide Web. Some tools used Boolean search, others, natural language based processing (NLP). Ontology-based techniques were proposed to improve the quality of the search but none were widely adopted since they did not statistically enhance either the recall or the precision of the search. However, when it comes to information extraction, they may be of significant help. Their integration in professional search engines has been rather slow, partially due to the fact that the ontology building process is time consuming. In this paper, we describe the SeseiOnto software, which uses simple artificial intelligence techniques to improve information extraction and retrieval. To assist the NLP-based information retrieval on a corpus of documents, SeseiOnto employs an automatically generated ontology. Under our experiments, we found that SeseiOnto obtained results comparable to a traditional search engine, while providing a natural language interface to its user", "venue": "2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "ie", "nlp", "onto", "nlp", "ie", "onto"], "mention_counts": {"nlp": 3, "onto": 4, "ie": 2}, "nlp_mention_counts": {"nlp": 3, "ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "4b1e6f34e9570376224cfd82655b83ca4909d825", "url": "https://www.semanticscholar.org/paper/4b1e6f34e9570376224cfd82655b83ca4909d825", "title": "Design and Construction of a NLP Based Knowledge Extraction Methodology in the Medical Domain Applied to Clinical Information", "abstract": "Objectives This research presents the design and development of a software architecture using natural language processing tools and the use of an ontology of knowledge as a knowledge base. Methods The software extracts, manages and represents the knowledge of a text in natural language. A corpus of more than 200 medical domain documents from the general medicine and palliative care areas was validated, demonstrating relevant knowledge elements for physicians. Results Indicators for precision, recall and F-measure were applied. An ontology was created called the knowledge elements of the medical domain to manipulate patient information, which can be read or accessed from any other software platform. Conclusions The developed software architecture extracts the medical knowledge of the clinical histories of patients from two different corpora. The architecture was validated using the metrics of information extraction systems.", "venue": "Healthcare informatics research", "citationCount": 3, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["kg", "ke", "nlp", "onto", "onto", "nlp", "ie", "kg"], "mention_counts": {"onto": 2, "nlp": 2, "ke": 1, "kg": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 2, "onto": 2, "ke": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "ceac023219023e792cc8671cd5ad35d79822f8b0", "url": "https://www.semanticscholar.org/paper/ceac023219023e792cc8671cd5ad35d79822f8b0", "title": "Ontologies as a Source for the Automatic Generation of Grammars for Information Extraction Systems", "abstract": "Grammars for Natural Language Processing (NLP) applications are generally built either by linguists \u2013 on the basis of their language competence, or by automated tools applied to existing large corpora of language data \u2014 using either supervised or unsupervised methods (or a combination of both). Domain knowledge usually played just a little role in this process. The increasing availability of extended knowledge representation systems, like taxonomies and ontologies, is giving the opportunity to consider new approaches to the (automated) generation of processing grammars, especially in the field of domain-oriented Information Extraction (IE). The reason for this being that most of the taxonomies and ontologies are equipped with natural language expressions included in ontology elements like labels, comments or definitions. These de facto established relations between (domain) knowledge and natural language expressions can be exploited for the automatic generation of domain specific NLP and IE grammars. We describe in this paper steps leading to this automation.", "venue": "SWAIE", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "ie", "ie", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 3, "onto": 4, "ie": 2}, "nlp_mention_counts": {"nlp": 3, "ie": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "03c07b2e46de3707bb2cd9caed138c629f16ce75", "url": "https://www.semanticscholar.org/paper/03c07b2e46de3707bb2cd9caed138c629f16ce75", "title": "Knowledge representation and management: benefits and challenges of the semantic web for the fields of KRM and NLP.", "abstract": "OBJECTIVES\nTo summarize excellent current research in the field of knowledge representation and management (KRM).\n\n\nMETHOD\nA synopsis of the articles selected for the IMIA Yearbook 2011 is provided and an attempt to highlight the current trends in the field is sketched.\n\n\nRESULTS\nThis last decade, with the extension of the text-based web towards a semantic-structured web, NLP techniques have experienced a renewed interest in knowledge extraction. This trend is corroborated through the five papers selected for the KRM section of the Yearbook 2011. They all depict outstanding studies that exploit NLP technologies whenever possible in order to accurately extract meaningful information from various biomedical textual sources.\n\n\nCONCLUSIONS\nBringing semantic structure to the meaningful content of textual web pages affords the user with cooperative sharing and intelligent finding of electronic data. As exemplified by the best paper selection, more and more advanced biomedical applications aim at exploiting the meaningful richness of free-text documents in order to generate semantic metadata and recently to learn and populate domain ontologies. These later are becoming a key piece as they allow portraying the semantics of the Semantic Web content. Maintaining their consistency with documents and semantic annotations that refer to them is a crucial challenge of the Semantic Web for the coming years.", "venue": "Yearbook of medical informatics", "citationCount": 3, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "ke", "onto", "nlp", "sw", "sw", "nlp", "sw"], "mention_counts": {"nlp": 3, "sw": 3, "onto": 1, "ke": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1}, "ld_mention_counts": {"ke": 1, "sw": 3, "onto": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "296247bad16a06df446dbc6897c9872056ed59de", "url": "https://www.semanticscholar.org/paper/296247bad16a06df446dbc6897c9872056ed59de", "title": "Rough Set Based Approach to Text Classification", "abstract": "Textual document set has become an important and rapidly growing information source in the web. Text classification is one of the crucial technologies for information organisation and management. Text classification has become more and more important and attracted wide attention of researchers from different research fields. In this paper, many feature selection methods, the implement algorithms and applications of text classification are introduced firstly. However, because there are much noise in the knowledge extracted by current data-mining techniques for text classification, it leads to much uncertainty in the process of text classification which is produced from both the knowledge extraction and knowledge usage, therefore, more innovative techniques and methods are needed to improve the performance of text classification. It has been a critical step with great challenge to further improve the process of knowledge extraction and effectively utilization of the extracted knowledge. Rough Set decision making approach is proposed to use Rough Set decision techniques to more precisely classify the textual documents which are difficult to separate by the classic text classification methods. The purpose of this paper is to give an overview of existing text classification technologies, to demonstrate the Rough Set concepts and the decision making approach based on Rough Set theory for building more reliable and effective text classification framework with higher precision, to set up an innovative evaluation metric named CEI which is very effective for the performance assessment of the similar research, and to propose a promising research direction for addressing the challenging problems in text classification, text mining and other relative fields.", "venue": "2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)", "citationCount": 12, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["ke", "tp", "ke", "ke", "ke"], "mention_counts": {"ke": 4, "tp": 1}, "nlp_mention_counts": {"ke": 4, "tp": 1}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "30c72c8c7973487d78a03f242fd46ec3947c562f", "url": "https://www.semanticscholar.org/paper/30c72c8c7973487d78a03f242fd46ec3947c562f", "title": "A Novel Patent Knowledge Extraction Method for Innovative Design", "abstract": "As an important source of inspiration, the great number of patent documents provides designers with valuable knowledge of design rationale (DR), including issues, intent, pros and cons of the solutions. Researchers have carried out a number of data analysis studies based on patent information, which is now a new discipline called Patinformatics, including the analysis of patent information from a macro perspective and the identification and extraction of patent knowledge from a micro perspective. If DR knowledge could be extracted automatically from the patent documents and provided to designers as a source of inspiration, it would greatly promote innovative design, and at the same time promote the reuse of patent documents and the wide application of DR theory, which can be like killing three birds with one stone. To address this issue, this study proposes an improved lexical-syntactic pattern method for DR centric patent knowledge extraction, including DR Vector Space model (DRVS), DRV Trigger Word (DRV-TW), Design Rationale Vector (DRV), DR credibility (DRC) and others, and DRV based knowledge extraction algorithms. Knowledge extraction experiments were conducted on 1491 patent documents to verify the feasibility and performance of the method. In addition, two other sets of comparative experiments were conducted using the FastText and BERT machine learning methods, and the results further confirmed the reliability of the proposed method for low-resource corpus.", "venue": "IEEE Access", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ke", "ke"], "mention_counts": {"ke": 4, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "1c26c5158e52b3cb101448322c6015a102d0f342", "url": "https://www.semanticscholar.org/paper/1c26c5158e52b3cb101448322c6015a102d0f342", "title": "Korean NLP2RDF Resources", "abstract": "The aim of Linked Open Data (LOD) is to improve information management and integration by enhancing accessibility to the existing various forms of open data. The goal of this paper is to make Korean resources linkable entities. By using NLP tools, which are suggested in this paper, Korean texts are converted to RDF resources and they can be connected with other RDF triples. It is worth noticing that to the best of our knowledge there is a few of publicy available Korean NLP tools. For this reason, the Korean NLP platform presented here will be available as open source. And it is shown in this paper that the result of this NLP platform can be used as Linked Data entities.", "venue": "ALR@COLING", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "nlp", "lod", "ld", "rdf", "lod", "nlp", "nlp", "nlp"], "mention_counts": {"ld": 1, "nlp": 4, "lod": 2, "rdf": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"ld": 1, "lod": 2, "rdf": 2}, "relevance_score": 0.8214593955366725}, {"paperId": "495015d21c26eac9a6bd64c836ee3370283641ec", "url": "https://www.semanticscholar.org/paper/495015d21c26eac9a6bd64c836ee3370283641ec", "title": "VisKE: Visual knowledge extraction and question answering by visual verification of relation phrases", "abstract": "How can we know whether a statement about our world is valid. For example, given a relationship between a pair of entities e.g., `eat(horse, hay)', how can we know whether this relationship is true or false in general. Gathering such knowledge about entities and their relationships is one of the fundamental challenges in knowledge extraction. Most previous works on knowledge extraction have focused purely on text-driven reasoning for verifying relation phrases. In this work, we introduce the problem of visual verification of relation phrases and developed a Visual Knowledge Extraction system called VisKE. Given a verb-based relation phrase between common nouns, our approach assess its validity by jointly analyzing over text and images and reasoning about the spatial consistency of the relative configurations of the entities and the relation involved. Our approach involves no explicit human supervision thereby enabling large-scale analysis. Using our approach, we have already verified over 12000 relation phrases. Our approach has been used to not only enrich existing textual knowledge bases by improving their recall, but also augment open-domain question-answer reasoning.", "venue": "Computer Vision and Pattern Recognition", "citationCount": 121, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "ke", "ke"], "mention_counts": {"ke": 4, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "e62cc6938ed468e97a63d49b80735d7a92fdd6f8", "url": "https://www.semanticscholar.org/paper/e62cc6938ed468e97a63d49b80735d7a92fdd6f8", "title": "Knowledge Extraction and Acquisition During Real-Time Navigation in Unknown Environments", "abstract": "This paper deals with the representation and acquisition of knowledge extracted during a real-time path planning process in an unknown space. In particular, the extracted knowledge from each current free navigation space is represented by attributed graphs. The graph knowledge representation forms are \u201cprocessed and combined\u201d appropriately by generating a scheme which acquires the extracted knowledge during the navigation. Illustrated examples are provided for simple navigation cases.", "venue": "Int. J. Pattern Recognit. Artif. Intell.", "citationCount": 12, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["ke", "ke", "kg", "ke", "ke"], "mention_counts": {"ke": 4, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "547f3e80bfbcfb0868d02aeaaf8261ba5ecceff9", "url": "https://www.semanticscholar.org/paper/547f3e80bfbcfb0868d02aeaaf8261ba5ecceff9", "title": "Knowledge extraction changes the way an expert thinks", "abstract": "The authors show an aspect of knowledge extraction that deserves attention: the process where a human domain expert becomes more cogent, in his or her own mind, about the algorithms, heuristics, and strategies that he or she uses in manipulating this factual knowledge base. The contention is that the knowledge engineer's constructs can be greatly strengthened by utilizing those constructs of the domain expert that may evolve through the knowledge extraction process. In addition, harnessing the domain expert's constructs by fostering such introspective processes early in the knowledge extraction stage will increase the cost effectiveness of the expert system design. The arguments are based on a case study of one particular human expert who has been involved from 1985 to 1992, in the construction of five expert systems, all of which pertain to the same domain and have the same objective to assist in the diagnosis of stuttering among young children.<<ETX>>", "venue": "[Proceedings] 1992 IEEE International Conference on Systems, Man, and Cybernetics", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ke", "ke"], "mention_counts": {"ke": 4, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "5ff3fc373fad4841fe98a4ef3986d76eb451844b", "url": "https://www.semanticscholar.org/paper/5ff3fc373fad4841fe98a4ef3986d76eb451844b", "title": "A framework for structured knowledge extraction and representation from natural language via deep sentence analysis", "abstract": "We present a framework that we are currently developing, that allows one to extract knowledge from natural language sentences using a deep analysis technique based on linguistic dependencies. The extracted knowledge is represented in OOLOT, an intermediate format that we have introduced, inspired by the Language of Thought (LOT) and based on Answer Set Programming (ASP). OOLOT uses an ontologyoriented lexicon and syntax. Therefore, it is possible to export the extracted knowledge into OWL and native ASP.", "venue": "Italian Conference on Computational Logic", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ke", "ke", "ke", "ke"], "mention_counts": {"ke": 4, "onto": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "onto": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "bf9bb5f28c1c9e014e453d745c8a6e83d9d365f2", "url": "https://www.semanticscholar.org/paper/bf9bb5f28c1c9e014e453d745c8a6e83d9d365f2", "title": "Knowledge Extraction and Analysis on Collaborative Interaction", "abstract": "E-learning is popularized so fast and Collaborative Learning (CL) becomes so important an instructional strategy. There are huge Group Session (GS) texts needed to be analyzed to evaluate CL, thus the automatic or semi-automatic methods of analyzing the GS texts become very important. \n \nIn this paper we present a method called Interaction Analysis depended on Knowledge Extraction (IAKE) to analyze collaborative learning by extracting knowledge from the GS texts. This method is based on a GS text analysis approach we proposed it as Theme based Knowledge Extraction (TKE).", "venue": "International Conference on Artificial Intelligence in Education", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "ke", "ke"], "mention_counts": {"ke": 4, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "kg": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "4274016dfe32971fcb5bb2043b35555f6fcf91c9", "url": "https://www.semanticscholar.org/paper/4274016dfe32971fcb5bb2043b35555f6fcf91c9", "title": "Building and Using a Lexical Knowledge Base of Near-Synonym Differences", "abstract": "Choosing the wrong word in a machine translation or natural language generation system can convey unwanted connotations, implications, or attitudes. The choice between near-synonyms such as error, mistake, slip, and blunderwords that share the same core meaning, but differ in their nuancescan be made only if knowledge about their differences is available. We present a method to automatically acquire a new type of lexical resource: a knowledge base of near-synonym differences. We develop an unsupervised decision-list algorithm that learns extraction patterns from a special dictionary of synonym differences. The patterns are then used to extract knowledge from the text of the dictionary. The initial knowledge base is later enriched with information from other machine-readable dictionaries. Information about the collocational behavior of the near-synonyms is acquired from free text. The knowledge base is used by Xenon, a natural language generation system that shows how the new lexical resource can be used to choose the best near-synonym in specific situations.", "venue": "Computational Linguistics", "citationCount": 80, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "kg", "nlg", "kg", "kg", "nlg", "kg", "ke"], "mention_counts": {"kg": 4, "nlg": 2, "ke": 1, "mt": 1}, "nlp_mention_counts": {"ke": 1, "nlg": 2, "mt": 1}, "ld_mention_counts": {"kg": 4, "ke": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "9cc8609f904c50b1be408abede7ab7f5cdbc9744", "url": "https://www.semanticscholar.org/paper/9cc8609f904c50b1be408abede7ab7f5cdbc9744", "title": "Research on Machine Translation of Deep Neural Network Learning Model Based on Ontology", "abstract": "To align different ontologies, it is necessary to find effective ways to achieve interoperability of information in the context of the Semantic Web. The development of accurate and reliable techniques to automatically perform this task, it is becoming more and more crucial as overlap between ontologies grows proportionally. In order to solve the problem that traditional machine translation cannot meet the needs of users because of the slow translation speed. According to the characteristics of Ontology's domain knowledge concept system, deep neural network learning model based machine translation method is proposed. Through the experimental design, we examine the translation time and BLEU score and other indicators. After junior translators use the tools, the translation time is reduced by 34.0% and the BLEU score increases by 7.59; after the senior translators use the tools, the translation time is reduced by 11.3%, and the BLEU score is increased by 1.67. Analysis of the experimental results shows that the essence of this method is to complement translation skills, so it is more effective for junior translators who are not good enough in translation skills. The machine translation method based on deep neural network learning can significantly improve the quality and efficiency of translation.", "venue": "Informatica", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "onto", "mt", "sw", "mt", "onto", "onto", "mt", "onto"], "mention_counts": {"sw": 1, "onto": 4, "mt": 4}, "nlp_mention_counts": {"mt": 4}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "9c01a3e29f62b7e8967b03af0016e2a555ddd2c2", "url": "https://www.semanticscholar.org/paper/9c01a3e29f62b7e8967b03af0016e2a555ddd2c2", "title": "FNLP\u2010ONT: A feasible ontology for improving NLP tasks in Persian", "abstract": "Natural language processing is a composition of several error\u2010prone and challenging tasks, including part of speech tagging, word sense disambiguation, named entity recognition, and compound verb detection. Studying intrasentence relations and roles is essential to improve the mentioned subtasks. Semi\u2010automatic schemes such as ontologies can be applied to clarify word's dependencies. This paper presents an ontology that is targeting to improve POS tagging, WSD, NER, and compound verb detection in Persian with extra properties that may ameliorate machine translation. The ontology is tested in combinations with several state\u2010of\u2010art algorithms on Dadegan corpus. The results show that coping semantic analysis with machine learning methods enhance relation detection and consequently precision of the mentioned subtasks, which is not widely addressed in Persian. Furthermore, the experimental results declare that the accuracy rate increases between 4.5 and 23% for different tasks.", "venue": "Expert Syst. J. Knowl. Eng.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "wsd", "wsd", "onto", "onto", "onto", "mt"], "mention_counts": {"nlp": 2, "wsd": 2, "onto": 4, "mt": 1}, "nlp_mention_counts": {"nlp": 2, "wsd": 2, "mt": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "aaabe8ed88532781c82d5eed36ef80f842a9f2e1", "url": "https://www.semanticscholar.org/paper/aaabe8ed88532781c82d5eed36ef80f842a9f2e1", "title": "Enhancing Linguistic Web Service Description with Non-functional NLP Properties", "abstract": "This paper deals with the enhancing of Linguistic Web Service (LingWS) description. It proposes an extension for the OWL-S approach and a Natural Language Processing (NLP) domain ontology based on linguistic standards. The proposed extension provides a classification of the Non-functional NLP properties which promotes the representation of their relationships. The extended OWL-S description is linked to the NLP domain ontology to semantically annotate the LingWS properties.", "venue": "ICSOFT", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "nlp", "nlp", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 5, "onto": 4}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "29421992b3908b5cb39e5705374ec7cf504ae2ea", "url": "https://www.semanticscholar.org/paper/29421992b3908b5cb39e5705374ec7cf504ae2ea", "title": "Design of GA and Ontology based NLP Frameworks for Online Opinion Mining", "abstract": "\n\n For almost every domain, a tremendous degree of data is accessible in an\nonline and offline mode. Billions of users are daily posting their views or opinions by using different\nonline applications like WhatsApp, Facebook, Twitter, Blogs, Instagram etc.\n\n\n\nThese reviews are constructive for the progress of the venture, civilization, state and even nation.\nHowever, this momentous amount of information is useful only if it is collectively and effectively\nmined.\n\n\n\nOpinion mining is used to extract the thoughts, expression, emotions, critics, appraisal\nfrom the data posted by different persons. It is one of the prevailing research techniques that coalesce\nand employ the features from natural language processing. Here, an amalgamated approach has been\nemployed to mine online reviews.\n\n\n\nTo improve the results of genetic algorithm based opining mining patent, here, a hybrid genetic\nalgorithm and ontology based 3-tier natural language processing framework named GAO_NLP_OM has\nbeen designed. First tier is used for preprocessing and corrosion of the sentences. Middle tier is composed\nof genetic algorithm based searching module, ontology for English sentences, base words for the\nreview, complete set of English words with item and their features. Genetic algorithm is used to expedite\nthe polarity mining process. The last tier is liable for semantic, discourse and feature summarization.\nFurthermore, the use of ontology assists in progressing more accurate opinion mining model.\n\n\n\nGAO_NLP_OM is supposed to improve the performance of genetic algorithm based opinion\nmining patent. The amalgamation of genetic algorithm, ontology and natural language processing\nseems to produce fast and more precise results. The proposed framework is able to mine simple as well\nas compound sentences. However, affirmative preceded interrogative, hidden feature and mixed language\nsentences still be a challenge for the proposed framework.\n", "venue": "Recent Patents on Engineering", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "onto", "nlp", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 4, "onto": 5}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.8214593955366725}, {"paperId": "2612f8294454aab719376b411185d8a90b3510b9", "url": "https://www.semanticscholar.org/paper/2612f8294454aab719376b411185d8a90b3510b9", "title": "The Theoretical Status of Ontologies in Natural Language Processing", "abstract": "This paper discusses the use of `ontologies' in Natural Language Processing. It classifies various kinds of ontologies that have been employed in NLP and discusses various benefits and problems with those designs. Particular focus is then placed on experiences gained in the use of the Upper Model, a linguistically-motivated `ontology' originally designed for use with the Penman text generation system. Some proposals for further NLP ontology design criteria are then made.", "venue": "ArXiv", "citationCount": 55, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 4, "onto": 5}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.8214593955366725}, {"paperId": "9ee97ae8d5ee07c4e5c7acb6c125ec7a38aa0564", "url": "https://www.semanticscholar.org/paper/9ee97ae8d5ee07c4e5c7acb6c125ec7a38aa0564", "title": "Natural language processing based ontology learning", "abstract": "Though the utility of domain Ontologies is now widely acknowledged in an increasing number of domains, a critical task of identifying, defining, and entering the concept definitions is still intractable. Nowadays, natural language becomes a more and more important information source, and natural language processing (NLP) becomes more and more practical. To reduce time, cost and making use of NLP, it is highly advisable to refer, in constructing or updating an ontology, to the documents available in the field. In this paper we describe NLTPOnto, a natural language processing based tool is devised to help people to semi-automatically construct ontology.", "venue": "2010 International Conference on Computer Application and System Modeling (ICCASM 2010)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "nlp", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 5, "onto": 4}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.8214593955366725}, {"paperId": "6a6e4f13a4577497a95ad8e1acddcee1d335130e", "url": "https://www.semanticscholar.org/paper/6a6e4f13a4577497a95ad8e1acddcee1d335130e", "title": "Integration of Domain Knowledge using Medical Knowledge Graph Deep Learning for Cancer Phenotyping", "abstract": "A key component of deep learning (DL) for natural language processing (NLP) is word embeddings. Word embeddings that effectively capture the meaning and context of the word that they represent can significantly improve the performance of downstream DL models for various NLP tasks. Many existing word embeddings techniques capture the context of words based on word co-occurrence in documents and text; however, they often cannot capture broader domain-specific relationships between concepts that may be crucial for the NLP task at hand. In this paper, we propose a method to integrate external knowledge from medical terminology ontologies into the context captured by word embeddings. Specifically, we use a medical knowledge graph, such as the unified medical language system (UMLS), to find connections between clinical terms in cancer pathology reports. This approach aims to minimize the distance between connected clinical concepts. We evaluate the proposed approach using a Multitask Convolutional Neural Network (MT-CNN) to extract six cancer characteristics \u2013 site, subsite, laterality, behavior, histology, and grade \u2013 from a dataset of 900K cancer pathology reports. The results show that the MT-CNN model which uses our domain informed embeddings outperforms the same MT-CNN using standard word2vec embeddings across all tasks, with an improvement in the overall microand macro-F1 scores by 4.97%and 22.5%, respectively. Keywords-Word embeddings; UMLS; convolutional neural networks; CNN; natural language processing; knowledge graph; This manuscript has been authored by UT Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a nonexclusive, paidup, irrevocable, world-wide license to publish or reproduce the published form of the manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-publicaccess-plan).", "venue": "ArXiv", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "nlp", "nlp", "nlp", "nlp", "kg", "kg", "onto"], "mention_counts": {"nlp": 5, "onto": 1, "kg": 3}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"kg": 3, "onto": 1}, "relevance_score": 0.8214593955366725}, {"paperId": "1c4dc3adf6911ee35699a99aaf93127f6c01e763", "url": "https://www.semanticscholar.org/paper/1c4dc3adf6911ee35699a99aaf93127f6c01e763", "title": "The Effect of Text Ambiguity on creating Policy Knowledge Graphs", "abstract": "A growing number of web and cloud-based products and services rely on data sharing between consumers, service providers, and their subsidiaries and third parties. There is a growing concern around the security and privacy of data in such large-scale shared architectures. Most organizations have a human-written privacy policy that discloses all the ways that data is shared, stored, and used. The organizational privacy policies must also be compliant with government and administrative regulations. This raises a major challenge for providers as they try to launch new services. Thus they are moving towards a system of automatic policy maintenance and regulatory compliance. This requires extracting policy from text documents and representing it in a semi-structured, machine-processable framework. The most popular method to this end is extracting policy information into a Knowledge Graph (KG). There exists a significant body of work that converts text descriptions of regulations into policies expressed in languages such as OWL and XACML and is grounded in the control-based schema by using NLP approaches. In this paper, we show that the NLP-based approaches to extract knowledge from written policy documents and representing them in enforceable Knowledge Graphs fail when the text policies are ambiguous. Ambiguity can arise from lack of clarity, misuse of syntax, and/or the use of complex language. We describe a system to extract features from a policy document that affect its ambiguity and classify the documents based on the level of ambiguity present. We validate this approach using human annotators. We show that a large number of documents in a popular privacy policy corpus (OPP-115) are ambiguous. This affects the ability to automatically monitor privacy policies. We show that for policies that are more ambiguous according to our proposed measure, NLP-based text segment classifiers are less accurate.", "venue": "2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "kg", "kg", "nlp", "nlp", "nlp", "kg"], "mention_counts": {"nlp": 3, "ke": 1, "onto": 1, "kg": 3}, "nlp_mention_counts": {"nlp": 3, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 1, "kg": 3}, "relevance_score": 0.8214593955366725}, {"paperId": "8a9ca89285620840ae770f8e90438a1df919ef22", "url": "https://www.semanticscholar.org/paper/8a9ca89285620840ae770f8e90438a1df919ef22", "title": "Template Driven Information Extraction for Populating Ontologies", "abstract": "We address the integration of information extraction (IE) and ontologies. In particular, using an ontology to aid the IE process, and using the IE results to help populate the ontology. We perform IE by means of domain specific templates and the lightweight use of Natural Languages Processing techniques (NLP). Our main goal is to learn information from text by the use of templates and in this way to alleviate the main bottleneck in creating knowledge-base systems that is \"the extraction of knowledge\". Our domain of study is \"KMi Planet\", a Web-based news server for communication of stories between members in our institute. The main goals of our system are to classify an incoming story, obtain the relevant objects within the story, deduce the relationships between them, and to populate the ontology. Furthermore, we aim to do this with minimal help from the user.", "venue": "Workshop on Ontology Learning", "citationCount": 47, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ie", "onto", "kg", "onto", "ie", "nlp", "onto", "onto", "onto", "ke"], "mention_counts": {"onto": 5, "nlp": 2, "ke": 1, "kg": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 1, "ie": 2}, "ld_mention_counts": {"kg": 1, "onto": 5, "ke": 1}, "relevance_score": 0.8183448250315903}, {"paperId": "be22b8a3b0662ddc7c75792ead3175d674b07bbc", "url": "https://www.semanticscholar.org/paper/be22b8a3b0662ddc7c75792ead3175d674b07bbc", "title": "The State of Knowledge Extraction from Text for Thai Language", "abstract": "With the emergence of the Semantic Web (or Linked Data), increased efforts have been made to automatically extract formalized semantic knowledge from natural language text. Most research work and tools for knowledge extraction are focusing on text in English language. In this work, wepresent our research-in-progress on evaluating the state-of-the-art in knowledge extraction from text for Thai language. For this purpose, we investigate the existing knowledge extraction literature and group the available research work and tools into eight knowledge extraction tasks. Our preliminary results from the survey of the state-of-the-art show that there exist large research gaps and therefore future research opportunities in Thaiknowledge extraction, and we provide hints for available research directions.", "venue": "IIAI International Conference on Advanced Applied Informatics", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ke", "ke", "ke", "ld", "ke", "ke"], "mention_counts": {"ld": 1, "sw": 1, "ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ld": 1, "sw": 1, "ke": 5}, "relevance_score": 0.8183448250315903}, {"paperId": "484d9a9ddcabb4e0125c4ba5886afc390d616c94", "url": "https://www.semanticscholar.org/paper/484d9a9ddcabb4e0125c4ba5886afc390d616c94", "title": "Improving Machine Translation through Linked Data", "abstract": "Abstract With the ever increasing availability of linked multilingual lexical resources, there is a renewed interest in extending Natural Language Processing (NLP) applications so that they can make use of the vast set of lexical knowledge bases available in the Semantic Web. In the case of Machine Translation, MT systems can potentially benefit from such a resource. Unknown words and ambiguous translations are among the most common sources of error. In this paper, we attempt to minimise these types of errors by interfacing Statistical Machine Translation (SMT) models with Linked Open Data (LOD) resources such as DBpedia and BabelNet. We perform several experiments based on the SMT system Moses and evaluate multiple strategies for exploiting knowledge from multilingual linked data in automatically translating named entities. We conclude with an analysis of best practices for multilingual linked data sets in order to optimise their benefit to multilingual and cross-lingual applications.", "venue": "Prague Bull. Math. Linguistics", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "lod", "sw", "lod", "ld", "ld", "nlp", "mt", "mt", "ld", "nlp", "kg"], "mention_counts": {"ld": 3, "sw": 1, "lod": 2, "nlp": 2, "mt": 3, "kg": 1}, "nlp_mention_counts": {"nlp": 2, "mt": 3}, "ld_mention_counts": {"ld": 3, "sw": 1, "lod": 2, "kg": 1}, "relevance_score": 0.8183448250315903}, {"paperId": "79c9c846e0f977868c034e8a40837409ab9b802a", "url": "https://www.semanticscholar.org/paper/79c9c846e0f977868c034e8a40837409ab9b802a", "title": "Enriching OWL Ontologies with Linguistic and User-Related Annotations: The ELEON System", "abstract": "This paper introduces ELEON, an editor that allows the enrichment of OWL ontologies with linguistic and user-related annotations. The enriched ontologies are used by natural language generation (NLG) engines to generate textual descriptions of the objects represented in the ontologies in the selected language and according to user's model. ELEON provides a well-defined interface that can be used by different NLG engines. The paper presents the relevant functionalities of ELEON, describes the provided interface to NLG engines and discusses the advantages of exploiting such enriched ontologies in NLG.", "venue": "IEEE International Conference on Tools with Artificial Intelligence", "citationCount": 32, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "nlg", "onto", "onto", "nlg", "nlg", "onto", "onto", "onto", "onto", "nlg"], "mention_counts": {"nlg": 5, "onto": 7}, "nlp_mention_counts": {"nlg": 5}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.8183448250315903}, {"paperId": "b130d7e38a59e1194bd6430696bf96d386114ce7", "url": "https://www.semanticscholar.org/paper/b130d7e38a59e1194bd6430696bf96d386114ce7", "title": "Natural Language Generation at Scale: A Case Study for Open Domain Question Answering", "abstract": "Current approaches to Natural Language Generation (NLG) for dialog mainly focus on domain-specific, task-oriented applications (e.g. restaurant booking) using limited ontologies (up to 20 slot types), usually without considering the previous conversation context. Furthermore, these approaches require large amounts of data for each domain, and do not benefit from examples that may be available for other domains. This work explores the feasibility of applying statistical NLG to scenarios requiring larger ontologies, such as multi-domain dialog applications or open-domain question answering (QA) based on knowledge graphs. We model NLG through an Encoder-Decoder framework using a large dataset of interactions between real-world users and a conversational agent for open-domain QA. First, we investigate the impact of increasing the number of slot types on the generation quality and experiment with different partitions of the QA data with progressively larger ontologies (up to 369 slot types). Second, we perform multi-task learning experiments between open-domain QA and task-oriented dialog, and benchmark our model on a popular NLG dataset. Moreover, we experiment with using the conversational context as an additional input to improve response generation quality. Our experiments show the feasibility of learning statistical NLG models for open-domain QA with larger ontologies.", "venue": "INLG", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "onto", "nlg", "onto", "nlg", "kg", "nlg", "nlg", "nlg", "onto", "nlg"], "mention_counts": {"kg": 1, "nlg": 7, "onto": 4}, "nlp_mention_counts": {"nlg": 7}, "ld_mention_counts": {"kg": 1, "onto": 4}, "relevance_score": 0.8183448250315903}, {"paperId": "799403bbb2f10fbfaf6381c4555fd0a9e987a34b", "url": "https://www.semanticscholar.org/paper/799403bbb2f10fbfaf6381c4555fd0a9e987a34b", "title": "An exploratory analysis: extracting materials science knowledge from unstructured scholarly data", "abstract": "\nPurpose\nThe output of academic literature has increased significantly due to digital technology, presenting researchers with a challenge across every discipline, including materials science, as it is impossible to manually read and extract knowledge from millions of published literature. The purpose of this study is to address this challenge by exploring knowledge extraction in materials science, as applied to digital scholarship. An overriding goal is to help inform readers about the status knowledge extraction in materials science.\n\n\nDesign/methodology/approach\nThe authors conducted a two-part analysis, comparing knowledge extraction methods applied materials science scholarship, across a sample of 22 articles; followed by a comparison of HIVE-4-MAT, an ontology-based knowledge extraction and MatScholar, a named entity recognition (NER) application. This paper covers contextual background, and a review of three tiers of knowledge extraction (ontology-based, NER and relation extraction), followed by the research goals and approach.\n\n\nFindings\nThe results indicate three key needs for researchers to consider for advancing knowledge extraction: the need for materials science focused corpora; the need for researchers to define the scope of the research being pursued, and the need to understand the tradeoffs among different knowledge extraction methods. This paper also points to future material science research potential with relation extraction and increased availability of ontologies.\n\n\nOriginality/value\nTo the best of the authors\u2019 knowledge, there are very few studies examining knowledge extraction in materials science. This work makes an important contribution to this underexplored research area.\n", "venue": "Electronic library", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "onto", "ke", "ke", "ke", "ke", "onto", "ke", "ke", "ke", "onto"], "mention_counts": {"ke": 9, "onto": 3, "kg": 1}, "nlp_mention_counts": {"ke": 9}, "ld_mention_counts": {"ke": 9, "onto": 3, "kg": 1}, "relevance_score": 0.8180808589832839}, {"paperId": "447058cb448648af0adbd34db3feaddcaeba625a", "url": "https://www.semanticscholar.org/paper/447058cb448648af0adbd34db3feaddcaeba625a", "title": "A Methodology for Extracting Knowledge about Controlled Vocabularies from Textual Data using FCA-Based Ontology Engineering", "abstract": "We introduce an end-to-end methodology (from text processing to querying a knowledge graph) for the sake of knowledge extraction from text corpora with a focus on a list of vocabularies of interest. We propose a pipeline that incorporates Natural Language Processing (NLP), Formal Concept Analysis (FCA), and Ontology Engineering techniques to build an ontology from textual data. We then extract the knowledge about controlled vocabularies by querying that knowledge graph, i.e., the engineered ontology. We demonstrate the significance of the proposed methodology by using it for knowledge extraction from a text corpus that consists of 800 news articles and reports about companies and products in the IT and pharmaceutical domain, where the focus is on a given list of 250 controlled vocabularies.", "venue": "IEEE International Conference on Bioinformatics and Biomedicine", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["tp", "ke", "nlp", "ke", "nlp", "onto", "ke", "onto", "onto", "kg", "onto", "kg"], "mention_counts": {"onto": 4, "nlp": 2, "ke": 3, "kg": 2, "tp": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 3, "tp": 1}, "ld_mention_counts": {"ke": 3, "onto": 4, "kg": 2}, "relevance_score": 0.7967438898272832}, {"paperId": "2fe7c6c1bb727a8a71af2d2b4c328937c6ba0e1f", "url": "https://www.semanticscholar.org/paper/2fe7c6c1bb727a8a71af2d2b4c328937c6ba0e1f", "title": "A Survey of Thai Knowledge Extraction for the Semantic Web Research and Tools", "abstract": "As the manual creation of domain models and also of linked data is very costly, the extraction of knowledge from structured and unstructured data has been one of the central research areas in the Semantic Web field in the last two decades. Here, we look specifically at the extraction of formalized knowledge from natural language text, which is the most abundant source of human knowledge available. There are many tools on hand for information and knowledge extraction for English natural language, for written Thai language the situation is different. The goal of this work is to assess the state-of-the-art of research on formal knowledge extraction specifically from Thai language text, and then give suggestions and practical research ideas on how to improve the state-of-the-art. To address the goal, first we distinguish nine knowledge extraction for the Semantic Web tasks defined in literature on knowledge extraction from English text, for example taxonomy extraction, relation extraction, or named entity recognition. For each of the nine tasks, we analyze the publications and tools available for Thai text in the form of a comprehensive literature survey. Additionally to our assessment, we measure the self-assessment by the Thai research community with the help of a questionnaire-based survey on each of the tasks. Furthermore, the structure and size of the Thai community is analyzed using complex literature database queries. Combining all the collected information we finally identify research gaps in knowledge extraction from Thai language. An extensive list of practical research ideas is presented, focusing on concrete suggestions for every knowledge extraction task \u2013 which can be implemented and evaluated with reasonable effort. Besides the task-specific hints for improvements of the state-of-the-art, we also include general recommendations on how to raise the efficiency of the respective research community. key words: knowledge extraction, Thai language text, landscape analysis, semantic web", "venue": "IEICE Trans. Inf. Syst.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "sw", "ke", "ke", "ke", "ke", "sw", "ke", "ke", "sw", "ke", "ld", "sw", "ke"], "mention_counts": {"ld": 1, "ke": 9, "sw": 4}, "nlp_mention_counts": {"ke": 9}, "ld_mention_counts": {"ld": 1, "ke": 9, "sw": 4}, "relevance_score": 0.7825501203436873}, {"paperId": "1ee2492cef96d3bc7d5a7b91fac7b4a70368b24a", "url": "https://www.semanticscholar.org/paper/1ee2492cef96d3bc7d5a7b91fac7b4a70368b24a", "title": "An Ontology-Based Knowledge Methodology in the Medical Domain in the Latin America: the Study Case of Republic of Panama", "abstract": "Introduction: Nowadays in Panama, there is a lot of patient information stored in textual form which cannot be manipulated to manage adequate knowledge. There are multiple resources created to represent knowledge, including specialized glossaries, ontologies, among others. The ontologies are an important part within the scope of the recovery and organization of the information and the semantic web. Also in recent works they are used in applications of natural language processing (NLP), as a knowledge base. Aim: This research was conducted with the aim of creating a methodology that allows from a text written in NL, extract the necessary elements using NLP tools and with them create a knowledge base represented by one domain ontology and extract knowledge to help medical specialists. Material and Methods: In this study we carried out a methodology that allows the extraction of knowledge of patient clinical records, general medicine and palliative care, in order to show relevant knowledge elements to specialists. The methodology was validated with a data corpus of approximately 200 patient records. Conclusion: We have created a knowledge representation methodology, combining NLP techniques and tools and the automatic instantiation of an ontology, which can serve as a software agent for other applications or used to visualize the patient\u2019s clinical information. The study was validated using the traditional metrics of information retrieval systems precision, recall, F-measure obtaining excellent results, and can be used as a software agent or methodology for the development of information extraction software systems in the medical domain.", "venue": "Acta informatica medica : AIM : journal of the Society for Medical Informatics of Bosnia & Herzegovina : casopis Drustva za medicinsku informatiku BiH", "citationCount": 6, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["ke", "kg", "nlp", "kg", "ke", "onto", "onto", "kg", "sw", "nlp", "nlp", "onto", "ie", "onto", "onto", "nlp"], "mention_counts": {"onto": 5, "nlp": 4, "ke": 2, "sw": 1, "kg": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 4, "ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "sw": 1, "onto": 5, "kg": 3}, "relevance_score": 0.777069182404355}, {"paperId": "0c0d1816ebe35fa9f00d4f7c720b6a89c9acdb58", "url": "https://www.semanticscholar.org/paper/0c0d1816ebe35fa9f00d4f7c720b6a89c9acdb58", "title": "A Proposal for Screening Inconsistencies in Ontologies based on Query Languages using WSD", "abstract": "In this paper, we discuss a method to screen inconsistencies in ontologies by applying a natural language processing (NLP) technique, especially, those used for word sense disambiguation (WSD). In the database research field, it is claimed that queries over target ontologies should play a significant role because they represent every aspect of the terms described in each ontology. According to (Calvanese et al., 2001), considering the global and the local ontologies, the terms in the global ontology can be viewed as the query over the local ontology, and the mapping between the global and the local ontologies is given by, associating each term in the global ontology with a view. On the other hand, ontology screening systems should be able to take advantage of some popular techniques for WSD, which is supposed to decide the right sense where the target word is used in a specific context. We present several examples regarding inconsistencies in ontologies with the aid of DAML+OIL notation(DAML+OIL, 2001), and propose that WSD can be one of the promising method to screen such as inconsistencies.", "venue": "NLPXML@COLING", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "wsd", "onto", "wsd", "onto", "onto", "wsd", "onto", "onto", "onto", "wsd", "nlp", "wsd", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "wsd": 5, "onto": 11}, "nlp_mention_counts": {"nlp": 2, "wsd": 5}, "ld_mention_counts": {"onto": 11}, "relevance_score": 0.777069182404355}, {"paperId": "451dde1a7755831b46705faf44b2497fd975e295", "url": "https://www.semanticscholar.org/paper/451dde1a7755831b46705faf44b2497fd975e295", "title": "Towards Computational Guessing of Unknown Word Meanings: The Ontological Semantic Approach", "abstract": "Towards Computational Guessing of Unknown Word Meanings: The Ontological Semantic Approach Julia M. Taylor (jtaylor1@purdue.edu) CERIAS, Purdue University & RiverGlass, Inc West Lafayette, IN 47907 & Champaign, IL 61820 Victor Raskin (vraskin@purdue.edu) Linguistics & CERIAS, Purdue University West Lafayette, IN 47907 Christian F. Hempelmann (chempelm@purdue.edu) Linguistics, Purdue University & RiverGlass, Inc West Lafayette, IN 47907 & Champaign, IL 61820 Abstract The paper describes a computational approach for guessing the meanings of previously unaccounted words in an implemented system for natural language processing. Interested in comparing the results to what is known about human guessing, it reviews a largely educational approach, partially based on cognitive psychology, to teaching humans, mostly children, to acquire new vocabulary from contextual clues, as well as the lexicographic efforts to account for neologisms. It then goes over the previous NLP efforts in processing new words and establishes the difference\u2014mostly, much richer semantic resources\u2014of the proposed approach. Finally, the results of a computer experiment that guesses the meaning of a non-existent word, placed as the direct object of 100 randomly selected verbs, from the known meanings of these verbs, with methods of the ontological semantics technology, are presented and discussed. While the results are promising percentage-wise, ways to improve them within the approach are briefly outlined. Keywords: guessing word meaning, natural language understanding, ontological semantic technology Unknown Words in Text Along with ambiguity, unattested input is one of the major problems for natural language processing systems. An NLP system is robust only if it can deal with unknown words. Yet, to deal with such words only makes sense when the rest of the sentence is understood. We take an approach here similar to that of a human learner that encounters an unfamiliar word and is able to approximate its meaning based on the rest of the sentence or its subsequent usages in other sentences. There are some suggested strategies in the human acquisition and understanding of unknown words. Some cases stand out as easy and almost self-explanatory. One of these cases is when a word is immediately explained. Such an explanation may be introduced by a that is phrase (To lose weight, one may have to follow a diet, that is, to limit the amount of food and to avoid eating certain foods.), or by apposition (Computers programs follow algorithms, ordered lists of instructions to perform.), or by examples (The earliest records of felines, for example, cats, tigers, lions, or leopards, are from millions of years ago.), or by providing the presumably known opposites for comparison through words like but, rather then, not (It is frigid outside, rather than warm and comfortable like yesterday.). Both in the case of human acquisition of new vocabulary and the machine attempt at guessing its meaning, these somewhat trivial instances, where the meaning of a new word is immediately explained, either by giving its definition or by examples, present no particular interest for us here. Besides, such cases are rather rare in regular expository texts because most writers do not bother to allow for vocabulary deficiency with regards to words with which they are well familiar themselves. Thus, it is the non-trivial cases, those without an attached explanation or description, that it is necessary to address when one is interested in designing a computer system for natural language understanding. On the other side of the spectrum lie words that can only be guessed through their functional description, not necessarily following the first use of an unknown word. These functional descriptions should be gathered throughout the document, or a number of documents, narrowing the original functional description, if necessary, or supplying other facets of it. For example, They used a Tim-Tim to navigate their way to the cabin on the lake. It took them almost half a day. They hadn\u2019t checked if the maps had been recently updated on the device, and spent hours looking for roads that no longer existed. From the clues in the first sentence, Tim-Tim can be understood as a navigation instrument (including an atlas or a map) through an inverse function of the instrument of navigation. Since no other devices are mentioned, this navigation instrument can be considered the device from the third sentence whose maps can be periodically updated. It is essential, therefore, in situations of dispersed clues that co-reference (or antecedence) be established correctly\u2014in this case, between device and Tim-Tim. Towards the middle of the spectrum are the cases where the description may immediately follow the first use of the word but without being helpfully triggered by phrases like for example or that is (He was rather taciturn. He didn\u2019t like", "venue": "CogSci", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "onto", "nlp", "nlp", "nlu", "onto", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 4, "nlu": 2}, "nlp_mention_counts": {"nlp": 4, "nlu": 2}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "e3a49ee315cb92da348beee18c8effcec5db9aa0", "url": "https://www.semanticscholar.org/paper/e3a49ee315cb92da348beee18c8effcec5db9aa0", "title": "NLP for the Generation of Training Data Sets for Ontology-Guided Weakly-Supervised Machine Learning in Digital Pathology", "abstract": "The combination of ontologies with machine learning (ML) approaches is a hot topic and not yet extensively investigated but having great future potential. This is due to the general fact that both, ontologies and ML, constitute two indispensable technologies for domain-specific knowledge extraction, actively used in knowledge-based systems. Whilst the primary goal of both these approaches are the same, knowledge discovery, little is yet known about how the two sources of knowledge can be successfully integrated. The main data source in digital pathology are whole slide images. For the effective generation of sufficiently large and high-quality training data we need to extract in addition information from medical reports, containing non-standardized text. Since full annotation on pixel level would be impracticably expensive, a practical solution is in weakly-supervised ML. In the project described in this paper we used ontology-guided natural language processing (NLP) for term extraction and a decision tree built with an expert-curated classification system. This demonstrates the practical value of our solution to analyze and structure training data sets for ML and as a tool for the generation of biobank catalogues.", "venue": "2019 IEEE Symposium on Computers and Communications (ISCC)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "onto", "nlp", "nlp", "onto", "onto", "nlp", "ke"], "mention_counts": {"nlp": 3, "kg": 1, "onto": 4, "ke": 1}, "nlp_mention_counts": {"nlp": 3, "ke": 1}, "ld_mention_counts": {"kg": 1, "onto": 4, "ke": 1}, "relevance_score": 0.7620593014579468}, {"paperId": "7415b111a760207d240b9cac859a91f6fd4b118e", "url": "https://www.semanticscholar.org/paper/7415b111a760207d240b9cac859a91f6fd4b118e", "title": "Web information extraction based on news domain ontology theory", "abstract": "For the current web information extraction can't adapt to the various page structures, this paper proposes a Web Information Extraction Method based on News Domain Ontology. The areas are accurately found out and the interested information was extracted exactly based on information extraction rules which is generated by news domain ontology. Using the technology of page processing, page conversion, XPath etc, the information extraction system based on news domain ontology is implemented. Testing from news site shows that the approach proposed doesn 't rely on the page structure and it can increase the recall and precision of information extraction.", "venue": "2010 IEEE 2nd Symposium on Web Society", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie", "ie", "ie", "ie", "onto", "ie", "onto"], "mention_counts": {"onto": 4, "ie": 6}, "nlp_mention_counts": {"ie": 6}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "5904186456ce1bd12574096b65a7c76960e1b98e", "url": "https://www.semanticscholar.org/paper/5904186456ce1bd12574096b65a7c76960e1b98e", "title": "Application of Domain Ontologies to Natural Language Processing: A Case Study for Drug-Drug Interactions", "abstract": "Natural Language Processing NLP techniques can provide an interesting way to mine the growing biomedical literature, and a promising approach for new knowledge discovery. However, the major bottleneck in this area is that these systems rely on specific resources providing the domain knowledge. Domain ontologies provide a contextual framework and a semantic representation of the domain, and they can contribute to a better performance of current NLP systems. However, their contribution to information extraction has not been well studied yet. The aim of this paper is to provide insights into the potential role that domain ontologies can play in NLP. To do this, the authors apply the drug-drug interactions ontology DINTO to named entity recognition and relation extraction from pharmacological texts. The authors use the DDI corpus, a gold-standard for the development and evaluation of IE systems in this domain, and evaluate their results in the framework of the last SemEval-2013 DDI Extraction task.", "venue": "Int. J. Inf. Retr. Res.", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "ie", "nlp", "onto", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 5, "onto": 4, "ie": 1}, "nlp_mention_counts": {"nlp": 5, "ie": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "2e614312ca22950b08697943c8a6a715ab285210", "url": "https://www.semanticscholar.org/paper/2e614312ca22950b08697943c8a6a715ab285210", "title": "Biomedical Concept Recognition Using Deep Neural Sequence Models", "abstract": "Background the automated identification of mentions of ontological concepts in natural language texts is a central task in biomedical information extraction. Despite more than a decade of effort, performance in this task remains below the level necessary for many applications. Results recently, applications of deep learning in natural language processing have demonstrated striking improvements over previously state-of-the-art performance in many related natural language processing tasks. Here we demonstrate similarly striking performance improvements in recognizing biomedical ontology concepts in full text journal articles using deep learning techniques originally developed for machine translation. For example, our best performing system improves the performance of the previous state-of-the-art in recognizing terms in the Gene Ontology Biological Process hierarchy, from a previous best F1 score of 0.40 to an F1 of 0.70, nearly halving the error rate. Nearly all other ontologies show similar performance improvements. Conclusions A two-stage concept recognition system, which is a conditional random field model for span detection followed by a deep neural sequence model for normalization, improves the state-of-the-art performance for biomedical concept recognition. Treating the biomedical concept normalization task as a sequence-to-sequence mapping task similar to neural machine translation improves performance.", "venue": "bioRxiv", "citationCount": 7, "fieldsOfStudy": ["Computer Science", "Biology"], "mentions": ["nll", "ie", "mt", "mt", "onto", "onto", "onto", "onto", "nlp", "nlp"], "mention_counts": {"onto": 4, "nll": 1, "nlp": 2, "mt": 2, "ie": 1}, "nlp_mention_counts": {"mt": 2, "nlp": 2, "nll": 1, "ie": 1}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "992bedb7cf40b5f7f72f4d5a5d10ec1967f92d48", "url": "https://www.semanticscholar.org/paper/992bedb7cf40b5f7f72f4d5a5d10ec1967f92d48", "title": "Automatic Knowledge Extraction with Human Interface", "abstract": "OrbWeaver, an automatic knowledge extraction system paired with a human interface, streamlines the use of unintuitive natural language processing software for modeling systems from their documentation. OrbWeaver enables the indirect transfer of knowledge about legacy systems by leveraging open source tools in document understanding and processing as well as using web based user interface constructs. By design, OrbWeaver is scalable, extensible, and usable; we demonstrate its utility by evaluating its performance in processing a corpus of documents related to advanced persistent threats in the cyber domain. The results indicate better knowledge extraction by revealing hidden relationships, linking co-related entities, and gathering evidence. Keywords\u2014knowledge extraction, human interface, natural language processing, system modeling, model based system engineering, cyber, advanced persistent threat", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "ke", "nlp", "ke", "ke"], "mention_counts": {"nlp": 2, "ke": 4}, "nlp_mention_counts": {"nlp": 2, "ke": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "128d207b22353cbde0ed321156dc5187befa25cf", "url": "https://www.semanticscholar.org/paper/128d207b22353cbde0ed321156dc5187befa25cf", "title": "A High Precision Pipeline for Financial Knowledge Graph Construction", "abstract": "Motivated by applications such as question answering, fact checking, and data integration, there is significant interest in constructing knowledge graphs by extracting information from unstructured information sources, particularly text documents. Knowledge graphs have emerged as a standard for structured knowledge representation, whereby entities and their inter-relations are represented and conveniently stored as (subject,predicate,object) triples in a graph that can be used to power various downstream applications. The proliferation of financial news sources reporting on companies, markets, currencies, and stocks presents an opportunity for extracting valuable knowledge about this crucial domain. In this paper, we focus on constructing a knowledge graph automatically by information extraction from a large corpus of financial news articles. For that purpose, we develop a high precision knowledge extraction pipeline tailored for the financial domain. This pipeline combines multiple information extraction techniques with a financial dictionary that we built, all working together to produce over 342,000 compact extractions from over 288,000 financial news articles, with a precision of 78% at the top-100 extractions.The extracted triples are stored in a knowledge graph making them readily available for use in downstream applications.", "venue": "International Conference on Computational Linguistics", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "kg", "ie", "kg", "kg", "kg", "ie", "kg"], "mention_counts": {"ke": 1, "kg": 5, "ie": 3}, "nlp_mention_counts": {"ke": 1, "ie": 3}, "ld_mention_counts": {"ke": 1, "kg": 5}, "relevance_score": 0.7620593014579468}, {"paperId": "620e797f4bd23f2343a240f6c7e84a78be39bdd1", "url": "https://www.semanticscholar.org/paper/620e797f4bd23f2343a240f6c7e84a78be39bdd1", "title": "The PISAB Question Answering System", "abstract": "The PISAB Question Answering system is based on a combination of Information Extraction and Information Retrieval techniques. Knowledge extracted from documents is modeled as a set of entities extracted from text and by relations between them. During the learning phase we index documents using the entities they contain. In the answering phase we exploit the index previously built in order to focus the search for the answer to just the most relevant documents. As answers to a question we select from these documents the paragraphs containing entities most similar to those in the question. PISAB has been submitted to the TREC-9 Conference, achieving encouraging results despite it current prototypical development stage. Introduction The problem of finding answers to questions on a large document collection, could in principle be solved by creating a knowledge base with the information extracted from documents and then querying such knowledge base. Unfortunately this approach is not yet feasible, since it requires advanced techniques of natural language processing, knowledge extraction, knowledge representation and reasoning, which are beyond the current state of the art. On the other hand, Information Retrieval techniques are quite effective in retrieving documents relevant to a certain subject, so in particular those which might contain the answer to a question. Information Extraction techniques help identifying certain kinds of information, but their capabilities are quite domain dependent and limited to entities with predefined patterns. Neither of these techniques is sufficient to address the Question Answering problem, but we have explored a way of combining them to build a complete question answering system. The approach The meaning of a document might be expressed in terms of entities and relations between them. Entities are the semantic equivalent of nouns present in the document, while relations correspond to verbs. For example, the information contained in the following phrase: \u201cJohn reads a book\u201d can be represented by means of the entities \u201cJohn\u201d and \u201cbook\u201d, and by the relation \u201creads\u201d that links subject and object. Relations need not be binary: prepositional phrases and various kinds of syntactic adjuncts allow expressing n-ary relations.", "venue": "TREC", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "nlp", "ke", "ie", "ie", "kg", "ie"], "mention_counts": {"nlp": 1, "kg": 2, "ke": 2, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 2, "ie": 3}, "ld_mention_counts": {"kg": 2, "ke": 2}, "relevance_score": 0.7620593014579468}, {"paperId": "3aa14bd47c6177248e7ebd356436e9d894b202d4", "url": "https://www.semanticscholar.org/paper/3aa14bd47c6177248e7ebd356436e9d894b202d4", "title": "Joint Posterior Revision of NLP Annotations via Ontological Knowledge", "abstract": "Different well-established NLP tasks contribute to elicit the semantics of entities mentioned in natural language text, such as Named Entity Recognition and Classification (NERC) and Entity Linking (EL). However, combining the outcomes of these tasks may result in NLP annotations \u2014 such as a NERC organization linked by EL to a person \u2014 that are unlikely or contradictory when interpreted in the light of common world knowledge about the entities these annotations refer to. We thus propose a general probabilistic model that explicitly captures the relations between multiple NLP annotations for an entity mention, the ontological entity classes implied by those annotations, and the background ontological knowledge those classes may be consistent with. We use the model to estimate the posterior probability of NLP annotations given their confidences (prior probabilities) and the ontological knowledge, and consequently revise the best annotation choice performed by the NLP tools. In a concrete scenario with two stateof-the-art tools for NERC and EL, we experimentally show on three reference datasets that for these tasks, the joint annotation revision performed by the model consistently improves on the original results of the tools.", "venue": "International Joint Conference on Artificial Intelligence", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "onto", "nlp", "onto", "nlp", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 6, "onto": 4}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "9c6048f927762837820ee0c1d1e1b2ebb6c16be8", "url": "https://www.semanticscholar.org/paper/9c6048f927762837820ee0c1d1e1b2ebb6c16be8", "title": "Calculating Word Sense Probability Distributions for Semantic Web Applications", "abstract": "Researchers have found that Word Sense Disambiguation (WSD) is useful for tasks such as ontology alignment. Many other Semantic Web applications could also be enhanced with WSD results of Semantic Web documents. A system that can provide reusable intermediate WSD results is desirable. Compared to the top sense or a rank of senses, an output of meaningful scores of each possible sense informs subsequent processes of the certainty in results, and facilitates the application of other knowledge in choosing the correct sense. We propose that probabilistic models, which have proved successful in many other fields, can also be applied to WSD. Based on such observations, we focus on the problem of calculating probability distributions of senses for terms. In this paper we propose our novel WSD approach with our probability model, derive the problem formula into small computable pieces, and propose ways to estimate the values of these pieces.", "venue": "2010 IEEE Fourth International Conference on Semantic Computing", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "onto", "sw", "sw", "wsd", "wsd", "sw", "wsd", "wsd", "wsd"], "mention_counts": {"sw": 3, "wsd": 6, "onto": 1}, "nlp_mention_counts": {"wsd": 6}, "ld_mention_counts": {"sw": 3, "onto": 1}, "relevance_score": 0.7620593014579468}, {"paperId": "4ecf1f8c380f0e75dbf597b550e8193df368e926", "url": "https://www.semanticscholar.org/paper/4ecf1f8c380f0e75dbf597b550e8193df368e926", "title": "Semantic pattern learning through maximum entropy-based WSD technique", "abstract": "This paper describes a Natural Language Learning method that extracts knowledge in the form of semantic patterns with ontology elements associated to syntactic components in the text. The method combines the use of EuroWordNet\u2019s ontological concepts and the correct sense of each word assigned by a Word Sense Disambiguation(WSD) module to extract three sets of patterns: subject-verb, verb-direct object and verb-indirect object. These sets define the semantic behaviour of the main textual elements based on their syntactic role. On the one hand, it is shown that Maximum Entropy models applied to WSD tasks provide good results. The evaluation of the WSD module has revealed a accuracy rate of 64% in a preliminary test. On the other hand, we explain how an adequate set of semantic or ontological patterns can improve the success rate of NLP tasks such us pronoun resolution. We have implemented both modules in C++ and although the evaluation has been performed for English, their general features allow the treatment of other languages like Spanish. This paper has been partially supported by the Spanish Government (CICYT) project number TIC2000-0664-C0202.", "venue": "CoNLL", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["nll", "onto", "ke", "nlp", "onto", "wsd", "wsd", "wsd", "onto"], "mention_counts": {"onto": 3, "nll": 1, "nlp": 1, "ke": 1, "wsd": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "wsd": 3, "nll": 1}, "ld_mention_counts": {"ke": 1, "onto": 3}, "relevance_score": 0.7620593014579468}, {"paperId": "9b5cea95de709e889eb41572d9e404667e5a330b", "url": "https://www.semanticscholar.org/paper/9b5cea95de709e889eb41572d9e404667e5a330b", "title": "Arabic NLP tools for ontology construction from Arabic text: An overview", "abstract": "Natural Language Processing (NLP) is a technique used to extract data in natural language text that is used in many applications. In the last years, NLP techniques have been adapted for ontology construction and population. For example, Part-of-Speech (POS) tagging, filtering, lexical semantics tagging or tagged named entities used to extract concepts and relationships among entities. The effect of the NLP tools on ontology construction from Arabic text is an area of research. However, to the best of our knowledge, there are no comparative studies that show the effectiveness of these preprocessing techniques. The main goal of this paper is to present a brief review for the existing Arabic NLP tools and assess their capabilities on Arabic ontology construction.", "venue": "2015 International Conference on Electrical and Information Technologies (ICEIT)", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "onto", "nlp", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 6, "onto": 4}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "d25ed54de4aad27e86720d47b5dd7ee3764f3e63", "url": "https://www.semanticscholar.org/paper/d25ed54de4aad27e86720d47b5dd7ee3764f3e63", "title": "Towards Symbiosis in Knowledge Representation and Natural Language Processing for Structuring Clinical Practice Guidelines", "abstract": "The successful adoption by clinicians of evidence-based clinical practice guidelines (CPGs) contained in clinical information systems requires efficient translation of free-text guidelines into computable formats. Natural language processing (NLP) has the potential to improve the efficiency of such translation. However, it is laborious to develop NLP to structure free-text CPGs using existing formal knowledge representations (KR). In response to this challenge, this vision paper discusses the value and feasibility of supporting symbiosis in text-based knowledge acquisition (KA) and KR. We compare two ontologies: (1) an ontology manually created by domain experts for CPG eligibility criteria and (2) an upper-level ontology derived from a semantic pattern-based approach for automatic KA from CPG eligibility criteria text. Then we discuss the strengths and limitations of interweaving KA and NLP for KR purposes and important considerations for achieving the symbiosis of KR and NLP for structuring CPGs to achieve evidence-based clinical practice.", "venue": "Nursing Informatics", "citationCount": 5, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlp", "onto", "nlp", "nlp", "nlp", "onto", "kg", "nlp", "nlp"], "mention_counts": {"nlp": 6, "onto": 3, "kg": 1}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"kg": 1, "onto": 3}, "relevance_score": 0.7620593014579468}, {"paperId": "6a762246fbc4084def7d4745fce341cd5a90dad6", "url": "https://www.semanticscholar.org/paper/6a762246fbc4084def7d4745fce341cd5a90dad6", "title": "Natural Language Generation From Ontologies Using Grammatical Framework", "abstract": "The paper addresses the problem of automatic generation of natural language descriptions for ontology-described artifacts. The motivation for the work is the challenge of providing textual descriptions of automatically generated scientific workflows (e.g., paragraphs that scientists can include in their publications). The extended abstract presents a system which generates descriptions of sets of atoms derived from a collection of ontologies. The system, called nlgPhylogeny, demonstrates the feasibility of the task in the Phylotastic project, that aims at providing evolutionary biologists with a platform for automatic generation of phylogenetic trees given some suitable inputs. nlgPhylogeny utilizes the fact that the Grammatical Framework (GF) is suitable for the natural language generation (NLG) task; the abstract shows how elements of the ontologies in Phylotastic, such as web services, inputs and outputs of web services, can be encoded in GF for the NLG task. 2012 ACM Subject Classification Computing methodologies \u2192 Logic programming and answer set programming, Information systems \u2192 Web services, Computing methodologies \u2192 Natural language generation", "venue": "ICLP", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "onto", "onto", "nlg", "nlg", "nlg", "nlg", "onto", "nlg"], "mention_counts": {"nlg": 6, "onto": 4}, "nlp_mention_counts": {"nlg": 6}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7620593014579468}, {"paperId": "cfc47e5748bbeb071b2035ba691f3c5ea303a9c9", "url": "https://www.semanticscholar.org/paper/cfc47e5748bbeb071b2035ba691f3c5ea303a9c9", "title": "A syntactic method of extracting terms from special texts for replenishing domain ontologies", "abstract": "Natural Language Processing (NLP) is one of the principal areas of artificial intelligence. It can be argued that the use of ontologies increases the efficiency of natural language processing. However, most ontologies are built manually and require a lot of work. Thus, the problem of automated ontology replenishment is very relevant. One approach is to develop methods for replenishing ontologies using NLP for specific texts of a certain area. We applied the developed method of replenishing the OntoMathPro mathematical ontology, by extracting new terminology from mathematical documents. We developed a method for processing complex syntactic structures (structures with coordination reduction). The method includes certain rule schemata, conditions under which they are to be applied, and conditions determining the sequence of subtrees for which they are to be performed. In our studies, we investigated typical coordination models for mathematical works and performed experiments with a big mathematical collection.", "venue": "2017 Second Russia and Pacific Conference on Computer Technology and Applications (RPC)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "onto", "nlp", "onto", "nlp", "onto", "onto"], "mention_counts": {"nlp": 4, "onto": 6}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.7620593014579468}, {"paperId": "cd349397fe11eae15f5b9fecb8f4ededb99ab112", "url": "https://www.semanticscholar.org/paper/cd349397fe11eae15f5b9fecb8f4ededb99ab112", "title": "A Knowledge-Based Sense Disambiguation Method to Semantically Enhanced NL Question for Restricted Domain", "abstract": "Within the space of question answering (QA) systems, the most critical module to improve overall performance is question analysis processing. Extracting the lexical semantic of a Natural Language (NL) question presents challenges at syntactic and semantic levels for most QA systems. This is due to the difference between the words posed by a user and the terms presently stored in the knowledge bases. Many studies have achieved encouraging results in lexical semantic resolution on the topic of word sense disambiguation (WSD), and several other works consider these challenges in the context of QA applications. Additionally, few scholars have examined the role of WSD in returning potential answers corresponding to particular questions. However, natural language processing (NLP) is still facing several challenges to determine the precise meaning of various ambiguities. Therefore, the motivation of this work is to propose a novel knowledge-based sense disambiguation (KSD) method for resolving the problem of lexical ambiguity associated with questions posed in QA systems. The major contribution is the proposed innovative method, which incorporates multiple knowledge sources. This includes the question\u2019s metadata (date/GPS), context knowledge, and domain ontology into a shallow NLP. The proposed KSD method is developed into a unique tool for a mobile QA application that aims to determine the intended meaning of questions expressed by pilgrims. The experimental results reveal that our method obtained comparable and better accuracy performance than the baselines in the context of the pilgrimage domain.", "venue": "Inf.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "wsd", "kg", "nlp", "nlp", "nlp", "onto", "wsd", "wsd", "kg"], "mention_counts": {"nlp": 3, "kg": 3, "wsd": 3, "onto": 1}, "nlp_mention_counts": {"nlp": 3, "wsd": 3}, "ld_mention_counts": {"kg": 3, "onto": 1}, "relevance_score": 0.7620593014579468}, {"paperId": "364760501bf9f1a8b7e8cbd0a8e01d19c9ae684e", "url": "https://www.semanticscholar.org/paper/364760501bf9f1a8b7e8cbd0a8e01d19c9ae684e", "title": "From hyperlinks to Semantic Web properties using Open Knowledge Extraction", "abstract": "Open information extraction approaches are useful but insufficient alone for populating the Web with machine read- able information as their results are not directly linkable to, and immediately reusable from, other Linked Data sources. This work proposes a novel paradigm, named Open Knowledge Extraction, and its implementation (Legalo) that performs unsuper- vised, open domain, and abstractive knowledge extraction from text for producing machine readable information. The imple- mented method is based on the hypothesis that hyperlinks (either created by humans or knowledge extraction tools) provide a pragmatic trace of semantic relations between two entities, and that such semantic relations, their subjects and objects, can be revealed by processing their linguistic traces (i.e. the sentences that embed the hyperlinks) and formalised as Semantic Web triples and ontology axioms. Experimental evaluations conducted on validated text extracted from Wikipedia pages, with the help of crowdsourcing, confirm this hypothesis showing high performances. A demo is available at http://wit.istc.cnr.it/stlab-tools/ legalo.", "venue": "Semantic Web", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "ke", "ke", "sw", "onto", "ie", "sw", "ke", "ke"], "mention_counts": {"onto": 1, "ld": 1, "ke": 4, "sw": 2, "ie": 1}, "nlp_mention_counts": {"ke": 4, "ie": 1}, "ld_mention_counts": {"ld": 1, "ke": 4, "sw": 2, "onto": 1}, "relevance_score": 0.7607792748995437}, {"paperId": "a7c9a6686d143bb03bbebac37b4deb90b281dea5", "url": "https://www.semanticscholar.org/paper/a7c9a6686d143bb03bbebac37b4deb90b281dea5", "title": "Boosting Document Retrieval with Knowledge Extraction and Linked Data", "abstract": "Given a document collection, Document Retrieval is the task of returning the most relevant documents for a specified user query. In this paper, we assess a document retrieval approach exploiting Linked Open Data and Knowledge Extraction techniques. Based on Natural Language Processing methods (e.g., Entity Linking, Frame Detection), knowledge extraction allows disambiguating the semantic content of queries and documents, linking it to established Linked Open Data resources (e.g., DBpedia, YAGO) from which additional semantic terms (entities, types, frames, temporal information) are imported to realize a semantic-based expansion of queries and documents. The approach, implemented in the KE4IR system, has been evaluated on different state-of-the-art datasets, on a total of 555 queries and with document collections spanning from few hundreds to more than a million of resources. The results show that the expansion with semantic content extracted from queries and documents enables consistently outperforming retrieval performances when only textual information is exploited; on a specific dataset for semantic search, KE4IR outperforms a reference ontology-based search system. The experiments also validate the feasibility of applying knowledge extraction techniques for document retrieval \u2014i.e., processing the document collection, building the expanded index, and searching over it\u2014 on large collections (e.g., TREC WT10g).", "venue": "Semantic Web", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["lod", "ke", "ke", "onto", "lod", "ld", "ke", "ke", "nlp"], "mention_counts": {"onto": 1, "ld": 1, "ke": 4, "lod": 2, "nlp": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 4}, "ld_mention_counts": {"ld": 1, "ke": 4, "lod": 2, "onto": 1}, "relevance_score": 0.7607792748995437}, {"paperId": "bf85ac3b7dcb2901526c749e83bf4542042d50ef", "url": "https://www.semanticscholar.org/paper/bf85ac3b7dcb2901526c749e83bf4542042d50ef", "title": "A hybrid ontology-based information extraction system", "abstract": "Information Extraction is the process of automatically obtaining knowledge from plain text. Because of the ambiguity of written natural language, Information Extraction is a difficult task. Ontology-based Information Extraction (OBIE) reduces this complexity by including contextual information in the form of a domain ontology. The ontology provides guidance to the extraction process by providing concepts and relationships about the domain. However, OBIE systems have not been widely adopted because of the difficulties in deployment and maintenance. The Ontology-based Components for Information Extraction (OBCIE) architecture has been proposed as a form to encourage the adoption of OBIE by promoting reusability through modularity. In this paper, we propose two orthogonal extensions to OBCIE that allow the construction of hybrid OBIE systems with higher extraction accuracy and a new functionality. The first extension utilizes OBCIE modularity to integrate different types of implementation into one extraction system, producing a more accurate extraction. For each concept or relationship in the ontology, we can select the best implementation for extraction, or we can combine both implementations under an ensemble learning schema. The second extension is a novel ontology-based error detection mechanism. Following a heuristic approach, we can identify sentences that are logically inconsistent with the domain ontology. Because the implementation strategy for the extraction of a concept is independent of the functionality of the extraction, we can design a hybrid OBIE system with concepts utilizing different implementation strategies for extracting correct or incorrect sentences. Our evaluation shows that, in the implementation extension, our proposed method is more accurate in terms of correctness and completeness of the extraction. Moreover, our error detection method can identify incorrect statements with a high accuracy.", "venue": "Journal of information science", "citationCount": 26, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "onto", "onto", "onto", "onto", "ie", "ie", "onto", "ie", "onto"], "mention_counts": {"onto": 8, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 8}, "relevance_score": 0.7607792748995437}, {"paperId": "7fcb917cd4b6d0c77c41b4a1b1dc0c4d1965ba7b", "url": "https://www.semanticscholar.org/paper/7fcb917cd4b6d0c77c41b4a1b1dc0c4d1965ba7b", "title": "Comprehensive Event Representations using Event Knowledge Graphs and Natural Language Processing", "abstract": "Recent work has utilised knowledge-aware approaches to natural language understanding, question answering, recommendation systems, and other tasks. These approaches rely on well-constructed and large-scale knowledge graphs that can be useful for many downstream applications and empower knowledge-aware models with commonsense reasoning. Such knowledge graphs are constructed through knowledge acquisition tasks such as relation extraction and knowledge graph completion. This work seeks to utilise and build on the growing body of work that uses findings from the field of natural language processing (NLP) to extract knowledge from text and build knowledge graphs. The focus of this research project is on how we can use transformer-based approaches to extract and contextualise event information, matching it to existing ontologies, to build comprehensive knowledge graph-based event representations. Specifically, sub-event extraction is used as a way of creating sub-event-aware event representations. These event representations are then further enriched through fine-grained location extraction and contextualised through the alignment of historically relevant quotes.", "venue": "WWW", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlu", "nlp", "kg", "kg", "kg", "onto", "kg", "ke", "kg", "kg", "nlp"], "mention_counts": {"onto": 1, "nlu": 1, "ke": 1, "nlp": 3, "kg": 6}, "nlp_mention_counts": {"nlp": 3, "ke": 1, "nlu": 1}, "ld_mention_counts": {"kg": 6, "onto": 1, "ke": 1}, "relevance_score": 0.7607792748995437}, {"paperId": "ba1bc10e15ed312ec30e3ff576e227961ee385ac", "url": "https://www.semanticscholar.org/paper/ba1bc10e15ed312ec30e3ff576e227961ee385ac", "title": "Corpus-based Ontology Learning for Word Sense Disambiguation", "abstract": "This paper proposes to disambiguate word senses by corpus-based ontology learning. Our approach is a hybrid method. First, we apply the previously-secured dictionary information to select the correct senses of some ambiguous words with high precision, and then use the ontology to disambiguate the remaining ambiguous words. The mutual information between concepts in the ontology was calculated before using the ontology as knowledge for disambiguating word senses. If mutual information is regarded as a weight between ontology concepts, the ontology can be treated as a graph with weighted edges, and then we locate the least weighted path from one concept to the other concept. In our practical machine translation system, our word sense disambiguation method achieved a 9% improvement over methods which do not use ontology for Korean translation.", "venue": "PACLIC", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "wsd", "onto", "onto", "wsd", "onto", "onto", "onto", "wsd", "onto", "wsd", "onto"], "mention_counts": {"wsd": 4, "onto": 8, "mt": 1}, "nlp_mention_counts": {"wsd": 4, "mt": 1}, "ld_mention_counts": {"onto": 8}, "relevance_score": 0.7607792748995437}, {"paperId": "edfb109298a04bfac6a1617be291fb44de3b0f0b", "url": "https://www.semanticscholar.org/paper/edfb109298a04bfac6a1617be291fb44de3b0f0b", "title": "Exploiting Ontology Lexica for Generating Natural Language Texts from RDF Data", "abstract": "The increasing amount of machinereadable data available in the context of the Semantic Web creates a need for methods that transform such data into human-comprehensible text. In this paper we develop and evaluate a Natural Language Generation (NLG) system that converts RDF data into natural language text based on an ontology and an associated ontology lexicon. While it follows a classical NLG pipeline, it diverges from most current NLG systems in that it exploits an ontology lexicon in order to capture context-specific lexicalisations of ontology concepts, and combines the use of such a lexicon with the choice of lexical items and syntactic structures based on statistical information extracted from a domain-specific corpus. We apply the developed approach to the cooking domain, providing both an ontology and an ontology lexicon in lemon format. Finally, we evaluate fluency and adequacy of the generated recipes with respect to two target audiences: cooking novices and advanced cooks.", "venue": "European Workshop on Natural Language Generation", "citationCount": 46, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "onto", "nlg", "rdf", "nlg", "nlg", "onto", "ie", "rdf", "nlg", "onto", "onto", "onto", "onto", "sw"], "mention_counts": {"onto": 7, "sw": 1, "nlg": 5, "rdf": 2, "ie": 1}, "nlp_mention_counts": {"nlg": 5, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 7, "rdf": 2}, "relevance_score": 0.748145532632524}, {"paperId": "b3a1323ae79be250758df20fd3308f3adf9c9e5e", "url": "https://www.semanticscholar.org/paper/b3a1323ae79be250758df20fd3308f3adf9c9e5e", "title": "Translating XBRL Into Description Logic. An Approach Using Protege, Sesame & OWL", "abstract": "In the context of the eTen project, WINS, a web-based business intelligence service to public and private financial institutions has been designed and implemented. One of the goals of the project was to provide new financial knowledge on companies from information gathered through interoperable information services. The services were implemented under the new emerging standard XBRL used for financial reporting. We sketch how relevant financial information was extracted from annual financial reportings. We also show at the same time the limitations we encountererd with the XBRL schema, due to the lack of reasoning support over XML-based data and information extracted from documents. To overcome these difficulties, we describe the \u201contologization\u201d of XBRL, which we assume to be a necessary requisite for large intelligent web-based financial information and decision support systems. 1 General Background In the context of the eTen project, WINS, a web-based business intelligence service to public and private financial institutions has been designed and implemented. One of the goals of the project was to provide new financial knowledge on companies from information gathered through interoperable information services [1]. The services were implemented under the new emerging standard called XBRL (eXtensible Business Reporting Language) used for financial reporting. In the following, we sketch how relevant financial information was extracted from annual financial reportings. We also show at the same time the limitations we encountererd with the XBRL schema, due to the lack of reasoning support over XML-based data and information extracted from documents that is finally mapped onto XBRL instances. In the first part of our submission, we just summarize our way of information extraction guided by XBRL, and afterwards describing our work dedicated to the ontologization of XBRL, which we assume to be a necessary requisite for large intelligent web-based financial information and decision support systems. The work described here will be further carried out within an Integrated Project of the 6th Framework, called MUSING (MUlti-Industry, Semantic-based next generation business Intelligence). 1 ETEN 2003/1, Grant agreement nr. C51083. WINS stands for Web-based Intelligence for common-interest fiscal Networked Services\u2019. 2 For more information, see XBRL International: http://www.xbrl.org. 2 Incremental Information Extraction Guided by XBRL The next three subsections present the basic setting, viz., XBRL-guided information extraction of structured and unstructured documents. 2.1 Knowledge-Driven Information Extraction from Structured and Unstructured Documents The actual input for the information extraction (IE) task in WINS consists of balance sheets in PDF format, containing structured forms (tables) and free text (included, for example, in the annexes of balance sheets). Relevant information extracted from these sources are merged and mapped onto the XBRL format. A terminological clarification should be given at this place. IE often refers to the task of filling useror application-defined templates with the result of information detected by natural language analysis tools in textual documents. For certain applications, knowledge bases are available, supporting the IE task. Such knowledge might consist of taxonomies, thesauri, or ontologies. In this case, knowledge-driven IE tries to populate knowledge bases with instances detected in the textual documents. This was the situation in WINS, where the XBRL taxonomy was guiding the IE task through the analysis of both tabular data and free text. In the end, an XBRL structure should be instantiated with the information extracted from the annual reports of companies. 2.2 The Mapping Process from Text to XBRL The mapping process has been implemented within a Web service made available to the WINS partners. The Web service operates on PDF/text files from WINS data providers and returns files, containing the data in XBRL format. In a first step, text and tables from the PDF documents were extracted. It was also necessary to apporimatively recontructs the original layout, which is getting lost in the PDF-to-text conversion. Once this has been done, the WINS information extraction module inspects the generated HTML documents, trying to find correspondences in the text of the tables for labels of concepts contained in the overall XBRL taxonomy. But not only the detection of realizations of XBRL concepts in the document is important. The extraction tools must also detect relevant dates in the tables as well as currencies used, so that the figures contained in the tables , e.g., balance and profit & loss (P&L) tables, are getting their correct interpretation. Since the XBRL taxonomy is also considering information about a company as such (name, address, number of employees, etc), the extraction tools need to detect this kind of information. In order to obtain such information, we implemented a simple named entity recognition algorithm for detecting names of companies, locations, and relevant persons. 456 BUSINESS INFORMATION SYSTEMS BIS 2006", "venue": "Business Information Systems", "citationCount": 32, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto", "onto", "ie", "kg", "ie", "ie", "ie", "ie", "kg", "ie", "ie", "onto", "ie"], "mention_counts": {"kg": 2, "onto": 4, "ie": 10}, "nlp_mention_counts": {"ie": 10}, "ld_mention_counts": {"kg": 2, "onto": 4}, "relevance_score": 0.748145532632524}, {"paperId": "97b45b01e294b8c3ea617d82e0c0c545a66bce6b", "url": "https://www.semanticscholar.org/paper/97b45b01e294b8c3ea617d82e0c0c545a66bce6b", "title": "Ontological semantics, formal ontology, and ambiguity", "abstract": "Ontological semantics is a theory of meaning in natural languageand an approach to natural language processing (NLP) which uses anontology as the central resource for extracting and representingmeaning of natural language texts, reasoning about knowledgederived from texts as well as generating natural language textsbased on representations of their meaning. Ontological semanticsdirectly supports such applications as machine translation ofnatural languages, information extraction, text summarization,question answering, advice giving, collaborative work of networksof human and software agents, etc. Ontological semantics paysserious attention to its theoretical foundations by explicating itspremises; therefore, formal ontology and its relations withontological semantics are important. Besides a general briefdiscussion of these relations, the paper focuses on the importanttheoretical and practical issue of the distinction between ontologyand natural language. It is argued that this crucial distinctionlies not in the (inaccurately) presumed nonambiguity of the one andthe well-established ambiguity of the other but rather in theconstructed and overtly defined nature of ontological concepts andlabels on which no human background knowledge can operateunintentionally to introduce ambiguity, as opposed to pervasiveuncontrolled and uncontrollable ambiguity in natural language. Theemphasis on this distinction, we argue, will provide bettertheoretical support for the central tenets of formal ontology byfreeing it from the Wittgensteinian and Rortyan retreats from theanalytical paradigm; it also reinforces the methodology of NLP bymaintaining a productive demarcation between thelanguage-independent nature of ontology and language-specificnature of the lexicons, a demarcation that has paid off well inconsecutive implementations of ontological semantics and theirapplications in practical computer systems.", "venue": "Formal Ontology in Information Systems", "citationCount": 52, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlg", "onto", "nlp", "onto", "onto", "nlp", "onto", "ie", "onto", "onto", "onto", "mt", "onto", "onto", "onto"], "mention_counts": {"onto": 10, "nlp": 3, "nlg": 1, "mt": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "nlg": 1, "mt": 1, "ie": 1}, "ld_mention_counts": {"onto": 10}, "relevance_score": 0.748145532632524}, {"paperId": "90f5e40f9706e031ba57163245b593419ab4f754", "url": "https://www.semanticscholar.org/paper/90f5e40f9706e031ba57163245b593419ab4f754", "title": "Reconciling Heterogeneous Descriptions of Language Resources", "abstract": "Language resources are a cornerstone of linguistic research and for the development of natural language processing tools, but the discovery of relevant resources remains a challenging task. This is due to the fact that relevant metadata records are spread among different repositories and it is currently impossible to query all these repositories in an integrated fashion, as they use different data models and vocabularies. In this paper we present a first attempt to collect and harmonize the metadata of different repositories, thus making them queriable and browsable in an integrated way. We make use of RDF and linked data technologies for this and provide a first level of harmonization of the vocabularies used in the different resources by mapping them to standard RDF vocabularies including Dublin Core and DCAT. Further, we present an approach that relies on NLP and in particular word sense disambiguation techniques to harmonize resources by mapping values of attributes \u2010 such as the type, license or intended use of a resource \u2010 into normalized values. Finally, as there are duplicate entries within the same repository as well as across different repositories, we also report results of detection of these duplicates.", "venue": "LDL@IJCNLP", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "rdf", "ld", "rdf", "nlp", "wsd"], "mention_counts": {"ld": 1, "nlp": 2, "wsd": 1, "rdf": 2}, "nlp_mention_counts": {"nlp": 2, "wsd": 1}, "ld_mention_counts": {"ld": 1, "rdf": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "b618f990830123fb87e033d0b7e18a040d5b55c4", "url": "https://www.semanticscholar.org/paper/b618f990830123fb87e033d0b7e18a040d5b55c4", "title": "Refining Search Performance through Semantic based CBR Model and QoS Ranking Methodology", "abstract": "Objectives:\u00a0To refine search performance using semantic web with an improved algorithm to retrieve the information efficiently.\u00a0Methods:\u00a0In order to establish the SCBR model and improve the performance of Web search, this paper adopts the Natural Language Processing (NLP) technology and the Quality of Service (QoS) ranking method, and endeavors to develop a relevant reliable and efficient search engine.\u00a0Findings:\u00a0Mean average precision tests revealed for quickness and precious of search results, and achieves the values from 82.98% to 99.53%. The experimental results show that the NLP technique improves the performance of SCBR model, and achieves higher average precision and recall values.\u00a0Novelty:\u00a0This research focuses to develop a related reliable and an efficient search engine to retrieve the accurate results for the user\u2019s complex query. It even bears the human error in typing, and suggests the expected word to search for. It also aims to retrieving the same result for synonym words which prevent the appearance of irrelevant search results. \n \nKeywords \n\u00ad Semantic Web, Information Retrieval, SCBR Model, Ontology, Semantic Search, Quality of Service", "venue": "Indian Journal of Science and Technology", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlp", "nlp", "onto", "sw", "nlp"], "mention_counts": {"nlp": 3, "sw": 2, "onto": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "ffba67307a1b002859f6b87739ba91d8c39804ae", "url": "https://www.semanticscholar.org/paper/ffba67307a1b002859f6b87739ba91d8c39804ae", "title": "Extending a model for ontology-based Arabic-English machine translation", "abstract": "The acceleration in telecommunication needs leads to many groups of research, especially in communication facilitating and Machine Translation fields. While people contact with others having different languages and cultures, they need to have instant translations. However, the available instant translators are still providing somewhat bad Arabic-English Translations, for instance when translating books or articles, the meaning is not totally accurate. Therefore, using the semantic web techniques to deal with the homographs and homonyms semantically, the aim of this research is to extend a model for the ontology-based Arabic-English Machine Translation, named NAN, which simulate the human way in translation. The experimental results show that NAN translation is approximately more similar to the Human Translation than the other instant translators. The resulted translation will help getting the translated texts in the target language somewhat correctly and semantically more similar to human translations for the Non-Arabic Natives and the Non-English natives.", "venue": "International Journal of Artificial Intelligence & Applications", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "sw", "onto", "mt", "mt"], "mention_counts": {"sw": 1, "onto": 2, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "384152f62d0db5995b90ad441c435c4abce8048b", "url": "https://www.semanticscholar.org/paper/384152f62d0db5995b90ad441c435c4abce8048b", "title": "Semantic text mining for lignocellulose research", "abstract": "Semantic technologies, including natural language processing (NLP), ontologies, semantic web services and web-based collaboration tools, promise to support users in dealing with complex data, thereby facilitating knowledge-intensive tasks. An ongoing challenge is to select the appropriate technologies and combine them in a coherent system that brings measurable improvements to the users. We present our ongoing development of a semantic infrastructure in support of genomics-based lignocellulose research. Part of this effort is the automated curation of knowledge from information on enzymes from fungi that is available in the literature and genome resources. Fungi naturally break down lignocellulose, hence the identification and characterization of the enzymes that they use in lignocellulose hydrolysis is an important part in research and development of biomass-derived products and fuels. Working close to the biology researchers who manually curate the existing literature, we developed ontological NLP pipelines integrated in a Web-based interface to help them in two main tasks: mining the literature for relevant information, and at the same time providing rich and semantically linked information.", "venue": "DTMBIO '11", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "nlp", "sw"], "mention_counts": {"nlp": 3, "sw": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "cc56e8b52b314f77bc3e85fffaf70559197453dc", "url": "https://www.semanticscholar.org/paper/cc56e8b52b314f77bc3e85fffaf70559197453dc", "title": "Building Multilingual Lexical Resources on Semiotic Principles", "abstract": "Multilinguality permeates the web so that multilingual resources are fundamental in several NLP applications as cross language information retrieval as well as machine translation. Nonetheless the manual creation of such resources is very expensive. Semantic Web technologies can represent a great enhancement for NLP applications. In this paper, we show how Semantic Web technologies as an upper ontology based on well-founded semiotic theories can be applied to build multilingual lexical resources as Machine Readable Dictionaries (MRDs).", "venue": "KEOD", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "onto", "mt", "nlp", "sw", "nlp"], "mention_counts": {"nlp": 2, "sw": 2, "onto": 1, "mt": 1}, "nlp_mention_counts": {"nlp": 2, "mt": 1}, "ld_mention_counts": {"sw": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "89756bfc1dfec78c00c043e41006f731b14265a7", "url": "https://www.semanticscholar.org/paper/89756bfc1dfec78c00c043e41006f731b14265a7", "title": "Nature: A Tool Resulting from the Union of Artificial Intelligence and Natural Language Processing for Searching Research Projects in Colombia", "abstract": "This paper presents the final results of the research project that aimed for the construction of a tool which is aided by Artificial Intelligence through an Ontology with a model trained with Machine Learning, and is aided by Natural Language Processing to support the semantic search of research projects of the Research System of the University of Nari\u00f1o. For the construction of NATURE, as this tool is called, a methodology was used that includes the following stages: appropriation of knowledge, installation and configuration of tools, libraries and technologies, collection, extraction and preparation of research projects, design and development of the tool. The main results of the work were three: a) the complete construction of the Ontology with classes, object properties (predicates), data properties (attributes) and individuals (instances) in Proteg\u00e9, SPARQL queries with Apache Jena Fuseki and the respective coding with Owlready2 using Jupyter Notebook with Python within the virtual environment of anaconda; b) the successful training of the model for which Machine Learning algorithms were used and specifically Natural Language Processing algorithms such as: SpaCy, NLTK, Word2vec and Doc2vec, this was also performed in Jupyter Notebook with Python within the virtual environment of anaconda and with Elasticsearch; and c) the creation of NATURE by managing and unifying the queries for the Ontology and for the Machine Learning model. The tests showed that NATURE was successful in all the searches that were performed as its results were satisfactory.", "venue": "International Journal of Artificial Intelligence & Applications", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "23976900d27bddcdbfbbba4ba5bea11cfe51bbbf", "url": "https://www.semanticscholar.org/paper/23976900d27bddcdbfbbba4ba5bea11cfe51bbbf", "title": "An Application of the Semantic Web Inspired by Human Learning and Natural Language Processing", "abstract": "The prototype in this paper presents a semantic web application that is inspired from psychology experiments on human learning and natural language processing. It aims at improving the proficiency of present search engines when dealing with specific queries (question-answering). The prototype makes use of the idea that the world wide web itself contains an enormous number of documents written in natural language (appearing in formats such as .html, .xml, .cfm, .pdf, .net, .asp etc.). It is consistently trained to improve its language proficiency by extracting knowledge from these documents and by storing redundant information in a database (i.e. information dealing with the same concept but expressed in different words).", "venue": "Int. J. Emerg. Technol. Learn.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "sw", "nlp", "nlp", "sw"], "mention_counts": {"nlp": 2, "ke": 1, "sw": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"ke": 1, "sw": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "073385657675a186ff6386f8520c490121ed1bc6", "url": "https://www.semanticscholar.org/paper/073385657675a186ff6386f8520c490121ed1bc6", "title": "World Wide Web in the Service of Schooling: Semantic Web as a Solution for Language Teaching in Cypriot Secondary Education", "abstract": "This paper examines some suitability aspects of existing web search engines in relation to the content and the stated learning objectives of language teaching in Cypriot secondary education, focusing on the language course of the third high school grade (G9). The end goal is to put the internet in the service of schooling; specifically to categorize the results returned by the search engine in- to genres in order to facilitate user (teacher or student) in choosing the most ap- propriate texts for their learning purposes. The tools for categorizing texts are being sought in the field of Semantic Web technology, such as metadata, ontol- ogies, software agents, and, the techniques in the fields of Natural Language Processing (NLP), Information Retrieval (IR), Information Extraction (IE) and Text Mining. The paper proposes the categorization of texts into six major ge- nre categories according to their external (structural) and internal (linguistic, stylistic) characteristics. For the purpose of this research, the MeDa13 metadata model was designed on the basis of the standard metadata model Dublin Core, and the Textual Genres Ontology (TeGO) was developed for describing the concepts mentioned in genres. In this work, we present the theoretical back- ground for the development of the proposed models (MeDa13 and TeGO), and also the methodological plan to achieve the research objective, which is the ca- tegorization of texts into genres considering the content and learning objectives for language teaching.", "venue": "EC-TEL Doctoral Consortium", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "onto", "nlp", "sw", "ie"], "mention_counts": {"nlp": 2, "sw": 2, "onto": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"sw": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "a4ffca7760507690e9f05758899aebfd7e96075a", "url": "https://www.semanticscholar.org/paper/a4ffca7760507690e9f05758899aebfd7e96075a", "title": "Linguistic Linked Open Data (LLOD). Introduction and Overview", "abstract": "The explosion of information technology has led to a substantial growth in quantity, diversity and complexity of linguistic data accessible over the internet. The lack of interoperability between linguistic and language resources represents a major challenge that needs to be addressed, in particular, if information from different sources is to be combined, like, say, machine-readable lexicons, corpus data and terminology repositories. For these types of resources, domainspecific standards have been proposed, yet, issues of interoperability between different types of resources persist, commonly accepted strategies to distribute, access and integrate their information have yet to be established, and technologies and infrastructures to address both aspects are still under development. The goal of the 2nd Workshop on Linked Data in Linguistics (LDL-2013) has been to bring together researchers from various fields of linguistics, natural language processing, and information technology to present and discuss principles, case studies, and best practices for representing, publishing and linking linguistic data collections, including corpora, dictionaries, lexical networks, translation memories, thesauri, etc., infrastructures developed on that basis, their use of existing standards, and the publication and distribution policies that were adopted. Background: Integrating Information from Different Sources In recent years, the limited interoperability between linguistic resources has been recognized as a major obstacle for data use and re-use within and across discipline boundaries. After half a century of computational linguistics [8], quantitative typology [12], empirical, corpus-based study of language [10], and computational lexicography [16], researchers in computational linguistics, natural language processing (NLP) or information technology, as well as in Digital Humanities, are confronted with an immense wealth of linguistic resources, that are not only growing in number, but also in their heterogeneity. Interoperability involves two aspects [14]: Structural (\u2018syntactic\u2019) interoperability: Resources use comparable formalisms to represent and to access data (formats, protocols, query languages, etc.),", "venue": "Workshop on Linked Data in Linguistics", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["llod", "nlp", "nlp", "nlp", "llod", "lod"], "mention_counts": {"nlp": 3, "llod": 2, "lod": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"llod": 2, "lod": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "1834fd2594feed8eeaccd7fc0698c8de7ab42ee3", "url": "https://www.semanticscholar.org/paper/1834fd2594feed8eeaccd7fc0698c8de7ab42ee3", "title": "Adaptive information extraction for document annotation in amilcare", "abstract": "Amilcare is a tool for Adaptive Information Extraction (IE) designed for supporting active annotation of documents for the Semantic Web (SW). It can be used either for unsupervised document annotation or as a support for human annotation. Amilcare is portable to new applications/domains without any knowledge of IE, as it just requires users to annotate a small training corpus with the information to be extracted. It is based on (LP)2, a supervised learning strategy for IE able to cope with different texts types, from newspaper-like texts, to rigidly formatted Web pages and even a mixture of them[1][5].Adaptation starts with the definition of a tag set for annotation, possibly organized as an ontology. Then users have to manually annotate a small training corpus. Amilcare provides a default mouse-based interface called Melita, where annotations are inserted by first selecting a tag from the ontology and then identifying the text area to annotate with the mouse. Differently from similar annotation tools [4, 5], Melita actively supports training corpus annotation. While users annotate texts, Amilcare runs in the background learning how to reproduce the inserted annotation. Induced rules are silently applied to new texts and their results are compared with the user annotation. When its rules reach a (user-defined) level of accuracy, Melita presents new texts with a preliminary annotation derived by the rule application. In this case users have just to correct mistakes and add missing annotations. User corrections are inputted back to the learner for retraining. This technique focuses the slow and expensive user activity on uncovered cases, avoiding requiring annotating cases where a satisfying effectiveness is already reached. Moreover validating extracted information is a much simpler task than tagging bare texts (and also less error prone), speeding up the process considerably. At the end of the corpus annotation process, the system is trained and the application can be delivered. MnM [6] and Ontomat annotizer [7] are two annotation tools adopting Amilcare's learner.In this demo we simulate the annotation of a small corpus and we show how and when Amilcare is able to support users in the annotation process, focusing on the way the user can control the tool's proactivity and intrusivity. We will also quantify such support with data derived from a number of experiments on corpora. We will focus on training corpus size and correctness of suggestions when the corpus is increased.", "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "sw", "onto", "ie", "onto", "ie"], "mention_counts": {"sw": 1, "onto": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "09510e0761bce39ea0b23804347ef8c4ee967b66", "url": "https://www.semanticscholar.org/paper/09510e0761bce39ea0b23804347ef8c4ee967b66", "title": "Machine Learning-Based Algorithms to Knowledge Extraction from Time Series Data: A Review", "abstract": "To predict the future behavior of a system, we can exploit the information collected in the past, trying to identify recurring structures in what happened to predict what could happen, if the same structures repeat themselves in the future as well. A time series represents a time sequence of numerical values observed in the past at a measurable variable. The values are sampled at equidistant time intervals, according to an appropriate granular frequency, such as the day, week, or month, and measured according to physical units of measurement. In machine learning-based algorithms, the information underlying the knowledge is extracted from the data themselves, which are explored and analyzed in search of recurring patterns or to discover hidden causal associations or relationships. The prediction model extracts knowledge through an inductive process: the input is the data and, possibly, a first example of the expected output, the machine will then learn the algorithm to follow to obtain the same result. This paper reviews the most recent work that has used machine learning-based techniques to extract knowledge from time series data.", "venue": "International Conference on Data Technologies and Applications", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "aa2889ca477809de939ad7b6dd2cb96b1825a829", "url": "https://www.semanticscholar.org/paper/aa2889ca477809de939ad7b6dd2cb96b1825a829", "title": "Knowledge of extraction from trained neural network by using decision tree", "abstract": "Inside the sets of data, hidden knowledge can be acquired by using neural network. These knowledge are described within topology, using activation function and connection weight at hidden neurons and output neurons. Is hardly to be understanding since neural networks act as a black box. The black box problem can be solved by extracting knowledge (rule) from trained neural network. Thus, the aim of this paper is to extract valuable information from trained neural networks using decision. Further, the Levenberg Marquardt algorithm was applied to training 30 networks for each datasets, using learning parameters and basis weights differences. As the number of hidden neurons increase, mean squared error and mean absolute percentage error decrease, and more time they need to deal with the dataset, that is result of investigation from neural network architectures. Decision tree induction generally performs better in knowledge extraction result with accuracy and precision level from 84.07 to 93.17 percent. The extracted rule can be used to explaining the process of the neural network systems and also can be applied in other systems like expert systems.", "venue": "International Conference on Science in Information Technology", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "29b559b912543d191c3ef9871ac18a857664a341", "url": "https://www.semanticscholar.org/paper/29b559b912543d191c3ef9871ac18a857664a341", "title": "Vulnerability knowledge extraction method based on joint extraction model", "abstract": "Information extraction is an important semantic processing task to construct network security knowledge graph. Extracting entities and relationships in vulnerability description from public data sets will inevitably lead to waste of manpower and difficulty in accurate positioning. Another challenge is that there are multiple relationships among vulnerable descriptors. This paper proposes a framework for the common vulnerabilities and exposures (CVE) analysis, which consists of entity annotation algorithm and relational classification model. In particular, we apply the model to CVE dataset to solve the problem of information extraction and relationship classification in the CVE vulnerability analysis. Moreover, the predicted relationship is used to construct vulnerability security knowledge graph. The experimental results show that the framework can deal with the CVE vulnerability description effectively, and has good relationship classification performance.", "venue": "2021 Ninth International Conference on Advanced Cloud and Big Data (CBD)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ie", "ie", "kg"], "mention_counts": {"ke": 1, "kg": 2, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "kg": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "1dc2cb77823d391954314aa67cb5603d72e5686a", "url": "https://www.semanticscholar.org/paper/1dc2cb77823d391954314aa67cb5603d72e5686a", "title": "Challenges and Opportunities for Journalistic Knowledge Platforms", "abstract": "Journalism is under pressure from loss of advertisement and revenues, while experiencing an increase in digital consumption and user demands for quality journalism and trusted sources. Journalistic Knowledge Platforms (JKPs) are an emerging generation of platforms which combine state-of-the-art artificial intelligence (AI) techniques such as knowledge graphs, linked open data (LOD), and natural-language processing (NLP) for transforming newsrooms and leveraging information technologies to increase the quality and lower the cost of news production. In order to drive research and design better JKPs that allow journalists to get most benefits out of them, we need to understand what challenges and opportunities JKPs are facing. This paper presents an overview of the main challenges and opportunities involved in JKPs which have been manually extracted from literature with the support of natural language processing and understanding techniques. These challenges and opportunities are organised in: stakeholders, information, functionalities, components, techniques and other aspects.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "lod", "nlp", "lod", "kg"], "mention_counts": {"nlp": 3, "lod": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 1, "lod": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "61645daf1c3a20bbe5570efb30f43b1645b26b33", "url": "https://www.semanticscholar.org/paper/61645daf1c3a20bbe5570efb30f43b1645b26b33", "title": "An Information Extraction Method for Digitized Textbooks of Traditional Chinese Medicine", "abstract": "Digital libraries have shouldered the mission of preserving and spreading human culture in the era of information. However, knowledge extraction for digital libraries is not well studied, and that holds back the role promotion of digital libraries from information collector to knowledge provider. This paper presents an ontology-based approach, which extracts detailed attributes of Traditional Chinese Medicine (TCM) from digitized textbooks. According to the characters of digitized textbooks, we propose an extraction ontology that is compatible with both textbook extraction and TCM theory. To improve extraction tolerance for OCR errors, we extract features of different aspects. Finally, a structured pattern based extraction method is adopted to minimize extraction supervision. The result shows that our method is a practical and robust exploration to address the problem of information extraction for digitized textbooks of TCM.", "venue": "2010 10th IEEE International Conference on Computer and Information Technology", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ke", "ie", "onto"], "mention_counts": {"ke": 1, "onto": 2, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "e0abebbcc12baaf5d84e0c66c8731347c2c52ed7", "url": "https://www.semanticscholar.org/paper/e0abebbcc12baaf5d84e0c66c8731347c2c52ed7", "title": "Knowledge Extraction and Extrapolation Using Ancient and Modern Biomedical Literature", "abstract": "Extraction of knowledge from biomedical literature is one of the major problems for researchers. This primarily involves identification of novel associations between biological objects (genes, proteins, diseases, medicines etc.). These associations are commonly extracted by mining biomedical resources such as the PUBMED which contains a large volume of information. An automated approach towards this end will reduce a substantial amount of time for biomedical researchers. In this paper we discuss a methodology to extract such associations and to assign a significance measure to the generated hypotheses. The computed significance value for the extracted knowledge can be considered as association strength between biological objects. The generated hypotheses with large significance can be considered for further experimental validation by biologists. In this paper we conduct two different validation studies of the results, which provide justification for the approach that was followed to generate the hypotheses.", "venue": "2009 International Conference on Advanced Information Networking and Applications Workshops", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "d0947127e4d5bb20c7ae4c965aa289958a399575", "url": "https://www.semanticscholar.org/paper/d0947127e4d5bb20c7ae4c965aa289958a399575", "title": "Precise tweet classification and sentiment analysis", "abstract": "The rise of social media in couple of years has changed the general perspective of networking, socialization, and personalization. Use of data from social networks for different purposes, such as election prediction, sentimental analysis, marketing, communication, business, and education, is increasing day by day. Precise extraction of valuable information from short text messages posted on social media (Twitter) is a collaborative task. In this paper, we analyze tweets to classify data and sentiments from Twitter more precisely. The information from tweets are extracted using keyword based knowledge extraction. Moreover, the extracted knowledge is further enhanced using domain specific seed based enrichment technique. The proposed methodology facilitates the extraction of keywords, entities, synonyms, and parts of speech from tweets which are then used for tweets classification and sentimental analysis. The proposed system is tested on a collection of 40,000 tweets. The proposed methodology has performed better than the existing system in terms of tweets classification and sentiment analysis. By applying the Knowledge Enhancer and Synonym Binder module on the extracted information we have achieved increase in information gain in a range of 0.1% to 55%. The increase in information gain has enabled our proposed system to better summarize the twitter data for user sentiments regarding a keyword from a particular category.", "venue": "International Conference on Interaction Sciences", "citationCount": 94, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "kg", "ke"], "mention_counts": {"ke": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "663e7fb1582fe3622863f5fc2234bc876bbb9ece", "url": "https://www.semanticscholar.org/paper/663e7fb1582fe3622863f5fc2234bc876bbb9ece", "title": "Ontology supported information extraction for document of evidence-based nursing domain", "abstract": "The increasing amount of information available in electronic media is a strategic resource for the health professional who uses this source to support decision making. However, it is not feasible to analyze a significant number of documents in a short time without the support of a computational tool. The Information Extraction aims to extract from textual documents only the relevant information defined by the user. This information can be mapped and classified in the field of health through ontologies. This paper proposes the design of an information extracting mechanism that, in a hybrid form, can combine: an extractor for textual documents; the construction and use of a domain ontology for evidence-based nursing, aiming to assist in the classification of documents which will be extracted by the extractor and in the representation of this domain with respect to the computational area.", "venue": "2014 9th Iberian Conference on Information Systems and Technologies (CISTI)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "ie"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "41350b24e0766b40a90fede59adec1531340dea9", "url": "https://www.semanticscholar.org/paper/41350b24e0766b40a90fede59adec1531340dea9", "title": "Fine-grained Information Extraction from Biomedical Literature based on Knowledge-enriched Abstract Meaning Representation", "abstract": "Biomedical Information Extraction from scientific literature presents two unique and non-trivial challenges. First, compared with general natural language texts, sentences from scientific papers usually possess wider contexts between knowledge elements. Moreover, comprehending the fine-grained scientific entities and events urgently requires domain-specific background knowledge. In this paper, we propose a novel biomedical Information Extraction (IE) model to tackle these two challenges and extract scientific entities and events from English research papers. We perform Abstract Meaning Representation (AMR) to compress the wide context to uncover a clear semantic structure for each complex sentence. Besides, we construct the sentence-level knowledge graph from an external knowledge base and use it to enrich the AMR graph to improve the model\u2019s understanding of complex scientific concepts. We use an edge-conditioned graph attention network to encode the knowledge-enriched AMR graph for biomedical IE tasks. Experiments on the GENIA 2011 dataset show that the AMR and external knowledge have contributed 1.8% and 3.0% absolute F-score gains respectively. In order to evaluate the impact of our approach on real-world problems that involve topic-specific fine-grained knowledge elements, we have also created a new ontology and annotated corpus for entity and event extraction for the COVID-19 scientific literature, which can serve as a new benchmark for the biomedical IE community.", "venue": "Annual Meeting of the Association for Computational Linguistics", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "ie", "kg", "kg"], "mention_counts": {"kg": 2, "onto": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "4c70eb2ef48ec1aae22546e202e52bc0d1c820e2", "url": "https://www.semanticscholar.org/paper/4c70eb2ef48ec1aae22546e202e52bc0d1c820e2", "title": "Ontology mediated information extraction in financial domain with Mastro System-T", "abstract": "Information extraction (IE) refers to the task of turning text documents into a structured form, in order to make the information contained therein automatically processable. Ontology Mediated Information Extraction (OMIE) is a new paradigm for IE that seeks to exploit the semantic knowledge expressed in ontologies to improve query answering over unstructured data (properly raw text). In this paper we present Mastro System-T, an OMIE tool born from a joint collaboration between the University of Rome \"La Sapienza\" and IBM Research Almaden and its first application in a financial domain, namely to facilitate the access to and the sharing of data extracted from the EDGAR system.", "venue": "DSMM@SIGMOD", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "986ae00cc06f76e773c438a54e6d93c4f4f72a1c", "url": "https://www.semanticscholar.org/paper/986ae00cc06f76e773c438a54e6d93c4f4f72a1c", "title": "An ontology-based Web information extraction approach", "abstract": "An approach supervised by ontology is proposed for Web information extraction after analyzing two types of methods based on wrapper and concept model. Using concepts and taxonomy relation between concepts provided by ontology, this method can locate the wanted information blocks in Web page quickly by judging if adjacent sub-trees which are included in HTML Tree are isomorphic. Furthermore, combining text's data-modes the method can filter information which are irrelevant to the wanted information and achieve higher accuracy of information extraction.", "venue": "2010 2nd International Conference on Future Computer and Communication", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "onto", "ie", "onto"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "c35b261f1fdca6cfa51abbe74c9cf942fc25d0e9", "url": "https://www.semanticscholar.org/paper/c35b261f1fdca6cfa51abbe74c9cf942fc25d0e9", "title": "Domain-Targeted, High Precision Knowledge Extraction", "abstract": "Our goal is to construct a domain-targeted, high precision knowledge base (KB), containing general (subject,predicate,object) statements about the world, in support of a downstream question-answering (QA) application. Despite recent advances in information extraction (IE) techniques, no suitable resource for our task already exists; existing resources are either too noisy, too named-entity centric, or too incomplete, and typically have not been constructed with a clear scope or purpose. To address these, we have created a domain-targeted, high precision knowledge extraction pipeline, leveraging Open IE, crowdsourcing, and a novel canonical schema learning algorithm (called CASI), that produces high precision knowledge targeted to a particular domain - in our case, elementary science. To measure the KB\u2019s coverage of the target domain\u2019s knowledge (its \u201ccomprehensiveness\u201d with respect to science) we measure recall with respect to an independent corpus of domain text, and show that our pipeline produces output with over 80% precision and 23% recall with respect to that target, a substantially higher coverage of tuple-expressible science knowledge than other comparable resources. We have made the KB publicly available.", "venue": "International Conference on Topology, Algebra and Categories in Logic", "citationCount": 47, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ie"], "mention_counts": {"ke": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "f485717097e721531452134332e158d75e1167d7", "url": "https://www.semanticscholar.org/paper/f485717097e721531452134332e158d75e1167d7", "title": "EULAide: Interpretation of End-User License Agreements using Ontology-Based Information Extraction", "abstract": "Ignoring End-User License Agreements (EULAs) for online services due to their length and complexity is a risk undertaken by the majority of online and mobile service users. This paper presents an Ontology-Based Information Extraction (OBIE) method for EULA term and phrase extraction to facilitate a better understanding by humans. An ontology capturing important terms and relationships has been developed and used to guide the OBIE process. Through a feedback cycle we have improved its domain-specific coverage by identifying additional concepts. In the detection and extraction, we focus on three key rights and conditions: permission, prohibition and duty. We present the EULAide system, which comprises a custom information extraction pipeline and a number of custom extraction rules tailored for EULA processing. To evaluate our approach, we created and manually annotated a corpus of 20 well-known licenses. For the gold standard we achieved an Inter-Annotator Agreement (IAA) of 90%, resulting in 193 permissions, 185 prohibitions and 168 duties. An evaluation of the OBIE pipeline against this gold standard resulted in an F-measure of 70-74% which, in the context of the IAA, proves the feasibility of the approach.", "venue": "SEMANTiCS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "d1f157f6b11de095690979c32f330bbea77436d3", "url": "https://www.semanticscholar.org/paper/d1f157f6b11de095690979c32f330bbea77436d3", "title": "KNEWS: Using Logical and Lexical Semantics to Extract Knowledge from Natural Language", "abstract": "We present KNEWS, a pipeline of NLP tools that accepts natural language text as input and outputs knowledge in a machine-readable format. The tool outputs frame-based knowledge as RDF triples or XML, including the word-level alignment with the surface form, as well as first-order logical formulae. KNEWS is freely available for download. Moreover, thanks to its versatility, KNEWS has already been employed for a number of different applications for information extraction and automatic reasoning.", "venue": "European Conference on Artificial Intelligence", "citationCount": 21, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "ie", "kg", "ke", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "kg": 1, "rdf": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 1, "rdf": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "cf2f6883a52e75e090e39b120c1a1dbf8790441c", "url": "https://www.semanticscholar.org/paper/cf2f6883a52e75e090e39b120c1a1dbf8790441c", "title": "Ontology-Based Enhanced Word Embedding for Automated Information Extraction from Geoscience Reports", "abstract": "Larger amount of geoscience reports brings both challenges and opportunities for data mining and analysis. This paper proposes an ontology-based enhanced word embedding (OEWE) information extraction methodology for extracting information about geoscience topic from regional geoscience reports. We first built the geoscience ontology to obtain a controlled vocabulary, and then the Skip-Gram model of word embedding was improved by Point-wise Mutual Information (PMI). Empirical experimental results on geoscience documents and benchmark datasets showed that the method is efficient.", "venue": "GEOINFORMATICS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "ie", "ie"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "36f2980b6e2698e3a97b2d1f980712ced96dc87f", "url": "https://www.semanticscholar.org/paper/36f2980b6e2698e3a97b2d1f980712ced96dc87f", "title": "Ontology Mediated Information Extraction with MASTRO SYSTEM-T", "abstract": "In several data-centric application domains, the need arises to extract valuable information from unstructured text documents. The recent paradigm of Ontology Mediated Information Extraction (OMIE) faces this problem by taking into account the knowledge expressed by a domain ontology, and reasoning over it to improve the quality of extracted data. MASTRO SYSTEM-T is a novel tool for OMIE, developed by Sapienza University and IBM Almaden Research. In this work, we demonstrate its usage for information extraction over real-world financial text documents from the U.S. EDGAR system.", "venue": "SEMWEB", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto", "ie", "onto"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "890f50f3c7418a98d4e88d7f7faa0d31c2b4fd3f", "url": "https://www.semanticscholar.org/paper/890f50f3c7418a98d4e88d7f7faa0d31c2b4fd3f", "title": "Concepts extraction for medical documents using ontology", "abstract": "In the biomedical domain large amount of text documents are unstructured information is available in digital text form. Text Mining is the method or technique to find for interesting and useful information from unstructured text. Text Mining is also an important task in medical domain. The technique uses for Information retrieval, Information extraction and natural language processing (NLP). Traditional approaches for information retrieval are based on key based similarity. These approaches are used to overcome these problems; Semantic text mining is to discover the hidden information from unstructured text and making relationships of the terms occurring in them. In the biomedical text, the text should be in the form of text which can be present in the books, articles, literature abstracts, and so forth. Most of information is stored in the text format, so in this paper we will focus on the role of ontology for semantic text mining by using WordNet. Specifically, we have presented a model for extracting concepts from text documents using linguistic ontology in the domain of medical.", "venue": "2015 International Conference on Advances in Computer Engineering and Applications", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "ie", "onto", "onto", "nlp"], "mention_counts": {"nlp": 2, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "eee552f14c70947f277395b3ebe2841e770767a2", "url": "https://www.semanticscholar.org/paper/eee552f14c70947f277395b3ebe2841e770767a2", "title": "Special issue on \u201cMulti\u2010modal information learning and analytics of cross\u2010media big data\u201d", "abstract": "We are living in the era of data deluge. Meanwhile, the world of big data includes a rich and complex set of cross\u2010media content, including text, images, video, audio, and graphics. To date, substantial research efforts have been dedicated to big data processing and cross\u2010media mining, yielding good theoretical underpinnings and great practical success. However, studies that jointly consider cross\u2010media big data analytics are relatively sparse (Zhang, Liu, Deng, Xu, and Choo (2018); Zhang, Wei, Wang, and Liao (2018)). This research gap needs more attention since it will benefit many real\u2010world applications. Despite its significance, it is important to analyse cross\u2010media big data due to the heterogeneity (Zhang, Zhao, Li, Chen, and Yuan (2015)), large\u2010scale volume, increasing size, lack of structure, correlations, and noise. Multi\u2010modal information learning (Demertzis, Iliadis, Avramidis, and El\u2010Kassaby (2017)), which can be treated as the most significant breakthrough in the past 10 years, has greatly affected the methodology of computer vision and achieved substantial progress in both academia and industry. Additionally, deep learning has been adopted in all kinds of computer vision applications, and many breakthroughs have been achieved in sub\u2010areas, such as DeepFace in the LFW competition for face verification and GoogleNet for the ImageNet Competition for object categorization. We can expect that more and more computer vision applications will benefit from multi\u2010modal information learning. This special issue focuses on learning methods to achieve high\u2010performance multi\u2010modal information analysis and understanding under uncontrolled environments on a large scale, which is also a very challenging problem. This problem has attracted considerable attention from both academia and industry. We hope this topic will aggregate top\u2010level research on new advances in multi\u2010modal information from cross\u2010media data. The purpose of this SI is to provide a forum for researchers and practitioners to exchange ideas and progress in related areas. After several rounds of review, six papers have been accepted for publication. Zhu, Ma, and Liu (2018) combined status\u2010striving motivation with opinion dynamics to study the decision mechanism and evolution law of employee knowledge sharing behaviour. A multiagent simulation method is used to design the decision\u2010making model of employee knowledge sharing behaviour. The influence of the network structure, number of people, and the proportion of employees with different statuses on knowledge\u2010sharing performance is considered. The simulation software is used to convert the theoretical model into a parametric experiment, and the sensitivity analysis is also employed. According to natural language processing and machine translation technology, to solve the problem, He (2018) aims to establish a parallel corpus for information extraction based on the OntoNotes corpus by combining automatic extraction and manual adjustment. To verify the validity of the parallel corpus constructed in this paper, a comparative experiment was carried out on the corpus. The corpus entity alignment rate, anaphora absence, and syntactic structure were analysed in detail based on statistics. To reduce the vagueness and subjectivity of customer demand in the process of product\u2013service system design, a fuzzy semantic calculation method is proposed by Zhang (2018) to obtain the importance of service demand. In addition, according to the demand of the clustering of service modules, a new clustering method is proposed to analyse discrete data based on the improved K\u2010means algorithm that is based on the Kruskal algorithm. Based on the actual situation of the knowledge flow during industrial transfer, a dynamic model of knowledge flow in a complex network is built by Zhou and Wu (2018). The mechanism of the knowledge flow in the network is analysed, and an analogue simulation is conducted using the Netlogo platform to identify the effects of the network's average degree, the number of enterprises involved in the transfer, the network size, and the effective transmission rate of the knowledge on the efficiency of the knowledge flow in the network. The work presented by Yang (2018) proposes an ontology mapping approach, in which the ontology element name and structure are combined. It uses the approaches based on linguistics and distance to generate a variable weight semantic graph. On this graph, the similarity of element names and structure are calculated through iterative computation. In the process of iteration, similarity result values are constantly adjusted. The approach avoids the problem of single methods that cannot use the entire amount of ontology information; therefore, it provides a more ideal mapping result. As an important resource of the company, the customer plays a vital role in the process of knowledge sharing. Therefore, research on the mechanism of customers' participation in knowledge sharing is especially important. In the work by Wang and Liu (2019), from the perspective of knowledge sharing between customers and enterprises, a customer participation model of a knowledge\u2010sharing mechanism is built based on three aspects: subjective influencing factors of knowledge sharing, objective influencing factors of knowledge sharing, and environmental influencing factors of knowledge sharing. As the special issue editors, we would like to take this opportunity to thank the various authors for their papers and the reviewers for their work. We are also grateful to Jon Hall, Editor\u2010in\u2010Chief of the Wiley journal Expert Systems.", "venue": "Expert Syst. J. Knowl. Eng.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "nlp", "mt"], "mention_counts": {"nlp": 1, "onto": 3, "mt": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "mt": 1, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "307e0ad03911072bc46639c14b30b053c2199867", "url": "https://www.semanticscholar.org/paper/307e0ad03911072bc46639c14b30b053c2199867", "title": "Identification des cat\u00e9gories de produits issus de catalogues publicitaires", "abstract": "In this paper, we propose an approach of information extraction, based on an ontology, and applied to documents from advertising catalogs. Documents are relatively poor descriptions of products. The information to be extracted, or annotations, concern the categories and features of the products, listed in a domain ontology. Thus, the information extraction about a product is actually an ontology population process, more precisely the population of concepts representing its categories and features. The poverty of the descriptions makes a fully automatic population impossible. We propose a two-step approach: (1) a first semi-automatic annotation step, which covers a small set of documents; (2) a second step, which annotates all other documents, in an entirely automatic way, based on machine learning mechanisms exploiting the results of the first step. The originality of this work relies on an incremental approach Revue d\u2019intelligence artificielle \u2013 n 5/2016, 557-578 558 RIA. Volume 30 \u2013 n 5/2016 to refine the extracted information. The work described has been applied on real data, in the toy domain. MOTS-CL\u00c9S : extraction d\u2019informations, peuplement d\u2019ontologie, annotation s\u00e9mantique, application dans le domaine du e-commerce.", "venue": "Rev. d'Intelligence Artif.", "citationCount": 0, "fieldsOfStudy": ["Geography", "Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "ie", "onto"], "mention_counts": {"onto": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "81ce8f3dabcfc48ac89d5fda2f3da961e59918f2", "url": "https://www.semanticscholar.org/paper/81ce8f3dabcfc48ac89d5fda2f3da961e59918f2", "title": "An Automated Learner for Extracting New Ontology Relations", "abstract": "Recently, the NLP community has shown a renewed interest in automatic recognition of semantic relations between pairs of words in text which called lexical semantics. This approach to semantics is concerned with psychological facts associated with the meaning of words. Lexical semantics is an important task with many potential applications including but not limited to, Information Retrieval, Information Extraction, Text Summarization, and Language Modeling. As this task \"automatic recognition of semantic relations between pairs of words in text\" can be used in many NLP applications, its implementation are demanding and may include many potential methodologies. And as it includes semantic processing, the results produced still need enhancements and the outcome was always limited in terms of domain or coverage. In this research we developed a buffered system that handle the whole process of extracting causation relations in general domain ontologies. The main achievement of this work is the heavy analysis of statistical and semantic information of causation relation context to generate the learner. The system also builds relation resources that made it possible to learn from itself, were each time it runs the resources incremented with new relations information recording all the statistics of such relation, making its performance enhanced each time it runs. Also we present a novel approach of learning based on the best lexical patterns extracted, besides two new algorithms the CIA and PS that provide the final set of rules for mining causation to enrich ontologies.", "venue": "International Conference on Advanced Computer Science Applications and Technologies", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "onto", "ie"], "mention_counts": {"nlp": 2, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "fec04ff73c63fa05b1f886f114bb4db779a944f4", "url": "https://www.semanticscholar.org/paper/fec04ff73c63fa05b1f886f114bb4db779a944f4", "title": "Semantic Relatedness Metric for Wikipedia Concepts Based on Link Analysis and its Application to Word Sense Disambiguation", "abstract": "Wikipedia has grown into a high quality up-todate knowledge base and can enable many knowledge-based applications, which rely on semantic information. One of the most general and quite powerful semantic tools is a measure of semantic relatedness between concepts. Moreover, the ability to efficiently produce a list of ranked similar concepts for a given concept is very important for a wide range of applications. We propose to use a simple measure of similarity between Wikipedia concepts, based on Dice\u2019s measure, and provide very efficient heuristic methods to compute top k ranking results. Furthermore, since our heuristics are based on statistical properties of scale-free networks, we show that these heuristics are applicable to other complex ontologies. Finally, in order to evaluate the measure, we have used it to solve the problem of word-sense disambiguation. Our approach to word sense disambiguation is based solely on the similarity measure and produces results with high accuracy.", "venue": "Spring Young Researchers Colloquium on Databases and Information Systems", "citationCount": 59, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "kg", "wsd", "kg", "wsd", "onto"], "mention_counts": {"kg": 2, "wsd": 3, "onto": 1}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "36e3add5e8c53c962d25b869d81bae5e13ad98eb", "url": "https://www.semanticscholar.org/paper/36e3add5e8c53c962d25b869d81bae5e13ad98eb", "title": "Internet Data Analysis Methodology for Cyberterrorism Vocabulary Detection, Combining Techniques of Big Data Analytics, NLP and Semantic Web", "abstract": "This article presents a methodology for the analysis of data on the Internet, combining techniques of Big Data analytics, NLP and semantic web in order to find knowledge about large amounts of information on the web. To test the effectiveness of the proposed method, webpages about cyberterrorism were analyzed as a case study. The procedure implemented a genetic strategy in parallel, which integrates (Crawler to locate and download information from the web; to retrieve the vocabulary, using techniques of NLP (tokenization, stop word, TF, TFIDF), methods of stemming and synonyms). For the pursuit of knowledge was built a dataset through the description of a linguistic corpus with semantic ontologies, considering the characteristics of cyber-terrorism, which was analyzed with the algorithms, Random Forests (parallel), Boosting, SVM, neural network, K-nn and Bayes. The results reveal a percentage of the 95.62% accuracy in the detection of the vocabulary of cyber-terrorism, which were approved through cross validation, reaching 576% time savings with parallel processing.", "venue": "International Journal on Semantic Web and Information Systems (IJSWIS)", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "sw", "nlp", "nlp", "sw"], "mention_counts": {"nlp": 3, "sw": 2, "onto": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "fc6eedc69f5da8fa7f1b2aeae58c403c0c4746f3", "url": "https://www.semanticscholar.org/paper/fc6eedc69f5da8fa7f1b2aeae58c403c0c4746f3", "title": "Knowledge extraction from software engineering repositories", "abstract": "Software engineering processes are hard to understand, and related tasks frequently produce lot of information which can be used for development of strategy for future Projects. In the last decade, a large number of software data sets have been created for different purposes, however as the challenges in the development and maintenance of software are increased the need for novel approaches to make use of the collected data is also increased. The demands for reduced development time and increased reliability of software also necessitated the need for knowledge extraction from the previously collected data sets. In this paper a detailed survey is conducted on the available methods for the knowledge extraction from the software engineering data bases to forecast and aid in improved development and maintenance phases of software.", "venue": "2017 Intelligent Systems Conference (IntelliSys)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "7ccfc1787a230a90c41c60cbfe8167e9e3df6d11", "url": "https://www.semanticscholar.org/paper/7ccfc1787a230a90c41c60cbfe8167e9e3df6d11", "title": "Efficient Parallel Wikipedia Internal Link Extraction for NLP-Assisted Requirements Understanding", "abstract": "Requirements engineering (RE) is a critical set of activities in the software development life cycle (SDLC). Without effective requirements elicitation, organization, communication, and understanding software engineers cannot build quality soft-ware. Thus, it is necessary for software stakeholders to facilitate the SDLC by following best practices and utilizing software tools as needed to ensure requirements are well understood. One area where RE still faces issues, despite stakeholders' best efforts, is the communication of requirements amongst the various stakeholders. Software stakeholders consist of the customers, developers, managers, end users, and others with a vested interest in the software, and they typically all have different skillsets, backgrounds, vernaculars, and understanding of the requirements. These differences naturally lead to miscommunications which can lead to redundant, missing, or conflicting requirements, especially when customer and end user domains include complex vocabularies developers may not be accustomed to, and vice versa, e.g., biology, physics, and medicine. One approach in recent works to address this challenge has been to bridge the communication gap between stakeholders by constructing domain-specific ontologies using natural language processing (NLP) and Wikipedia [1]. With these ontologies, stakeholders have a convenient tool they can use to translate and understand specific requirements in the terminologies they're accustomed to. These techniques have shown promising potential, however there are computational challenges associated with efficiently handling a large dataset like Wikipedia. In particular, parsing internal links from Wikipedia article metadata can be a bottleneck in such ontology-construction systems. In this work we address this issue by implementing a program for memory-efficient parallel internal link extraction from Wikipedia articles. This builds on the work of Rodriguez et al. [2] by optimizing additional phases in the knowledge acquisition process.", "venue": "2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "nlp", "onto", "onto"], "mention_counts": {"nlp": 3, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "10db7bab795cd3c57280594803ed838e0c443947", "url": "https://www.semanticscholar.org/paper/10db7bab795cd3c57280594803ed838e0c443947", "title": "The RDF-based Information Capturing System from Web Pages", "abstract": "It is an investigative purpose to acquire the event information in the municipality website and extraction information is converted into the XML form of the RDF model. There is a problem that the extraction performance is controlled by the structure of the HTML tag though there is Web-wrapper method that uses the HTML tag as an information extraction technique on the Web page. In this paper, we propose an extraction method from a HTML document based on dictionary. HTML tag is deleted from the HTML document and it converts it into the text. It proposes the method for extracting a target character string by comparing the text with the collection of words prepared beforehand. Finally, extraction information is converted into the XML form of the RDF model. The evaluation experiment was done to the municipality in 23 Tokyo district and 56 Chiba prefecture in Japan. The proposal method was able to extract event information on as a whole 73\\%. The LR-Wrapper was 52\\%. The Tree-Wrapper was 55\\%. The PLR-Wrapper was 32\\%. The proposal method confirmed event information was rating higher than an existing method extractive by the combination of a simple algorithm and the collection of words.", "venue": "International Conference on P2P, Parallel, Grid, Cloud and Internet Computing", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "rdf", "ie", "ie", "ie", "rdf"], "mention_counts": {"rdf": 3, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"rdf": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "0d4571f57b32a182a34ae49ac68257fbd174c1ba", "url": "https://www.semanticscholar.org/paper/0d4571f57b32a182a34ae49ac68257fbd174c1ba", "title": "PDFMEF: A Multi-Entity Knowledge Extraction Framework for Scholarly Documents and Semantic Search", "abstract": "We introduce PDFMEF, a multi-entity knowledge extraction framework for scholarly documents in the PDF format. It is implemented with a framework that encapsulates open-source extraction tools. Currently, it leverages PDFBox and TET for full text extraction, the scholarly document filter described in [5] for document classification, GROBID for header extraction, ParsCit for citation extraction, PDFFigures for figure and table extraction, and algorithm extraction [27]. While it can be run as a whole, the extraction tool in each module is highly customizable. Users can substitute default extractors with other extraction tools they prefer by writing a thin wrapper to implement the abstracts. The framework is designed to be scalable and is capable of running in parallel using a multi-processing technique in Python. Experiments indicate that the system with default setups is CPU bounded, and leaves a small footprint in the memory, which makes it best to run on a multi-core machine. The best performance using a dedicated server of 16 cores takes 1.3 seconds on average to process one PDF document. It is used to index extracted information and help users to quickly locate relevant results in published scholarly documents and to efficiently construct a large knowledge base in order to build a semantic scholarly search engine. Part of it is running on CiteSeerX digital library search engine.", "venue": "International Conference on Knowledge Capture", "citationCount": 39, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "b4c1c6895a588126db151dbe9fd6c2315be5e7bd", "url": "https://www.semanticscholar.org/paper/b4c1c6895a588126db151dbe9fd6c2315be5e7bd", "title": "Knowledge Extraction from Self-Organizing Map Using Minimization Entropy Principle Algorithm", "abstract": "Knowledge extraction using self-organizing map produced numeric values. This paper proposes knowledge extraction from self-organizing map using membership function from the minimization entropy principle algorithm to build linguistic intervals. The rough set theory was used in the rule extraction process for the minimum number of rules. The rules were in the form of linguistic \"if-then\" rule that user can understand easily. The benchmark data were iris database and Wisconsin breast cancer database. The experimental results received the fewer number of rules with high accuracy", "venue": "2006 International Symposium on Communications and Information Technologies", "citationCount": 1, "fieldsOfStudy": ["Mathematics", "Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "92e8fbd6a3dc4d571551a74f737dd8abe13c70c2", "url": "https://www.semanticscholar.org/paper/92e8fbd6a3dc4d571551a74f737dd8abe13c70c2", "title": "Knowledge Extraction from Construction Cost Databases Using Fuzzy Queries", "abstract": "Construction business abandons in data, such as: prices of individual work items, site productivity data, details on construction materials, and similar, that are used to estimate the works and offer services, at contractor's side, or evaluate construction alternatives and decide on money spending, at owner's side. Such a wealth of data is usually queried using rudimentary tools, producing plain spreadsheet-like reports, such as bills of quantities and lists of materials. Improvements in handling of existing hisorical data, especially cost data, would result in better knowledge extraction during estimating and could generally enhance the quality of construction project management.Construction estimating and evaluation of technology options are not always based on fully determined database queries. Therefore, \"flexible\" queries such as: \"retrieve materials with 'good daily output' and 'acceptable cost'\" are likely to be utilized on a historical cost database. Flexible queries are not possible in standard relational databases and there is no such command in SQL relational database language that can be used to retrieve materials with \"good daily output\" or \"acceptable cost\". Instead, relational query must state bottom limit for daily output and top limit for acceptable material cost.A standard relational construction cost estimating database has been created, with all necessary functions required by the modern construction practice. In order to enable usage of so-called linguistic variables in queries, relational cost data model has than been extended with concepts from fuzzy theory. User interface has been created in a transparent manner, so software may be used by professionals and alike without knowledge on fuzzy concepts. All extensions to data model have been implemented using standard relational database language, as both relational data model and fuzzy sets are based on theory of sets.Extended data model has been tested on a large construction project to simulate expert's reasoning and provide automated professional advice to the user. Results matched expert's opinion and model proved to be useful knowledge extraction tool in construction estimating practice.", "venue": "HIS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "47c8ec833415797328936781a53c02ba8f37cc56", "url": "https://www.semanticscholar.org/paper/47c8ec833415797328936781a53c02ba8f37cc56", "title": "Semantic Networks for Engineering Design: State of the Art and Future Directions", "abstract": "\n In the past two decades, there has been increasing use of semantic networks in engineering design for supporting various activities, such as knowledge extraction, prior art search, idea generation and evaluation. Leveraging large-scale pre-trained graph knowledge databases to support engineering design-related natural language processing (NLP) tasks has attracted a growing interest in the engineering design research community. Therefore, this paper aims to provide a survey of the state-of-the-art semantic networks for engineering design and propositions of future research to build and utilize large-scale semantic networks as knowledge bases to support engineering design research and practice. The survey shows that WordNet, ConceptNet and other semantic networks, which contain common-sense knowledge or are trained on non-engineering data sources, are primarily used by engineering design researchers to develop methods and tools. Meanwhile, there are emerging efforts in constructing engineering and technical-contextualized semantic network databases, such as B-Link and TechNet, through retrieving data from technical data sources and employing unsupervised machine learning approaches. On this basis, we recommend six strategic future research directions to advance the development and uses of large-scale semantic networks for artificial intelligence applications in engineering design.", "venue": "Journal of Mechanical Design", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ke", "nlp", "nlp"], "mention_counts": {"nlp": 2, "kg": 2, "ke": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "aa0da1fd33d1e46faa2bf60afd871bcf2aa89667", "url": "https://www.semanticscholar.org/paper/aa0da1fd33d1e46faa2bf60afd871bcf2aa89667", "title": "Data Guided Approach to Generate Multi-dimensional Schema for Targeted Knowledge Discovery", "abstract": "Data mining and data warehousing are two key technologies which have made significant contributions to the field of knowledge discovery in a variety of domains. More recently, the integrated use of traditional data mining techniques such as clustering and pattern recognition with data warehousing technique of Online Analytical Processing (OLAP) have motivated diverse research areas for leveraging knowledge discovery from complex real-world datasets. Recently, a number of such integrated methodologies have been proposed to extract knowledge from datasets but most of these methodologies lack automated and generic methods for schema generation and knowledge extraction. Mostly data analysts need to rely on domain specific knowledge and have to cope with technological constraints in order to discover knowledge from high dimensional datasets. In this paper we present a generic methodology which incorporates semi-automated knowledge extraction methods to provide data-driven assistance towards knowledge discovery. In particular, we provide a method for constructing a binary tree of hierarchical clusters and annotate each node in the tree with significant numeric variables. Additionally, we propose automated methods to rank nominal variables and to generate candidate multidimensional schema with highly significant dimensions. We have performed three case studies on three real-world datasets taken from the UCI machine learning repository in order to validate the generality and applicability of our proposed methodology.", "venue": "Australasian Data Mining Conference", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4777cc52b0f8d3510712ec8bc74b717d8f5245c0", "url": "https://www.semanticscholar.org/paper/4777cc52b0f8d3510712ec8bc74b717d8f5245c0", "title": "A review on knowledge extraction for Business operations using data mining", "abstract": "The Knowledge Economy is of great importance in various business fields which had resulted in increased demand for the people having high order thinking skills and unpredicted-problem-solving at workplace. Every organization has a Knowledge Management (KM) department as Knowledge itself is a precious resource of the organization. The latest trends in KM include Customer and Vendor knowledge, Mobile Applications for KM, Collaborative Knowledge Management System (KMS) and Social intranet which can be integrated with business processes. The knowledge extracted can be stored and processed to enhance business intelligence. KM works with various business fields like Marketing, Sales, Human Resource, Operations, Supply Chain, etc. Due to frequent changes in operation of processes and Quality policies, the knowledge extracted from these processes can play a vital role in enhancing business processes. In this paper we had proposed various models of KM & Business Operations and the need of data mining technique which can be used to deliver appropriate knowledge to the user.", "venue": "2017 7th International Conference on Cloud Computing, Data Science & Engineering - Confluence", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "bd09b846c49095070040d9ffb9d8015d2ede9f99", "url": "https://www.semanticscholar.org/paper/bd09b846c49095070040d9ffb9d8015d2ede9f99", "title": "Knowledge extraction through etymological networks: Synonym discovery in Sino-Korean words", "abstract": "Extracting knowledge from a text is a very active area of research. Techniques such as word embedding and LSA have brought great breakthroughs and have been used in applications such as automatic translation. We propose a novel approach to extract knowledge from text that relies on a graph to express the complex etymological structures formed by the historical roots of words. Our approach is specially fit for the study of Sino-Korean vo- cabulary, where the etymological roots of words are clearly shown in their writing. We use our approach to build a bipartite graph based on the Chinese etymological roots of Sino-Korean words, and then use the network structure to extract features describing pairs of nodes. We used these features in a classification scheme to discover pairs of nodes that represent synonym characters. Our model is simpler than previous work on synonym discovery with Chinese characters, and obtains good results. The code and data for our work are made openly available.", "venue": "2016 IEEE International Conference on Knowledge Engineering and Applications (ICKEA)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4459750d8bd1f8ae5277503c9767663aaa9a8722", "url": "https://www.semanticscholar.org/paper/4459750d8bd1f8ae5277503c9767663aaa9a8722", "title": "Discovering Knowledge by Comparing Silhouettes Using K-Means Clustering for Customer Segmentation", "abstract": "A large amount of data is generated every day from different sources. Knowledge extraction is the discovery of some useful and potential information from data that can help to make better decisions. Today's business process requires a technique that is intelligent and has the capability to discover useful patterns in data called data mining. This research is about using silhouettes created from K-means clustering to extract knowledge. This paper implements K-means clustering technique in order to group customers into K clusters according to deals purchased in two different scenarios using evolutionary algorithm for optimization and compare silhouettes for different K values to analyze the improvement in extracted knowledge.", "venue": "International Journal of Knowledge Management", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "bf36475cf160915944d1d32ae41c77f383255ec3", "url": "https://www.semanticscholar.org/paper/bf36475cf160915944d1d32ae41c77f383255ec3", "title": "Simulation knowledge extraction and reuse in constrained random processor verification", "abstract": "This work proposes a methodology of knowledge extraction from constrained-random simulation data. Feature-based analysis is employed to extract rules describing the unique properties of novel assembly programs hitting special conditions. The knowledge learned can be reused to guide constrained-random test generation towards uncovered corners. The experiments are conducted based on the verification environment of a commercial processor design, in parallel with the on-going verification efforts. The experimental results show that by leveraging the knowledge extracted from constrained-random simulation, we can improve the test templates to activate the assertions that otherwise are difficult to activate by extensive simulation.", "venue": "Design Automation Conference", "citationCount": 25, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "384c0b21af45678a1a3675c08520afd71a73221f", "url": "https://www.semanticscholar.org/paper/384c0b21af45678a1a3675c08520afd71a73221f", "title": "Knowledge Extraction of Adaptive Structural Learning of Deep Belief Network for Medical Examination Data", "abstract": "Deep learning has a hierarchical network structure to represent multiple features of input data. The adaptive structural learning method of Deep Belief Network (DBN) can reach the high classification capability while searching the optimal network structure during the training. The method can find the optimal number of hidden neurons for given input data in a Restricted Boltzmann Machine (RBM) by neuron generation\u2013annihilation algorithm, and generate a new hidden layer in DBN by the extension of the algorithm. In this paper, the proposed adaptive structural learning of DBN (Adaptive DBN) was applied to the comprehensive medical examination data for cancer prediction. The developed prediction system showed higher classification accuracy for test data (99.5% for the lung cancer and 94.3% for the stomach cancer) than the several learning methods such as traditional RBM, DBN, Non-Linear Support Vector Machine (SVM), and Convolutional Neural Network (CNN). Moreover, the explicit knowledge that makes the inference process of the trained DBN is required in deep learning. The binary patterns of activated neurons for given input in RBM and the hierarchical structure of DBN can represent the relation between input and output signals. These binary patterns were classified by C4.5 for knowledge extraction. Although the extracted knowledge showed slightly lower classification accuracy than the trained DBN network, it was able to improve inference speed by about 1/40. We report that the extracted IF-THEN rules from the trained DBN for medical examination data showed some interesting features related to initial condition of cancer.", "venue": "International Journal of Semantic Computing", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "e5edb891d05ee9db97c09ee6fdb18aa0d2c4b357", "url": "https://www.semanticscholar.org/paper/e5edb891d05ee9db97c09ee6fdb18aa0d2c4b357", "title": "Web Data Knowledge Extraction", "abstract": "A constantly growing amount of information is available through the web. Unfortunately, extracting useful content from this massive amount of data still remains an open issue. The lack of standard data models and structures forces developers to create adhoc solutions from the scratch. The figure of the expert is still needed in many situations where developers do not have the correct background knowledge. This forces developers to spend time acquiring the needed background from the expert. In other directions, there are promising solutions employing machine learning techniques. However, increasing accuracy requires an increase in system complexity that cannot be endured in many projects. In this work, we approach the web knowledge extraction problem using an expertcentric methodology. This methodology defines a set of configurable, extendible and independent components that permit the reutilisation of large pieces of code among projects. Our methodology differs from similar solutions in its expert-driven design. This design, makes it possible for subject-matter expert to drive the knowledge extraction for a given set of documents. Additionally, we propose the utilization of machine assisted solutions that guide the expert during this process. To demonstrate the capabilities of our methodology, we present a real use case scenario in which public procurement data is extracted from the web-based repositories of several public institutions across Europe. We provide insightful details about the challenges we had to deal with in this use case and additional discussions about how to apply our methodology.", "venue": "ArXiv", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4108915df0c8ee27f5863d79c6d603e557bd51e0", "url": "https://www.semanticscholar.org/paper/4108915df0c8ee27f5863d79c6d603e557bd51e0", "title": "Research on Flow Object Knowledge Extraction Method", "abstract": "This paper in view of flow object, through the research for data mining and knowledge discovery technology, has designed the system structure used for flow object knowledge extraction, and has shown related algorithm used for the flow object knowledge extraction. Take the cement production process for an example, it has extracted operation rule in cement calcinations process, proposed suggestive rule for big operation which can result in bigger fluctuation of key parameter, thus to achieve the goal of control effect improvement and the enhancement of installment running stability.", "venue": "2008 First International Conference on Intelligent Networks and Intelligent Systems", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "1f4a4769e4d2fb846e59c2f185e0377190739f18", "url": "https://www.semanticscholar.org/paper/1f4a4769e4d2fb846e59c2f185e0377190739f18", "title": "Learning Structured Embeddings of Knowledge Bases", "abstract": "\n \n Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigorous symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like nat- ural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning meth- ods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text.\n \n", "venue": "AAAI Conference on Artificial Intelligence", "citationCount": 808, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["nlu", "kg", "ke", "kg", "wsd"], "mention_counts": {"kg": 2, "wsd": 1, "ke": 1, "nlu": 1}, "nlp_mention_counts": {"ke": 1, "wsd": 1, "nlu": 1}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "113418a03ea3b5b66cee1103ec83dca2e6f1062a", "url": "https://www.semanticscholar.org/paper/113418a03ea3b5b66cee1103ec83dca2e6f1062a", "title": "Challenging Knowledge Extraction to Support the Curation of Documentary Evidence in the Humanities (short paper)", "abstract": "The identification and cataloguing of documentary evidence from textual corpora is an important part of empirical research in the humanities. In this position paper, we ponder the applicability of knowledge extraction techniques to support the data acquisition process. Initially, we characterise the task by analysing the end-to-end process occurring in the data curation activity. After that, we examine general knowledge extraction tasks and discuss their relation to the problem at hand. Considering the case of the Listening Experience Database (LED), we perform an empirical analysis focusing on two roles: the 'listener' and the 'place'. The results show, among other things, how the entities are often mentioned many paragraphs away from the evidence text or are not in the source at all. We discuss the challenges emerged from the point of view of scientific knowledge acquisition.", "venue": "SciKnow@K-CAP", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "2a3c577ff1c2fb73cebb2e1b76c09e16039353e0", "url": "https://www.semanticscholar.org/paper/2a3c577ff1c2fb73cebb2e1b76c09e16039353e0", "title": "Formal Concept Analysis With Background Knowledge: Attribute Priorities", "abstract": "This paper deals with background knowledge in knowledge extraction from binary data. A background knowledge represents an additional piece of information a user may have along with the input data. Such information can be considered as specifying the type of knowledge a user is looking for in the data. In particular, we emphasize the need for taking into account background knowledge in formal concept analysis. We present an approach to modeling background knowledge that represents user's priorities regarding attributes and their relative importance. Such priorities serve as a constraint-only those formal concepts that are compatible with user's priorities are considered relevant, extracted from data, and presented to the user. Our approach has two main practical features. First, the number of formal concepts presented to the user may get significantly reduced. As a result, the user is supplied with relevant formal concepts only and is not overloaded by a large number of possibly nonrelevant formal concepts. Second, different priorities lead to different pieces of knowledge extracted from data. This way, the input data may be repeatedly used in knowledge extraction for different purposes corresponding to different priorities. We concentrate on foundational aspects such as mathematical feasibility, reasoning with background knowledge, removing redundancy from background knowledge, and computational tractability, and present several illustrative examples. In addition, we discuss directions for future research.", "venue": "IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)", "citationCount": 79, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "e018d81158c97f0e19b9d7eeabc339a2309c0e56", "url": "https://www.semanticscholar.org/paper/e018d81158c97f0e19b9d7eeabc339a2309c0e56", "title": "Compressor Fault Diagnosis Knowledge: A Benchmark Dataset for Knowledge Extraction From Maintenance Log Sheets Based on Sequence Labeling", "abstract": "Compressor fault diagnosis requires expert knowledge. Using the sequence labeling technology, this expert knowledge can be automatically extracted from compressor maintenance log sheets. Previous studies indicate that sequence labeling methods often need a substantial amount of annotation data for knowledge extraction, Unfortunately, the annotation data are very scarce in the field of compressor fault diagnosis. In this paper, we introduce a benchmark dataset for extraction of knowledge suitable for air compressor fault diagnosis. First, we collected 11,418 pieces of information from air compressor maintenance log sheets. Fault description, service requests, causes and troubleshooting solutions were stored in a dataset for data preprocessing and masking. In addition, 6196 valid text pairs were developed after the \u201cnoises\u201d in the raw dataset were cleaned. Second, five kinds of entities and sequences, such as equipment, faults, service requests, causes and troubleshooting solutions, were annotated by three subject experts. The annotation consistency was assessed with F1 scores. Furthermore, our proposed baseline model (or the BERT-BI-LSTM-CRF model) was compared against other five sequence labeling models (BI-LSTM-CRF, Lattice LSTM, BERT NER, ZEN, and ERNIE). The BERT-BI-LSTM-CRF model gives superior performance in extracting expert knowledge from the subject dataset. Although the baseline model is not the most cutting-edge model in the sequence labeling and named entity recognition fields, it indeed presents a great potential for compressor fault diagnosis. The dataset is available at https://github.com/chentao1999/CFDK.", "venue": "IEEE Access", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "d564e3b76d05b6ba834f2e6a5f31617e5874841b", "url": "https://www.semanticscholar.org/paper/d564e3b76d05b6ba834f2e6a5f31617e5874841b", "title": "Symbolic Knowledge Extraction from Opaque Predictors Applied to Cosmic-Ray Data Gathered with LISA Pathfinder", "abstract": "Machine learning models are nowadays ubiquitous in space missions, performing a wide variety of tasks ranging from the prediction of multivariate time series through the detection of specific patterns in the input data. Adopted models are usually deep neural networks or other complex machine learning algorithms providing predictions that are opaque, i.e., human users are not allowed to understand the rationale behind the provided predictions. Several techniques exist in the literature to combine the impressive predictive performance of opaque machine learning models with human-intelligible prediction explanations, as for instance the application of symbolic knowledge extraction procedures. In this paper are reported the results of different knowledge extractors applied to an ensemble predictor capable of reproducing cosmic-ray data gathered on board the LISA Pathfinder space mission. A discussion about the readability/fidelity trade-off of the extracted knowledge is also presented.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Physics"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "7f5e959d0af68e932ba6e00fbd3a7801e1497084", "url": "https://www.semanticscholar.org/paper/7f5e959d0af68e932ba6e00fbd3a7801e1497084", "title": "Resource-aware Federated Learning using Knowledge Extraction and Multi-model Fusion", "abstract": "With increasing concern about user data privacy, feder- ated learning (FL) has been developed as a unique training paradigm for training machine learning models on edge de- vices without access to sensitive data. Traditional FL and existing methods directly employ aggregation methods on all edges of the same models and training devices for a cloud server. Although these methods protect data privacy, they are not capable of model heterogeneity, even ignore the heterogeneous computing power, and incur steep communica- tion costs. In this paper, we purpose a resource-aware FL to aggregate an ensemble of local knowledge extracted from edge models, instead of aggregating the weights of each local model, which is then distilled into a robust global knowledge as the server model through knowledge distillation. The local model and the global knowledge are extracted into a tiny size knowledge network by deep mutual learning. Such knowledge extraction allows the edge client to deploy a resource- aware model and perform multi-model knowledge fusion while maintaining communication ef\ufb01ciency and model het- erogeneity. Empirical results show that our approach has signi\ufb01cantly improved over existing FL algorithms in terms of communication cost and generalization performance in heterogeneous data and models. Our approach reduces the com- munication cost of VGG-11 by up to 102 \u00d7 and ResNet-32 by up to 30 \u00d7 when training ResNet-20 as the knowledge net- work.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "35f471394a99f1eb02c67ff6a7b01093c1fa44d8", "url": "https://www.semanticscholar.org/paper/35f471394a99f1eb02c67ff6a7b01093c1fa44d8", "title": "An architectural framework for knowledge extraction from meteorological data", "abstract": "The methods of knowledge extraction from spatio-temporal data such as meteorological domain suffer from various problems like missing values, noise, improper format, and large in volume. Therefore, data pre-processing and reorganisation are the important concerns of this domain. In this paper, we have discussed a layered framework for data fetching, pre-processing and persisting. Further, we develop knowledge extraction modules for supporting the future extensibility and reusing well established tools. We have presented experimental results achieved using this architecture, in addition to its other potential benefits.", "venue": "International Journal of Applied Management Science", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "dde4d58c428ca01b63efccff17d147a92e7d5693", "url": "https://www.semanticscholar.org/paper/dde4d58c428ca01b63efccff17d147a92e7d5693", "title": "Neural Relational Learning Through Semi-Propositionalization of Bottom Clauses", "abstract": "Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph mining and link analysis in social networks. The CILP++ system is a neural-symbolic system which can perform efficient relational learning, by being able to process first-order logic knowledge into a neural network. CILP++ relies on BCP, a recently discovered propositionalization algorithm, to perform relational learning. However, efficient knowledge extraction from such networks is an open issue and features generated by BCP do not have an independent relational description, which prevents sound knowledge extraction from such networks. We present a methodology for generating independent propositional features for BCP by using semi-propositionalization of bottom clauses. Empirical results obtained in comparison with the original version of BCP show that this approach has comparable accuracy and runtimes, while allowing proper relational knowledge representation of features for knowledge extraction from CILP++ networks.", "venue": "AAAI Spring Symposia", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "8597da824cfdb3e1b212fad236dea66b835d1ce7", "url": "https://www.semanticscholar.org/paper/8597da824cfdb3e1b212fad236dea66b835d1ce7", "title": "Knowledge extraction from SID epidemiological data using neural networks", "abstract": "Knowledge extraction is an important problem that has been little addressed by neural networks. In this work, we try to analyse the knowledge stored in a trained three-layer perceptron, using sudden infant death syndrome data. It is shown that when analysing the internal structure of the network, the classification solution realised by the network may be optimal in terms of classification results but not optimal in terms of knowledge representation. A simple method is proposed in order to reorganise and to extract knowledge stored in the synaptic weights.<<ETX>>", "venue": "Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "9fc210d4f461cac8ceb334dbcd3560e568b22767", "url": "https://www.semanticscholar.org/paper/9fc210d4f461cac8ceb334dbcd3560e568b22767", "title": "Discrete approach for automatic knowledge extraction from precedent large-scale data, and classification", "abstract": "The proposed method for automatic knowledge extraction from large-scale data is based on the idea of analysing neighborhoods of \"supporting\" objects and construction of data covered by sets of hyper parallelepipeds. A simple procedure to choose the supporting objects is applied. Knowledge extraction (logical regularities search) is based on the solution of special discrete linear optimization tasks associated with supporting objects. Two practical tasks are considered for method illustration.", "venue": "Object recognition supported by user interaction for service robots", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "a2bbb8f9631b1965cfd2e851f61355142913f535", "url": "https://www.semanticscholar.org/paper/a2bbb8f9631b1965cfd2e851f61355142913f535", "title": "Multi-knowledge extraction algorithm using Group Search Optimization for brain dataset analysis", "abstract": "Knowledge is formed by a kind of mapping from the condition space to the decision space in rough set. This paper presents multi-knowledge extraction approaches with fuzzy population algorithms. The Group Search Optimization (GSO) and Particle Swarm Optimization (PSO) are compared. GSO not only has the rapid convergence speed, but also has low time complexity, especially for high dimensional datasets. We use the multi-knowledge extraction algorithm based on GSO to analyze the data of brain cognition datasets. The experimental results illustrate our algorithm is very promising to seek for the relationship between the active brain regions and stimuli.", "venue": "2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "781f6f2d95eba6825ad3d0b69784b3881c1c0c2f", "url": "https://www.semanticscholar.org/paper/781f6f2d95eba6825ad3d0b69784b3881c1c0c2f", "title": "A Knowledge Extraction System from Manager's Operation Sequences in System Development Project", "abstract": "In order to support training project managers with a project simulator, we address extracting knowledge from Manager's operation sequences in system development projects. Because it is hard to collect many operation sequences from project managers, the proposed knowledge extraction system generates operation sequences automatically by agent programs. The generated operation sequences are classified into clusters that consist of similar operation sequences. The clusters represent characteristic operation sequences to improve evaluation of a project. As a preliminary experiment based on clustering for time-series categorical data, we confirm that the generated clusters represent characteristic operations as knowledge to determine operations.", "venue": "2016 5th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "9073c82625d040c6d31abb38c73d2ced899b5005", "url": "https://www.semanticscholar.org/paper/9073c82625d040c6d31abb38c73d2ced899b5005", "title": "Knowledge Extraction in Digit Recognition Using MNIST Dataset: Evolution in Handwriting Analysis", "abstract": "In handwriting recognition, traditional systems have relied heavily on handcrafted features and a massive amount of prior data and knowledge. Deep learning techniques have been the focus of research in the field of handwriting digit recognition and have achieved breakthrough performance in the last few years for knowledge extraction and management. KM and knowledge pyramid helps the project with its relationship with big data and IoT. The layers were selected randomly by which the performance of all the cases was found different. Data layers of the knowledge pyramid are formed by the sensors and input devices, whereas knowledge layers are the result of knowledge extraction applied on data layers. The knowledge pyramid and KM helps in making the use of IoT and big data easily. In this manuscript, the knowledge management principles capture the handwritten gestures numerically and get it recognized correctly by the software. The application of AI and DNN has increased the acceptability significantly. The accuracy is better than other available software on the market.", "venue": "Int. J. Knowl. Manag.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "2f5b99ae60a17aef14cdd03f53d784ee4c7edb3a", "url": "https://www.semanticscholar.org/paper/2f5b99ae60a17aef14cdd03f53d784ee4c7edb3a", "title": "A Variable Precision Rough Set Model for Knowledge-assisted Management in Distance Education", "abstract": "To enable the teaching administrator to better obtain effective knowledge from a large amount of information to assist management and improve the efficiency and level of teaching management, a variable precision rough set model for knowledge assisted management of distance education was proposed. First, based on the theory of complete reduction and knowledge extraction, the proposed pedigree ambiguity tree was used as a strategy for obtaining complete reduction. An algorithm for obtaining a complete set of reductions was given. Then, by studying the process of knowledge extraction, a multi-knowledge extraction framework was put forward. The process of data conversion was completely realized. Finally, experimental verification was performed. The results showed that the proposed model overcame the effect of noise data in real data and improved the efficiency of the algorithm. Therefore, the model has high universality.", "venue": "Int. J. Emerg. Technol. Learn.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "3e4dd6bab08734aff28f6c19f0245e5d98198135", "url": "https://www.semanticscholar.org/paper/3e4dd6bab08734aff28f6c19f0245e5d98198135", "title": "Knowledge Transfer for Feature Generation in Document Classification", "abstract": "One important problem in machine learning is how to extract knowledge from prior experience, then transfer and apply this knowledge in new learning tasks. To address this problem, transfer learning leverages information from (supervised) learning on related tasks to facilitate the current learning task. Self-taught learning uses information extracted from (unsupervised) learning on related data. In this paper, we propose a new method for knowledge extraction, transfer and application in classification. We consider document classification where we mine correlation relationships among the words from a set of documents and compile a collection of correlation relationships as prior knowledge. This knowledge is then applied to generate new features for classifying documents in classes/types different from the ones from which we obtain the correlation relationships. Our experiment results show that the correlation-based knowledge transfer helps to reduce classification errors.", "venue": "2009 International Conference on Machine Learning and Applications", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "ke", "kg"], "mention_counts": {"ke": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "699313a1d735c32b06b8bf36367df6178953b4d3", "url": "https://www.semanticscholar.org/paper/699313a1d735c32b06b8bf36367df6178953b4d3", "title": "Objects and Goals Extraction from Semantic Networks : Applications of Fuzzy SetS Theory", "abstract": "Department of Mathematical and Data Processing, Preparatory Institute of Engineering Studies of Monastir Kairouan Road, 5019 Monastir, Tunisia E-mail : Nazih.omri@ipeim.rnu.tn , E-mail : Noureddine.chouigui@ipeim.rnu.tn Abstract. In this paper we present a short survey of fuzzy and Semantic approaches to Knowledge Extraction. The goal of such approaches is to define flexible Knowledge Extraction Systems able to deal with the inherent vagueness and uncertainty of the Extraction process. In this survey we address if and how some approaches met their goal. Keywords: Knowledge extraction, fuzzy Goal, Fuzzy Object, Semantic Network, Fuzzy sets.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "3f4b86a94ef1c8d58a07ca72396e2f203d5bf124", "url": "https://www.semanticscholar.org/paper/3f4b86a94ef1c8d58a07ca72396e2f203d5bf124", "title": "Natural Language Processing for Forecasting Innovative Development of the Energy Infrastructure", "abstract": "The article deals with the application of natural language processing methods to support research and forecasting the innovative development of energy infrastructure. The main methods of NLP, which are used to build an intelligent system to support scientific research, are considered. Methods of building infrastructure for processing Open Linked Data and Big Data are described. Semantic analysis and knowledge integration are based on ontology system. Applying suggested methods allow increasing quality of scientific research in this area and make it more effectively", "venue": "E3S Web of Conferences", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ld", "nlp", "onto", "nlp", "lod"], "mention_counts": {"ld": 1, "nlp": 3, "lod": 1, "onto": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"ld": 1, "lod": 1, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "fee3fd2b000ca3e8d3a09606fd15f0a3c1cf69bc", "url": "https://www.semanticscholar.org/paper/fee3fd2b000ca3e8d3a09606fd15f0a3c1cf69bc", "title": "K4ThaiHealth: A Prototype for Thai Routine Medical Research Knowledge Extraction Sharing", "abstract": "\u201cRoutine to Research\u201d (R2R) is well known for Thai research related to the development of routine works of medical and public health practitioners. R2R research contains useful practical knowledge beneficial to the health of Thai people. However, this knowledge cannot be shared easily because it is unstructured and not classified text, moreover, no tool for R2R Thai knowledge sharing yet. In this research, we attempt to use text mining techniques to get insights of R2R research data and the K4ThaiHealth is first implemented as a prototype for basic R2R knowledge sharing. A set of basic medical corpus are developed using Thai medical International Statistical Classification of Diseases and Related Health Problems (ICD10TM) and several resources. They are used for R2R Thai medical text classification and key terms relationship extraction. The results are classified into diseases, organs, symptoms, and others. K4ThaiHealth is then used as a knowledge sharing prototype to offer health and medical practice knowledge extracted from R2R data sharing to Thai people. R2R WordCloud and R2R WordNet are used to display the diseases knowledge extracted from R2R research data and their relationships to diseases, organs, symptoms and others are visualized.", "venue": "2018 Seventh ICT International Student Project Conference (ICT-ISPC)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "56ac2d00f0466fba8498d56658639ad91ac972e9", "url": "https://www.semanticscholar.org/paper/56ac2d00f0466fba8498d56658639ad91ac972e9", "title": "Knowledge Extraction from Auto-Encoders on Anomaly Detection Tasks Using Co-activation Graphs", "abstract": "Deep neural networks have exploded in popularity and different types of networks are used to solve a multitude of complex tasks. One such task is anomaly detection, that a type of deep neural network called auto-encoder has become extremely proficient at solving. The low level neural activity, produced by such a network, generates extremely rich representations of the data, which can be used to extract task specific knowledge. In this paper, we built upon previous work and used co-activation graph analysis to extract knowledge from auto-encoders, that were trained for the specific task of anomaly detection. First, we outlined a method for extracting co-activation graphs from auto-encoders. Then, we performed graph analysis to discover that task specific knowledge from the auto-encoder was being encoded into the co-activation graph, and that the extracted knowledge could be used to reveal the role of individual neurons in the network.", "venue": "K-CAP", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "e757589891759f70b50fb972135fac4c5c1d65a6", "url": "https://www.semanticscholar.org/paper/e757589891759f70b50fb972135fac4c5c1d65a6", "title": "Identifying likely student dropouts using fuzzy inferencing", "abstract": "Fuzzy logic provides a methodology for reasoning using imprecise rules and assertions. Whereas a statement can only be true or false in classical logic, statements in fuzzy logic may be true or false to varying degrees. This enables fuzzy logic to deal with data and rules that are expressed in an imprecise manner using inexact linguistic expressions. Fuzzy inference is the process of formulating the mapping from a given input to an output using fuzzy logic. The mapping then provides a basis from which decisions can be made, or patterns discerned. Fuzzy expert systems are proving to be a powerful tool in business intelligence and decision support. This project concerns the development of a fuzzy expert system for identifying likely student dropouts at Columbus State University (CSU).\n According to a report released by the National Center for Public Policy and Higher Education, a low rate of college completion is a key concern in American higher education. According to ACT (the college testing service), the national average freshmen retention rate is 65.7%. From 2005 to 2010, this rate at CSU was 71%. Colleges and universities across the country, including CSU, are investigating this problem of student Retention, Progression and Graduation (RPG) in order to address it more effectively. The main aim of this research project is to build a fuzzy inference based model using a hybrid knowledge extraction process to predict how likely each freshman student will be to drop their program of study at the end of their first semester. Columbus State University database has student Retention, Progression and Graduation (RPG) data dating back to 1998. This historical data is being utilized to develop and evaluate the proposed fuzzy rule-based inferencing system.\n Knowledge extraction for the system will be performed using a top down (symbolic) as well as a bottom-up (data-based) approach. In the top-down approach, rules for the fuzzy model will be derived using the traditional knowledge extraction process involving domain expert interviews. Several persons in charge of university departments that have low retention rates will be interviewed to identify parameters that are significant determinants of student success. Fuzzy-rules designed using this knowledge will be weighted appropriately to reflect their level of significance. In the second phase of fuzzy rule derivation, results of data mining performed on student data will be utilized. A feedforward artificial neural network already trained using the student data will be subjected to weight analysis to derive additional rules for the fuzzy rule base, as well as for adjusting the significance of all rules. This hybrid neuro-fuzzy approach is expected to yield better performance than either a conventional fuzzy inferencing system or an artificial neural network.", "venue": "ACMSE '13", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "cd4bc7aa872fca023d2b5b03117cebf0979abbac", "url": "https://www.semanticscholar.org/paper/cd4bc7aa872fca023d2b5b03117cebf0979abbac", "title": "Constructing Bayesian networks by harvesting knowledge from online resources", "abstract": "In this paper, the development of a human-like intelligent system, named AKEOS (Automatic Knowledge Extraction from Online Sources), is introduced. AKEOS can automatically harvest knowledge from online resources to build a Bayesian network inference engine. Starting from a single event, the AKEOS system performs unsupervised knowledge extraction to convert unstructured text into structured knowledge. By performing repeated knowledge extraction for multiple similar events, AKEOS produces structured databases. Thus, various machine learning algorithms can be directly applied to explore relations between attributes, to discover patterns hidden in the data, or to build inference engines such as Bayesian networks for predictive and diagnostic reasoning. AKEOS is an end-to-end system with lists of events as input, structured databases as intermediate products, and inference engines as end products.", "venue": "2016 19th International Conference on Information Fusion (FUSION)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "3e81b09417029e09e01bd3bca7e2c590986505af", "url": "https://www.semanticscholar.org/paper/3e81b09417029e09e01bd3bca7e2c590986505af", "title": "A novel method for extracting interpretable knowledge from a spiking neural classifier with time-varying synaptic weights", "abstract": "This paper presents a novel method for information interpretability in an MC-SEFRON classifier. To develop a method to extract knowledge stored in a trained classifier, first, the binary-class SEFRON classifier developed earlier is extended to handle multi-class problems. MC-SEFRON uses the population encoding scheme to encode the real-valued input data into spike patterns. MC-SEFRON is trained using the same supervised learning rule used in the SEFRON. After training, the proposed method extracts the knowledge for a given class stored in the classifier by mapping the weighted postsynaptic potential in the time domain to the feature domain as Feature Strength Functions (FSFs). A set of FSFs corresponding to each output class represents the extracted knowledge from the classifier. This knowledge encoding method is derived to maintain consistency between the classification in the time domain and the feature domain. The correctness of the FSF is quantitatively measured by using FSF directly for classification tasks. For a given input, each FSF is sampled at the input value to obtain the corresponding feature strength value (FSV). Then the aggregated FSVs obtained for each class are used to determine the output class labels during classification. FSVs are also used to interpret the predictions during the classification task. Using ten UCI datasets and the MNIST dataset, the knowledge extraction method, interpretation and the reliability of the FSF are demonstrated. Based on the studies, it can be seen that on an average, the difference in the classification accuracies using the FSF directly and those obtained by MC-SEFRON is only around 0.9% & 0.1\\% for the UCI datasets and the MNIST dataset respectively. This clearly shows that the knowledge represented by the FSFs has acceptable reliability and the interpretability of classification using the classifier's knowledge has been justified.", "venue": "ArXiv", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "097a988d32639aa2d79f4858b692075b00440ca3", "url": "https://www.semanticscholar.org/paper/097a988d32639aa2d79f4858b692075b00440ca3", "title": "Social Computing and User Generated Content potential Pros and Cons: A Review", "abstract": "Social Computing has gained multidisplinary key research areas from academicians to professionals and to researchers. In couple of past decades several implementations, framework and research theories had been proposed and justified in such regards. Moreover Knowledge Extraction (KE) is a trending and emerging domain that addresses various proven techniques for extracting knowledge out of heavy and resilient social data. In this paper we have presented our literature review for novel approach of data intensive social computing for the purpose of knowledge extraction in social ties. Social Computing is a trending fuzzy term that act as a super set of Social Network Analysis, crowd management, crowd sourcing and many more. And as bigger population in involved in social computing therefore it becomes very crucial to cater overall monitoring functionality on such huge user generated data.", "venue": "International Journal of Computer Applications", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4d15d83ae7e61fded91a9455b4a268273718fd7d", "url": "https://www.semanticscholar.org/paper/4d15d83ae7e61fded91a9455b4a268273718fd7d", "title": "TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories", "abstract": "Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce. State-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-Commerce scenarios, which often contain thousands of diverse categories. This paper proposes TXtract, a taxonomy-aware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy. Through category conditional self-attention and multi-task learning, our approach is both scalable, as it trains a single model for thousands of categories, and effective, as it extracts category-specific attribute values. Experiments on products from a taxonomy with 4,000 categories show that TXtract outperforms state-of-the-art approaches by up to 10% in F1 and 15% in coverage across all categories.", "venue": "Annual Meeting of the Association for Computational Linguistics", "citationCount": 35, "fieldsOfStudy": ["Computer Science", "Mathematics"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "9ce4599f0e7529c0324c0b25e72b8e7ac1f21178", "url": "https://www.semanticscholar.org/paper/9ce4599f0e7529c0324c0b25e72b8e7ac1f21178", "title": "Interpretable Multi-Step Reasoning with Knowledge Extraction on Complex Healthcare Question Answering", "abstract": "Healthcare question answering assistance aims to provide customer healthcare information, which widely appears in both Web and mobile Internet. The questions usually require the assistance to have proficient healthcare background knowledge as well as the reasoning ability on the knowledge. Recently a challenge involving complex healthcare reasoning, HeadQA dataset, has been proposed, which contains multiple-choice questions authorized for the public healthcare specialization exam. Unlike most other QA tasks that focus on linguistic understanding, HeadQA requires deeper reasoning involving not only knowledge extraction, but also complex reasoning with healthcare knowledge. These questions are the most challenging for current QA systems, and the current performance of the state-of-the-art method is slightly better than a random guess. In order to solve this challenging task, we present a Multi-step reasoning with Knowledge extraction framework (MurKe). The proposed framework first extracts the healthcare knowledge as supporting documents from the large corpus. In order to find the reasoning chain and choose the correct answer, MurKe iterates between selecting the supporting documents, reformulating the query representation using the supporting documents and getting entailment score for each choice using the entailment model. The reformulation module leverages selected documents for missing evidence, which maintains interpretability. Moreover, we are striving to make full use of off-the-shelf pre-trained models. With less trainable weight, the pre-trained model can easily adapt to healthcare tasks with limited training samples. From the experimental results and ablation study, our system is able to outperform several strong baselines on the HeadQA dataset.", "venue": "ArXiv", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "287603a70267bbe0d11d4cdf9d3241fabefcb6f2", "url": "https://www.semanticscholar.org/paper/287603a70267bbe0d11d4cdf9d3241fabefcb6f2", "title": "Layerwise Knowledge Extraction from Deep Convolutional Networks", "abstract": "Knowledge extraction is used to convert neural networks into symbolic descriptions with the objective of producing more comprehensible learning models. The central challenge is to find an explanation which is more comprehensible than the original model while still representing that model faithfully. The distributed nature of deep networks has led many to believe that the hidden features of a neural network cannot be explained by logical descriptions simple enough to be comprehensible. In this paper, we propose a novel layerwise knowledge extraction method using M-of-N rules which seeks to obtain the best trade-off between the complexity and accuracy of rules describing the hidden features of a deep network. We show empirically that this approach produces rules close to an optimal complexity-error tradeoff. We apply this method to a variety of deep networks and find that in the internal layers we often cannot find rules with a satisfactory complexity and accuracy, suggesting that rule extraction as a general purpose method for explaining the internal logic of a neural network may be impossible. However, we also find that the softmax layer in Convolutional Neural Networks and Autoencoders using either tanh or relu activation functions is highly explainable by rule extraction, with compact rules consisting of as little as 3 units out of 128 often reaching over 99% accuracy. This shows that rule extraction can be a useful component for explaining parts (or modules) of a deep neural network.", "venue": "ArXiv", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "e9ecc83042f6d439a32815a19c1bc11cc82b6a8b", "url": "https://www.semanticscholar.org/paper/e9ecc83042f6d439a32815a19c1bc11cc82b6a8b", "title": "Introduction to MAchine Learning & Knowledge Extraction (MAKE)", "abstract": "The grand goal of Machine Learning is to develop software which can learn from previous experience\u2014similar to how we humans do. Ultimately, to reach a level of usable intelligence, we need (1) to learn from prior data, (2) to extract knowledge, (3) to generalize\u2014i.e., guessing where probability function mass/density concentrates, (4) to fight the curse of dimensionality, and (5) to disentangle underlying explanatory factors of the data\u2014i.e., to make sense of the data in the context of an application domain. To address these challenges and to ensure successful machine learning applications in various domains an integrated machine learning approach is important. This requires a concerted international effort without boundaries, supporting collaborative, cross-domain, interdisciplinary and transdisciplinary work of experts from seven sections, ranging from data pre-processing to data visualization, i.e., to map results found in arbitrarily high dimensional spaces into the lower dimensions to make it accessible, usable and useful to the end user. An integrated machine learning approach needs also to consider issues of privacy, data protection, safety, security, user acceptance and social implications. This paper is the inaugural introduction to the new journal of MAchine Learning & Knowledge Extraction (MAKE). The goal is to provide an incomplete, personally biased, but consistent introduction into the concepts of MAKE and a brief overview of some selected topics to stimulate future research in the international research community.", "venue": "Machine Learning and Knowledge Extraction", "citationCount": 50, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "0fdc8c3be4f0059c201beee2a6f61c8c06dfe9ee", "url": "https://www.semanticscholar.org/paper/0fdc8c3be4f0059c201beee2a6f61c8c06dfe9ee", "title": "Knowledge Extraction and Integration for Information Gathering in Penetration Testing", "abstract": "Assets identification is an important aspect of penetration test on which security practitioner develop their defense mechanism. In addition, assets identification is an essential piece of information for penetration testers to find a weakness in the targeted organization. Information gathering is the process of extracting knowledge to recognize the organizations' assets available on the internet. There are many open source tools available for information gathering. However, penetration tester needs to put manual effort (during several hours to multiple days) to extract useful knowledge from the output of one tool and integrate that knowledge in another tool. Penetration tester can increase speed and accuracy of the overall information gathering process by automating the knowledge extraction and integration. This paper review and identify open source subdomain enumeration and service scanning tools and present an approach to integrate and automate identified tools. The result reveals that there is a significant improvement of the information gathering process by using our approach due to the reduction of manual tasks.", "venue": "IEEE International Conference on Software Quality, Reliability and Security Companion", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "1162c416c4f0ac5d20cb2d636b1424435ca4bced", "url": "https://www.semanticscholar.org/paper/1162c416c4f0ac5d20cb2d636b1424435ca4bced", "title": "A rule-based named-entity recognition method for knowledge extraction of evidence-based dietary recommendations", "abstract": "Evidence-based dietary information represented as unstructured text is a crucial information that needs to be accessed in order to help dietitians follow the new knowledge arrives daily with newly published scientific reports. Different named-entity recognition (NER) methods have been introduced previously to extract useful information from the biomedical literature. They are focused on, for example extracting gene mentions, proteins mentions, relationships between genes and proteins, chemical concepts and relationships between drugs and diseases. In this paper, we present a novel NER method, called drNER, for knowledge extraction of evidence-based dietary information. To the best of our knowledge this is the first attempt at extracting dietary concepts. DrNER is a rule-based NER that consists of two phases. The first one involves the detection and determination of the entities mention, and the second one involves the selection and extraction of the entities. We evaluate the method by using text corpora from heterogeneous sources, including text from several scientifically validated web sites and text from scientific publications. Evaluation of the method showed that drNER gives good results and can be used for knowledge extraction of evidence-based dietary recommendations.", "venue": "PLoS ONE", "citationCount": 81, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "840a4bd0234c5ced74e71583121995cb4ac5d18e", "url": "https://www.semanticscholar.org/paper/840a4bd0234c5ced74e71583121995cb4ac5d18e", "title": "Design of a conceptual knowledge extraction framework for a social learning environment based on Social Network Analysis methods", "abstract": "The advent of social media in education has the potential to foster collaborative learning. Exploring students' interactions on the social media tools is an important research direction, which could bring an insight into the collaborative learning process. Therefore, our aim is to propose a conceptual framework for knowledge extraction and visualization from a social media-based learning environment. In particular, we focus on our in-house platform, called eMUSE, which has been successfully used in a project-based learning scenario. The paper addresses the construction of a social graph starting from students' interactions on the social media tools; the objective is to identify appropriate social network analysis techniques that can answer specific educational needs and integrate them in a conceptual knowledge extraction framework. The basis, rationale and analysis levels of the framework are discussed in the paper.", "venue": "International Conference on Innovative Computing and Cloud Computing", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "127744222fe31086c2c998a7fae2f16a3fb330a4", "url": "https://www.semanticscholar.org/paper/127744222fe31086c2c998a7fae2f16a3fb330a4", "title": "Lexicon Knowledge Extraction with Sentiment Polarity Computation", "abstract": "Sentiment analysis is one of the most popular natural language processing techniques. It aims to identify the sentiment polarity (positive, negative, neutral or mixed) within a given text. The proper lexicon knowledge is very important for the lexicon-based sentiment analysis methods since they hinge on using the polarity of the lexical item to determine a text's sentiment polarity. However, it is quite common that some lexical items appear positive in the text of one domain but appear negative in another. In this paper, we propose an innovative knowledge building algorithm to extract sentiment lexicon knowledge through computing their polarity value based on their polarity distribution in text dataset, such as in a set of domain specific reviews. The proposed algorithm was tested by a set of domain microblogs. The results demonstrate the effectiveness of the proposed method. The proposed lexicon knowledge extraction method can enhance the performance of knowledge based sentiment analysis.", "venue": "2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "nlp", "ke"], "mention_counts": {"nlp": 1, "ke": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "240f59d5f72e7c08a1de28a29f84a3b3be54e5bc", "url": "https://www.semanticscholar.org/paper/240f59d5f72e7c08a1de28a29f84a3b3be54e5bc", "title": "Relational Knowledge Extraction from Neural Networks", "abstract": "The effective integration of learning and reasoning is a well-known and challenging area of research within artificial intelligence. Neural-symbolic systems seek to integrate learning and reasoning by combining neural networks and symbolic knowledge representation. In this paper, a novel methodology is proposed for the extraction of relational knowledge from neural networks which are trainable by the efficient application of the backpropagation learning algorithm. First-order logic rules are extracted from the neural networks, offering interpretable symbolic relational models on which logical reasoning can be performed. The wellknown knowledge extraction algorithm TREPAN was adapted and incorporated into the first-order version of the neural-symbolic system CILP++. Empirical results obtained in comparison with a probabilistic model for relational learning, Markov Logic Networks, and a state-of-the-art Inductive Logic Programming system, Aleph, indicate that the proposed methodology achieves competitive accuracy results consistently in all datasets investigated, while either Markov Logic Networks or Aleph show considerably worse results in at least one dataset. It is expected that effective knowledge extraction from neural networks can contribute to the integration of heterogeneous knowledge representations.", "venue": "CoCo@NIPS", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "1d4ad0da9581c8d9d5a7d56765e53782995cbaae", "url": "https://www.semanticscholar.org/paper/1d4ad0da9581c8d9d5a7d56765e53782995cbaae", "title": "Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia", "abstract": "Wikipedia infoboxes are a valuable source of structured knowledge for global knowledge sharing. However, infobox information is very incomplete and imbalanced among the Wikipedias in different languages. It is a promising but challenging problem to utilize the rich structured knowledge from a source language Wikipedia to help complete the missing infoboxes for a target language. In this paper, we formulate the problem of cross-lingual knowledge extraction from multilingual Wikipedia sources, and present a novel framework, called WikiCiKE, to solve this problem. An instancebased transfer learning method is utilized to overcome the problems of topic drift and translation errors. Our experimental results demonstrate that WikiCiKE outperforms the monolingual knowledge extraction method and the translation-based method.", "venue": "ACL", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "66040e6c4261a9bdc9847c429440995105cdd2da", "url": "https://www.semanticscholar.org/paper/66040e6c4261a9bdc9847c429440995105cdd2da", "title": "Framework for web application domain knowledge extraction", "abstract": "A decade ago a web application e-Student was built with aim to provide electronic support for student enrolment and examination/alumni records management at the University of Ljubljana. Due to issues emerging from the Bologna reform a new e-Student is to be build using a modern technology in the near future. The old e-Student encapsulates a huge amount of domain knowledge. Unfortunately, it was developed using agile approach resulting in poor technical documentation, thus an alternative approach for the domain knowledge extraction has to be defined. In the paper a framework for an effective web application domain knowledge extraction is defined. It has five elements. The main principles (1) of extraction are defined to perform effective reengineering of different application views at a defined abstract level. A proper knowledge representation using diverse models (2) has to be determined next, and the Model Driven Architecture using UML models is considered a suitable choice. The procedure (3) for extraction has to be defined using appropriate (usually custom made) tools (4) and performed by skilled staff (5), possibly members of the old development team. The use of framework is demonstrated on the web application e-Student outlining several custom made tools, the results and the most valuable lessons learnt.", "venue": "International Convention on Information and Communication Technology, Electronics and Microelectronics", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "91fe19171bdb6925785d8b01a7d06d6bebb01f9b", "url": "https://www.semanticscholar.org/paper/91fe19171bdb6925785d8b01a7d06d6bebb01f9b", "title": "Parameterized Contrast in Second Order Soft Co-occurrences: A Novel Text Representation Technique in Text Mining and Knowledge Extraction", "abstract": "In this article, we present a novel statistical representation method for knowledge extraction from a corpus containing short texts. Then we introduce the contrast parameter which could be adjusted for targeting different conceptual levels in text mining and knowledge extraction. The method is based on second order co-occurrence vectors whose efficiency for representing meaning has been established in many applications, especially for representing word senses in different contexts and for disambiguation purposes. We evaluate our method on two tasks: classification of textual description of dreams, and classification of medical abstracts for systematic reviews.", "venue": "2009 IEEE International Conference on Data Mining Workshops", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "c3deb9745320563d5060e568aeb294469f6582f6", "url": "https://www.semanticscholar.org/paper/c3deb9745320563d5060e568aeb294469f6582f6", "title": "Knowledge Extraction from Structured Engineering Drawings", "abstract": "As a typical type of structured documents, table drawings are widely used in engineering fields. Knowledge extraction of such structured documents plays an important role in automatic interpretation systems. In this paper, we propose a new knowledge extraction method based on automatically analyzing drawing layout and extracting physical or logical structures from the given engineering table drawings. Then based on the automatic interpretation results, we further propose normalization method to integrate varied types of engineering tables with other engineering drawings and extract implied domain knowledge.", "venue": "2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "60269cd30224d2b75b202822052bb2bbff635f8a", "url": "https://www.semanticscholar.org/paper/60269cd30224d2b75b202822052bb2bbff635f8a", "title": "Knowledge extraction from human motion", "abstract": "Observing people is currently one of the most active application areas in computer vision. This strong interest is driven by a wide spectrum of promising applications in many areas such as virtual reality, smart surveillance, perceptual interface, etc [13]. This paper presents the concept of knowledge extraction from single human motion via a fixed camera in an enclosed environment in order to mine some movement attributes. We propose a framework based on five mining tools. The five measurements are extracting pixel coverage of a particular object, time domain, frequency distribution of pixels of interest, distances crossed in each frame and considering the object velocity. We assume that, taking into account the measurements mentioned above will introduce a robust knowledge extraction approach.", "venue": "2008 International Conference on Computer and Communication Engineering", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "ddbf8a207b3dde78881a1d20a27a5ea49d20e453", "url": "https://www.semanticscholar.org/paper/ddbf8a207b3dde78881a1d20a27a5ea49d20e453", "title": "Application of Genetic Algorithm and Rough Set Theory for Knowledge Extraction", "abstract": "This paper proposes a hybrid approach using the rough set theory and genetic algorithm (RS-GA) for knowledge extraction as one part of a substation level decision support system. The technique involved a process which learns and extracts knowledge from a set of events into a form of rules to identify the most probable faulted section in a network. Numerous case studies performed on a simulated distribution network [1] that consists of several relays models [2] using PSCAD/EMTDC have revealed the usefulness of the proposed technique for fault diagnosis. The test results demonstrated that the extracted rules are capable of identifying and isolating the faulted section and hence improve the outage response time.", "venue": "2007 IEEE Lausanne Power Tech", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4bd85ca5d9608b55624f9f5f3a9b191848c4d89f", "url": "https://www.semanticscholar.org/paper/4bd85ca5d9608b55624f9f5f3a9b191848c4d89f", "title": "Deep Logic Networks: Inserting and Extracting Knowledge From Deep Belief Networks", "abstract": "Developments in deep learning have seen the use of layerwise unsupervised learning combined with supervised learning for fine-tuning. With this layerwise approach, a deep network can be seen as a more modular system that lends itself well to learning representations. In this paper, we investigate whether such modularity can be useful to the insertion of background knowledge into deep networks, whether it can improve learning performance when it is available, and to the extraction of knowledge from trained deep networks, and whether it can offer a better understanding of the representations learned by such networks. To this end, we use a simple symbolic language\u2014a set of logical rules that we call confidence rules\u2014and show that it is suitable for the representation of quantitative reasoning in deep networks. We show by knowledge extraction that confidence rules can offer a low-cost representation for layerwise networks (or restricted Boltzmann machines). We also show that layerwise extraction can produce an improvement in the accuracy of deep belief networks. Furthermore, the proposed symbolic characterization of deep networks provides a novel method for the insertion of prior knowledge and training of deep networks. With the use of this method, a deep neural\u2013symbolic system is proposed and evaluated, with the experimental results indicating that modularity through the use of confidence rules and knowledge insertion can be beneficial to network performance.", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "citationCount": 85, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "6d355e6fc39865a1bc00b109719d72a981aeb494", "url": "https://www.semanticscholar.org/paper/6d355e6fc39865a1bc00b109719d72a981aeb494", "title": "Knowledge extraction from hierarchical fuzzy model obtained by fuzzy neural networks and genetic algorithm", "abstract": "Knowledge extraction from trained artificial neural networks has been studied by many researchers. Modeling of nonlinear systems using fuzzy neural networks (FNNs) is a promising approach to the knowledge acquisition, and a FNN is specially designed for knowledge extraction. The authors have proposed a hierarchical fuzzy modeling method using FNNs and a genetic algorithm (GA). This method can identify fuzzy models of nonlinear objects with strong nonlinearity. The disadvantage of the method is that the training of the FNN is time consuming. This paper presents a quick method for a rough search for proper structures in the antecedent of fuzzy models. The fine tuning of the acquired rough model is done by the FNN. This modeling method is quite efficient to identify precise fuzzy models of systems with strong nonlinearities. A simulation is done to show the effectiveness of the proposed method.", "venue": "International Conference on Neural Networks", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "7b5949433e482dec0acb9b6aa24a92b26417e916", "url": "https://www.semanticscholar.org/paper/7b5949433e482dec0acb9b6aa24a92b26417e916", "title": "Pertinent Knowledge Extraction from a Semantic Network: Application of Fuzzy Sets Theory", "abstract": "In this paper we describe an approach to define flexible Knowledge Extraction Systems able to deal with the inherent vagueness and uncertainty of the Extraction process. We also present a short survey of fuzzy and semantic approaches to knowledge extraction. The goal of such approaches is to address if and how some approaches met their goal.", "venue": "Int. J. Artif. Intell. Tools", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "c83352aeefffa5db36f5bad7944e8e0fd13eb01c", "url": "https://www.semanticscholar.org/paper/c83352aeefffa5db36f5bad7944e8e0fd13eb01c", "title": "HILDA: knowledge extraction from neural networks in legal rule based and case based reasoning", "abstract": "A major requirement for legal expert systems involved in generating legal advice and purporting to adjudicate on disputes is that they explain their reasoning. Even systems involved in predicting the outcomes of legal disputes are enhanced by this facility. Difficulties in extracting knowledge from neural networks (\"NNs\") have made their application to legal expert systems somewhat limited. HILDA incorporates some aspects of rule based reasoning (\"RBR\") and case based reasoning (\"CBR\") to assist the user in predicting case outcomes and generating arguments and case decisions. The system can use the NN to guide RBR and CBR in a number of ways. Knowledge extracted from a NN could also be used to iteratively refine the system's domain theory. This refined domain theory is one way in which HILDA can carry out RBR and CBR.", "venue": "Proceedings of ICNN'95 - International Conference on Neural Networks", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "c88ccba0cd963569e535b44e1c29071c93f92421", "url": "https://www.semanticscholar.org/paper/c88ccba0cd963569e535b44e1c29071c93f92421", "title": "Sub-symbolic knowledge extraction environment for teaching process assistance", "abstract": "We aim with this research is to build an integrated environment to serve as an assistant in the educational process. When we deal with unstructured knowledge, getting of useful information for the teaching process is very difficult. Neural networks can store subsymbolic knowledge, but until recently it was believed to be only in a \"black-box\" format. Knowledge extraction from NNs is a relatively new field, which tries to reduce these disadvantages and build a bridge between subsymbolic and symbolic knowledge. As the teaching process requires only symbolic knowledge, we believe this to be a chance for teachers to significantly improve their teaching materials and/or style by combining the symbolic knowledge of the domain theory with the rules extracted from the empirical subsymbolic knowledge stored in NNs trained on examples. Therefore, we developed a neural network's subsymbolic knowledge extraction environment for the teaching process assistance and also built a study case of teaching stock exchange developments.", "venue": "1998 Second International Conference. Knowledge-Based Intelligent Electronic Systems. Proceedings KES'98 (Cat. No.98EX111)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "f78ec72f044d23c8c55b7ce935ec34ac2edb8ac5", "url": "https://www.semanticscholar.org/paper/f78ec72f044d23c8c55b7ce935ec34ac2edb8ac5", "title": "Automatic knowledge extraction from OCR documents using hierarchical document analysis", "abstract": "Industries can improve their business efficiency by analyzing and extracting relevant knowledge from large numbers of documents. Knowledge extraction manually from large volume of documents is labor intensive, unscalable and challenging. Consequently there have been a number of attempts to develop intelligent systems to automatically extract relevant knowledge from OCR documents. Moreover, the automatic system can improve the capability of search engine by providing application-specific domain knowledge. However, extracting the efficient information from OCR documents is challenging due to highly unstructured format [1, 11, 18, 26]. In this paper, we propose an efficient framework for a knowledge extraction system that takes keywords based queries and automatically extracts their most relevant knowledge from OCR documents by using text mining techniques. The framework can provide relevance ranking of knowledge to a given query. We tested the proposed framework on corpus of documents at GE Power where document consists of more than hundred pages in PDF.", "venue": "Research in Adaptive and Convergent Systems", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "aa15ef60c2fb0164f06b650187b8e9b5592602d1", "url": "https://www.semanticscholar.org/paper/aa15ef60c2fb0164f06b650187b8e9b5592602d1", "title": "Methodology for Knowledge Extraction from Trained Artificial Neural Networks", "abstract": "Artificial neural networks are widely spread models that outperform more basic, but explainable machine learning models like classification decision tree. Although their lack of explainability severely limits their area of application. All mission critical areas or law regulated areas (like European GDPR) require model to be explained. Explainability allows model validation for correctness and lack of bias. Thus methods for knowledge extraction from artificial neural networks have gained attention and development efforts. Current paper addresses this problem and describes knowledge extraction methodology which can be applied to classification problems. It is based on previous research and allows knowledge to be extracted from trained fully connected feed-forward artificial neural network, from radial basis function neural network and from hyper-polytope based classifier in the form of binary classification decision tree, elliptical rules and If-Then rules.", "venue": "Information Technology and Management Science", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "a5f964d87101b52cdaf52e612b5f01353c4874de", "url": "https://www.semanticscholar.org/paper/a5f964d87101b52cdaf52e612b5f01353c4874de", "title": "Hyperspectral Imagery Semantic Interpretation Based on Adaptive Constrained Band Selection and Knowledge Extraction Techniques", "abstract": "In this paper, we propose a novel adaptive band selection approach for hyperspectral image semantic interpretation. This approach is based on constrained band selection (CBS) method and extracted knowledge coming from tensor locality preserving projection. The extracted knowledge is presented as a set of rules which are used to evaluate the importance of spectral bands for classes discrimination. Based on these extracted rules and the CBS approach, relevant bands are selected to enhance the hyperspectral image semantic interpretation. The main advantage of the proposed adaptive band selection approach is to allow the automatic selection of discriminant, distinctive and informative spectral bands, and improve the semantic interpretation of hyperspectral images. Experimental results on real images show that the proposed band selection approach reaches competitive good performances, in terms of classification accuracy.", "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "583159b66af823fc91c1113c68896d3c7645a4a1", "url": "https://www.semanticscholar.org/paper/583159b66af823fc91c1113c68896d3c7645a4a1", "title": "AKECP: Adaptive Knowledge Extraction from Feature Maps for Fast and Efficient Channel Pruning", "abstract": "Pruning can remove redundant parameters and structures of Deep Neural Networks (DNNs) to reduce inference time and memory overhead. As an important component of neural networks, the feature map (FM) has stated to be adopted for network pruning. However, the majority of FM-based pruning methods do not fully investigate effective knowledge in the FM for pruning. In addition, it is challenging to design a robust pruning criterion with a small number of images and achieve parallel pruning due to the variability of FMs. In this paper, we propose Adaptive Knowledge Extraction for Channel Pruning (AKECP), which can compress the network fast and efficiently. In AKECP, we first investigate the characteristics of FMs and extract effective knowledge with an adaptive scheme. Secondly, we formulate the effective knowledge of FMs to measure the importance of corresponding network channels. Thirdly, thanks to the effective knowledge extraction, AKECP can efficiently and simultaneously prune all the layers with extremely few or even one image. Experimental results show that our method can compress various networks on different datasets without introducing additional constraints, and it has advanced the state-of-the-arts. Notably, for ResNet-110 on CIFAR-10, AKECP achieves 59.9% of parameters and 59.8% of FLOPs reduction with negligible accuracy loss. For ResNet-50 on ImageNet, AKECP saves 40.5% of memory footprint and reduces 44.1% of FLOPs with only 0.32% of Top-1 accuracy drop.", "venue": "ACM Multimedia", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4e49a590cb119f0707d643fb1157dfdb33f6ef0d", "url": "https://www.semanticscholar.org/paper/4e49a590cb119f0707d643fb1157dfdb33f6ef0d", "title": "Conceptualisation and Annotation of Drug Nonadherence Information for Knowledge Extraction from Patient-Generated Texts", "abstract": "Approaches to knowledge extraction (KE) in the health domain often start by annotating text to indicate the knowledge to be extracted, and then use the annotated text to train systems to perform the KE. This may work for annotat- ing named entities or other contiguous noun phrases (drugs, some drug effects), but be- comes increasingly difficult when items tend to be expressed across multiple, possibly non- contiguous, syntactic constituents (e.g. most descriptions of drug effects in user-generated text). Other issues include that it is not al- ways clear how annotations map to actionable insights, or how they scale up to, or can form part of, more complex KE tasks. This paper reports our efforts in developing an approach to extracting knowledge about drug nonadher- ence from health forums which led us to con- clude that development cannot proceed in sep- arate steps but that all aspects\u2014from concep- tualisation to annotation scheme development, annotation, KE system training and knowl- edge graph instantiation\u2014are interdependent and need to be co-developed. Our aim in this paper is two-fold: we describe a generally ap- plicable framework for developing a KE ap- proach, and present a specific KE approach, developed with the framework, for the task of gathering information about antidepressant drug nonadherence. We report the conceptual- isation, the annotation scheme, the annotated corpus, and an analysis of annotated texts.", "venue": "Conference on Empirical Methods in Natural Language Processing", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "48597f5aaebf9cb280df0e2fe6d7723074e0a734", "url": "https://www.semanticscholar.org/paper/48597f5aaebf9cb280df0e2fe6d7723074e0a734", "title": "PharmKE: Knowledge Extraction Platform for Pharmaceutical Texts using Transfer Learning", "abstract": "Even though named entity recognition (NER) has seen tremendous development in recent years, some domain-specific use-cases still require tagging of unique entities, which is not well handled by pre-trained models. Solutions based on enhancing pre-trained models or creating new ones are efficient, but creating reliable labeled training for them to learn on is still challenging. In this paper, we introduce PharmKE, a text analysis platform tailored to the pharmaceutical industry that uses deep learning at several stages to perform an in-depth semantic analysis of relevant publications. The proposed methodology is used to produce reliably labeled datasets leveraging cutting-edge transfer learning, which are later used to train models for specific entity labeling tasks. By building models for the well-known text-processing libraries spaCy and AllenNLP, this technique is used to find Pharmaceutical Organizations and Drugs in texts from the pharmaceutical domain. The PharmKE platform also incorporates the NER findings to resolve co-references of entities and examine the semantic linkages in each phrase, creating a foundation for further text analysis tasks, such as fact extraction and question answering. Additionally, the knowledge graph created by DBpedia Spotlight for a specific pharmaceutical text is expanded using the identified entities. The obtained results with the proposed methodology result in about a 96% F1-score on the NER tasks, which is up to 2% better than those of the fine-tuned BERT and BioBERT models developed using the same dataset. The ultimate benefits of the platform are that pharmaceutical domain specialists may more easily identify the knowledge extracted from the input texts thanks to the platform\u2019s visualization of the model findings. Likewise, the proposed techniques can be integrated into mobile and pervasive systems to give patients more relevant and comprehensive information from scanned medication guides. Similarly, it can provide preliminary insights to patients and even medical personnel on whether a drug from a different vendor is compatible with the patient\u2019s prescription medication.", "venue": "Computers", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "tp"], "mention_counts": {"ke": 2, "tp": 1, "kg": 1}, "nlp_mention_counts": {"ke": 2, "tp": 1}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "65a2d594a595196a4faf066b821112eed05ef1bd", "url": "https://www.semanticscholar.org/paper/65a2d594a595196a4faf066b821112eed05ef1bd", "title": "A reasoning system about knowledge extraction in human-computer interaction", "abstract": "The development of Intelligent Decision Support System is a key application for artificial intelligence technology. And human-computer interaction is the most common action in Intelligent Decision Support System. In the process of human-computer interaction, the input of decision problem and the output of the solution will be converted into the knowledge that can be extracted by machine. Human-computer interaction is a dynamic process, which involves knowledge extraction and sharing between machines and people. In this paper, we establish a Knowledge Extraction System (KES) based on the analysis of the usual knowledge types that are involved in the interaction process. And then we prove its soundness and completeness, explore some properties about reasoning in human-computer interaction. In this system, we can find the interpretation as to how the machine understands human's questions and reasons intelligently. It also reflects how the cognitive agent extracts each other's private knowledge into common knowledge that later becomes its own knowledge during the interactive process.", "venue": "Chinese Control and Decision Conference", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "90afd8fa7ee5583be04812e95b719f12d80b4f84", "url": "https://www.semanticscholar.org/paper/90afd8fa7ee5583be04812e95b719f12d80b4f84", "title": "Medical knowledge extraction scheme for cloudlet-based healthcare system to avoid malicious attacks", "abstract": "The medical information sharing involves information collection, information storage, and sharing of this information. Protection to medical information against malicious attacks is an important concern due to its storage on remote cloud. In current online question answering system for health-related issues, extraction of medical knowledge from the clamorous question- answers pair is a challenge. To overcome these challenges, medical knowledge extraction scheme for cloudlet based healthcare system is proposed. The proposed and developed medical knowledge extraction (MKE) scheme finds valid remedial Triples from clamorous question-answer (Q-A) pairs and evaluate the reliability along with doctor's proficiency using truth discovery method the modified number theory research unit algorithm (NTRU) and collaborative intrusion detection system (CIDS) is used to avoid and detect malicious attacks. The response time of the proposed system is 6 to 8 seconds which results in a substantial reduction in time and cost for the end-user.", "venue": "International Journal of Cloud Computing", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "c54c81182aa7fb3dc38459c26c94287ba013ed4b", "url": "https://www.semanticscholar.org/paper/c54c81182aa7fb3dc38459c26c94287ba013ed4b", "title": "Knowledge extraction for assisted curation of summaries of bacterial transcription factor properties", "abstract": "Abstract Transcription factors (TFs) play a main role in transcriptional regulation of bacteria, as they regulate transcription of the genetic information encoded in DNA. Thus, the curation of the properties of these regulatory proteins is essential for a better understanding of transcriptional regulation. However, traditional manual curation of article collections to compile descriptions of TF properties takes significant time and effort due to the overwhelming amount of biomedical literature, which increases every day. The development of automatic approaches for knowledge extraction to assist curation is therefore critical. Here, we show an effective approach for knowledge extraction to assist curation of summaries describing bacterial TF properties based on an automatic text summarization strategy. We were able to recover automatically a median 77% of the knowledge contained in manual summaries describing properties of 177 TFs of Escherichia coli K-12 by processing 5961 scientific articles. For 71% of the TFs, our approach extracted new knowledge that can be used to expand manual descriptions. Furthermore, as we trained our predictive model with manual summaries of E. coli, we also generated summaries for 185 TFs of Salmonella enterica serovar Typhimurium from 3498 articles. According to the manual curation of 10 of these Salmonella typhimurium summaries, 96% of their sentences contained relevant knowledge. Our results demonstrate the feasibility to assist manual curation to expand manual summaries with new knowledge automatically extracted and to create new summaries of bacteria for which these curation efforts do not exist. Database URL: The automatic summaries of the TFs of E. coli and Salmonella and the automatic summarizer are available in GitHub (https://github.com/laigen-unam/tf-properties-summarizer.git).", "venue": "Database J. Biol. Databases Curation", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "c404755bedf4209f42e258610d32059e5e0b35d7", "url": "https://www.semanticscholar.org/paper/c404755bedf4209f42e258610d32059e5e0b35d7", "title": "Knowledge Extraction Using Image Features", "abstract": "A feature-based picture knowledge extraction methodology is developed in this research in order to acquire a more completed and systematic search result via the target picture. In the proposed methodology, as the target picture from the knowledge acquirer is obtained, a set of prototype pictures highly correlated with the target picture can be determined based on the image features of the target picture. Then the knowledge regarding the extracted prototype pictures are integrated to represent the knowledge of the target picture. The verification results demonstrate that the developed system can be applied to real cases to effectively assist picture knowledge extraction of distinct application domains.", "venue": "Int. J. Electron. Bus. Manag.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "6229c16188aa99d83f0afe7732a381daecb74b51", "url": "https://www.semanticscholar.org/paper/6229c16188aa99d83f0afe7732a381daecb74b51", "title": "Relevance Feedback for Goal's Extraction from Fuzzy Semantic Networks", "abstract": "In this paper we present a short survey of fuzzy a nd Semantic approaches to Knowledge Extraction. The goal of such approaches is to define flexible Knowl edge Extraction Systems able to deal with the inher ent vagueness and uncertainty of the Extraction process . It has long been recognised that interactivity im proves the effectiveness of Knowledge Extraction systems. Novice users queries is the most natural and interactiv e medium of communication and recent progress in recognitio n is making it possible to build systems that inter act with the user. However, given the typical novice users queri es submitted to Knowledge Extraction systems, it is easy to imagine that the effects of goal recognition errors in novice user\u2019s queries must be severely destruct ive on the system\u2019s effectiveness. The experimental work repor ted in this paper shows that the use of classical K nowledge Extraction techniques for novice user\u2019s query proce ssing is robust to considerably high levels of goal recognition errors. Moreover, both standard relevance feedback and pseudo relevance feedback can be effectively employed to improve the effectiveness of novice user\u2019s query processing.", "venue": "ArXiv", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "0230c06c771bb6411614d685432ef076927e66f4", "url": "https://www.semanticscholar.org/paper/0230c06c771bb6411614d685432ef076927e66f4", "title": "Extracting Biomedical Factual Knowledge Using Pretrained Language Model and Electronic Health Record Context", "abstract": "Language Models (LMs) have performed well on biomedical natural language processing applications. In this study, we conducted some experiments to use prompt methods to extract knowledge from LMs as new knowledge Bases (LMs as KBs). However, prompting can only be used as a low bound for knowledge extraction, and perform particularly poorly on biomedical domain KBs. In order to make LMs as KBs more in line with the actual application scenarios of the biomedical domain, we specifically add EHR notes as context to the prompt to improve the low bound in the biomedical domain. We design and validate a series of experiments for our Dynamic-Context-BioLAMA task. Our experiments show that the knowledge possessed by those language models can distinguish the correct knowledge from the noise knowledge in the EHR notes, and such distinguishing ability can also be used as a new metric to evaluate the amount of knowledge possessed by the model.", "venue": "ArXiv", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "nlp", "kg"], "mention_counts": {"nlp": 1, "ke": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "7a4e33214fa35b444ac3594343bdfe69bb0c7536", "url": "https://www.semanticscholar.org/paper/7a4e33214fa35b444ac3594343bdfe69bb0c7536", "title": "Dual Track Multimodal Automatic Learning through Human-Robot Interaction", "abstract": "Human beings are constantly improving their cognitive ability via automatic learning from the interaction with the environment. Two important aspects of automatic learning are the visual perception and knowledge acquisition. The fusion of these two aspects is vital for improving the intelligence and interaction performance of robots. Many automatic knowledge extraction and recognition methods have been widely studied. However, little work focuses on integrating automatic knowledge extraction and recognition into a unified framework to enable jointly visual perception and knowledge acquisition. To solve this problem, we propose a Dual Track Multimodal Automatic Learning (DTMAL) system, which consists of two components: Hybrid Incremental Learning (HIL) from the vision track and Multimodal Knowledge Extraction (MKE) from the knowledge track. HIL can incrementally improve recognition ability of the system by learning new object samples and new object concepts. MKE is capable of constructing and updating the multimodal knowledge items based on the recognized new objects from HIL and other knowledge by exploring the multimodal signals. The fusion of the two tracks is a mutual promotion process and jointly devote to the dual track learning. We have conducted the experiments through human-machine interaction and the experimental results validated the effectiveness of our proposed system.", "venue": "IJCAI", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "245a23daee7ebb4ad2c83da7526522b96c862ce0", "url": "https://www.semanticscholar.org/paper/245a23daee7ebb4ad2c83da7526522b96c862ce0", "title": "A medical diagnosis support system based on automatic knowledge extraction from databases through differential evolution", "abstract": "An intelligent system for supporting medical diagnosis is presented in this paper. The system automatically extracts knowledge from databases as sets of IF-THEN rules. The approach chosen to fulfil this task is based on the differential evolution DE algorithm and its implementation results in a tool called DEREx. This tool is aimed at supporting clinicians in their decision making in the diagnostic process, by providing them with clear explanations on the reasons why each item is assigned to a given class. Performance of the tool has been evaluated over seven medical databases and compared against that of fifteen well-known classification tools. Numerical results in terms of classification accuracy and their statistical analysis, have evidenced the effectiveness of the proposed approach, so DEREx is preferable because of its added value, i.e. the knowledge extracted automatically and provided to users in an easily comprehensible form.", "venue": "Int. J. Data Min. Bioinform.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "aae5d379820447cb6c45a490c224dda6c41263fe", "url": "https://www.semanticscholar.org/paper/aae5d379820447cb6c45a490c224dda6c41263fe", "title": "CHCI: A Crowdsourcing Human-computer Interaction Framework for Cultural Heritage Knowledge", "abstract": "This paper aims to extract knowledge including entities and relationships, from multi-source heterogeneous cultural heritage (CH) resources. The proposed crowdsourcing human-computer interaction framework utilizes museum-user-algorithm cooperation to achieve high-quality and scalable CH knowledge extraction. This paper also proposes crowdsourcing optimization mechanisms to improve participation and quality of crowdsourcing project. Finally, this paper discusses how extracted knowledge can support CH digital resource construction and knowledge-driven intelligent applications in Museum.", "venue": "ACM/IEEE Joint Conference on Digital Libraries", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "4123075b5d2232b154be457c3493321182d8fa2e", "url": "https://www.semanticscholar.org/paper/4123075b5d2232b154be457c3493321182d8fa2e", "title": "Controlling Complexity and Accuracy of Classification Decision Tree Extracted from Trained Artificial Neural Network", "abstract": "There is growing number of publications devoted to knowledge extraction from fully connected feed-forward artificial neural networks. Although there are not many publications covering ways allowing to control extracted knowledge complexity and precision. The higher complexity is, the higher accuracy can be gained. But in case ANN should be validated by domain expert or just be explainable it should be simple enough - this will lower accuracy of extracted knowledge. The current paper explores influence of parameters used for ANN pruning and neurons outputs discretization and clustering onto accuracy of extracted classification decision tree. Hence reader is presented with experimental validation of effects produced by variation in parameters combination.", "venue": "2019 60th International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "711966fb0cbc93285fa907726239ad0d94ad6d62", "url": "https://www.semanticscholar.org/paper/711966fb0cbc93285fa907726239ad0d94ad6d62", "title": "Extracting heuristically acceptable information from fuzzy/neural architectures via heuristic constraint enforcement. I. Foundation", "abstract": "Knowledge extraction from systems where the existing knowledge is limited is a difficult task. Using fuzzy/neural architectures to extract heuristic information from systems has received increasing attention. In most cases, using output error measures to validate extracted knowledge is not sufficient; extracted knowledge may not make heuristic sense even if the output error may meet the specified criterion. Using the principles of set theoretic estimation, the paper proposes a method for enforcing heuristic constraints on the membership functions of fuzzy/neural architectures. The proposed method ensures that the final membership functions conform to a priori heuristic knowledge. Although the method is described on a specific fuzzy/neural architecture, it is applicable to other realizations of fuzzy inference systems including adaptive or static implementations. The organized yet flexible characteristic of the heuristic constraint enforcement method enables its application to a wide range of problems.", "venue": "1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36227)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "a45cd816a5151e29ea22dfaf31f6482ea9387b86", "url": "https://www.semanticscholar.org/paper/a45cd816a5151e29ea22dfaf31f6482ea9387b86", "title": "A Sampling-Based Framework for Transductive Classification in Information Networks", "abstract": "Knowledge extraction from large information networks has received increasing attention in recent years. Among existing methods for knowledge extraction, transductive classification is a well-known semi-supervised learning method, where both labeled and unlabeled vertices are used in the learning process. However, transductive classification tasks become impractical in large information networks and the use of network sampling techniques in the transductive classification setting is not a trivial task, since it is required that all the vertices of the original network be classified during the transductive learning \u2013 and not only the vertices of the sample. In this paper, we present a framework called TCSN (Transductive Classification for Sampled Networks). TCSN allows the use of various network sampling techniques, as well as enables the use of various methods of transductive classification for information networks. We present a variation of the Chernoff Bounds method to calculate the minimum size of a sampled network, thereby bounding sampling error within a pre-specified tolerance level. Moreover, TCSN extends the concept of evidence accumulation to combine the results of several rounds of transductive classification into a final classification. Experimental results from different information networks reveals that TCSN statistically outperformed the classification performance in the whole original network. These promising results show that the TCSN enables transductive classification in large information networks without loss of quality in the knowledge extraction process.", "venue": "2019 8th Brazilian Conference on Intelligent Systems (BRACIS)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke"], "mention_counts": {"ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "1772dde69bce9910a62442aa418e04d45a1dada9", "url": "https://www.semanticscholar.org/paper/1772dde69bce9910a62442aa418e04d45a1dada9", "title": "1+1>2: Programming Know-What and Know-How Knowledge Fusion, Semantic Enrichment and Coherent Application", "abstract": "\u2014Software programming requires both API reference (know-what) knowledge and programming task (know-how) knowledge. Lots of programming know-what and know-how knowledge is documented in text, for example, API reference documentation and programming tutorials. To improve knowledge accessibility and usage, several recent studies use Natural Language Processing (NLP) methods to construct API know-what knowledge graph (API-KG) and programming task know-how knowledge graph (Task-KG) from software documentation. Although being promising, current API-KG and Task-KG are independent of each other, and thus are void of inherent connections between the two types of knowledge. Our empirical study on Stack Over\ufb02ow questions con\ufb01rms that only 36% of the API usage problems can be answered by the know-how or the know-what knowledge alone, while the rest questions requires a fusion of both. Inspired by this observation, we make the \ufb01rst attempt to fuse API-KG and Task-KG by API entity linking. This fusion creates nine categories of API semantic relations and two types of task semantic relations which are not present in the stand-alone API-KG or Task-KG. According to the de\ufb01nitions of these new API and task semantic relations, our approach dives deeper than surface-level API linking of API-KG and Task-KG, and infer nine categories of API semantic relations from task descriptions and two types of task semantic relations with the assistance of API-KG, which enrich the declaration or syntactic relations in the current API-KG and Task-KG. Our fused and semantically-enriched API-Task KG supports coherent API/Task-centric knowledge search by text or code queries. We have implemented our approach on Java programming documentation and built a web tool to search and explore API and programming task knowledge. Our evaluation con\ufb01rms the high-accuracy of our knowledge extraction, fusion and enrichment methods, and the effectiveness and usefulness of our API-Task KG for answering Stack Over\ufb02ow questions.", "venue": "IEEE Transactions on Services Computing", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "kg", "nlp", "nlp"], "mention_counts": {"nlp": 2, "ke": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"ke": 1, "kg": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "2b4a9bab3474f7695c10d1aa276ccaa7b5f6cebb", "url": "https://www.semanticscholar.org/paper/2b4a9bab3474f7695c10d1aa276ccaa7b5f6cebb", "title": "Mobile based data retrieval using RDF and NLP in an efficient approach", "abstract": "Nowadays Youngsters like IT people and various sectors are showing massive interest and moving towards agriculture farming for self sustainable energy for our generation. People are in metro environment with family owned lands they are in need of proper guidance from farmers having huge experience But there is a gap in medium in approaching the farmers by staying with them for months together to get Knowledge Transfer for facing issues and clarifications. The farmers and ancestors traditional methods and subject knowledge need to be transferred to upcoming generations. In Digital world, farmers are using smart phones so to document the knowledge from farmers the Subject Matter Expertise of agriculture (SME) under database and to provide solution by queries in efficient approach through RDF (Resource Description Framework) and Natural Language Processing Technique (NLP) in the form of mobile application internally using SPARQL queries. In this paper, WordNet text mining processing method is used.", "venue": "2017 Third International Conference on Science Technology Engineering & Management (ICONSTEM)", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "rdf", "nlp", "rdf", "rdf"], "mention_counts": {"nlp": 3, "rdf": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"rdf": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "9d2c55d6132f3c7f438f9427a64a88ee042616af", "url": "https://www.semanticscholar.org/paper/9d2c55d6132f3c7f438f9427a64a88ee042616af", "title": "G\u00e9n\u00e9ration de phrase : entr\u00e9e, algorithmes et applications (Sentence Generation: Input, Algorithms and Applications)", "abstract": "Joint work with Paul Bedaride, Eric Kow, Shashi Narayan and Laura Perez-Beltrachini) Sentence Generation maps abstract linguistic representations into sentences. A necessary part of any natural language generation system, sentence generation has also recently received increasing attention in applications such as transfer based machine translation (cf. the LOGON project) and natural language interfaces to knowledge bases (e.g., to verbalise, to author and/or to query ontologies). One outstanding issue in Sentence Generation is what it starts from. What is the abstract linguistic representation it generates from? In my talk, I will explore sentence generation from two main input formats (flat semantic formulae and dependency structures) and discuss their impact on efficiency, algorithms and applications. I will start by describing an algorithm that generates from flat semantic formulae, explain why it is computationally intractable and presenting ways of optimising it to make it usable in practice. I will then show how this algorithm can be used to generate paraphrases; to support error mining and to generate teaching material for language learners from an ontology. In the second part of the talk, I will focus on generation from dependency structures. Based on the input data recently made available by the Generation Challenges Surface Realisation Shared Task, I will show how the algorithm previously used to generate from flat semantic formulae can be adapted to generate from dependency structures. I will moreover discuss various issues raised by the GenChal data such as, missing lexical entries and mismatches between dependency and grammar structures. Bio of Claire Gardent Claire Gardent is a senior researcher at the French National Center for Scientific Research (CNRS). She graduated in linguistics at the University of Toulouse in 1986, received an MSc in Artificial Intelligence from the University of Essex in 1987 and defended a PhD in Cognitive Science at the University of Edinburgh in 1991. From 1991 to 2000, she worked as a reseacher at the Universities of Utrecht and Amsterdam (The Netherlands), Clermont-Ferrand and Sarrebruecken (Germany). Since 2001 she has been working for the CNRS at the Lorraine Laboratory for Research in Computer Science (LORIA) in Nancy, France. Claire Gardent's research focuses on the computational treatment of natural language meaning. She has worked on the automatic acquisition of lexical resources for French, on syntactic parsing and semantic role labelling and on text generation. Recently, she has become interested in in exploring the interaction between virtual worlds and natural language processing. Claire Gardent has published a textbook on analysis and generation (with Karine Baschung) and about 100 articles in journals and conference proceedings. She has been nominated Chair of the European Chapter for the Association of Computational Linguistics (EACL), editor in chief of the french journal \"Traitement Automatique des Langues\" and member of the editorial board of the journals \"Computational Linguistics\", \"Journal of Semantics\". Each year she is on the programme committee of half a dozen international conferences or workshops, she also acted as scientific chair for various international conferences (EACL), workshops (TAG+, ENLG, DIALOR, SIGDIAL) and summer schools (ESSLLI).", "venue": "JEPTALNRECITAL", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlg", "kg", "nlp", "mt", "onto"], "mention_counts": {"onto": 2, "nlp": 1, "nlg": 1, "mt": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "nlg": 1, "mt": 1}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "a4571e19a53a9ee571e66a710a2bcc69ede5dff1", "url": "https://www.semanticscholar.org/paper/a4571e19a53a9ee571e66a710a2bcc69ede5dff1", "title": "A multilingual ontology matcher", "abstract": "State-of-the-art multilingual ontology matchers use machine translation to reduce the problem to the monolingual case. We investigate an alternative, self-contained solution based on semantic matching where labels are parsed by multilingual natural language processing and then matched using a language-independent knowledge base acting as an interlingua. As the method relies on the availability of domain vocabularies in the languages supported, matching and vocabulary enrichment become joint, mutually reinforcing tasks. In particular, we propose a vocabulary enrichment method that uses the matcher\u2019s output to detect and generate missing items semi-automatically. Vocabularies developed in this manner can then be reused for other domain-specific natural language understanding tasks.", "venue": "OM", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "nlp", "nlu", "onto", "kg"], "mention_counts": {"onto": 2, "nlu": 1, "nlp": 1, "mt": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "nlu": 1, "mt": 1}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "6c086a542ccac6fa52089513676924ab9256b733", "url": "https://www.semanticscholar.org/paper/6c086a542ccac6fa52089513676924ab9256b733", "title": "Representation of linguistic and domain knowledge for second language learning in virtual worlds", "abstract": "There has been much debate, both theoretical and practical, on how to link ontologies and lexicons in natural language processing (NLP) applications. In this paper, we focus on an application in which lexicon and ontology are used to generate teaching material. We briefly describe the application (a serious game for language learning). We then zoom in on the representation and interlinking of the lexicon and of the ontology. We show how the use of existing standards and of good practice principles facilitates the design of our resources while satisfying the expressivity requirements set by natural language generation.", "venue": "LREC", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 2, "nlg": 1, "onto": 3}, "nlp_mention_counts": {"nlp": 2, "nlg": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "9c0864fb89db95b43824aab93d666136b4ef4730", "url": "https://www.semanticscholar.org/paper/9c0864fb89db95b43824aab93d666136b4ef4730", "title": "Reflections on KOS Based Data Alignment", "abstract": "This paper briefly reviews two contrasting case studies by the authors on semantic data integration within the archaeology field and reflects on some of the key issues encountered, relevant to the NKOS Workshop theme on strategies for alignment of metadata to KOS linked data. Both projects involved diverse datasets with different schema and which employed different terminology. Both projects combined datasets with information extracted from archaeological reports via natural language processing (NLP). In both cases, the semantic framework that afforded data integration was a combination of metadata element sets organised by an ontology with a relevant value vocabulary (eg thesauri and term lists). Ontologies and value vocabularies have been seen as complementary resources for this purpose [1, 2].", "venue": "NKOS@TPDL", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "ie", "onto", "ld"], "mention_counts": {"ld": 1, "nlp": 2, "onto": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"ld": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "1edf4d90c9ba9a9b29b429ab5838998bda6be07f", "url": "https://www.semanticscholar.org/paper/1edf4d90c9ba9a9b29b429ab5838998bda6be07f", "title": "Open-source Tools for Creation, Maintenance, and Storage of Lexical Resources for Language Generation from Ontologies", "abstract": "This paper describes reusable, open-source tools for creation, maintenance, storage, and access of Language Resources (LR) needed for generating natural language texts from ontologies. One advantage of these tools is that they provide a user-friendly interface for NLG LR manipulation. They also provide unified models for accessing NLG lexicons and mappings between lexicons and ontologies.", "venue": "LREC", "citationCount": 35, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "onto", "nlg", "onto", "onto", "nlg"], "mention_counts": {"nlg": 3, "onto": 3}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "f91c9a379b35455b6042793e9037d58e1ecdd7d9", "url": "https://www.semanticscholar.org/paper/f91c9a379b35455b6042793e9037d58e1ecdd7d9", "title": "A general ontology based multi-lingual multi-function multimedia intelligent system", "abstract": "A large variety of information processing applications deal with natural language texts. Many of these applications require extracting and processing the meanings of natural language texts, in addition to processing their syntactic forms. In order to extract meanings from texts and manipulate them, a natural language processing system must have a significant amount of knowledge about the world and the domain of discourse. However, different kinds of knowledge are required for different languages, for different functions, and for different media. We design a new general ontology based natural language processing system referred to as the multi-lingual multi-function multimedia intelligent system (MMM-IS). MMM-IS is a complex system with multiple functions that can deal with multiple languages and multiple media. The general ontology consists of a static layer and a dynamic layer, and provides a real world knowledge model for the natural language processing system. The paper presents the concept of MMM-IS and how to build MMM-IS for real applications.", "venue": "Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "5e579ba17a674b8dbfdf7670bbcd2784512095b9", "url": "https://www.semanticscholar.org/paper/5e579ba17a674b8dbfdf7670bbcd2784512095b9", "title": "Ontology-Based Approach to Semantically Enhanced Question Answering for Closed Domain: A Review", "abstract": "For many users of natural language processing (NLP), it can be challenging to obtain concise, accurate and precise answers to a question. Systems such as question answering (QA) enable users to ask questions and receive feedback in the form of quick answers to questions posed in natural language, rather than in the form of lists of documents delivered by search engines. This task is challenging and involves complex semantic annotation and knowledge representation. This study reviews the literature detailing ontology-based methods that semantically enhance QA for a closed domain, by presenting a literature review of the relevant studies published between 2000 and 2020. The review reports that 83 of the 124 papers considered acknowledge the QA approach, and recommend its development and evaluation using different methods. These methods are evaluated according to accuracy, precision, and recall. An ontological approach to semantically enhancing QA is found to be adopted in a limited way, as many of the studies reviewed concentrated instead on NLP and information retrieval (IR) processing. While the majority of the studies reviewed focus on open domains, this study investigates the closed domain.", "venue": "Inf.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 3, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "582089a00a6c9fb534f16d1dbbafc50cc4e3912a", "url": "https://www.semanticscholar.org/paper/582089a00a6c9fb534f16d1dbbafc50cc4e3912a", "title": "Schema Aware Semantic Reasoning for Interpreting Natural Language Queries in Enterprise Settings", "abstract": "Natural Language Query interfaces allow the end-users to access the desired information without the need to know any specialized query language, data storage, or schema details. Even with the recent advances in NLP research space, the state-of-the-art QA systems fall short of understanding implicit intents of real-world Business Intelligence (BI) queries in enterprise systems, since Natural Language Understanding still remains an AI-hard problem. We posit that deploying ontology reasoning over domain semantics can help in achieving better natural language understanding for QA systems. In this paper, we specifically focus on building a Schema Aware Semantic Reasoning Framework that translates natural language interpretation as a sequence of solvable tasks by an ontology reasoner. We apply our framework on top of an ontology based, state-of-the-art natural language question-answering system ATHENA, and experiment with 4 benchmarks focused on BI queries. Our experimental numbers empirically show that the Schema Aware Semantic Reasoning indeed helps in achieving significantly better results for handling BI queries with an average accuracy improvement of ~30%", "venue": "COLING", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlu", "nlp", "nlu"], "mention_counts": {"nlp": 1, "onto": 3, "nlu": 2}, "nlp_mention_counts": {"nlp": 1, "nlu": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "9ffc808f0ad8ef25d59f9b86ba1dc87d86505afb", "url": "https://www.semanticscholar.org/paper/9ffc808f0ad8ef25d59f9b86ba1dc87d86505afb", "title": "Gold-standard ontology-based anatomical annotation in the CRAFT Corpus", "abstract": "Abstract Gold-standard annotated corpora have become important resources for the training and testing of natural-language-processing (NLP) systems designed to support biocuration efforts, and ontologies are increasingly used to facilitate curational consistency and semantic integration across disparate resources. Bringing together the respective power of these, the Colorado Richly Annotated Full-Text (CRAFT) Corpus, a collection of full-length, open-access biomedical journal articles with extensive manually created syntactic, formatting and semantic markup, was previously created and released. This initial public release has already been used in multiple projects to drive development of systems focused on a variety of biocuration, search, visualization, and semantic and syntactic NLP tasks. Building on its demonstrated utility, we have expanded the CRAFT Corpus with a large set of manually created semantic annotations relying on Uberon, an ontology representing anatomical entities and life-cycle stages of multicellular organisms across species as well as types of multicellular organisms defined in terms of life-cycle stage and sexual characteristics. This newly created set of annotations, which has been added for v2.1 of the corpus, is by far the largest publicly available collection of gold-standard anatomical markup and is the first large-scale effort at manual markup of biomedical text relying on the entirety of an anatomical terminology, as opposed to annotation with a small number of high-level anatomical categories, as performed in previous corpora. In addition to presenting and discussing this newly available resource, we apply it to provide a performance baseline for the automatic annotation of anatomical concepts in biomedical text using a prominent concept recognition system. The full corpus, released with a CC BY 3.0 license, may be downloaded from http://bionlp-corpora.sourceforge.net/CRAFT/index.shtml. Database URL: http://bionlp-corpora.sourceforge.net/CRAFT/index.shtml", "venue": "Database J. Biol. Databases Curation", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "onto", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 3, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "5bd466cb7b6ce832e695fb0c5438f90eb75b4e48", "url": "https://www.semanticscholar.org/paper/5bd466cb7b6ce832e695fb0c5438f90eb75b4e48", "title": "Learning by Reading: A Prototype System, Performance Baseline and Lessons Learned", "abstract": "A traditional goal of Artificial Intelligence research has been a system that can read unrestricted natural language texts on a given topic, build a model of that topic and reason over the model. Natural Language Processing advances in syntax and semantics have made it possible to extract a limited form of meaning from sentences. Knowledge Representation research has shown that it is possible to model and reason over topics in interesting areas of human knowledge. It is useful for these two communities to reunite periodically to see where we stand with respect to the common goal of text understanding. \n \nIn this paper, we describe a coordinated effort among researchers from the Natural Language and Knowledge Representation and Reasoning communities. We routed the output of existing NL software into existing KR software to extract knowledge from texts for integration with engineered knowledge bases. We tested the system on a suite of roughly 80 small English texts about the form and function of the human heart, as well as a handful of \"confuser\" texts from other domains. We then manually evaluated the knowledge extracted from novel texts. \n \nOur conclusion is that the technology from these fields is mature enough to start producing unified machine reading systems. The results of our exercise provide a performance baseline for systems attempting to acquire models from text.", "venue": "AAAI", "citationCount": 57, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "nlp"], "mention_counts": {"nlp": 1, "ke": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "6d1f1829167420402edce708b3dc1208039b9107", "url": "https://www.semanticscholar.org/paper/6d1f1829167420402edce708b3dc1208039b9107", "title": "Managing Mathematical Texts with OWL and Their Graphical Representation", "abstract": "Mathematical knowledge contained in scientific digital publications poses a challenge for intelligent retrieval mechanisms. Many current approaches use statistical (e.g. Google) or natural language processing methods to find correlations in texts and annotate texts semantically. However both kinds of approaches face the problem of extracting and processing knowledge from mathematical equations. The presented system is based on natural language processing techniques, and benefits from characteristic linguistic structures defined by the language used in mathematical texts. It accumulates extracted information snippets from texts, symbols, and equations in knowledge bases. These knowledge bases provide the foundation for the information retrieval. This article describes the concepts and the prototypical technical implementation.", "venue": "Proceedings of the 41st Annual Hawaii International Conference on System Sciences (HICSS 2008)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "onto", "ie", "nlp", "kg"], "mention_counts": {"nlp": 2, "kg": 2, "onto": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.7310585786300049}, {"paperId": "5ddeb7cbf6d879a31091eed56b80c2dc47838474", "url": "https://www.semanticscholar.org/paper/5ddeb7cbf6d879a31091eed56b80c2dc47838474", "title": "An ontology clarification tool for word sense disambiguation", "abstract": "This paper presents a method for word sense disambiguation based on Lesk algorithm which uses lexical database WordNet as knowledge base. The word sense disambiguation is the process of automatically clarifying a meaning of a word in its context. In general, ontology means the meaning or analogous term. It can be interpreted by relating the word with other words in the sentence. This tool accepts English statement as input and gives best possible meaning of given word. Method is experimented with senseval-2 test data for lexical sample task. The results show the betterment over the original Lesk algorithm.", "venue": "2011 3rd International Conference on Electronics Computer Technology", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "wsd", "wsd", "onto", "wsd"], "mention_counts": {"kg": 1, "wsd": 3, "onto": 2}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.7310585786300049}, {"paperId": "9fba7523c25feb27ea8e65ed07160fc0bc150fe5", "url": "https://www.semanticscholar.org/paper/9fba7523c25feb27ea8e65ed07160fc0bc150fe5", "title": "Ontology Based Query Expansion Using Word Sense Disambiguation", "abstract": "Abstract - The existing information retrieval techniques do not consider the context of the keywords present in the user\u2019s queries. Therefore, the search engines sometimes do not provide sufficient information to the users. New methods based on the semantics of user keywords must be developed to search in the vast web space without incurring loss of information. The semantic based information retrieval techniques need to understand the meaning of the concepts in the user queries. This will improve the precision-recall of the search results. Therefore, this approach focuses on the concept based semantic information retrieval. This work is based on Word sense disambiguation, thesaurus WordNet and ontology of any domain for retrieving information in order to capture the context of particular concept(s) and discover semantic relationships between them. reaction. Index terms \u2013 Word Sense Disambiguation, Semantic Information Retrieval, Clustering, Ontology. I. INTRODUCTION", "venue": "ArXiv", "citationCount": 19, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "wsd", "wsd", "onto", "onto", "wsd"], "mention_counts": {"wsd": 3, "onto": 3}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7310585786300049}, {"paperId": "8976101ac01ff3d97f818f0bbc6a4af5d85e49d5", "url": "https://www.semanticscholar.org/paper/8976101ac01ff3d97f818f0bbc6a4af5d85e49d5", "title": "Ontological Approach for Knowledge Extraction from Clinical Documents", "abstract": "In clinical NLP (Natural Language Processing), Knowledge extraction is a very important task to develop a highly accurate information retrieval system. The various approaches used to develop such systems include rule-based approach, statistical approach, shortest path algorithm or hybrid of these approaches. Accuracy and coverage are the most important parameters while comparing different approaches. Some methodologies have good accuracy but low coverage and vice-versa. In this paper, our focus is to extract domain relationships, for example to extract the relationship between \u2018Disease\u2019 and \u2018Procedure\u2019 or \u2018Symptom\u2019 and \u2018Disease\u2019 etc. from the clinical documents using three different approaches. These three approaches are i) Statistical ii) Shortest Path iii) Shortest Path Using Body System. All three approaches use our in-house existing NLP system to extract entities from the un-structured documents. The Statistical approach applies a probabilistic algorithm on clinical documents, whereas the Shortest Path algorithm uses the Ontological knowledge base for the hierarchical relationship between entities. This Ontological knowledge base is built upon the curated Unified Medical Language System (UMLS). For the Shortest Path Using Body System approach, we have used the domain relationship as well as hierarchical relationship. The output of these approaches is further validated by a domain expert and this validated relationship is used to enrich our ontological knowledge base. We have presented the details of these approaches one-by-one along with the comparative results of these approaches. We finally go through the analysis of the result and conclude on further work.", "venue": "2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "onto", "nlp", "ke", "ke", "onto", "onto", "kg", "nlp", "kg", "onto"], "mention_counts": {"nlp": 3, "onto": 4, "kg": 3, "ke": 2}, "nlp_mention_counts": {"nlp": 3, "ke": 2}, "ld_mention_counts": {"kg": 3, "onto": 4, "ke": 2}, "relevance_score": 0.7095051064826536}, {"paperId": "2523efbccb6aa4a3fc9aac074a76db7395a78dc0", "url": "https://www.semanticscholar.org/paper/2523efbccb6aa4a3fc9aac074a76db7395a78dc0", "title": "The LODeXporter: Flexible Generation of Linked Open Data Triples from NLP Frameworks for Automatic Knowledge Base Construction", "abstract": "We present LODeXporter, a novel approach for exporting Natural Language Processing (NLP) results to a graph-based knowledge base, following Linked Open Data (LOD) principles. The rules for transforming NLP entities into Resource Description Framework (RDF) triples are described in a custom mapping language, which is defined in RDF Schema (RDFS) itself, providing a separation of concerns between NLP pipeline engineering and knowledge base engineering. LODeXporter is available as an open source component for the GATE (General Architecture for Text Engineering) framework.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "lod", "kg", "kg", "rdf", "nlp", "lod", "nlp", "nlp", "kg", "rdf", "rdf", "lod"], "mention_counts": {"nlp": 5, "lod": 3, "kg": 3, "rdf": 3}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"kg": 3, "lod": 3, "rdf": 3}, "relevance_score": 0.7095051064826536}, {"paperId": "e02076dcf9f5218d42d46980b84b6c74a84dfae5", "url": "https://www.semanticscholar.org/paper/e02076dcf9f5218d42d46980b84b6c74a84dfae5", "title": "BERT-based knowledge extraction method of unstructured domain text", "abstract": "With the development and business adoption of knowledge graph, there is an increasing demand for extracting entities and relations of knowledge graphs from unstructured domain documents. This makes the automatic knowledge extraction for domain text quite meaningful. This paper proposes a knowledge extraction method based on BERT, which is used to extract knowledge points from unstructured specific domain texts (such as insurance clauses in the insurance industry) automatically to save manpower of knowledge graph construction. Different from the commonly used methods which are based on rules, templates or entity extraction models, this paper converts the domain knowledge points into question and answer pairs and uses the text around the answer in documents as the context. The method adopts a BERT-based model similar to BERT\u2019s SQuAD reading comprehension task. The model is fine-tuned. And it is used to directly extract knowledge points from more insurance clauses. According to the test results, the model performance is good.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "kg", "ke", "ke", "ke", "ke", "ke", "kg"], "mention_counts": {"kg": 4, "ke": 5}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"kg": 4, "ke": 5}, "relevance_score": 0.7095051064826536}, {"paperId": "8cf32a719d23a97a0b796108381e59579f92f67f", "url": "https://www.semanticscholar.org/paper/8cf32a719d23a97a0b796108381e59579f92f67f", "title": "HISTORIAE, History of Socio-Cultural Transformation as Linguistic Data Science. A Humanities Use Case", "abstract": "The paper proposes an interdisciplinary approach including methods from disciplines such as history of concepts, linguistics, natural language processing (NLP) and Semantic Web, to create a comparative framework for detecting semantic change in multilingual historical corpora and generating diachronic ontologies as linguistic linked open data (LLOD). Initiated as a use case (UC4.2.1) within the COST Action Nexus Linguarum, European network for Web-centred linguistic data science, the study will explore emerging trends in knowledge extraction, analysis and representation from linguistic data science, and apply the devised methodology to datasets in the humanities to trace the evolution of concepts from the domain of socio-cultural transformation. The paper will describe the main elements of the methodological framework and preliminary planning of the intended workflow. 2012 ACM Subject Classification Computing methodologies \u2192 Semantic networks; Computing methodologies \u2192 Ontology engineering; Computing methodologies \u2192 Temporal reasoning; Computing methodologies \u2192 Lexical semantics; Computing methodologies \u2192 Language resources; Computing methodologies \u2192 Information extraction", "venue": "LDK", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["llod", "lod", "ke", "sw", "llod", "onto", "nlp", "nlp", "onto", "ie"], "mention_counts": {"onto": 2, "sw": 1, "llod": 2, "lod": 1, "ke": 1, "nlp": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1, "ie": 1}, "ld_mention_counts": {"onto": 2, "sw": 1, "llod": 2, "lod": 1, "ke": 1}, "relevance_score": 0.7059547412717408}, {"paperId": "abfbc294890b9b1ed038ce91a43f46c883d8e1ba", "url": "https://www.semanticscholar.org/paper/abfbc294890b9b1ed038ce91a43f46c883d8e1ba", "title": "Learning from Mistakes: Combining Ontologies via Self-Training for Dialogue Generation", "abstract": "Natural language generators (NLGs) for task-oriented dialogue typically take a meaning representation (MR) as input, and are trained end-to-end with a corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue acts and domain attributes. Creation of such datasets is labor intensive and time consuming. Therefore, dialogue systems for new domain ontologies would benefit from using data for pre-existing ontologies. Here we explore, for the first time, whether it is possible to train an NLG for a new larger ontology using existing training sets for the restaurant domain, where each set is based on a different ontology. We create a new, larger combined ontology, and then train an NLG to produce utterances covering it. For example, if one dataset has attributes for family friendly and rating information, and the other has attributes for decor and service, our aim is an NLG for the combined ontology that can produce utterances that realize values for family friendly, rating, decor and service. Initial experiments with a baseline neural sequence-to-sequence model show that this task is surprisingly challenging. We then develop a novel self-training method that identifies (errorful) model outputs, automatically constructs a corrected MR input to form a new (MR, utterance) training pair, and then repeatedly adds these new instances back into the training data. We then test the resulting model on a new test set. The result is a self-trained model whose performance is an absolute 75.4% improvement over the baseline model. We also report a human qualitative evaluation of the final model showing that it achieves high naturalness, semantic coherence and grammaticality.", "venue": "SIGDIAL", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlg", "onto", "nlg", "nlg", "onto", "onto", "onto", "onto", "nlg"], "mention_counts": {"nlg": 4, "onto": 7}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.7059547412717408}, {"paperId": "6373a66a0af4eafba9b76053e6ba82fd438b5118", "url": "https://www.semanticscholar.org/paper/6373a66a0af4eafba9b76053e6ba82fd438b5118", "title": "Populating a domain ontology from web historical dictionaries and encyclopedias", "abstract": "An increasing volume of information is available on the web and usually is expressed as text, representing unstructured or semi-structured data. Thus, semantic information is implicit in these texts, since they are mainly intended for human consumption and interpretation. Therefore, it is not easy to automatically identify concepts or establish relations among them inside the texts. In particular, some web sites contain information on historical data about artistic manifestations like literature or music. This kind of site contains a body of knowledge on the domain, and usually is constructed with some format and content patterns that may be useful for information extraction. In order to make this information available as a structured knowledge base, an information extraction process is necessary. Ontologies are an appropriate way to represent structured knowledge bases, enabling sharing, reuse and inference. In this paper, it is described an information extraction process cycle for populating a domain ontology using texts available on the internet to extract instances of concepts, events and relations, based on existing ontology development methodologies and information extraction techniques. Through this process, latent concepts and relations expressed in natural language can be extracted and represented as an ontology, allowing new uses of the available content. A case study that applies this process is presented.", "venue": "EATIS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "onto", "ie", "onto", "ie", "kg", "onto", "ie", "onto", "ie"], "mention_counts": {"kg": 2, "onto": 5, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"kg": 2, "onto": 5}, "relevance_score": 0.7059547412717408}, {"paperId": "6dab169937c969e346bdf408d8e4ab39f1deda60", "url": "https://www.semanticscholar.org/paper/6dab169937c969e346bdf408d8e4ab39f1deda60", "title": "Using Ontology-Driven Methods to Develop Frameworks for Tackling NLP Problems", "abstract": "In this paper, we present the meta-tooling framework named TAISim that can be used both as a developer\u201fs tool for creating NLP systems and as a NLP learning environment, which allows helping students to construct NLP systems by example in a flexible way. TAISim enables the end user to combine different components of a typical NLP system in order to tackle specific NLP problems. We use ontology-engineering methods to accumulate meta-knowledge about the system construction and about users\u201f activities to control the process of development and using the NLP system. Thanks to ontologydriven methods TAISim can be modified and enriched with additional information resources and program modules by means of a high-level interface. Additionally, we demonstrate how the using of meta-ontology helps us to improve TAISim to tackle ontology design automation problems.", "venue": "AIST", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "nlp", "nlp", "onto", "onto", "nlp", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 7, "onto": 4}, "nlp_mention_counts": {"nlp": 7}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7059547412717408}, {"paperId": "3b6496a6947578049706600db09b70ab094ef535", "url": "https://www.semanticscholar.org/paper/3b6496a6947578049706600db09b70ab094ef535", "title": "Combining the Best of Two Worlds: NLP and IR for Intranet Search", "abstract": "Natural language processing (NLP) is becoming much more robust and applicable in realistic applications. One area in which NLP has still not been fully exploited is information retrieval (IR). In particular we are interested in search over intranets and other local Web sites. We see dialogue-driven search which is based on a largely automated knowledge extraction process as one of the next big steps. Instead of replying with a set of documents for a user query the system would allow the user to navigate through the extracted knowledge base by making use of a simple dialogue manager. Here we support this idea with a first task-based evaluation that we conducted on a university intranet. We automatically extracted entities like person names, organizations and locations as well as relations between entities and added visual graphs to the search results whenever a user query could be mapped into this knowledge base. We found that users are willing to interact and use those visual interfaces. We also found that users preferred such a system that guides a user through the result set over a baseline approach. The results represent an important first step towards full NLP-driven intranet search.", "venue": "2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "kg", "nlp", "nlp", "ke", "kg", "nlp", "nlp"], "mention_counts": {"nlp": 5, "kg": 2, "ke": 2}, "nlp_mention_counts": {"nlp": 5, "ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 2}, "relevance_score": 0.7059547412717408}, {"paperId": "6e621feba6fa2ff894807d38ee7b648be89b227b", "url": "https://www.semanticscholar.org/paper/6e621feba6fa2ff894807d38ee7b648be89b227b", "title": "Small in Size, Big in Precision: A Case for Using Language-Specific Lexical Resources for Word Sense Disambiguation", "abstract": "Linked open data (LOD) presents an ideal platform for connecting the multilingual lexical resources used in natural language processing (NLP) tasks, but the use of machine translation to fill in gaps in lexical coverage for resource-poor languages means that large amounts of data are potentially unverified. For graph-based word sense disambiguation (WSD), one approach has been to first translate terms into English in order to disambiguate using richer, fuller lexical knowledge bases (LKBs) such as WordNet. In this paper, we show that this approach actually creates more ambiguity and is far less accurate than using languagespecific resources, which, regardless of their smaller size, can provide results comparable in accuracy to the state-of-theart reported for graph-based WSD in English. For LOD, this demonstrates the importance of continuing to grow and extend language-specific resources in order to continually verify and reintegrate them as accurate resources.", "venue": "NLPLOD@RANLP", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["lod", "wsd", "nlp", "lod", "wsd", "wsd", "nlp", "wsd", "lod", "mt", "kg"], "mention_counts": {"nlp": 2, "lod": 3, "mt": 1, "kg": 1, "wsd": 4}, "nlp_mention_counts": {"nlp": 2, "wsd": 4, "mt": 1}, "ld_mention_counts": {"kg": 1, "lod": 3}, "relevance_score": 0.7059547412717408}, {"paperId": "d2152059477ca5ac6424e6890879fce18469c8b1", "url": "https://www.semanticscholar.org/paper/d2152059477ca5ac6424e6890879fce18469c8b1", "title": "Semi-Automatic Practical Ontology Construction by Using a Thesaurus, Computational Dictionaries, and Large Corpora", "abstract": "This paper presents the semi-automatic construction method of a practical ontology by using various resources. In order to acquire a reasonably practical ontology in a limited time and with less manpower, we extend the Kadokawa thesaurus by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. The former can be obtained by converting valency information and case frames from previously-built computational dictionaries used in machine translation. The latter can be acquired from concept co-occurrence information, which is extracted automatically from large corpora. The ontology stores rich semantic constraints among 1,110 concepts, and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology. In our practical machine translation system, our ontology-based word sense disambiguation method achieved an 8.7% improvement over methods which do not use an ontology for Korean translation.", "venue": "HTLKM@ACL", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "wsd", "onto", "mt", "nlp", "mt", "onto", "onto"], "mention_counts": {"nlp": 1, "wsd": 1, "onto": 7, "mt": 2}, "nlp_mention_counts": {"nlp": 1, "wsd": 1, "mt": 2}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.7059547412717408}, {"paperId": "83b4f1037269b4b7621cb249520262c3d3b12104", "url": "https://www.semanticscholar.org/paper/83b4f1037269b4b7621cb249520262c3d3b12104", "title": "Research on Automatically Building Tibetan Folk Culture Ontology Library for Sino-Tibetan Machine Translation", "abstract": "In order to achieve the Sino-Tibetan machine translation system, the key problem is the automatic construction of the domain ontology. We use a combination of concept hierarchy theory and KDD (knowledge discovery in databases) technology to automatically build domain ontology. For building domain ontology, we should first extract information and build model of information in this field to create the initial concepts for ontology. We will automatically build Tibetan folk culture ontology library. In it, we will collect more than 100 kinds of Tibetan folk culture characteristics and find the relationship between them. Then we will establish their concept of a hierarchical tree, and finally achieve the expansion of the ontology knowledge through KDD technology for Sino-Tibetan Machine translation.", "venue": "2013 6th International Conference on Intelligent Networks and Intelligent Systems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "onto", "mt", "onto", "onto", "onto", "onto", "ie", "onto", "mt", "onto"], "mention_counts": {"onto": 7, "mt": 3, "ie": 1}, "nlp_mention_counts": {"mt": 3, "ie": 1}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.7059547412717408}, {"paperId": "cae30d5914fcf473a7a7169dcb8dcff881b8910d", "url": "https://www.semanticscholar.org/paper/cae30d5914fcf473a7a7169dcb8dcff881b8910d", "title": "Ontology-Driven Self-Supervision for Adverse Childhood Experiences Identification Using Social Media Datasets", "abstract": ": Adverse Childhood Experiences (ACEs) are defined as a collection of highly stressful, and potentially traumatic, events or circumstances that occur throughout childhood and/or adolescence. They have been shown to be associated with increased risks of mental health diseases or other abnormal behaviours in later lives. However, the identification of ACEs from textual data with Natural Language Processing (NLP) is challenging because (a) there are no NLP ready ACE ontologies; (b) there are few resources available for machine learning, necessitating the data annotation from clinical experts; (c) costly annotations by domain experts and large number of documents for supporting large machine learning models. In this paper, we present an ontology-driven self-supervised approach (derive concept embeddings using an auto-encoder from baseline NLP results) for producing a publicly available resource that would support large-scale machine learning (e.g., training transformer based large language models) on social media corpus. This resource as well as the proposed approach are aimed to facilitate the community in training transferable NLP models for effectively surfacing ACEs in low-resource scenarios like NLP on clinical notes within Electronic Health Records. The resource including a list of ACE ontology terms, ACE concept embeddings and the NLP annotated corpus is available at https://github.com/knowlab/ACE-NLP.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "nlp", "onto", "nlp", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 7, "onto": 4}, "nlp_mention_counts": {"nlp": 7}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7059547412717408}, {"paperId": "9a95053b3175ee2a90707d6816571333173948f2", "url": "https://www.semanticscholar.org/paper/9a95053b3175ee2a90707d6816571333173948f2", "title": "Semantic Annotation and Querying Framework based on Semi-structured Ayurvedic Text", "abstract": "Knowledge bases (KB) are an important resource in a number of natural language processing (NLP) and information retrieval (IR) tasks, such as semantic search, automated question-answering etc. They are also useful for researchers trying to gain information from a text. Unfortunately, however, the state-of-the-art in Sanskrit NLP does not yet allow automated construction of knowledge bases due to unavailability or lack of sufficient accuracy of tools and methods. Thus, in this work, we describe our efforts on manual annotation of Sanskrit text for the purpose of knowledge graph (KG) creation. We choose the chapter Dh\u0101nyavarga from Bh\u0101vaprak\u0101\u015banigha\u1e47\u1e6du of the Ayurvedic text Bh\u0101vaprak\u0101\u015ba for annotation. The constructed knowledge graph contains 410 entities and 764 relationships. Since Bh\u0101vaprak\u0101\u015banigha\u1e47\u1e6du is a technical glossary text that describes various properties of different substances, we develop an elaborate ontology to capture the semantics of the entity and relationship types present in the text. To query the knowledge graph, we design 31 query templates that cover most of the common question patterns. For both manual annotation and querying, we customize the Sangrahaka framework previously developed by us. The entire system including the dataset is available from https://sanskrit.iitk.ac.in/ayurveda/. We hope that the knowledge graph that we have created through manual annotation and subsequent curation will help in development and testing of NLP tools in future as well as studying of the Bh\u0101vaprak\u0101\u015banigha\u1e47\u1e6du text.", "venue": "Online World Conference on Soft Computing in Industrial Applications", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "kg", "kg", "nlp", "kg", "kg", "nlp", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 4, "kg": 6, "onto": 1}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 6, "onto": 1}, "relevance_score": 0.7059547412717408}, {"paperId": "c147f5eaf647dfd22a011784f16c37d9002b1af9", "url": "https://www.semanticscholar.org/paper/c147f5eaf647dfd22a011784f16c37d9002b1af9", "title": "PhenoGO: Assigning Phenotypic Context to Gene Ontology Annotations with Natural Language Processing", "abstract": "Natural language processing (NLP) is a high throughput technology because it can process vast quantities of text within a reasonable time period. It has the potential to substantially facilitate biomedical research by extracting, linking, and organizing massive amounts of information that occur in biomedical journal articles as well as in textual fields of biological databases. Until recently, much of the work in biological NLP and text mining has revolved around recognizing the occurrence of biomolecular entities in articles, and in extracting particular relationships among the entities. Now, researchers have recognized a need to link the extracted information to ontologies or knowledge bases, which is a more difficult task. One such knowledge base is Gene Ontology annotations (GOA), which significantly increases semantic computations over the function, cellular components and processes of genes. For multicellular organisms, these annotations can be refined with phenotypic context, such as the cell type, tissue, and organ because establishing phenotypic contexts in which a gene is expressed is a crucial step for understanding the development and the molecular underpinning of the pathophysiology of diseases. In this paper, we propose a system, PhenoGO, which automatically augments annotations in GOA with additional context. PhenoGO utilizes an existing NLP system, called BioMedLEE, an existing knowledge-based phenotype organizer system (PhenOS) in conjunction with MeSH indexing and established biomedical ontologies. More specifically, PhenoGO adds phenotypic contextual information to existing associations between gene products and GO terms as specified in GOA. The system also maps the context to identifiers that are associated with different biomedical ontologies, including the UMLS, Cell Ontology, Mouse Anatomy, NCBI taxonomy, GO, and Mammalian Phenotype Ontology. In addition, PhenoGO was evaluated for coding of anatomical and cellular information and assigning the coded phenotypes to the correct GOA; results obtained show that PhenoGO has a precision of 91% and recall of 92%, demonstrating that the PhenoGO NLP system can accurately encode a large number of anatomical and cellular ontologies to GO annotations. The PhenoGO Database may be accessed at the following URL: http://www.phenoGO.org", "venue": "Pacific Symposium on Biocomputing", "citationCount": 93, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "nlp", "onto", "onto", "nlp", "onto", "onto", "onto", "onto", "nlp", "nlp", "onto", "kg", "kg", "ie", "kg", "onto"], "mention_counts": {"nlp": 5, "onto": 8, "kg": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 5, "ie": 1}, "ld_mention_counts": {"kg": 3, "onto": 8}, "relevance_score": 0.7048226948775973}, {"paperId": "4116b467f5f9ad167917215214cfcbda7a002683", "url": "https://www.semanticscholar.org/paper/4116b467f5f9ad167917215214cfcbda7a002683", "title": "An Efficient Approach for Semantic Relatedness Evaluation Based on Semantic Neighborhood", "abstract": "In the context of natural language processing and information retrieval, ontologies can improve the results of the word sense disambiguation (WSD) techniques. By making explicit the semantics of the term, ontology-based semantic measures play a crucial role to determine how different ontology classes have a similar meaning. In this context, it is common to use semantic similarity as a basis for WSD. However, the measures generally consider only taxonomic relationships, which affects negatively the discrimination of two ontology classes that are related by the other relationship types. On the other hand, semantic relatedness measures consider diverse types of relationships to determine how much two classes on the ontology are related. However, these measures, especially the path-based approaches, has as the main drawback a high computational complexity to be calculated in query execution time. Also, for both types of semantic measures, it is unpractical to store all similarity or relatedness values between all ontology classes in memory, especially for large ontologies. In this work, we propose a novel approach based on semantic neighbors that aim to improve the query time in path-based semantic measures without losing their effectiveness in relatedness analysis. We also propose an efficient algorithm to calculate the semantic distance between two ontology classes. We evaluate our proposal in WSD using a pre-existent domain ontology for well-core description. This ontology contains 929 classes related to rock facies and a set of sentences from four different corpora about geology in the Oil\\&Gas domain. In the experiments, we compared our approach with state-of-the-art semantic relatedness measures, such as path-based, feature-based, information content, and hybrid methods regarding the F-score, query time and the total number of classes in memory. The experimental results show that the proposed method obtains F-score gains in WSD, as well as an improvement in the query time concerning the traditional path-based approaches. Also, we reduce the total number of classes stored in memory for each ontology class.", "venue": "2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "wsd", "nlp", "onto", "wsd", "onto", "onto", "wsd", "onto", "wsd", "onto", "wsd", "onto", "onto", "onto"], "mention_counts": {"nlp": 1, "wsd": 5, "onto": 11}, "nlp_mention_counts": {"nlp": 1, "wsd": 5}, "ld_mention_counts": {"onto": 11}, "relevance_score": 0.7048226948775973}, {"paperId": "56bb03730f52a69de3abedef03584582011e36fc", "url": "https://www.semanticscholar.org/paper/56bb03730f52a69de3abedef03584582011e36fc", "title": "Enriching Answers in Question Answering Systems using Linked Data", "abstract": "Linked Data has emerged as the most widely used and the most powerful knowledge source for Question Answering (QA). Although Question Answering using Linked Data (QALD) fills in many gaps in the traditional QA models, the answers are still presented as factoids. This research introduces an answer presentation model for QALD by employing Natural Language Generation (NLG) to generate natural language descriptions to present an informative answer. The proposed approach employs lexicalization, aggregation, and referring expression generation to build a human-like enriched answer utilizing the triples extracted from the entities mentioned in the question as well as the entities contained in the answer.", "venue": "SEMWEB", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "nlg", "llod", "ld", "nlg", "ld", "lod"], "mention_counts": {"ld": 2, "llod": 1, "nlg": 3, "lod": 1}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"ld": 2, "llod": 1, "lod": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "f1a1cb1a2d585bec5c66230c28fefc46444db263", "url": "https://www.semanticscholar.org/paper/f1a1cb1a2d585bec5c66230c28fefc46444db263", "title": "Cross-lingual Knowledge Extraction (XLike)", "abstract": "The goal of the XLike project is to develop technology to monitor and aggregate knowledge that is currently spread across mainstream and social media, and to enable cross-lingual ser vices for publishers, media monitoring and business intelligence. The effort will combine scientific capabilities and insights from several areas of science \u2013 modern computational linguistics and NLP, machine learning, text mining and semantic technologies \u2013 in order to enable cross-lingual text \u201cunderstanding\u201d by machines. Specifically, we plan to pursue the following two key open research problems: (1) to extract and integrate formal knowledge relations from multilingual texts with cross-lingual knowledge bases, and (2) to adapt linguistic techniques and crowdsourcing to deal with irregularities in the informal language used primarily in social media. The developed technology will be language-independent to the largest possible extent, while within the project we will specifically address English, German, Spanish, Chinese and Hindi as major world languages and Catalan, Slovenian, and Croatian as less resourced languages. Knowledge resources from Linked Open Data cloud will be used, with special focus paid to using general common sense knowledge base CycKB as \u201csemantic Interlingua\u201d. The use of Machine Translation will be oriented towards translation from a natural language as the source language into a formal language of semantic representation as source language. For languages where not enough required linguistic resources are available, we will use a probabilistic Interlingua representation trained from a parallel corpora and/or from comparable corpus derived from the Wikipedia, or use MT as a fallback option for translating the text from less resourced language to English and then process this translation. Specifically, developed solutions will be applied and evaluated in two use cases: a \u201cBloomberg\u201d use case, covering the domain of financial news, and a \u201cSlovenian Press Agency\u201d use case, covering the domain of general news.", "venue": "MTSUMMIT", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "lod", "kg", "nlp", "mt"], "mention_counts": {"nlp": 1, "ke": 1, "lod": 1, "mt": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "mt": 1}, "ld_mention_counts": {"ke": 1, "lod": 1, "kg": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "cf2450d035df95ff44ee1c154ced3a48012fbc6f", "url": "https://www.semanticscholar.org/paper/cf2450d035df95ff44ee1c154ced3a48012fbc6f", "title": "Session details: Poster presentations", "abstract": "We are very pleased to welcome the 100 posters of this WWW 2012 poster track. We wish to thank all Program Committee members for the quality (and the quantity) of their work. And naturally, we also congratulate authors of the selected contributions that reflect the variety of the WWW series conferences: all posters present high quality work in various domains, poster authors come from various countries and represent both academic and industrial research teams. The poster track provides an opportunity for authors and conference attendees to interact and exchange about innovative theoretical and technical work in progress around, about and on the Web. In addition to being displayed during three whole days from 4/18 to 4/20, WWW'2012 posters will also be shortly presented on Thursday 4/19 PM. Seven \"spotlight events\" are organized to give conference attendees a 5-minute glimpse on each poster content, in the following areas: 1 - Web search / Web mining: WWW contains rich data, yet how to effectively use such rich data is challenging. Web search is mainly about searching information with a proper query, while web mining has a boarder scope of knowledge exaction from the Web. 2 - Natural Language Processing / Information Search and Retrieval: NLP and IR are two major foundations of Web search. With more and more applications on the Web, the boundary between NLP and IR is getting smaller. How to efficiently and effectively handle large scale online data is a challenge for NLP and IR communities. 3 - Web engineering / Performance / Security / Privacy: this session copes with the everyday challenges of Web development: how to improve contents and applications design processes, how to serve them efficiently and securely to users and how to face privacy issues. 4 - Recommender Systems / Semantic Web: Recommender Systems allow Web communities access the best of users' opinions. Semantic Web and Linked Data rely on communities' data to harvest and process complex information. This session presents posters that cope with graph algorithms and semantic/social data. 5 - Social Web / Human Factors / Accessibility: this session deals with human-oriented topics. The presented posters mainly concern analysis and prediction of social behavior and influence, as well as of individual behavior, affective computing and Web accessibility. 6 - Microblogging / Social media / Catch up TV: social media and microblogging provide rich online information with a different perspective from traditional static Web page data. How to effectively search, mine and utilize such valuable information is a very stimulating research area. 7 - Monetization: posters in this session explore efficient ways to spread ads on the Web or analyze the efficiency of existing algorithms on actual e-commerce activity. This year's edition aims at \"augmenting posters\" in several ways: poster views are also available in virtual worlds, and attendees can access poster metadata and additional information and even chat with authors using their smartphones and tabs. Numerous efforts have been made to encourage discussions about posters for the largest audience, inside and outside the conference. So come, see and participate!", "venue": "Proceedings of the 21st International Conference on World Wide Web", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "sw", "sw", "nlp", "nlp", "ld", "nlp"], "mention_counts": {"ld": 1, "nlp": 4, "sw": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"ld": 1, "sw": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "b641ab44b1ab70665c58b10f7d175be99bb9a16f", "url": "https://www.semanticscholar.org/paper/b641ab44b1ab70665c58b10f7d175be99bb9a16f", "title": "Ontology-Based Interpretation of Natural Language Philipp Cimiano, Christina Unger, and John McCrae (University of Arminia Bielefeld, Germany) Morgan & Claypool, Synthesis Lectures on Human Language Technologies, March 2014, 178 pages, (doi:10.2200/S00561ED1V01Y201401HLT024) , $45.00", "abstract": "A book aiming to build a bridge between two fields that share the subject of research but do not share the same views necessarily puts itself in a difficult position: The authors have either to strike a fair balance at peril of dissatisfying both sides or nail their colors to the mast and cater mainly to one of two communities. For semantic processing of natural language with either NLP methods or Semantic Web approaches, the authors clearly favor the latter and propose a strictly ontology-driven interpretation of natural language. The main contribution of the book, driving semantic processing from the ground up by a formal domain-specific ontology, is elaborated in ten well-structured chapters spanning 143 pages of content.", "venue": "Computational Linguistics", "citationCount": 1, "fieldsOfStudy": ["Sociology", "Computer Science"], "mentions": ["onto", "sw", "onto", "hlt", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 3, "hlt": 1}, "nlp_mention_counts": {"nlp": 2, "hlt": 1}, "ld_mention_counts": {"sw": 1, "onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "c376766991bb481c200b9ebdefb073d5b610d97c", "url": "https://www.semanticscholar.org/paper/c376766991bb481c200b9ebdefb073d5b610d97c", "title": "Natural Multi-language Interaction between Firefighters and Fire Fighting Robots", "abstract": "Due to rapid development of agent systems and robotics, more and more chances are available for humans to interact with agent-based robotic technology (e.g., Robotic vacuums, robotic surgery, etc.), this trend increases the importance of human-robot interaction including human-robot communication. For the robust human-robot communication, natural language processing (NLP) can be implemented, among various existing natural language processing techniques, Ontological Semantic Technology (OST), which addresses meanings in an easily comprehensible way as a human does, was selected, the OST is a system in an ontology-based structure to deal with multiple natural languages. This research specifically addresses a concept of natural language-based communication with fire fighting robots and humans, and the main domain of this study targets fire fighting situations. In order to implement ontology-based communication with different languages, Korean and English were used for this particular study. This study extends the domain of Ontological Semantic Technology, specifically for communication with robots in a fire fighting domain using Korean and English.", "venue": "2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "5bb28ae9adf604c5acc56fd51d0be3ea392048ce", "url": "https://www.semanticscholar.org/paper/5bb28ae9adf604c5acc56fd51d0be3ea392048ce", "title": "The Value of Paraphrase for Knowledge Base Predicates", "abstract": "Paraphrase, i.e., differing textual realizations of the same meaning, has proven useful for many natural language processing (NLP) applications. Collecting paraphrase for predicates in knowledge bases (KBs) is the key to comprehend the RDF triples in KBs. Existing works have published some paraphrase datasets automatically extracted from large corpora, but have too many redundant pairs or don't cover enough predicates, which cannot be improved by computer only and need the help of human beings. This paper shows a full process of collecting large-scale and high-quality paraphrase dictionaries for predicates in knowledge bases, which takes advantage of existing datasets and combines the technologies of machine mining and crowdsourcing. Our dataset comprises 2284 distinct predicates in DBpedia and 31130 paraphrase pairs in total, the quality of which is a great leap over previous works. Then it is demonstrated that such good paraphrase dictionaries can do great help to natural language processing tasks such as question answering and language generation. We also publish our own dictionary for further research.", "venue": "AAAI Conference on Artificial Intelligence", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "rdf", "kg", "nlp", "kg", "nlp"], "mention_counts": {"nlp": 3, "kg": 3, "rdf": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 3, "rdf": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "4b87c1e8798b172ea00e788963628f8695f6501e", "url": "https://www.semanticscholar.org/paper/4b87c1e8798b172ea00e788963628f8695f6501e", "title": "Semantic analysis of natural language software requirement", "abstract": "In the recent past, domain specific solutions for detailed semantic analysis have got acceptable by natural language processing community and use of applications involving natural language based user interface. Different approaches that has been previously used is focusing on quality of text and improving the text contents by adding semantic information with text then the existing approaches used for semantic analysis can provide better results. In this, an approach was presented to address the problem of non-availability of semantic information required for better semantic analysis. This problem is solved by using semantic technology to annotate text of software requirements expressed in a natural language with their domain specific semantics and investigate the effect of semantic analysis with attached semantics. The presented approach uses a semantic framework specifically designed for interpretation and detailed semantic analysis of natural language software requirement specifications. The used framework is based on semantic technology involves knowledge extracted from existing software requirement documents and knowledge extracted from existing applications. The presented approach shows that by adapting and combing existing ontologies to support knowledge management, developing system and performing experiments on requirement of real world software systems. In this approach start with software requirement specification, after this clean the irrelevant requirements, convert the cleaned requirements into graph that represents inter related different elements. Represent the requirement graph into sparse matrix, after these all steps; we generate ontology with the help of OntoGen tool.", "venue": "InTech", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ke", "onto", "ke", "onto"], "mention_counts": {"nlp": 1, "onto": 2, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "8c557b7df46e6c54a5fef4a78f38151aa0c4ede5", "url": "https://www.semanticscholar.org/paper/8c557b7df46e6c54a5fef4a78f38151aa0c4ede5", "title": "Natural Language Processing Model for Automatic Analysis of Cybersecurity-Related Documents", "abstract": "This paper describes the development and implementation of a natural language processing model based on machine learning which performs cognitive analysis for cybersecurity-related documents. A domain ontology was developed using a two-step approach: (1) the symmetry stage and (2) the machine adjustment. The first stage is based on the symmetry between the way humans represent a domain and the way machine learning solutions do. Therefore, the cybersecurity field was initially modeled based on the expertise of cybersecurity professionals. A dictionary of relevant entities was created; the entities were classified into 29 categories and later implemented as classes in a natural language processing model based on machine learning. After running successive performance tests, the ontology was remodeled from 29 to 18 classes. Using the ontology, a natural language processing model based on a supervised learning model was defined. We trained the model using sets of approximately 300,000 words. Remarkably, our model obtained an F1 score of 0.81 for named entity recognition and 0.58 for relation extraction, showing superior results compared to other similar models identified in the literature. Furthermore, in order to be easily used and tested, a web application that integrates our model as the core component was developed.", "venue": "Symmetry", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 4, "onto": 3}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "5c162159543b068e20d6ec73d7bb0ef86bee4646", "url": "https://www.semanticscholar.org/paper/5c162159543b068e20d6ec73d7bb0ef86bee4646", "title": "Application of Natural Language Processing and Evidential Analysis to Web-Based Intelligence Information Acquisition", "abstract": "The quality of decisions made in business and government relates directly to the quality of the information used to formulate the decision. This information may be retrieved from an organization's knowledge base (Intranet) or from the World Wide Web. Intelligence services Intranet held information can be efficiently manipulated by technologies based upon either semantics such as ontologies, or statistics such as meaning-based computing. These technologies require complex processing of large amount of textual information. However, they cannot currently be effectively applied to Web-based search due to various obstacles, such as lack of semantic tagging. A new approach proposed in this paper supports Web-based search for intelligence information utilizing evidence-based natural language processing (NLP). This approach combines traditional NLP methods for filtering of Web-search results, Grounded Theory to test the completeness of the evidence, and Evidential Analysis to test the quality of gathered information. The enriched information derived from the Web-search will be transferred to the intelligence services knowledge base for handling by an effective Intranet search system thus increasing substantially the information for intelligence analysis. The paper will show that the quality of retrieved information is significantly enhanced by the discovery of previously unknown facts derived from known facts.", "venue": "European Intelligence and Security Informatics Conference", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "kg", "kg", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "c6e4e528c5ea975f04fadb462c6b6e3284a471a1", "url": "https://www.semanticscholar.org/paper/c6e4e528c5ea975f04fadb462c6b6e3284a471a1", "title": "Issues and Challenges in Annotating Urdu Action Verbs on the IMAGACT4ALL Platform", "abstract": "In South-Asian languages such as Hindi and Urdu, action verbs having compound constructions and serial verbs constructions pose serious problems for natural language processing and other linguistic tasks. Urdu is an Indo-Aryan language spoken by 51, 500, 0001 speakers in India. Action verbs that occur spontaneously in day-to-day communication are highly ambiguous in nature semantically and as a consequence cause disambiguation issues that are relevant and applicable to Language Technologies (LT) like Machine Translation (MT) and Natural Language Processing (NLP). IMAGACT4ALL is an ontology-driven web-based platform developed by the University of Florence for storing action verbs and their inter-relations. This group is currently collaborating with Jawaharlal Nehru University (JNU) in India to connect Indian languages on this platform. Action verbs are frequently used in both written and spoken discourses and refer to various meanings because of their polysemic nature. The IMAGACT4ALL platform stores each 3d animation image, each one of them referring to a variety of possible ontological types, which in turn makes the annotation task for the annotator quite challenging with regard to selecting verb argument structure having a range of probability distribution. The authors, in this paper, discuss the issues and challenges such as complex predicates (compound and conjunct verbs), ambiguously animated video illustrations, semantic discrepancies, and the factors of verb-selection preferences that have produced significant problems in annotating Urdu verbs on the IMAGACT ontology.", "venue": "LREC", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "nlp", "onto", "onto", "mt"], "mention_counts": {"nlp": 3, "onto": 3, "mt": 1}, "nlp_mention_counts": {"nlp": 3, "mt": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "28386672818a489af94879534e6e644ae5231371", "url": "https://www.semanticscholar.org/paper/28386672818a489af94879534e6e644ae5231371", "title": "plWordNet as the Cornerstone of a Toolkit of Lexico-semantic Resources", "abstract": "A wordnet is many things to many people: a graph of inter-related lexicalised concepts, a taxonomy, a thesaurus, and so on. A wordnet makes good sense as the mainstay of any deep automated semantic analysis of text. We have begun the construction of a multi-component, multi-use toolkit of natural language processing tools with plWordNet, a very large Polish wordnet, at its centre. The components will include plWordNet and its mapping onto an ontology (the upper level and elements of the middle level), a lexicon of proper names and a semantic valency lexicon. Some of those elements will be aligned with plWordNet, and there will be a mapping onto Princeton WordNet. Several challenging applications will show the utility of the toolkit in practice. 1 How wordnets evolve Wordnets start small but quickly grow to account for much of the lexical material of the given language. The size of version 3.1 of Princeton WordNet (PWN) (Fellbaum, 1998) is a de facto standard, even if this mature wordnet also keeps growing, albeit slowly.1 One of the resources which approach this size standard is plWordNet (Piasecki et al., 2009), now in version 2.1. Languages change continually, so lexicographers never rest, but one can still ask when the development of a wordnet ought to slow down, and whether there is an appropriate steady state of a wordnet. That clearly is a loaded question, and much depends on the language. For example, suppose that a wordnet for PWN began as a test of a theory of human semantic representation and memory (Collins and Quillian, 1969). It now features a comprehensive vocabulary, a set of universally useful semantic relations, glosses, links to ontologies, and more. a richly inflected language with complex and varied derivation was originally a translation of PWN. Such a wordnet should, sooner or later, acquire semantic relations which account accurately for its unique lexical system.. A wordnet, even as developed as PWN, GermaNet (Hamp and Feldweg, 1997) or plWordNet (Maziarz et al., 2013a), serves many natural language processing (NLP) applications, yet it seems neither feasible nor necessary to remake wordnets into universal NLP resources. Instead, we propose to mark clear boundaries around a wordnet (what it should and what it should not include), and treat it as a pivotal element of an organic toolkit of inter-connected tools and resources for the semantic analysis of texts, along with the auxiliary morphological and syntactic analysis tools. Our case study is such a toolkit, now under development, centred on plWordNet 3.0 (also in development), and intended first and foremost for research in the humanities. In the remainder of the paper, we present the main design assumptions and principles of that project. We explain how comprehensive we want plWordNet 3.0 to become, what size and what coverage we envisage. We attempt to describe how the toolkit will be built around plWordNet, and we outline plans for its large-scale illustrative applications in several domains. We discuss how the components of the toolkit will be expanded or constructed: plWordNet 3.0, its mapping to an ontology, and a semantic lexicon of proper names. We also briefly present resources for morphological and structural description, associated with the plWordNet system, among them a lexicon of lexico-syntactic structures of multiword expressions and a valency lexicon linked to plWordNet but developed independently. This work is meant to take several years of initial effort and years of maintenance. We cannot answer many design questions yet, but many will be answered as the project unfolds. That is to say. we want to interlace theory and practice.", "venue": "GWC", "citationCount": 16, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "nlp", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 3}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "5f2dd73a4437008e792023f0a8a51662b66edc51", "url": "https://www.semanticscholar.org/paper/5f2dd73a4437008e792023f0a8a51662b66edc51", "title": "Combining NLP And Semantics For Mining Software Technologies From Research Publications", "abstract": "The natural language processing (NLP) community has developed a variety of methods for extracting and disambiguating information from research publications. However, they usually focus only on standard research entities such as authors, affiliations, venues, references and keywords. We propose a novel approach, which combines NLP and semantic technologies for generating from the text of research publications an OWL ontology describing software technologies used or introduced by researchers, such as applications, systems, frameworks, programming languages, and formats. The method was tested on a sample of 300 publications in the Semantic Web field, yielding promising results.", "venue": "WWW", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "sw", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 4, "sw": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "2f6bb3c5dd032fcb37b79828be446f26c2f47654", "url": "https://www.semanticscholar.org/paper/2f6bb3c5dd032fcb37b79828be446f26c2f47654", "title": "Research and Design of Knowledge System Construction System Based on Natural Language Processing", "abstract": "The digital processing of content resources has subverted the traditional paper content processing model and has also spread widely. The digital resources processed by text structure need to be structured and processed by professional knowledge, which can be saved as a professional digital content resource of knowledge base and provide basic metadata for intelligent knowledge service platform. The professional domain-based knowledge system construction system platform explored in this study is designed based on natural language processing. Natural language processing is an important branch of artificial intelligence, which is the application of artificial intelligence technology in linguistics. The system first extracts the professional thesaurus and domain ontology in the digital resources and then uses the new word discovery algorithm based on the label weight designed by artificial intelligence technology to intelligently extract and clean the new words of the basic thesaurus. At the same time, the relationship system between knowledge points and elements is established to realize the association extraction of targeted knowledge points, and finally the output content is enriched from knowledge points into related knowledge systems. In order to improve the scalability and universality of the system, the extended architecture of the thesaurus, algorithms, computational capabilities, tags, and exception thesaurus was taken into account when designing. At the same time, the implementation of \u201cartificial intelligence [Formula: see text] manual assistance\u201d was adopted. On the basis of improving the system availability, the experimental basis of the optimization algorithm is provided. The results of this research will bring an artificial intelligence innovation after the digitization to the publishing industry and will transform the content service into an intelligent service based on the knowledge system.", "venue": "Int. J. Pattern Recognit. Artif. Intell.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "kg", "tp", "kg", "onto"], "mention_counts": {"nlp": 3, "tp": 1, "kg": 2, "onto": 1}, "nlp_mention_counts": {"nlp": 3, "tp": 1}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "4585294526e2a0c3c9bc6d1c0b742ad16d00a7d6", "url": "https://www.semanticscholar.org/paper/4585294526e2a0c3c9bc6d1c0b742ad16d00a7d6", "title": "NLP Technologies and Semantic Web: Risks, Opportunities and Challenges", "abstract": "In this paper we provide a set of hypotheses about possible interactions between the raising paradigm of the Semantic Web and NLP technologies. We show that there is some role to be played by NLP both on the ground of creation and maintenance of the Semantic Web and on the one of its access by humans. We also provide the skeleton of a running application which emulates a situation where the Semantic Web has reached its mature state.", "venue": "Intelligenza Artificiale", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "nlp", "sw", "nlp", "sw", "nlp"], "mention_counts": {"nlp": 3, "sw": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "1486ceab09a38b31160840d66ec9bd4c76bc943d", "url": "https://www.semanticscholar.org/paper/1486ceab09a38b31160840d66ec9bd4c76bc943d", "title": "Semantic Web based Machine Translation", "abstract": "This paper describes the experimental combination of traditional Natural Language Processing (NLP) technology with the Semantic Web building stack in order to extend the expert knowledge required for a Machine Translation (MT) task. Therefore, we first give a short introduction in the state of the art of MT and the Semantic Web and discuss the problem of disambiguation being one of the common challenges in MT which can only be solved using world knowledge during the disambiguation process. In the following, we construct a sample sentence which demonstrates the need for world knowledge and design a prototypical program as a successful solution for the outlined translation problem. We conclude with a critical view on the developed approach.", "venue": "ESIRMT/HyTra@EACL", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "sw", "nlp", "mt", "mt", "nlp"], "mention_counts": {"nlp": 2, "sw": 3, "mt": 2}, "nlp_mention_counts": {"nlp": 2, "mt": 2}, "ld_mention_counts": {"sw": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "5016c3ed744329a4e17aee25b9742267bfce70ed", "url": "https://www.semanticscholar.org/paper/5016c3ed744329a4e17aee25b9742267bfce70ed", "title": "\"Linked data as background knowledge for information extraction on the web\" by Ziqi Zhang, Anna Lisa Gentile and Isabelle Augenstein with Martin Vesely as coordinator", "abstract": "Information Extraction (IE) is the technique for transforming textual data into structured representation that can be understood by machines. It is a crucial technique in enabling the Semantic Web, where increasing interest has been seen in recent years. This article reports recent progress in the LODIE project - Linked Open Data for Information Extraction, aimed at advancing Web IE to a new frontier by exploiting largely available, semantically annotated, Linked Open Data as background knowledge. We cover topics of wrapper induction, IE from semi-structured content such as tables and lists, and IE from free-text. We describe new challenges in the research and methods proposed to address them, together with summaries of recent evaluations showing encouraging results.", "venue": "LINK", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "ie", "ie", "lod", "ie", "lod", "sw"], "mention_counts": {"ld": 1, "sw": 1, "lod": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"ld": 1, "sw": 1, "lod": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "3b61d2494e7ca8405b64d4f0634103a821dedaa1", "url": "https://www.semanticscholar.org/paper/3b61d2494e7ca8405b64d4f0634103a821dedaa1", "title": "Integrating Open and Closed Information Extraction: Challenges and First Steps", "abstract": "Over the past years, state-of-the-art information extraction (IE) systems such as NELL [5] and ReVerb [9] have achieved impressive results by producing very large knowledge resources at web scale with minimal supervision. However, these resources lack the schema information, exhibit a high degree of ambiguity, and are difficult even for humans to interpret. Working with such resources becomes easier if there is a structured information base to which the resources can be linked. In this paper, we introduce the integration of open information extraction projects with Wikipedia-based IE projects that maintain a logical schema, as an important challenge for the NLP, semantic web, and machine learning communities. We describe the problem, present a gold-standard benchmark, and take the first steps towards a data-driven solution to the problem. This is especially promising, since NELL and ReVerb typically achieve a very large coverage, but still still lack a fullfl edged clean ontological structure which, on the other hand, could be provided by large-scale ontologies like DBpedia [2] or YAGO [13].", "venue": "NLP-DBPEDIA@ISWC", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "ie", "nlp", "onto", "onto", "ie", "ie"], "mention_counts": {"nlp": 1, "sw": 1, "onto": 2, "ie": 3}, "nlp_mention_counts": {"nlp": 1, "ie": 3}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "2d6a08ab89d5b5b6dbcb779c8162392f9cbb261a", "url": "https://www.semanticscholar.org/paper/2d6a08ab89d5b5b6dbcb779c8162392f9cbb261a", "title": "Representing Texts as Contextualized Entity-Centric Linked Data Graphs", "abstract": "The integration of a small fraction of the information present in the Web of Documents to the Linked Data Web can provide a significant shift on the amount of information available to data consumers. However, information extracted from text does not easily fit into the usually highly normalized structure of ontology-based datasets. While the representation of structured data assumes a high level of regularity, relatively simple and consistent conceptual models, the representation of information extracted from texts need to take into account large terminological variation, complex contextual/dependency patterns, and fuzzy or conflicting semantics. This work focuses on bridging the gap between structured and unstructured data, proposing the representation of text as structured discourse graphs (SDGs), targeting an RDF representation of unstructured data. The representation focuses on a semantic best-effort information extraction scenario, where information from text is extracted under a pay-as-you-go data quality perspective, trading terminological normalization for domain-independency, context capture, wider representation scope and maximization of textual information capture.", "venue": "2013 24th International Workshop on Database and Expert Systems Applications", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "ie", "ld", "rdf", "ld", "onto"], "mention_counts": {"ld": 2, "onto": 1, "rdf": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"ld": 2, "onto": 1, "rdf": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "d2c499c6ffa9a95e9404faacacd43f01f7298491", "url": "https://www.semanticscholar.org/paper/d2c499c6ffa9a95e9404faacacd43f01f7298491", "title": "Benchmarking the Extraction and Disambiguation of Named Entities on the Semantic Web", "abstract": "Named entity recognition and disambiguation are of primary importance for extracting information and for populating knowledge bases. Detecting and classifying named entities has traditionally been taken on by the natural language processing community, whilst linking of entities to external resources, such as those in DBpedia, has been tackled by the Semantic Web community. As these tasks are treated in different communities, there is as yet no oversight on the performance of these tasks combined. We present an approach that combines the state-of-the art from named entity recognition in the natural language processing domain and named entity linking from the semantic web community. We report on experiments and results to gain more insights into the strengths and limitations of current approaches on these tasks. Our approach relies on the numerous web extractors supported by the NERD framework, which we combine with a machine learning algorithm to optimize recognition and linking of named entities. We test our approach on four standard data sets that are composed of two diverse text types, namely newswire and microposts.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 69, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "kg", "nlp", "sw", "sw", "ie", "nlp"], "mention_counts": {"nlp": 2, "sw": 3, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"sw": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "0ba745b89f6e5e71821a9d88e9e989d233e09dba", "url": "https://www.semanticscholar.org/paper/0ba745b89f6e5e71821a9d88e9e989d233e09dba", "title": "RULIE: Rule Unification for Learning Information Extraction", "abstract": "In this paper we are presenting RULIE (Rule Unification for Learning Information Extraction), an adaptive information extraction algorithm which works by employing a hybrid technique of Rule Learning and Rule Unification in order to extract relevant information from all types of documents which can be found and used in the semantic web. This algorithm combines the techniques of the LP2 and the BWI algorithms for improved performance. In this paper we are also presenting the experimental results of this algorithm and respective details of evaluation. This evaluation compares RULIE to other information extraction algorithms based on their respective performance measurements and in almost all cases RULIE outruns the other algorithms which are namely: LP2, BWI, RAPIER, SRV and WHISK. This technique would aid current techniques of linked data which would eventually lead to fullier realisation of the semantic web.", "venue": "LDH", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "sw", "ie", "ld", "ie", "ie", "sw"], "mention_counts": {"ld": 1, "sw": 2, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"ld": 1, "sw": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "d2e5207bfe52eb75b79414bda95b6c91d02e8490", "url": "https://www.semanticscholar.org/paper/d2e5207bfe52eb75b79414bda95b6c91d02e8490", "title": "KESeDa: Knowledge Extraction from Heterogeneous Semi-Structured Data Sources", "abstract": "A large part of the free knowledge existing on the Web is available as heterogeneous, semi-structured data, which is only weakly interlinked and in general does not include any semantic classification. Due to the enormous amount of information the necessary preparation of this data for integrating it in the Web of Data requires automated processes. The extraction of knowledge from structured as well as unstructured data has already been the topic of research. But especially for the semi-structured data format JSON, which is widely used as a data exchange format e.g., in social networks, extraction solutions are missing. Based on the findings we made by analyzing existing extraction methods, we present our KESeDa approach for extracting knowledge from heterogeneous, semi-structured data sources. We show how knowledge can be extracted by describing different analysis and processing steps. With the resulting semantically enriched data the potential of Linked Data can be utilized.", "venue": "International Conference on Semantic Systems", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ld"], "mention_counts": {"ld": 1, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ld": 1, "ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "8ca06ceaf94d8d25fbdc845f67d1b4806e19d0fa", "url": "https://www.semanticscholar.org/paper/8ca06ceaf94d8d25fbdc845f67d1b4806e19d0fa", "title": "Knowledge Extraction for Art History: the Case of Vasari's The Lives of The Artists", "abstract": "Knowledge Extraction (KE) techniques are used to convert unstructured information present in texts to Knowledge Graphs (KGs) which can be queried and explored. Despite their potential for cultural heritage domains, such as Art History, these techniques often encounter limitations if applied to domain-specific data. In this paper we present the main challenges that KE has to face on art-historical texts, by using as case study Giorgio Vasari\u2019s The Lives of The Artists . This paper discusses the following NLP tasks for art-historical texts, namely entity recognition and linking, coreference resolution, time extraction, motif extraction and artwork extraction. Several strategies to annotate art-historical data for these tasks and evaluate NLP models are also proposed.", "venue": "Qurator", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "nlp", "kg", "ke"], "mention_counts": {"nlp": 2, "ke": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "bfd71e2e6bf1974f2194fb66cccc855d00b7a1a9", "url": "https://www.semanticscholar.org/paper/bfd71e2e6bf1974f2194fb66cccc855d00b7a1a9", "title": "A Semantic Extraction and Sentimental Assessment of Risk Factors (SESARF): An NLP Approach for Precision Medicine: A Medical Decision Support Tool for Early Diagnosis from Clinical Notes", "abstract": "Clinical notes contain information that is crucial for the diagnosis process. However, it is usually not properly manually analyzed due to the tremendous efforts and time it takes. Hence, an automated approach is eagerly needed to maximize clinical knowledge management and reduce cost. In this paper, we propose a framework SESARF: a Semantic Extractor to identify hidden risk factors in clinical notes and a Sentimental Analyzer to assess the severity levels associated with the identified Risk Factors. This tool can be customized to any disease using Linked Open Data (LOD) by selecting a specific disease and collecting its risk factors list from medical ontologies. The extracted knowledge can serve two purposes: 1) a feature vector is prepared, for any classifier in machine learning, containing risk factors and their weights based on our semantic enrichment and sentimental analyzer and 2) a proper comparison of the extracted information with wearable body sensors that can alert any major changes in a patient's health status to personalize treatment.", "venue": "2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "lod", "ie", "ke", "nlp", "lod"], "mention_counts": {"onto": 1, "nlp": 1, "ke": 1, "lod": 2, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "lod": 2, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "dca3c960ec60b321c0bc5c546aa8e3fbe0eb7e82", "url": "https://www.semanticscholar.org/paper/dca3c960ec60b321c0bc5c546aa8e3fbe0eb7e82", "title": "RAKER: Resource-Aware Knowledge Extraction aRchitecture on Mobile Grid", "abstract": "In this paper, we, based on mobile agent technology, propose a Resource-Aware Knowledge Extraction aRchitecture on mobile grid, named RAKER. RAKER can dynamically determine the processing and policy for achieving high-performance and high availability of knowledge extracting based on our previous proposed Resource Estimation Model. On the RAKER, users can extract information or knowledge in efficient,effective and transparent way that kept on the mobile grid without caring about the energy consumption that is most important issue in mobile computing. We show the implementation and an example to demonstrate the use of RAKER. In addition, we also measure the latency and the energy consumption and simulate the system availability to show that the performance of RAKER.", "venue": "2009 First Asian Conference on Intelligent Information and Database Systems", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "ie"], "mention_counts": {"ke": 3, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "fe2b08135fbfe3d3a8846f5b328afb8a0ab1567d", "url": "https://www.semanticscholar.org/paper/fe2b08135fbfe3d3a8846f5b328afb8a0ab1567d", "title": "KIM \u2013 a semantic platform for information extraction and retrieval", "abstract": "The KIM platform provides a novel Knowledge and Information Management framework and services for automatic semantic annotation, indexing, and retrieval of documents. It provides a mature and semantically enabled infrastructure for scalable and customizable information extraction (IE) as well as annotation and document management, based on GATE.General Architecture for Text Engineering (GATE) (http://gate.ac.uk), leading NLP and IE platform developed at the University of Sheffield. Our understanding is that a system for semantic annotation should be based upon a simple model of real-world entity concepts, complemented with quasi-exhaustive instance knowledge. To ensure efficiency, easy sharing, and reusability of the metadata we introduce an upper-level ontology. Based on the ontology, a large-scale instance base of entity descriptions is maintained. The knowledge resources involved are handled by use of state-of-the-art Semantic Web technology and standards, including RDF(S) repositories, ontology middleware and reasoning. From a technical point of view, the platform allows KIM-based applications to use it for automatic semantic annotation, for content retrieval based on semantic queries, and for semantic repository access. As a framework, KIM also allows various IE modules, semantic repositories and information retrieval engines to be plugged into it. This paper presents the KIM platform, with an emphasis on its architecture, interfaces, front-ends, and other technical issues.", "venue": "Natural Language Engineering", "citationCount": 338, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "sw", "ie", "nlp", "onto", "ie", "onto"], "mention_counts": {"nlp": 1, "sw": 1, "onto": 3, "ie": 2}, "nlp_mention_counts": {"nlp": 1, "ie": 2}, "ld_mention_counts": {"sw": 1, "onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "49badb6822d79d7c3954b6da257972c3d14a2896", "url": "https://www.semanticscholar.org/paper/49badb6822d79d7c3954b6da257972c3d14a2896", "title": "Discovering Inconsistencies in PubMed Abstracts through Ontology-Based Information Extraction", "abstract": "Searching for a cure for cancer is one of the most vital pursuits in modern medicine. In that aspect microRNA research plays a key role. Keeping track of the shifts and changes in established knowledge in the microRNA domain is very important. In this paper, we introduce an Ontology-Based Information Extraction method to detect occurrences of inconsistencies in microRNA research paper abstracts. We propose a method to first use the Ontology for MIcroRNA Targets (OMIT) to extract triples from the abstracts. Then we introduce a new algorithm to calculate the oppositeness of these candidate relationships. Finally we present the discovered inconsistencies in an easy to read manner to be used by medical professionals. To our best knowledge, this study is the first ontology-based information extraction model introduced to find shifts in the established knowledge in the medical domain using research paper abstracts. We downloaded 36877 abstracts from the PubMed database. From those, we found 102 inconsistencies relevant to the microRNA domain.", "venue": "ACM International Conference on Bioinformatics, Computational Biology and Biomedicine", "citationCount": 19, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 4, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "5f4eb3e0ee8e0e6e842d1b855bb6ef22dbc098e0", "url": "https://www.semanticscholar.org/paper/5f4eb3e0ee8e0e6e842d1b855bb6ef22dbc098e0", "title": "NLP Techniques for Term Extraction and Ontology Population", "abstract": "This chapter investigates NLP techniques for ontology population, using a combination of rule-based approaches and machine learning. We describe a method for term recognition using linguistic and statistical techniques, making use of contextual information to bootstrap learning. We then investigate how term recognition techniques can be useful for the wider task of information extraction, making use of similarity metrics and contextual information. We describe two tools we have developed which make use of contextual information to help the development of rules for named entity recognition. Finally, we evaluate our ontology-based information extraction results using a novel technique we have developed which makes use of similarity-based metrics first developed for term recognition.", "venue": "Ontology Learning and Population", "citationCount": 149, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "ie", "ie", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 3, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "3b8ef03b38c2bb52053709b7a1c2a9f554590bf5", "url": "https://www.semanticscholar.org/paper/3b8ef03b38c2bb52053709b7a1c2a9f554590bf5", "title": "Ontology-Based Scalable and Portable Information Extraction System to Extract Biological Knowledge from Huge Collection of Biomedical Web Documents", "abstract": "Automated discovery and extraction of biological knowledge from biomedical web documents has become essential because of the enormous amount of biomedical literature published each year. In this paper we present an ontology-based scalable and portable information extraction system to automatically extract biological knowledge from huge collection of online biomedical web documents. Our method integrates ontology-based semantic tagging, information extraction and data mining together, automatically learns the patterns based on a few user seed tuples, and then extract new tuples from the biomedical web documents based on the discovered patterns. A novel system SPIE (Scalable and Portable Information Extraction) is implemented and tested on the PuBMed to find the chromatin protein-protein interaction and the experimental results indicate our approach is very effective in extracting biological knowledge from huge collection of biomedical web documents.", "venue": "International Conference on Wirtschaftsinformatik", "citationCount": 19, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "ie", "ie"], "mention_counts": {"onto": 3, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "8f1bdfc8ed2a1420d32c5f3b81e2b31c8fdc2cb7", "url": "https://www.semanticscholar.org/paper/8f1bdfc8ed2a1420d32c5f3b81e2b31c8fdc2cb7", "title": "The Use of Ontology in Clinical Information Extraction", "abstract": "Extracting clinical data from medical or clinical reports is a crucial effort. These records contain the most valuable pieces of evidence of treatments in humans. Integration of information extraction (IE) and ontology can produce a great tool for clinical concept extraction. The aim of this paper is to present a quick overview of the research work which has applied IE and ontology approaches in medical or clinical concepts extraction. This paper also presents our proposed framework based on the integration of both approaches mentioned above for extracting clinical concepts.", "venue": "Journal of Physics: Conference Series", "citationCount": 7, "fieldsOfStudy": ["Physics", "Computer Science"], "mentions": ["ke", "onto", "onto", "ie", "onto", "ie"], "mention_counts": {"ke": 1, "onto": 3, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "2fd2729412765cfaebdf78272aba7068bdbdc19b", "url": "https://www.semanticscholar.org/paper/2fd2729412765cfaebdf78272aba7068bdbdc19b", "title": "Open Knowledge Extraction through Compositional Language Processing", "abstract": "We present results for a system designed to perform Open Knowledge Extraction, based on a tradition of compositional language processing, as applied to a large collection of text derived from the Web. Evaluation through manual assessment shows that well-formed propositions of reasonable quality, representing general world knowledge, given in a logical form potentially usable for inference, may be extracted in high volume from arbitrary input sentences. We compare these results with those obtained in recent work on Open Information Extraction, indicating with some examples the quite different kinds of output obtained by the two approaches. Finally, we observe that portions of the extracted knowledge are comparable to results of recent work on class attribute extraction.", "venue": "Conference on Semantics in Text Processing", "citationCount": 53, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke", "ke"], "mention_counts": {"ke": 3, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "dec1e7340b69ed6b7044ffa2b73b27169e0cf000", "url": "https://www.semanticscholar.org/paper/dec1e7340b69ed6b7044ffa2b73b27169e0cf000", "title": "Ontology-Based Information Extraction System in E-Commerce Websites", "abstract": "Information extraction may help the users to query their needed information from the endless useful information. Now there are a variety of information extraction technologies, and we usually use the search engines to find our information we need from the Internet. But it is difficult to find the \"real\" information we want. So in this paper, we use the concept of the ontology to analyze the structure and content of the website, to build ontology model, in order to extract the information based on ontology from the e-commerce website for the users. In the end, the paper makes an experiment test of the text tool GATE to extract from websites and evaluate the results objectively.", "venue": "CASE", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "ie", "onto"], "mention_counts": {"onto": 4, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "97cbb076fd2bda102a8d27170669229c680cc705", "url": "https://www.semanticscholar.org/paper/97cbb076fd2bda102a8d27170669229c680cc705", "title": "Information extraction from unstructured data using RDF", "abstract": "The Internet exhibits a gigantic measure of helpful data which is generally designed for its users, which makes it hard to extract applicable information from different sources. Accordingly, the accessibility of strong, adaptable Information Extraction framework that consequently concentrate structured data such as, entities, relationships between entities, and attributes from unstructured or semi-structured sources. But somewhere during extraction of information may lead to the loss of its meaning, which is absolutely not feasible. Semantic Web adds solution to this problem. It is about providing meaning to the data and allow the machine to understand and recognize these augmented data more accurately. The proposed system is about extracting information from research data of IT domain like journals of IEEE, Springer, etc., which aid researchers and the organizations to get the data of journals in an optimized manner so the time and hard work of surfing and reading the entire journal's papers or articles reduces. Also the accuracy of the system is taken care of using RDF, the data extracted has a specific declarative semantics so that the meaning of the research papers or articles during extraction remains unchanged. In addition, the same approach shall be applied on multiple documents, so that time factor can get saved.", "venue": "2016 International Conference on ICT in Business Industry & Government (ICTBIG)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "ie", "ie", "ie", "rdf", "sw", "ie"], "mention_counts": {"sw": 1, "rdf": 2, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"sw": 1, "rdf": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "226716d236d08cb291779a2b96347ea8bf532711", "url": "https://www.semanticscholar.org/paper/226716d236d08cb291779a2b96347ea8bf532711", "title": "Ontology-Based Information Extraction of Crop Diseases on Chinese Web Pages", "abstract": "This paper proposes a method for extracting information of crop diseases on Chinese web pages. First, we define some special labels of the DOM tree[1] to partition the web page into some content blocks. Then the noise content in the web pages is eliminated according to the location and the word number of a content block. We employ an ontology-based way to implement information extraction from the content blocks. A top-down method is adopted to construct the ontology of crop diseases. In the extraction process, the concepts, relations and instances of ontology is used to extract the entities. The event is extracted by an optimal classification of paragraph groups in a content block. Experiments demonstrate the performance of the proposed method is satisfactory.", "venue": "J. Comput.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 4, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "b9dd3d0315b8811fd819b76f3d8b778c885593fa", "url": "https://www.semanticscholar.org/paper/b9dd3d0315b8811fd819b76f3d8b778c885593fa", "title": "Automatic Knowledge Base Construction using Probabilistic Extraction, Deductive Reasoning, and Human Feedback", "abstract": "We envision an automatic knowledge base construction system consisting of three inter-related components. MADden is a knowledge extraction system applying statistical text analysis methods over database systems (DBMS) and massive parallel processing (MPP) frameworks; ProbKB performs probabilistic reasoning over the extracted knowledge to derive additional facts not existing in the original text corpus; CAMeL leverages human intelligence to reduce the uncertainty resulting from both the information extraction and probabilistic reasoning processes.", "venue": "AKBC-WEKEX@NAACL-HLT", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ke", "ke", "ie"], "mention_counts": {"kg": 2, "ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"kg": 2, "ke": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "505251351b44f8db8ce7219fcea960f56eb3ae0b", "url": "https://www.semanticscholar.org/paper/505251351b44f8db8ce7219fcea960f56eb3ae0b", "title": "A system for coreference resolution for the clinical narrative", "abstract": "OBJECTIVE\nTo research computational methods for coreference resolution in the clinical narrative and build a system implementing the best methods.\n\n\nMETHODS\nThe Ontology Development and Information Extraction corpus annotated for coreference relations consists of 7214 coreferential markables, forming 5992 pairs and 1304 chains. We trained classifiers with semantic, syntactic, and surface features pruned by feature selection. For the three system components--for the resolution of relative pronouns, personal pronouns, and noun phrases--we experimented with support vector machines with linear and radial basis function (RBF) kernels, decision trees, and perceptrons. Evaluation of algorithms and varied feature sets was performed using standard metrics.\n\n\nRESULTS\nThe best performing combination is support vector machines with an RBF kernel and all features (MUC score=0.352, B(3)=0.690, CEAF=0.486, BLANC=0.596) outperforming a traditional decision tree baseline.\n\n\nDISCUSSION\nThe application showed good performance similar to performance on general English text. The main error source was sentence distances exceeding a window of 10 sentences between markables. A possible solution to this problem is hinted at by the fact that coreferent markables sometimes occurred in predictable (although distant) note sections. Another system limitation is failure to fully utilize synonymy and ontological knowledge. Future work will investigate additional ways to incorporate syntactic features into the coreference problem.\n\n\nCONCLUSION\nWe investigated computational methods for coreference resolution in the clinical narrative. The best methods are released as modules of the open source Clinical Text Analysis and Knowledge Extraction System and Ontology Development and Information Extraction platforms.", "venue": "J. Am. Medical Informatics Assoc.", "citationCount": 42, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ie", "onto", "ie", "onto", "onto", "ke"], "mention_counts": {"ke": 1, "onto": 3, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "2533e6ef309f625f891d506d013af47715f8b95f", "url": "https://www.semanticscholar.org/paper/2533e6ef309f625f891d506d013af47715f8b95f", "title": "Components for information extraction: ontology-based information extractors and generic platforms", "abstract": "Information Extraction (IE) has existed as a field for several decades and has produced some impressive systems in the recent past. Despite its success, widespread usage and commercialization remain elusive goals for this field. We identify the lack of effective mechanisms for reuse as one major reason behind this situation. Here, we mean not only the reuse of the same IE technique in different situations but also the reuse of information related to the application of IE techniques (e.g., features used for classification). We have developed a comprehensive component-based approach for information extraction that promotes reuse to address this situation. We designed this approach starting from our previous work on the use of multiple ontologies in information extraction [24]. The key ideas of our approach are \"information extractors,\" which are components of an IE system that make extractions with respect to particular components of an ontology and \"platforms for IE,\" which are domain and corpus independent implementations of IE techniques. A case study has shown that this component-based approach can be successfully applied in practical situations.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 39, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 3, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "459474126c67ee0c5517ac3eac9f2e1c0fcafda2", "url": "https://www.semanticscholar.org/paper/459474126c67ee0c5517ac3eac9f2e1c0fcafda2", "title": "Named Entity Disambiguation using Freebase and Syntactic Parsing", "abstract": "Named Entity Disambiguation (NED) is a fundamental task of semantic annotation for the Semantic Web. The task of Word Sense Disambiguation (WSD) in Ontology-Based Information Extraction (OBIE) aims to establish a link between the textual entity mention and the corresponding class in the ontology. In this paper, we propose a NED process integrated in a rule-based OBIE system for French. We show that our SVM approach can improve disambiguation efficiency using syntactic features provided by the Fips parser and popularity score features extracted from the Freebase knowledge base.", "venue": "LD4IE@ISWC", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "sw", "wsd", "ie", "onto", "onto", "kg"], "mention_counts": {"onto": 2, "sw": 1, "kg": 1, "wsd": 2, "ie": 1}, "nlp_mention_counts": {"wsd": 2, "ie": 1}, "ld_mention_counts": {"sw": 1, "onto": 2, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "b3488844ad80daf10a3d6bceef64058769390306", "url": "https://www.semanticscholar.org/paper/b3488844ad80daf10a3d6bceef64058769390306", "title": "Knowledge Extraction System from Unstructured Documents", "abstract": "In this paper, we design and implement a knowledge extraction system from unstructured documents (unstructured documents are documents where the information is in natural language, and require natural language processing techniques for processing) in HTML format. Basically, the system allows to transform the content of a text that is in natural language, into structured and organized knowledge, semantically described (a Semantic Ontology). Therefore, it is proposed to generate semantic knowledge based on the extraction of entities and relationships, where entities are anything about which something can be said, and relations the interactions between entities. From the generated semantic knowledge model, it is possible to infer new knowledge, such as lexicons, taxonomies and specialized terminological bases. The system can be used by any semantic processing application, in its processes of enriching its information and knowledge", "venue": "IEEE Latin America Transactions", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "ke", "ke", "kg"], "mention_counts": {"nlp": 1, "kg": 1, "onto": 1, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"ke": 2, "onto": 1, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "9d41230ef26d7aef8ded112f712081ad9c6ecbf8", "url": "https://www.semanticscholar.org/paper/9d41230ef26d7aef8ded112f712081ad9c6ecbf8", "title": "Identifying semantic and syntactic relations from text documents", "abstract": "Semantic and syntactic relations play an important role of applications in recent years, especially on Semantic Web, Information Retrieval, Information Extraction, and Question Answering. Semantic and syntactic relations content main ideas in the sentences or paragraphs. This paper presents our proposed algorithms for identifying semantic and syntactic relations between objects and their properties in order to enrich a domain specific ontology, namely Computing Domain Ontology, which is used in Information extraction system. We combine the methodologies of Natural Language Processing with Machine Learning in these proposed algorithms in order to extract the explicit and implicit relations. We exploit these relations from distinct resources, such as WordNet, Wikipedia and text documents of ACM Digital Libraries. We also use Natural Language Processing tools, such as OpenNLP, Stanford Lexical Dependency Parser in order to analyze and parse sentences. A random sample among 245 categories of ACM Categories is used to evaluate. Results generated show that our proposed approach achieves high precision.", "venue": "The 2015 IEEE RIVF International Conference on Computing & Communication Technologies - Research, Innovation, and Vision for Future (RIVF)", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "sw", "nlp", "ie", "ie", "onto"], "mention_counts": {"nlp": 2, "sw": 1, "onto": 2, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"sw": 1, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "1df13ee06629e5d19623855f98b32fde6f4fb6f4", "url": "https://www.semanticscholar.org/paper/1df13ee06629e5d19623855f98b32fde6f4fb6f4", "title": "Financial Knowledge Graph Based Financial Report Query System", "abstract": "Annual Financial Reports are the core in the Banking Sector to publish its financial statistics. Extracting useful information from these complex and lengthy reports involves manual process to resolve the financial queries, resulting in delays and ambiguity in investment decisions. One of the major reasons is the lack of any standardization in the format and vocabulary used in the reports. An automated system for resolution of intelligent financial queries is therefore difficult to design. Several works have been proposed to overcome these problems using Information Extraction; however, they do not address the semantic interoperability of the reports across different institutions. This work proposed an automated querying engine to answer the financial queries using Ontology based Information Extraction. For Semantic modeling of financial reports, a Financial Knowledge Graph, assisted by Financial Ontology, has been proposed. The nodes are populated with entities, while links are populated with relationships using Information Extraction applied on annual reports. Two benefits have been provided by this system to stakeholders through automation: decision making through queries and generation of custom financial stories. The work can further be extended to other domains including healthcare and academia where physical reports are used for communication.", "venue": "IEEE Access", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "kg", "ie", "kg", "ie", "onto"], "mention_counts": {"kg": 2, "onto": 2, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "c533fc1b75166f16604611240950fbfb7ec2b019", "url": "https://www.semanticscholar.org/paper/c533fc1b75166f16604611240950fbfb7ec2b019", "title": "Constrained Semi-supervised Learning in the Presence of Unanticipated Classes", "abstract": "Traditional semi-supervised learning (SSL) techniques consider the missing labels of unlabeled datapoints as latent/unobserved variables, and model these variables, and the parameters of the model, using techniques like Expectation Maximization (EM). Such semisupervised learning techniques are widely used for Automatic Knowledge Base Construction (AKBC) tasks.\n We consider two extensions to traditional SSL methods which make it more suitable for a variety of AKBC tasks. First, we consider jointly assigning multiple labels to each instance, with a flexible scheme for encoding constraints between assigned labels: this makes it possible, for instance, to assign labels at multiple levels from a hierarchy. Second, we account for another type of latent variable, in the form of unobserved classes. In open-domain webscale information extraction problems, it is an unrealistic assumption that the class ontology or topic hierarchy we are using is complete. Our proposed framework combines structural search for the best class hierarchy with SSL, reducing the semantic drift associated with erroneously grouping unanticipated classes with expected classes. Together, these extensions allow a single framework to handle a large number of knowledge extraction tasks, including macro-reading, noun-phrase classification, word sense disambiguation, alignment of KBs to wikipedia or on-line glossaries, and ontology extension.\n To summarize, this thesis argues that many AKBC tasks which have previously been addressed separately can be viewed as instances of single abstract problem: multiview semisupervised learning with an incomplete class hierarchy. In this thesis we present a generic EM framework for solving this abstract task.", "venue": "SIGF", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "wsd", "kg", "ke"], "mention_counts": {"onto": 2, "ke": 1, "kg": 1, "wsd": 1, "ie": 1}, "nlp_mention_counts": {"ke": 1, "wsd": 1, "ie": 1}, "ld_mention_counts": {"kg": 1, "onto": 2, "ke": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "70afaf892e1e88f8e9e0c98b3fcc1bf3baadd226", "url": "https://www.semanticscholar.org/paper/70afaf892e1e88f8e9e0c98b3fcc1bf3baadd226", "title": "Knowledge extraction and integration for semi-structural information in digital libraries", "abstract": "Recent years, the development of digital library builds up a tremendous digital world with millions of digitized books. However, despite the fact that digital libraries now have much more data compared with traditional libraries, extracting information and providing knowledge access are still challenges for the development of library services. Our application aims to extract information from traditional Chinese medicine (TCM) textbooks of China America Digital Academic Library (CADAL). Specifically, we extract both detailed properties of TCM entities (herbal medicines, prescriptions and attending illnesses) and their relations. The advantage is that with the extracted entity properties, relations can be discovered on semantic level. Besides, external information like famous TCM doctors as well as relevant images and videos is also integrated according to a predefined ontology model, so that the result of the integration tends to make library a knowledge provider. The primary contribution is that our application extracts knowledge out of digitized textbooks and comes to form a platform of preserving and popularizing TCM, one of the greatest culture heritages that have served health of Chinese people for over 4,000 years.", "venue": "JCDL '09", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "onto", "ke", "ke"], "mention_counts": {"ke": 2, "onto": 1, "ie": 2}, "nlp_mention_counts": {"ke": 2, "ie": 2}, "ld_mention_counts": {"ke": 2, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "ad384645fbef115e5b2648c20d8dd58f4f3505be", "url": "https://www.semanticscholar.org/paper/ad384645fbef115e5b2648c20d8dd58f4f3505be", "title": "The research of information extraction based on knowledge engineering", "abstract": "The system of arms information extraction based on the ontology, consists of two parts: knowledge base, processing program. It realizes the arms category determination based on text categorization, and realizes the arms object determination based on named entity recognition. It realizes the information extraction according to information extraction rules based on syntax and semantic constraint. It realizes the information integration in semantic level to some extent based on the ontology.", "venue": "2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "ie", "ie", "onto", "kg"], "mention_counts": {"kg": 1, "onto": 2, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "419d6ca6faf224c98a62ddbb5f75bd0d4ea31b6c", "url": "https://www.semanticscholar.org/paper/419d6ca6faf224c98a62ddbb5f75bd0d4ea31b6c", "title": "SPOT: Knowledge-Enhanced Language Representations for Information Extraction", "abstract": "Knowledge-enhanced pre-trained models for language representation have been shown to be more effective in knowledge base construction tasks (i.e.,~relation extraction) than language models such as BERT. These knowledge-enhanced language models incorporate knowledge into pre-training to generate representations of entities or relationships. However, existing methods typically represent each entity with a separate embedding. As a result, these methods struggle to represent out-of-vocabulary entities and a large amount of parameters, on top of their underlying token models (i.e., the transformer), must be used and the number of entities that can be handled is limited in practice due to memory constraints. Moreover, existing models still struggle to represent entities and relationships simultaneously. To address these problems, we propose a new pre-trained model that learns representations of both entities and relationships from token spans and span pairs in the text respectively. By encoding spans efficiently with span modules, our model can represent both entities and their relationships but requires fewer parameters than existing models. We pre-trained our model with the knowledge graph extracted from Wikipedia and test it on a broad range of supervised and unsupervised information extraction tasks. Results show that our model learns better representations for both entities and relationships than baselines, while in supervised settings, fine-tuning our model outperforms RoBERTa consistently and achieves competitive results on information extraction tasks.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ke", "ie", "ie", "ie"], "mention_counts": {"kg": 2, "ke": 1, "ie": 3}, "nlp_mention_counts": {"ke": 1, "ie": 3}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "4fcbf12ed7391d9863c4a39d2d27b359c964562e", "url": "https://www.semanticscholar.org/paper/4fcbf12ed7391d9863c4a39d2d27b359c964562e", "title": "Text Mining for Personalized Knowledge Extraction From Online Support Groups", "abstract": "The traditional approach to health care is being revolutionized by the rapid adoption of patient\u2010centered healthcare models. The successful transformation of patients from passive recipients to active participants is largely attributed to increased access to healthcare information. Online support groups present a platform to seek and exchange information in an inclusive environment. As the volume of text on online support groups continues to grow exponentially, it is imperative to improve the quality of retrieved information in terms of relevance, reliability, and usefulness. We present a text\u2010mining approach that generates a knowledge extraction layer to address this void in personalized information retrieval from online support groups. The knowledge extraction layer encapsulates an ensemble of text\u2010mining techniques with a domain ontology to interpose an investigable and extensible structure on hitherto unstructured text. This structure is not limited to personalized information retrieval for patients, as it also imparts aggregates for crowdsourcing analytics by healthcare researchers. The proposed approach was successfully trialed on an active online support group consisting of 800,000 posts by 72,066 participants. Demonstrations for both patient and researcher use cases accentuate the value of the proposed approach to unlock a broad spectrum of personalized and aggregate knowledge concealed within crowdsourced content.", "venue": "J. Assoc. Inf. Sci. Technol.", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "onto", "ke"], "mention_counts": {"ke": 3, "onto": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "2d03b7f30a7ab644b842bba4ae6f0f35b5721032", "url": "https://www.semanticscholar.org/paper/2d03b7f30a7ab644b842bba4ae6f0f35b5721032", "title": "A Knowledge-Based Method for Grammatical Knowledge Extraction Process", "abstract": "\u2014 This paper describes a new approach for the development of systems that requires natural language parsing or generation. This method is based on the use of Descriptive Grammars \u2013in particular, the descriptive grammar for Spanish is used\u2013 as the source for linguistic knowledge extraction. This knowledge source allows the use of classical knowledge-engineering methodologies for the extraction of rules that represent partial or complete aspects of the language, without the necessity of appealing to linguistic theories or experts. This easy method opens a new range of possibilities to the development of reliable applications that require parsing or language generation, dialog systems, information extraction, or semantic web applications.", "venue": "IKE", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ie", "ke", "sw", "ke"], "mention_counts": {"kg": 1, "sw": 1, "ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"kg": 1, "sw": 1, "ke": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "affad79c3df7719b209a3dad7e1be5324281fa9a", "url": "https://www.semanticscholar.org/paper/affad79c3df7719b209a3dad7e1be5324281fa9a", "title": "Enabling type/condition-specified entity/fact retrieval using semantic knowledge extracted from wikipedia", "abstract": "Wikipedia has recently become an important semantic knowledge resource, thanks to its semi-structured semantic features and the huge amount of user-generated content covering a wide range of topics. The mode of information retrieval on Wikipedia, as on the Web in general, however, remains that of conventional keyword-based page/document retrieval. The project presented in this paper, entitled PanAnthropon FilmWorld, aims at demonstrating direct, sophisticated entity/fact retrieval by extracting/deriving semantic knowledge from Wikipedia and by representing facts using domain-relevant classes, entities, attributes, and categories. To this end, a semantic knowledge base containing the extracted data and a semantic search interface demonstrating the proposed retrieval capability have been constructed. The focus of this paper is on the details concerning semantic knowledge extraction and derivation. However, the interface is fully functional. The results of evaluation confirm both the quality of knowledge extraction and the effectiveness of entity/fact retrieval using the interface.", "venue": "SMER '11", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "ke"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "fc2145325a3e5612d936d0da3e90de26ab85015d", "url": "https://www.semanticscholar.org/paper/fc2145325a3e5612d936d0da3e90de26ab85015d", "title": "Description and Evaluation of Semantic Similarity Measures Approaches", "abstract": "ABSTRACT In recent years, semantic similarity measure has a great interest in Semantic Web and Natural Language Processing (NLP). Several similarity measures have been developed, being given the existence of a structured knowledge representation offered by ontologies and corpus which enable semantic interpretation of terms. Semantic similarity measures compute the similarity between concepts/terms included in knowledge sources in order to perform estimations. This paper discusses the existing semantic similarity methods based on structure, information content and feature approaches. Additionally, we present a critical evaluation of several categories of semantic similarity approaches based on two standard benchmarks. The aim of this paper is to give an efficient evaluation of all these measures which help researcher and practitioners to select the measure that best fit for their requirements. General Terms Similarity Measures, Ontology, Semantic Web, NLP", "venue": "ArXiv", "citationCount": 112, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "nlp", "onto", "nlp", "sw", "onto", "nlp"], "mention_counts": {"nlp": 3, "sw": 2, "onto": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 2, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "a05dc7e7e554df8a72bb154e022aaefe24c2489f", "url": "https://www.semanticscholar.org/paper/a05dc7e7e554df8a72bb154e022aaefe24c2489f", "title": "Knowledge Extraction From Texts Based on Wikidata", "abstract": "This paper presents an effort within our company of developing knowledge extraction pipeline for English, which can be further used for constructing an entreprise-specific knowledge base. We present a system consisting of entity detection and linking, coreference resolution, and relation extraction based on the Wikidata schema. We highlight existing challenges of knowledge extraction by evaluating the deployed pipeline on real-world data. We also make available a database, which can serve as a new resource for sentential relation extraction, and we underline the importance of having balanced data for training classification models.", "venue": "NAACL", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 1, "ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "13af5acb07914748d8372af6c4c43d94c7cfead3", "url": "https://www.semanticscholar.org/paper/13af5acb07914748d8372af6c4c43d94c7cfead3", "title": "Automated knowledge extraction from the federal acquisition regulations system (FARS)", "abstract": "With increasing regulation of Big Data, it is becoming essential for organizations to ensure compliance with various data protection standards. The Federal Acquisition Regulations System (FARS) within the Code of Federal Regulations (CFR) includes facts and rules for individuals and organizations seeking to do business with the US Federal government. Parsing and gathering knowledge from such lengthy regulation documents is currently done manually and is time and human intensive. Hence, developing a cognitive assistant for automated analysis of such legal documents has become a necessity. We have developed semantically rich approach to automate the analysis of legal documents and have implemented a system to capture various facts and rules contributing towards building an efficient legal knowledge base that contains details of the relationships between various legal elements, semantically similar terminologies, deontic expressions and cross-referenced legal facts and rules. In this paper, we describe our framework along with the results of automating knowledge extraction from the FARS document (Title 48, CFR). Our approach can be used by Big Data Users to automate knowledge extraction from Large Legal documents.", "venue": "2017 IEEE International Conference on Big Data (Big Data)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "871f1562a709cadad79d3ba299d311f7c68e15fa", "url": "https://www.semanticscholar.org/paper/871f1562a709cadad79d3ba299d311f7c68e15fa", "title": "Using Frame Semantics for Knowledge Extraction from Twitter", "abstract": "\n \n Knowledge bases have the potential to advance artificial intelligence, but often suffer from recall problems, i.e., lack of knowledge of new entities and relations. On the contrary, social media such as Twitter provide abundance of data, in a timely manner: information spreads at an incredible pace and is posted long before it makes it into more commonly used resources for knowledge extraction. In this paper we address the question whether we can exploit social media to extract new facts, which may at first seem like finding needles in haystacks. We collect tweets about 60 entities in Freebase and compare four methods to extract binary relation candidates, based on syntactic and semantic parsing and simple mechanism for factuality scoring. The extracted facts are manually evaluated in terms of their correctness and relevance for search. We show that moving from bottom-up syntactic or semantic dependency parsing formalisms to top-down frame-semantic processing improves the robustness of knowledge extraction, producing more intelligible fact candidates of better quality. In order to evaluate the quality of frame semantic parsing on Twitter intrinsically, we make a multiply frame-annotated dataset of tweets publicly available.\n \n", "venue": "AAAI Conference on Artificial Intelligence", "citationCount": 27, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "f424b6029a507c145aba8aef6646228565ec09ca", "url": "https://www.semanticscholar.org/paper/f424b6029a507c145aba8aef6646228565ec09ca", "title": "KEFST: a knowledge extraction framework using finite-state transducers", "abstract": "\nPurpose\nThe purpose of this research study is to extract and identify named entities from Hadith literature. Named entity recognition (NER) refers to the identification of the named entities in a computer readable text having an annotation of categorization tags for information extraction. NER is an active research area in information management and information retrieval systems. NER serves as a baseline for machines to understand the context of a given content and helps in knowledge extraction. Although NER is considered as a solved task in major languages such as English, in languages such as Urdu, NER is still a challenging task. Moreover, NER depends on the language and domain of study; thus, it is gaining the attention of researchers in different domains.\n\n\nDesign/methodology/approach\nThis paper proposes a knowledge extraction framework using finite-state transducers (FSTs) \u2013 KEFST \u2013 to extract the named entities. KEFST consists of five steps: content extraction, tokenization, part of speech tagging, multi-word detection and NER. An extensive empirical analysis using the data corpus of Urdu translation of Sahih Al-Bukhari, a widely known hadith book, reveals that the proposed method effectively recognizes the entities to obtain better results.\n\n\nFindings\nThe significant performance in terms of f-measure, precision and recall validates that the proposed model outperforms the existing methods for NER in the relevant literature.\n\n\nOriginality/value\nThis research is novel in this regard that no previous work is proposed in the Urdu language to extract named entities using FSTs and no previous work is proposed for Urdu hadith data NER.\n", "venue": "Electron. Libr.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "ke", "ke"], "mention_counts": {"ke": 3, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "608088c0135c469812cfb99f6d764bcfa1c5609c", "url": "https://www.semanticscholar.org/paper/608088c0135c469812cfb99f6d764bcfa1c5609c", "title": "Contour tracking based knowledge extraction and object recognition using deep learning neural networks", "abstract": "Object recognition in digital images are carried out using syntactic or spectral domain pattern recognition techniques. Due to ever increasing size of data collected by digital image acquisition systems there is a need to go in for developing faster, reliable and intelligent pattern recognition methods which would mostly supplement human intelligence in recognizing objects which otherwise remain latent and unnoticed. One such effort is use of deep learning neural networks for object recognition. The input to this system is knowledge extracted from the contours of various objects pre valent in a digital image. This paper advocates a novel method for extracting knowledge about the contours of various objects and components in a digital image and for recognizing objects using a neural network.", "venue": "International Conference on Next Generation Computing Technologies", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ke"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "acbfa46c0cf6d95a3ad77a0d696b8527a546f1e6", "url": "https://www.semanticscholar.org/paper/acbfa46c0cf6d95a3ad77a0d696b8527a546f1e6", "title": "Knowledge Extraction Experiment Based on Tourism Knowledge Graph Q & A Data Set", "abstract": "At present, the most natural language processing tasks use common data sets for experiments. However, as the concept of domain knowledge graphs is proposed, domain-based data sets have gradually become a demand. In this article, we collect data from various travel websites and official websites of tourist attractions, and use this to build a question and answer data set. At the same time, we also introduce the current Bert model with outstanding effect in the nlp field, and use this model to conduct experiments in the travelling question and answer data set. The experimental results not only show the feasibility of the constructed tourism data set, but also lay a foundation for the subsequent construction of a knowledge question answering system for tourism knowledge graph.", "venue": "2020 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "kg", "kg", "kg", "ke"], "mention_counts": {"nlp": 2, "kg": 3, "ke": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"kg": 3, "ke": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "29324138bcbb3ce0abde7fc281f4b2e917746b6d", "url": "https://www.semanticscholar.org/paper/29324138bcbb3ce0abde7fc281f4b2e917746b6d", "title": "Using Deep Learning for Extracting User-Generated Knowledge from Web Communities", "abstract": "The interest in user-generated content (UGC) is steadily growing, as it provides a valuable data source for companies to extract information that can be used for competitive advantage. However, mining UGC and extracting knowledge is a costly and labor-intense endeavour. Against this backdrop, the steep advancements in deep learning (DL) during the last years offer the potential to counteract this. However, DL is still in its infancy in the realm of UGC. Thus, we aim at contributing to the field of knowledge extraction of UGC by comparing traditional machine learning (ML) approaches with state-of-the-art DL models (e.g., BERT) for mining usergenerated instructions. We follow the knowledge discovery process to construct a novel corpus of user-generated instructions and develop a best-practice approach to mine knowledge from UGC. Thereby, we intend to foster a better understanding of knowledge extraction systems for UGC and provide a valuable solution to extend existing information systems.", "venue": "ECIS", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ie", "ke", "ke"], "mention_counts": {"ke": 3, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "b7287ef449e765c7dea2175a35fcc028b4b9bdb9", "url": "https://www.semanticscholar.org/paper/b7287ef449e765c7dea2175a35fcc028b4b9bdb9", "title": "Towards Building Open Knowledge Base From Programming Question-Answering Communities", "abstract": "In this paper, we propose the first system, so-called Open Programming Knowledge Extraction (OPKE), to automatically extract knowledge from programming Question-Answering (QA) communities. OPKE is the first step of building a programming-centric knowledge base. Data mining and Natural Language Processing techniques are leveraged to identify duplicate questions and construct structured information. Preliminary evaluation shows the effectiveness of OPKE.", "venue": "SEMWEB", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "ke", "ke", "kg"], "mention_counts": {"nlp": 1, "kg": 2, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"kg": 2, "ke": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "3de2db6e0fd81767599cb23197a3da2a883947f4", "url": "https://www.semanticscholar.org/paper/3de2db6e0fd81767599cb23197a3da2a883947f4", "title": "PERLEX: A Bilingual Persian-English Gold Dataset for Relation Extraction", "abstract": "Relation extraction is the task of extracting semantic relations between entities in a sentence. It is an essential part of some natural language processing tasks such as information extraction, knowledge extraction, and knowledge base population. The main motivations of this research stem from a lack of a dataset for relation extraction in the Persian language as well as the necessity of extracting knowledge from the growing big-data in the Persian language for different applications. In this paper, we present \"PERLEX\" as the first Persian dataset for relation extraction, which is an expert-translated version of the \"Semeval-2010-Task-8\" dataset. Moreover, this paper addresses Persian relation extraction utilizing state-of-the-art language-agnostic algorithms. We employ six different models for relation extraction on the proposed bilingual dataset, including a non-neural model (as the baseline), three neural models, and two deep learning models fed by multilingual-BERT contextual word representations. The experiments result in the maximum f-score 77.66% (provided by BERTEM-MTB method) as the state-of-the-art of relation extraction in the Persian language.", "venue": "Sci. Program.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "kg", "ke", "nlp"], "mention_counts": {"nlp": 1, "ke": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "4665f291020b54f758e47081f00ba4c4014006c9", "url": "https://www.semanticscholar.org/paper/4665f291020b54f758e47081f00ba4c4014006c9", "title": "EKDB&W'05: Workshop on Extraction of Knowledge from Databases and Warehouses", "abstract": "This chapter includes 13 selected papers for the 2005 EKDB&W Extracting Knowledge from Databases and Warehouses workshop which objective was to attract contributions related to methods for non-trivial extraction of information from data. Unsupervised Learning (Clustering, in particular) was addressed in 2 papers referring to the following proposals (i) The use of K-Means algorithm to yield clusters, based on attributes which summarize TV watching temporal behaviour; (ii) A weighted combination of the results of multiple clustering procedures. Supervised Learning was addressed in 6 papers including: (i) A new approach to evaluate the trade-off between redundancy and learning investment on the binary coding of multi-class problems' output (ECOCS); (ii) A two-stage approach for the discretization of continuous attributes which specifically provided means to evaluate incremental learning in classification problems (iii) The comparison of several supervised techniques in order to predict algae blooms.(iv) A LVQ network trained to discriminate classes of breast tissue. (v) A new evolutionary algorithm to train realistic ANN's (vi) Rough sets to generate decision rules from biometric databases which eventually helped to reduce the load of a verification system. Market Basket Analysis was addressed in 2 papers: (i) The generalization of Association Rules using a new algorithm GART based on the use of items' taxonomies; (ii) A new approach for finding the next-item for each customer in large database marketing presented a new solution to improve automatic cross-selling strategies. Finally, 3 papers addressed the issue of data and knowledge extraction dealing directly with databases and data warehouses gathering the following contributions: (i) The concept of staging schema mappings which helped to solve issues of consistency and integrity when uploading small and medium sized data repositories into a data warehouse; (ii) A flexible, and yet inexpensive, metadata repository based on standard XML technologies that eased the task of managing an Information System. (iii) A new tool to explore large amounts of spacial data with On-Line Analytical Processing techniques. Application domains were very diverse and illustrated the practical utility of the presented methodologies. Data originating from TV Audiometer systems, Retail, Water metrics, Biometric and Medical databases were considered. The EKDB&W Workshop would not have been possible without the contribution of Authors, Program Committee members and EPIA 2005 Organizers. All deserve our thanks and appreciation.", "venue": "2005 portuguese conference on artificial intelligence", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ke", "ke", "ke"], "mention_counts": {"ke": 3, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "e24d0ed38b08b5542d8a0ca5cee3ebdfdcad31d2", "url": "https://www.semanticscholar.org/paper/e24d0ed38b08b5542d8a0ca5cee3ebdfdcad31d2", "title": "Towards Knowledge Enhanced Language Model for Machine Reading Comprehension", "abstract": "Machine reading comprehension is a crucial and challenging task in natural language processing (NLP). Recently, knowledge graph (KG) embedding has gained massive attention as it can effectively provide side information for downstream tasks. However, most previous knowledge-based models do not take into account the structural characteristics of the triples in KGs, and only convert them into vector representations for direct accumulation, leading to deficiencies in knowledge extraction and knowledge fusion. In order to alleviate this problem, we propose a novel deep model KCF-NET, which incorporates knowledge graph representations with context as the basis for predicting answers by leveraging capsule network to encode the intrinsic spatial relationship in triples of KG. In KCF-NET, we fine-tune BERT, a highly performance contextual language representation model, to capture complex linguistic phenomena. Besides, a novel fusion structure based on multi-head attention mechanism is designed to balance the weight of knowledge and context. To evaluate the knowledge expression and reading comprehension ability of our model, we conducted extensive experiments on multiple public datasets such as WN11, FB13, SemEval-2010 Task 8 and SQuAD. Experimental results show that KCF-NET achieves state-of-the-art results in both link prediction and MRC tasks with negligible parameter increase compared to BERT-Base, and gets competitive results in triple classification task with significantly reduced model size.", "venue": "IEEE Access", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "ke", "kg", "kg", "nlp"], "mention_counts": {"nlp": 2, "kg": 3, "ke": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"kg": 3, "ke": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "686f51d2b291268c86c68129285211160c1c4886", "url": "https://www.semanticscholar.org/paper/686f51d2b291268c86c68129285211160c1c4886", "title": "Coupled Semi-Supervised Learning for Chinese Knowledge Extraction", "abstract": "Robust intelligent systems may leverage knowledge about the world to cope with a variety of contexts. While automatic knowledge extraction algorithms have been successfully used to build knowledge bases in En-glish, little progress has been made in extracting non- alphabetic languages, e.g. Chinese. This paper iden-ti\ufb01es the key challenge in instance and pattern ex- traction for Chinese and presents the Coupled Chinese Pattern Learner that utilizes part-of-speech tagging and language-dependent grammar rules for gener- alized matching in the Chinese never-ending language learner framework for large-scale knowledge extraction from online documents. Experiments showed that the proposed system is scalable and achieves a precision of 79 . 9% in learning categories after a small number of iterations.", "venue": "AAAI Workshop: Knowledge Extraction from Text", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ke"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "9c2511f90acf52ae436ea81177476b23b7409728", "url": "https://www.semanticscholar.org/paper/9c2511f90acf52ae436ea81177476b23b7409728", "title": "A Korean Knowledge Extraction System for Enriching a KBox", "abstract": "The increased demand for structured knowledge has created considerable interest in knowledge extraction from natural language sentences. This study presents a new Korean knowledge extraction system and web interface for enriching a KBox knowledge base that expands based on the Korean DBpedia. The aim is to create an endpoint where knowledge can be extracted and added to KBox anytime and anywhere.", "venue": "International Conference on Computational Linguistics", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "119a5d9902a67e15e6e62d068459ad26fe749f41", "url": "https://www.semanticscholar.org/paper/119a5d9902a67e15e6e62d068459ad26fe749f41", "title": "Knowledge Extraction from C-Code", "abstract": "In this paper we present first ideas for extracting knowledge from C source code of control programs. The extracted knowledge is intended to be used in our smart control engine which takes a rule set and decides which rules to use based on the internal and environmental conditions. The extraction of rules is based on the control-flow graph of the supplied C program: Basically, our method extracts rules that correspond to paths to given high-level function calls. The advantage of this method is to get a first knowledge-base from available source code which makes using a smart control engine more applicable for industry. We use an industrial control program as example within the paper in order to justify the usefulness of our approach.", "venue": "2007 Fifth Workshop on Intelligent Solutions in Embedded Systems", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 1, "ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "fb4de23525a425c673c719771cd6593499843213", "url": "https://www.semanticscholar.org/paper/fb4de23525a425c673c719771cd6593499843213", "title": "A Novel End-to-End Multiple Tagging Model for Knowledge Extraction", "abstract": "It is an emerging research topic in NLP to joint extraction of knowledge including entities and relations from unstructured text and representing them as meaningful triplets. Despite significant progresses made by recent deep neural network based solutions, these methods still confront the overlapping issue that different relational triplets may have overlapped entities in a sentence, and it is troublesome to address this issue by current solutions. In this paper, we propose a novel multiple tagging model to address the overlapping issue and extract knowledge from unstructured text. Specifically, we devise a multiple tagging scheme that transforms the problem of joint entity and relation extraction into a multiple sequence tagging problem. By using GRU as the building block for encoding-decoding, the proposed model is capable of handling the triplet overlapping problem because the decoder layer allows one entity to take part in more than one triplet. The whole network is end-to-end trianable and outputs all triplets in a sentence directly. Experimental results on the NYT and KBP benchmarks demonstrate that the proposed model siginificantly improves the recall of triplet, and consequently, achieving the new state-of-the-art in the task of triplet extraction on both datasets.", "venue": "2019 International Joint Conference on Neural Networks (IJCNN)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "nlp", "ke"], "mention_counts": {"nlp": 1, "ke": 3}, "nlp_mention_counts": {"nlp": 1, "ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "2ee39a9a7d7186a351f2170ce7ebd61f8264a860", "url": "https://www.semanticscholar.org/paper/2ee39a9a7d7186a351f2170ce7ebd61f8264a860", "title": "Representing Knowledge by Spans: A Knowledge-Enhanced Model for Information Extraction", "abstract": "Knowledge-enhanced pre-trained models for language representation have been shown to be more effective in knowledge base construction tasks (i.e., relation extraction) than language models such as BERT. These knowledge-enhanced language models incorporate knowledge into pre-training to generate representations of entities or relationships. However, existing methods typically represent each entity with a separate embedding. As a result, these methods struggle to represent out-of-vocabulary entities and a large amount of parameters, on top of their underlying token models (i.e., the transformer), must be used and the number of entities that can be handled is limited in practice due to memory constraints. Moreover, existing models still struggle to represent entities and relationships simultaneously. To address these problems, we propose a new pre-trained model that learns representations of both entities and relationships from token spans and span pairs in the text respectively. By encoding spans efficiently with span modules, our model can represent both entities and their relationships but requires fewer parameters than existing models. We pre-trained our model with the knowledge graph extracted from Wikipedia and test it on a broad range of supervised and unsupervised information extraction tasks. Results show that our model learns better representations for both entities and relationships than baselines, while in supervised settings, fine-tuning our model outperforms RoBERTa consistently and achieves competitive results on information extraction tasks.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "kg", "ie", "ke", "kg", "ie"], "mention_counts": {"kg": 2, "ke": 1, "ie": 3}, "nlp_mention_counts": {"ke": 1, "ie": 3}, "ld_mention_counts": {"kg": 2, "ke": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "a2900b21897ca70e02afee1a94550c9b6a15dcb4", "url": "https://www.semanticscholar.org/paper/a2900b21897ca70e02afee1a94550c9b6a15dcb4", "title": "Probabilistic Task Content Modeling for Episodic Textual Narratives", "abstract": "Episodic knowledge is often stored in the form of textual narratives written in natural language. However, a large repository of such narratives will contain both repetitive and novel knowledge. In this paper, we propose an approach for discovering interesting pieces of knowledge by using a priori task knowledge. By considering the narratives as generated by an underlying task structure, the elements of the task can be regarded as topics that generate the text. Then, by capturing task content in a probabilistic model, the model can be used, e.g., to identify the semantic orientation of textual phrases. An evaluation for a real world corpus of episodic narratives provides strong evidence for the feasibility of the proposed approach. Episodic Textual Narratives Discussing the use of knowledge in knowledge systems, (Richter 1998) distinguishes among three types of knowledge: background, contextual, and episodic knowledge. Out of all types of knowledge systems, episodic knowledge\u2014 which is of narrative character, because it tells the story of something that happened in the past\u2014is directly employed in Case-Based Reasoning (CBR) systems only. Since episodic knowledge has a narrative character, a natural means of preserving it is in the form of textual narratives written in natural language by human users. Extraction of valuable pieces of knowledge from such narratives, which can serve as cases in the context of a Textual CBR (TCBR) system, is the focus of our research. A common way of extracting knowledge from text documents in the context of a TCBR system is by considering a priori domain knowledge (Lenz 1999). The underlying idea is that text can be regarded as a container of domain objects (or information entities) and by using different types of knowledge acquisition, text documents can be reduced to a set of such information entities. In contrast, we take an alternative perspective. We consider text as generated by an underlying process, which consists of a series of related events. Domain objects are then participants of such events. By recognizing events and their participants in text, it is not only possible to discover domain objects, but also their respective roles in the event. In this way, text will not be a Copyright c \u00a9 2007, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. mere set of information entities, but a network of interconnected, semantically labeled entities. Additionally, a second difference to (Lenz 1999)\u2019s approach is that instead of a priori domain knowledge, we use a priori task knowledge to perform knowledge extraction. Task Knowledge: An Example Consider the task MONITOR-and-DIAGNOSE, a task that starts with the monitoring of an object and continues with diagnosis only if something problematic is observed during the monitoring step. In general, there is a distinct temporal order in the way the MONITOR-and-DIAGNOSE task is performed. 1. Some entities of interest are observed and the respective findings are noticed. 2. These findings are then explained and evaluated. 3. If findings are evaluated as negative, actions for maintenance are recommended. These three steps can be referred to as observation (OBS), explanation (EXP), and take action (ACT), and can be regarded as events that occur during the execution of the MONITOR-and-DIAGNOSE task. In the same way in which these events occur in reality, they will be described in written form, too. Each of the events is constituted by relations between different elements of the task. For example, an OBS relates an observed object to a finding, or an EXP relates a symptom to a possible cause. In our previous research, we have described an active learning approach (named LARC) that learns to annotate episodic narratives with task knowledge roles such as observed object, finding, cause, etc. (Mustafaraj, Hoof, & Freisleben 2006). Probabilistic Task Content Modeling The previous description of a task as a series of interconnected events and participant roles constitutes an abstract model of task structure. An instantiation of the task structure in a real situation produces the task content. The instantiation consists of the verbalization of the abstract events and roles with concrete sentences and phrases. Because realworld events and natural language are of stochastic nature, a probabilistic model is an appropriate means for capturing task content.", "venue": "FLAIRS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "4f152e62986f887f252b8b51859f2297ca1296f2", "url": "https://www.semanticscholar.org/paper/4f152e62986f887f252b8b51859f2297ca1296f2", "title": "Ontology learning from Italian legal texts", "abstract": "The paper reports on the methodology and preliminary results of a case study in automatically extracting ontological knowledge from Italian legislative texts. We use a fully--implemented ontology learning system (T2K) that includes a battery of tools for Natural Language Processing (NLP), statistical text analysis and machine language learning. Tools are dynamically integrated to provide an incremental representation of the content of vast repositories of unstructured documents. Evaluated results, however preliminary, show the great potential of NLP--powered incremental systems like T2K for accurate large--scale semi--automatic extraction of legal ontologies.", "venue": "Law, Ontologies and the Semantic Web", "citationCount": 35, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "onto", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "0b556bf90622f059dcc5667156127e85e1af4c49", "url": "https://www.semanticscholar.org/paper/0b556bf90622f059dcc5667156127e85e1af4c49", "title": "Enhancing Question Answering by Injecting Ontological Knowledge through Regularization", "abstract": "Deep neural networks have demonstrated high performance on many natural language processing (NLP) tasks that can be answered directly from text, and have struggled to solve NLP tasks requiring external (e.g., world) knowledge. In this paper, we present OSCR (Ontology-based Semantic Composition Regularization), a method for injecting task-agnostic knowledge from an Ontology or knowledge graph into a neural network during pre-training. We evaluated the performance of BERT pre-trained on Wikipedia with and without OSCR by measuring the performance when fine-tuning on two question answering tasks involving world knowledge and causal reasoning and one requiring domain (healthcare) knowledge and obtained 33.3%, 18.6%, and 4% improved accuracy compared to pre-training BERT without OSCR.", "venue": "Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out", "citationCount": 3, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["kg", "nlp", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 3, "kg": 1, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 1, "onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "dea3e3f809e7ec2ec6ac5346f11e67c77987c0cd", "url": "https://www.semanticscholar.org/paper/dea3e3f809e7ec2ec6ac5346f11e67c77987c0cd", "title": "Knowledge Extraction from Learning Traces in Continuous Domains", "abstract": "A method is introduced to extract and transfer knowledge between a source and a target task in continuous domains and for direct policy search algorithms. The principle is (1) to use a direct policy search on the source task, (2) extract knowledge from the learning traces and (3) transfer this knowledge with a reward shaping approach. The knowledge extraction process consists in analyzing the learning traces, i.e. the behaviors explored while learning on the source task, to identify the behavioral features specific to successful solutions. Each behavioral feature is then attributed a value corresponding to the average reward obtained by the individuals exhibiting it. These values are used to shape rewards while learning on a target task. The approach is tested on a simulated ball collecting task in a continuous arena. The behavior of an individual is analyzed with the help of the generated knowledge bases.", "venue": "AAAI Fall Symposia", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "ke", "ke"], "mention_counts": {"kg": 1, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 1, "ke": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "d5010c762c8dea4c55aeb5e9fd7b3675aa7480df", "url": "https://www.semanticscholar.org/paper/d5010c762c8dea4c55aeb5e9fd7b3675aa7480df", "title": "KB-NLG: From Knowledge Base to Natural Language Generation", "abstract": "We perform the natural language generation (NLG) task by mapping sets of Resource Description Framework (RDF) triples into text. First we investigate the impact of increasing the number of entity types in delexicalisaiton on the generation quality. Second we conduct different experiments to evaluate two widely applied language generation systems, encoder-decoder with attention and the Transformer model on a large benchmark dataset. We evaluate different models on automatic metrics, as well as the training time. To our knowledge, we are the first to apply Transformer model to this task.", "venue": "WNLP@ACL", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "rdf", "nlg", "nlg", "kg", "nlg", "rdf"], "mention_counts": {"kg": 1, "nlg": 4, "rdf": 2}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"kg": 1, "rdf": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "223b0cc07f7fae7d4e17ea3227a6006ff6e0d7fe", "url": "https://www.semanticscholar.org/paper/223b0cc07f7fae7d4e17ea3227a6006ff6e0d7fe", "title": "Translating Domain-Specific Expressions in Knowledge Bases with Neural Machine Translation", "abstract": "Our work presented in this paper focuses on the translation of domain-specific expressions represented in semantically structured resources, like ontologies or knowledge graphs. To make knowledge accessible beyond language borders, these resources need to be translated into different languages. The challenge of translating labels or terminological expressions represented in ontologies lies in the highly specific vocabulary and the lack of contextual information, which can guide a machine translation system to translate ambiguous words into the targeted domain. Due to the challenges, we train and translate the terminological expressions in the medial and financial domain with statistical as well as with neural machine translation methods. We evaluate the translation quality of domainspecific expressions with translation systems trained on a generic dataset and experiment domain adaptation with terminological expressions. Furthermore we perform experiments on the injection of external knowledge into the translation systems. Through these experiments, we observed a clear advantage in domain adaptation and terminology injection of NMT methods over SMT. Nevertheless, through the specific and unique terminological expressions, subword segmentation within NMT does not outperform a word based neural translation model.", "venue": "ArXiv", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "onto", "mt", "kg", "kg", "mt"], "mention_counts": {"kg": 2, "onto": 2, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "1f6c3532cc16467b2a9aecb679c358d4589cf04f", "url": "https://www.semanticscholar.org/paper/1f6c3532cc16467b2a9aecb679c358d4589cf04f", "title": "The Multilingual Procedural Semantic Web", "abstract": "The stated goal of the Semantic Web community is to turn the Web into a richly annotated resource, making its content more amenable to applications that involve machine reasoning. The most widely discussed language-oriented aspect of this vision involves the creation and use of an inventory of markup tags that indicate select semantic types. So, the \u201csemantics\u201d of the Semantic Web is not the semantics of full texts or even full sentences, but rather of select elements of text and extra-textual information. Moreover, the annotations are expected to be largely carried out manually, so broad coverage is unlikely, as are consistency and universal public-spiritedness on the part of annotators (cf. Doctorow, no date). Compare this to the ideal semantic web, which would be automatically generated from the unadorned web by processors that would carry out lexical disambiguation, referential disambiguation, and the interpretation of textual implicatures, such as the recognition of irony and indirect speech acts. Such full semantic interpretations of web content would serve as optimal input for machine reasoners. It is common practice in the field of AI to assume the availability of such knowledge structures \u2013 in fact, practically all work on machine reasoning over the past decades has used hand-crafted, complete, unambiguous knowledge structures as input. How that could be achieved automatically was always considered a separate issue, delegated to the NLP community. The NLP community, however, by and large abandoned the task of deep semantic analysis some 20 years ago, opting to pursue either (a) knowledge lean, \u201clowhanging fruit\u201d tasks that contribute to the configuration of better NLP applications in the near term but do not contribute to the ultimate goal of automatic text understanding or (b) method-oriented work, in which the methods themselves are of first priority and natural language serves primarily as a source of data sets. 1", "venue": "MSW", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "nlp", "nlp", "sw", "sw", "nlp"], "mention_counts": {"nlp": 3, "sw": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "d50a4877f02762a8c0e3ec6a2619575f2e759c3c", "url": "https://www.semanticscholar.org/paper/d50a4877f02762a8c0e3ec6a2619575f2e759c3c", "title": "An Ontology-Based Approach to Natural Language Generation from Coded Data in Electronic Health Records", "abstract": "The worldwide adoption of the HL7 Clinical Document Architecture (CDA) is promoting the availability of coded data (CDA entries) within sections of clinical documents. At the moment, an increasing number of studies are investigating ways to transform the narratives of CDA documents into machine process able CDA entries. This paper addresses the reverse problem, i.e. obtaining linguistic representations (sentences) from CDA entries. The approach presented employs Natural Language Generation (NLG) techniques and deals with two major tasks: content selection and content expression. The current research proposes a formal semantic representation of CDA entries and investigates how expressive domain ontologies in OWL and SPARQL SELECT queries can contribute to NLG. To validate the proposal, the study has focused on CDA entries from the History of Present Illness sections of CDA consultation notes. The results obtained are encouraging, as the clinical narratives automatically generated from these CDA entries fulfil the clinicians' expectations.", "venue": "2011 UKSim 5th European Symposium on Computer Modeling and Simulation", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "nlg", "onto", "onto", "onto", "nlg", "nlg"], "mention_counts": {"nlg": 4, "onto": 3}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "448d72e3059fc7cc2852b26236e5b11acf1ca898", "url": "https://www.semanticscholar.org/paper/448d72e3059fc7cc2852b26236e5b11acf1ca898", "title": "A Repository of Frame Instance Lexicalizations for Generation", "abstract": "Robust, statistical Natural Language Generation from Web knowledge bases is hindered by the lack of text-aligned resources. We aim to fill this gap by presenting a method for extracting knowledge from natural language text, and encode it in a format based on frame semantics and ready to be distributed in the Linked Open Data space. We run an implementation of such methodology on a collection of short documents and build a repository of frame instances equipped with fine-grained lex-icalizations. Finally, we conduct a pilot stody to investigate the feasibility of an approach to NLG based on said resource. We perform error analysis to assess the quality of the resource and manually evaluate the output of the NLG prototype.", "venue": "WebNLG", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["lod", "kg", "ke", "nlg", "nlg", "nlg"], "mention_counts": {"kg": 1, "lod": 1, "nlg": 3, "ke": 1}, "nlp_mention_counts": {"ke": 1, "nlg": 3}, "ld_mention_counts": {"kg": 1, "lod": 1, "ke": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "4b28157e005f885cb91cb582cdd965f08372f731", "url": "https://www.semanticscholar.org/paper/4b28157e005f885cb91cb582cdd965f08372f731", "title": "Integration of Neuroimaging and Microarray Datasets through Mapping and Model-Theoretic Semantic Decomposition of Unstructured Phenotypes", "abstract": "An approach towards heterogeneous neuroscience dataset integration is proposed that uses Natural Language Processing (NLP) and a knowledge-based phenotype organizer system (PhenOS) to link ontology-anchored terms to underlying data from each database, and then maps these terms based on a computable model of disease (SNOMED CT\u00ae). The approach was implemented using sample datasets from fMRIDC, GEO, The Whole Brain Atlas and Neuronames, and allowed for complex queries such as \u201cList all disorders with a finding site of brain region X, and then find the semantically related references in all participating databases based on the ontological model of the disease or its anatomical and morphological attributes\u201d. Precision of the NLP-derived coding of the unstructured phenotypes in each dataset was 88% (n = 50), and precision of the semantic mapping between these terms across datasets was 98% (n = 100). To our knowledge, this is the first example of the use of both semantic decomposition of disease relationships and hierarchical information found in ontologies to integrate heterogeneous phenotypes across clinical and molecular datasets.", "venue": "Summit on translational bioinformatics", "citationCount": 1, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "onto", "kg", "nlp", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 3, "kg": 1, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 1, "onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "798489d2b0d3619246e95c0b553133183f35186a", "url": "https://www.semanticscholar.org/paper/798489d2b0d3619246e95c0b553133183f35186a", "title": "Business Insights Using Knowledge Graphs by Text Analytics in Dynamic Environments", "abstract": "Business Intelligence (BI) requires the collection and organization of important pieces of information (i.e. entities) from multiple sources to provide valuable insights (e.g. business trends) as events (i.e. a specific happening linked with a specific location and time) to users. Online news articles are one of the important information sources that present business news offered by various companies in the market every day all around the world. These news articles often cover the same events and report redundant information. Existing news platforms aim at collecting the key entities from news articles and providing a mechanism to view the latest and relevant business events based on user interest. However, they do not provide a method to model business events and understand them temporally, spatially, and contextually (i.e. changes in the event). For instance, it is crucial to know for how long a business event has been active? How important is its evolution locally, or worldwide? Or how did different companies come up with this event as competitors in the market? The contribution of this research is the exploration of the possibilities of modeling spatial, temporal, and contextual information evolution related to business events through the application of knowledge graphs and text analytics, more specifically, Natural Language Processing (NLP) methods. The constructed knowledge graphs through Named-Entity Recognition (NER), i.e., an NLP technique, present a compact news representation that tells the key entities of the business event at one glance using linked open data concepts. It enables the assessment of other related news events as well as provides the means for analysis of the influence and evolution of business events.", "venue": "International ACM Conference on Management of Emergent Digital EcoSystems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "lod", "kg", "kg", "nlp", "nlp"], "mention_counts": {"nlp": 3, "kg": 3, "lod": 1}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 3, "lod": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "01fd5d3a300b6b8f8f2478d0d5e5250b1a851df0", "url": "https://www.semanticscholar.org/paper/01fd5d3a300b6b8f8f2478d0d5e5250b1a851df0", "title": "The lexical properties of the gene ontology", "abstract": "The Gene Ontology (GO) is a construct developed for the purpose of annotating molecular information about genes and their products. The ontology is a shared resource developed by the GO Consortium, a group of scientists who work on a variety of model organisms. In this paper we investigate the nature of the strings found in the Gene Ontology and evaluate them for their usefulness in natural language processing (NLP). We extend previous work that identified a set of properties that reliably identifies natural language phrases in the Unified Medical Language System (UMLS). The results indicate that a large percentage (79%) of GO terms are potentially useful for NLP applications. Some 35% of the GO terms were found in a corpus derived from the MEDLINE bibliographic database, and 27% of the terms were found in the current edition of the UMLS.", "venue": "American Medical Informatics Association Annual Symposium", "citationCount": 49, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlp", "onto", "nlp", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "9a36f572553dd3e1c69ff16dd694ad2f07a55a13", "url": "https://www.semanticscholar.org/paper/9a36f572553dd3e1c69ff16dd694ad2f07a55a13", "title": "Semantic Restructuring of Natural Language Image Captions to Enhance Image Retrieval", "abstract": "The rapid growth in the volume of visual information can make the task of finding and accessing visual information of interest, overwhelming for users. Semantic analysis of image captions can be used in conjunction with image retrieval systems (IMR) to retrieve selected images more precisely. To do this, we first exploit a Natural Language Processing (NLP) framework in order to extract concepts from image captions. Next, an ontology-based framework is deployed in order to resolve natural language ambiguities. The novelty of the proposed framework is that the combination of LSI with the Ontology framework enables the combined framework to tolerate ambiguities and variations in the Ontology. A key feature is that the system can find indirectly relevant concepts in image captions and thus leverage these to represent the semantics of images at a higher level. Experimental results show that the use of LSI based NLP combined with an ontological framework significantly enhances image retrieval.", "venue": "J. Multim.", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "7d02d1e7e1b65cf2877d8167360c8a8881617574", "url": "https://www.semanticscholar.org/paper/7d02d1e7e1b65cf2877d8167360c8a8881617574", "title": "Coupling Ontology Driven Semantic Representation with Multilingual Natural Language Generation for Tuning International Terminologies", "abstract": "OBJECTIVES\nThe importance of clinical communication between providers, consumers and others, as well as the requisite for computer interoperability, strengthens the need for sharing common accepted terminologies. Under the directives of the World Health Organization (WHO), an approach is currently being conducted in Australia to adopt a standardized terminology for medical procedures that is intended to become an international reference.\n\n\nMETHOD\nIn order to achieve such a standard, a collaborative approach is adopted, in line with the successful experiment conducted for the development of the new French coding system CCAM. Different coding centres are involved in setting up a semantic representation of each term using a formal ontological structure expressed through a logic-based representation language. From this language-independent representation, multilingual natural language generation (NLG) is performed to produce noun phrases in various languages that are further compared for consistency with the original terms.\n\n\nRESULTS\nOutcomes are presented for the assessment of the International Classification of Health Interventions (ICHI) and its translation into Portuguese. The initial results clearly emphasize the feasibility and cost-effectiveness of the proposed method for handling both a different classification and an additional language.\n\n\nCONCLUSION\nNLG tools, based on ontology driven semantic representation, facilitate the discovery of ambiguous and inconsistent terms, and, as such, should be promoted for establishing coherent international terminologies.", "venue": "MedInfo", "citationCount": 2, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["nlg", "nlg", "nlg", "onto", "onto", "nlg", "onto"], "mention_counts": {"nlg": 4, "onto": 3}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "1667ba6154ed69d8870ad27ea2db1fba92f063af", "url": "https://www.semanticscholar.org/paper/1667ba6154ed69d8870ad27ea2db1fba92f063af", "title": "Construcci\u00f3n de una base de conocimiento l\u00e9xico multil\u00edng\u00fce de amplia cobertura: Multilingual Central Repository", "abstract": "The use of wide coverage and general domain semantic resources has become a common practice and often necesary by existing systems Natural Language Processing (NLP). WordNet is by far the most widely used semantic resource in NLP. Following the success of WordNet, the EuroWordNet project has designed a multilingual semantic infrastructure to develop wordnets for a set of European languages. In EuroWordNet, these wordnets are interconnected with links stored in the Inter-Lingual Index (ILI). Following the EuroWordNet architecture, the MEANING project has developed the first versions of Multilingual Central Repository (MCR) using WordNet 1.6 as ILI. Thus, maintaining the compatibility between wordnets of different languages \u200b\u200band versions. This version of the MCR integrates six different versions of the English WordNet (1.6 to 3.0) and wordnets in Spanish, Catalan, Basque and Italian, along with more than a million semantic relationships between concepts and semantic properties different ontologies. We recently developed a new version of MCR using WordNet 3.0 as ILI. This new version of the MCR integrates wordnets of five different languages: English, Spanish, Catalan, Basque and Galician. The current version of MCR, like the previous one, systematically integrates thousands of semantic relations between concepts. In addition, the MCR is enriched with about 460,000 semantic and ontological properties including Base Level Concepts, Top Ontology, WordNet Domains and AdimenSUMO, providing all ontological consistency the integrated semantic wordnets and resources on it.", "venue": "Linguam\u00e1tica", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "2c13b0d69a5a1c05d231ce3eeb45a95cc42976ec", "url": "https://www.semanticscholar.org/paper/2c13b0d69a5a1c05d231ce3eeb45a95cc42976ec", "title": "An NLP-based ontology population for a risk management generic structure", "abstract": "In this paper we propose an NLP-based Ontology Population approach for a Generic Structure instantiation from natural language texts, in the domain of Risk Management. The approach is semi-automatic and based on combined NLP techniques using domain expert intervention for control and validation. It relies on the predicative power of verbs in the instantiation process. It is not domain dependent since it heavily relies on linguistic knowledge.\n We demonstrate the effectiveness of our method on the ontology of the PRIMA project (supported by the European community) and we populate this generic domain ontology via an available corpus. A first validation of the approach is done through an experiment with Chemical Fact Sheets from Environmental Protection Agency.", "venue": "International Conference on Soft Computing as Transdisciplinary Science and Technology", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "6a714159fd367d904309544632d7b4633d9be119", "url": "https://www.semanticscholar.org/paper/6a714159fd367d904309544632d7b4633d9be119", "title": "Information retrieval in folktales using natural language processing", "abstract": "Our aim is to extract information about literary characters in unstructured texts. We employ natural language processing and reasoning on domain ontologies. The first task is to identify the main characters and the parts of the story where these characters are described or act. We illustrate the system in a scenario in the folktale domain. The system relies on a folktale ontology that we have developed based on Propp's model for folktales morphology. Index Terms\u2014Natural language processing, ontologies, literary character, folktales.", "venue": "ArXiv", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 3, "onto": 3, "ie": 1}, "nlp_mention_counts": {"nlp": 3, "ie": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "4e223a0864b4f79b21a3b21c2575a589d7c02975", "url": "https://www.semanticscholar.org/paper/4e223a0864b4f79b21a3b21c2575a589d7c02975", "title": "An Ontological Metro Accident Case Retrieval Using CBR and NLP", "abstract": "Metro accidents are apt to cause serious consequences, such as casualties or heavy economic loss. Once accidents occur, quick and accurate decision-making is essential to prevent emergent accidents from getting worse, which remains a challenge due to the lack of efficient knowledge representation and retrieval. In this research, an ontological method that integrates case-based reasoning (CBR) and natural language processing (NLP) techniques was proposed for metro accident case retrieval. An ontological model was developed to formalize the representation of metro accident knowledge, and then, the CBR aimed to retrieve similar past cases for supporting decision-making after the accident cases were annotated by the NLP technique. Rule-based reasoning (RBR), as a complementary of CBR, was used to decide the appropriate measures based on those that are recorded in regulations, such as emergency plans. A total of 120 metro accident cases were extracted from the safety monthly reports during metro operations and then built into the case library. The proposed method was tested in MyCBR and evaluated by expert reviews, which had an average precision of 91%.", "venue": "Applied Sciences", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "onto", "onto", "nlp", "onto"], "mention_counts": {"nlp": 4, "onto": 3}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "02dcfe30ec70f75b8471fc255c94d8f1ed671c10", "url": "https://www.semanticscholar.org/paper/02dcfe30ec70f75b8471fc255c94d8f1ed671c10", "title": "Knowledge-based best of breed approach for automated detection of clinical events based on German free text digital hospital discharge letters", "abstract": "Objectives The secondary use of medical data contained in electronic medical records, such as hospital discharge letters, is a valuable resource for the improvement of clinical care (e.g. in terms of medication safety) or for research purposes. However, the automated processing and analysis of medical free text still poses a huge challenge to available natural language processing (NLP) systems. The aim of this study was to implement a knowledge-based best of breed approach, combining a terminology server with integrated ontology, a NLP pipeline and a rules engine. Methods We tested the performance of this approach in a use case. The clinical event of interest was the particular drug-disease interaction \u201cproton-pump inhibitor [PPI] use and osteoporosis\u201d. Cases were to be identified based on free text digital discharge letters as source of information. Automated detection was validated against a gold standard. Results Precision of recognition of osteoporosis was 94.19%, and recall was 97.45%. PPIs were detected with 100% precision and 97.97% recall. The F-score for the detection of the given drug-disease-interaction was 96,13%. Conclusion We could show that our approach of combining a NLP pipeline, a terminology server, and a rules engine for the purpose of automated detection of clinical events such as drug-disease interactions from free text digital hospital discharge letters was effective. There is huge potential for the implementation in clinical and research contexts, as this approach enables analyses of very high numbers of medical free text documents within a short time period.", "venue": "PLoS ONE", "citationCount": 10, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["nlp", "nlp", "onto", "nlp", "nlp", "kg", "kg"], "mention_counts": {"nlp": 4, "onto": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "9109aeeb4225fb438f3dcd4403a5e80087c511ee", "url": "https://www.semanticscholar.org/paper/9109aeeb4225fb438f3dcd4403a5e80087c511ee", "title": "Domain Experts and Natural language Processing in the Evaluation of Circular Economy Business Model Ontology", "abstract": "There are efforts been made globally, to switch from the take-make-dispose linear economy to Circular Economy (CE) due to the effects and wasteful nature of linear economic model. However, there are challenges and barriers that are keeping businesses from this transition despite the benefits. One of such challenge is uncertainty regarding the residual value of used products which often lead to incentives misalignment among others. While research efforts are being made to proffer solutions by proposing circular business models, the question of how reliable the new models are remains unanswered. In this paper, we propose a hybrid model of domain experts and Natural Language Processing (NLP) techniques for evaluating and validating CE Business Model (CEBM) ontology. Textual data on competency questions and comments is collected from domain experts and used in the proposed model. The model identified gaps in the ontology used here, calling for further work on the ontology and the result appear satisfactory to the domain experts who participated.", "venue": "2021 IEEE 15th International Conference on Semantic Computing (ICSC)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 3, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.7007781224516946}, {"paperId": "4b002556fd2a95beb50024be18379acd322632d3", "url": "https://www.semanticscholar.org/paper/4b002556fd2a95beb50024be18379acd322632d3", "title": "Cognitive Computing based Question-Answering System for Teaching Electrical Motor Concepts", "abstract": "Today is the era of \u201dBig Data\u201d, and one has to spend ample amount of time to extract the meaningful information from such a huge store of data. It leads towards such a question answering system which can offer exact and precise answers to user queries. For that there is a requirement of understanding user queries effectively. Thus this paper proposes a cognitive computing powered question answering system in the field of education, which posses the power of Natural Language Processing (NLP). Here, cognitive computing provides the methods for synergism of several powers into a single architecture, NLP provides understanding of the user questions effectively, and Ontology endows with the techniques for the construction of robust knowledge base. So for the realistic implementation of the proposed architecture, the education domain has chosen and will be teaching electrical motor concepts to the edification of the students. General Terms Question-Answering System, NLP, Ontology.", "venue": "International Journal of Computer Applications", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "nlp", "onto", "nlp", "nlp", "nlp"], "mention_counts": {"nlp": 4, "kg": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "0389b12b6f752cb060652f0f05b83612db8f9788", "url": "https://www.semanticscholar.org/paper/0389b12b6f752cb060652f0f05b83612db8f9788", "title": "Knowledge-leveraged Computational Thinking through Natural Language Processing and Statistical Logic (NII Shonan Meeting 2011-4)", "abstract": "This talk describes a recent effort on the development of a textual entailment data set. Rather than assuming a sub-component of applications like question answering and multi-document summarization, we focus on a realworld task to judge whether a natural language proposition is true or false according to a given text. I will describe the design of resource development and features of the obtained resource. Unsupervised Semantic Parsing Pedro Domingos, Washington University Extracting knowledge from text has long been a goal of AI and NLP, but progress has been difficult. Manual approaches are too brittle, and supervised learning ones require an unrealistic quantity and quality of labeled data. To address this problem, we have recently developed the first unsupervised approach to semantic parsing (i.e., translating raw text into a formal meaning representation). It is based on Markov logic, which combines Markov networks and first-order logic. Our USP system transforms dependency trees into quasilogical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The most probable semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We have evaluated our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP greatly outperforms previous approaches on this task. (Joint work with Hoifung Poon.) A general MCMC method for logic-based probabilistic modeling Taisuke Sato, Titech I present a general MCMC method for PRISM, a logic-based probabilistic modeling language. It is a generalization of an MCMC method for PCFGs to the one for PRISM that covers from Bayesian networks to probabilistic grammars. I describe how to estimate the marginal probability of data from MCMC samples and how to perform Bayesian Viterbi inference using an example of Naive Bayes model augmented with a hidden variable.", "venue": "NII Shonan Meet. Rep.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "kg", "nlp", "ke"], "mention_counts": {"nlp": 2, "ke": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 2}, "ld_mention_counts": {"ke": 2, "kg": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "ef38c6e46b7dea556c4253d7fdcc533b4db08ebf", "url": "https://www.semanticscholar.org/paper/ef38c6e46b7dea556c4253d7fdcc533b4db08ebf", "title": "PoliGraph: Automated Privacy Policy Analysis using Knowledge Graphs", "abstract": "Privacy policies disclose how an organization collects and handles personal information. Recent work has made progress in leveraging natural language processing (NLP) to automate privacy policy analysis and extract collection statements from different sentences, considered in isolation from each other. In this paper, we view and analyze, for the \ufb01rst time, the entire text of a privacy policy in an integrated way. In terms of methodology: (1) we de\ufb01ne P OLI G RAPH , a type of knowledge graph that captures different relations between different parts of the text in a privacy policy; and (2) we develop an NLP-based tool, P OLI G RAPH - ER , to automatically extract P OLI G RAPH from the text. In addition, (3) we revisit the notion of ontologies, previously de\ufb01ned in heuristic ways, to capture subsumption relations between terms. We make a clear distinction between local and global ontologies to capture the context of individual privacy policies, application domains, and privacy laws. Using a public dataset for evaluation, we show that P OLI G RAPH - ER identi\ufb01es 61% more collection statements than prior state-of-the-art, with over 90% precision. In terms of applications, P OLI G RAPH enables automated analysis of a corpus of privacy policies and allows us to: (1) reveal common patterns in the texts across different privacy policies, and (2) assess the correctness of the terms as de\ufb01ned within a privacy policy. We also apply P OLI G RAPH to: (3) detect contradictions in a privacy policy\u2014we show false posi-tives by prior work, and (4) analyze the consistency of privacy policies and network traf\ufb01c, where we identify signi\ufb01cantly more clear disclosures than prior work.", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "onto", "onto", "kg", "kg"], "mention_counts": {"nlp": 3, "onto": 2, "kg": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 2, "onto": 2}, "relevance_score": 0.7007781224516946}, {"paperId": "8853907ba99148819d309702b566149fbaebee0c", "url": "https://www.semanticscholar.org/paper/8853907ba99148819d309702b566149fbaebee0c", "title": "The MultiTal NLP tool infrastructure", "abstract": "This paper gives an overview of the MultiTal project, which aims to create a research infrastructure that ensures long-term distribution of NLP tools descriptions. The goal is to make NLP tools more accessible and usable to end-users of different disciplines. The infrastructure is built on a meta-data scheme modelling and standardising multilingual NLP tools documentation. The model is conceptualised using an OWL ontology. The formal representation of the ontology allows us to automatically generate organised and structured documentation in different languages for each represented tool.", "venue": "LT4DH@COLING", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 4, "onto": 3}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.7007781224516946}, {"paperId": "03b7fe82dd7b08a9d86e39ed9a4d4000d90308b4", "url": "https://www.semanticscholar.org/paper/03b7fe82dd7b08a9d86e39ed9a4d4000d90308b4", "title": "Abstractive Text Summarization: Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization", "abstract": "Abstract Nowadays, most research conducted in the field of abstractive text summarization focuses on neural-based models alone, without considering their combination with knowledge-based approaches that could further enhance their efficiency. In this direction, this work presents a novel framework that combines sequence-to-sequence neural-based text summarization along with structure and semantic-based methodologies. The proposed framework is capable of dealing with the problem of out-of-vocabulary or rare words, improving the performance of the deep learning models. The overall methodology is based on a well-defined theoretical model of knowledge-based content generalization and deep learning predictions for generating abstractive summaries. The framework is composed of three key elements: (i) a pre-processing task, (ii) a machine learning methodology, and (iii) a post-processing task. The pre-processing task is a knowledge-based approach, based on ontological knowledge resources, word sense disambiguation, and named entity recognition, along with content generalization, that transforms ordinary text into a generalized form. A deep learning model of attentive encoder-decoder architecture, which is expanded to enable a coping and coverage mechanism, as well as reinforcement learning and transformer-based architectures, is trained on a generalized version of text-summary pairs, learning to predict summaries in a generalized form. The post-processing task utilizes knowledge resources, word embeddings, word sense disambiguation, and heuristic algorithms based on text similarity methods in order to transform the generalized version of a predicted summary to a final, human-readable form. An extensive experimental procedure on three popular data sets evaluates key aspects of the proposed framework, while the obtained results exhibit promising performance, validating the robustness of the proposed approach.", "venue": "International Conference on Computational Logic", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "wsd", "kg", "wsd", "kg", "wsd", "onto"], "mention_counts": {"kg": 3, "wsd": 3, "onto": 1}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"kg": 3, "onto": 1}, "relevance_score": 0.7007781224516946}, {"paperId": "fa562cd4a1e794273780296e77d9a72dc1dd9433", "url": "https://www.semanticscholar.org/paper/fa562cd4a1e794273780296e77d9a72dc1dd9433", "title": "Triple Prediction from Texts by Using Distributed Representations of Words", "abstract": "Knowledge graphs have been shown to be useful to many tasks in artificial intelligence. Triples of knowledge graphs are traditionally structured by human editors or extracted from semi-structured information; however, editing is expensive, and semi-structured information is not common. On the other hand, most such information is stored as text. Hence, it is necessary to develop a method that can extract knowledge from texts and then construct or populate a knowledge graph; this has been attempted in various ways. Currently, there are two approaches to constructing a knowledge graph. One is open information extraction (Open IE), and the other is knowledge graph embedding; however, neither is without problems. Stanford Open IE, the current best such system, requires labeled sentences as training data, and knowledge graph embedding systems require numerous triples. Recently, distributed representations of words have become a hot topic in the field of natural language processing, since this approach does not require labeled data for training. These require only plain text, but Mikolov showed that it can perform well with the word analogy task, answering questions such as, \u201ca is to b as c is to ?.\u201d This can be considered as a knowledge extraction task from a text for finding the missing entity of a triple. However, the accuracy is not sufficiently high when applied in a straightforward manner to relations in knowledge graphs, since the method uses only one triple as a positive example. In this paper, we analyze why distributed representations perform such tasks well; we also propose a new method for extracting knowledge from texts that requires much less annotated data. Experiments show that the proposed method achieves considerable improvement compared with the baseline; in particular, the improvement in HITS@10 was more than doubled for some relations. key words: distributed representations of words, knowledge extraction, knowledge graph completion", "venue": "IEICE Trans. Inf. Syst.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "kg", "ke", "kg", "kg", "kg", "kg", "kg", "ke", "ie", "ke", "kg", "kg"], "mention_counts": {"nlp": 1, "ke": 4, "kg": 8, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 4, "ie": 1}, "ld_mention_counts": {"ke": 4, "kg": 8}, "relevance_score": 0.666059299203733}, {"paperId": "6a878e4b4e92d213a376a381957ee23adb379163", "url": "https://www.semanticscholar.org/paper/6a878e4b4e92d213a376a381957ee23adb379163", "title": "Ontology Guided Information Extraction from Unstructured Text", "abstract": "In this paper, we describe an approach topopulate an existing ontology with instance information present in the natural language text provided as input.An ontology is defined as an explicit conceptualizatio n of a shared domain [18]. This approach starts with a list of relevant domain ontologies created by human experts, and techniques for identifying the most appropriateontology to be extended with information from a given text. Then we demonstrate heuristics to extract information from the unstructured text and for adding it as structured information to the selected ontology . This identification of the relevant ontology is critical, as it is used in identifying relevant informationin the text. We extract information in the form of semantic triples from the text, guided by the concepts in the ontology. We then convert the extracted information about the semantic class instances into Resource Description Framework (RDF 3 ) and append it to the existing domain ontology. This enables us to perform more precise semantic queries over the semantic triple store thus created. We have achieved 95% accuracy of information extraction in our implementation.", "venue": "ArXiv", "citationCount": 41, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "ie", "onto", "rdf", "onto", "ie", "onto", "rdf", "onto", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 8, "rdf": 2, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 8, "rdf": 2}, "relevance_score": 0.663953241522736}, {"paperId": "be389da5786bde0fd9e9f8fdda83fba730ab1463", "url": "https://www.semanticscholar.org/paper/be389da5786bde0fd9e9f8fdda83fba730ab1463", "title": "HO2IEV: Heavyweight ontology based web information extraction technique for visionless users", "abstract": "As the internet grows rapidly, millions of web pages are being added on a daily basis. The extraction of precise information is becoming more and more difficult as the volume of data on the internet increases. Several search engines and information fetching tools are available on the internet, all of which claim to provide the best crawling facilities. For the most part, these search engines are keyword based. This poses a problem for visually impaired people who want to get the full use from online resources available to other users. Visually impaired users require special aid to get along with any given computer system. Interface and content management are no exception, and special tools are required to facilitate the extraction of relevant information from the internet for visually impaired users. The HO2IEV (Heavyweight Ontology Based Information Extraction for Visually impaired User) architecture provides a mechanism for highly precise information extraction using heavyweight ontology and built-in vocal command system for visually impaired internet users. Our prototype intelligent system not only integrates and communicates among different tools, such as voice command parsers, domain ontology extractors and short message engines, but also introduces an autonomous mechanism of information extraction (hereafter referred to as IE) using heavyweight ontology. In this paper we designed domain specific heavyweight ontology using OWL (Web Ontology Language) for ontology modeling and PAL (Prot\u00e9g\u00e9 Axiom Language) for axiom writing. We introduced a novel autonomous mechanism for IE by developing prototype software. A series of experiments were designed for the testing and analysis of the performance of heavyweight ontology in general, and our information extraction prototype specifically.", "venue": "The 7th International Conference on Networked Computing and Advanced Information Management", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "onto", "ie", "onto", "onto", "ie", "ie", "onto", "ie", "onto", "onto", "onto"], "mention_counts": {"onto": 10, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 10}, "relevance_score": 0.663953241522736}, {"paperId": "b046a50a36589ae031bbf2924be9b2888c3100c8", "url": "https://www.semanticscholar.org/paper/b046a50a36589ae031bbf2924be9b2888c3100c8", "title": "Knowledge Extraction Method for Power Grid Fault Text Based on Ontology", "abstract": "The power industry usually records equipment failures, defects and other information in the form of text, which contains lots of regular patterns. Knowledge extraction in fault text is of great significance to improve efficiency and reduce the labor cost in the power industry. However, the research for knowledge extraction of text information in this field is rare, it is even more difficult to use machine learning algorithms to mine the deep patterns. To solve this problem, a method of knowledge extraction is proposed in this field. We use power equipment fault texts and relevant guidance as raw materials. Firstly, the knowledge base of this field is designed and constructed based on the ontology concepts, including ontology concept base, description base and regular expression base. Then, the knowledge extraction algorithm is designed according to the knowledge base. After that we conduct the knowledge merge operation to make the extraction results more accurate. Experiments on the real fault texts shows the feasibility and the high accuracy of our method when compared with artificial extraction.", "venue": "Journal of Physics: Conference Series", "citationCount": 0, "fieldsOfStudy": ["Physics", "Computer Science"], "mentions": ["ke", "ke", "kg", "ke", "onto", "kg", "onto", "ke", "ke", "onto"], "mention_counts": {"ke": 5, "onto": 3, "kg": 2}, "nlp_mention_counts": {"ke": 5}, "ld_mention_counts": {"ke": 5, "onto": 3, "kg": 2}, "relevance_score": 0.663953241522736}, {"paperId": "148941d7167df6a75659eca4dad7cd9143b5f71c", "url": "https://www.semanticscholar.org/paper/148941d7167df6a75659eca4dad7cd9143b5f71c", "title": "Ontology-Driven Human Language Technology for Semantic-Based Business Intelligence", "abstract": "In this poster submission, we describe the actual state of development of textual analysis and ontology-based information extraction in real world applications, as they are defined in the context of the European R&D project \u201cMUSING\u201d dealing with Business Intelligence. We present in some details the actual state of ontology development, including a time and domain ontologies, which are guiding information extraction onto an ontology population task.", "venue": "ECAI", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "onto", "ie", "onto", "hlt"], "mention_counts": {"hlt": 1, "onto": 5, "ie": 2}, "nlp_mention_counts": {"hlt": 1, "ie": 2}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "2140e45b351137f53bd85c70f80c19d0c82c1614", "url": "https://www.semanticscholar.org/paper/2140e45b351137f53bd85c70f80c19d0c82c1614", "title": "Feeding OWL: Extracting and Representing the Content of Pathology Reports", "abstract": "This paper reports on an ongoing project that combines NLP with semantic web technologies to support a content-based storage and retrieval of medical pathology reports. We describe the NLP component of the project (a robust parser) and the background knowledge component (a domain ontology represented in OWL), and how they work together during extraction of domain specific information from natural language reports. The system provides a good example of how NLP techniques can be used to populate the Semantic Web.", "venue": "NLPXML@ACL", "citationCount": 22, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "sw", "nlp", "onto", "sw", "nlp", "onto"], "mention_counts": {"nlp": 3, "sw": 2, "onto": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 2, "onto": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "a98f7b2bd334973a8ecaacf4aa5b4fe8e48ac3b9", "url": "https://www.semanticscholar.org/paper/a98f7b2bd334973a8ecaacf4aa5b4fe8e48ac3b9", "title": "A Heuristic Grafting Strategy for Manufacturing Knowledge Graph Extending and Completion Based on Nature Language Processing: KnowTree", "abstract": "Applied to search, question answering, and semantic web of close-or-open domain, knowledge graph (KG) is known for its incompleteness subject to the rapid knowledge growing pace. Inspired by the agricultural grafting technology to fruit variety, this paper proposes a heuristic knowledge grafting strategy (HGS) for manufacturing knowledge graph (MKG) named KnowTree extending and completion with natural language processing (NLP) mining engineering cases document. Based on similarity analysis, firstly the grafting related definitions and mechanisms (completeness, relatedness, connectivity and reutilization) are defined. Then, focused on the four mechanisms, HGS takes a pair same engineering documents as input. KnowWords is built as a collection of KnowScion, and each scion is mined from engineering documents based on the SAO structure network, whose importance is evaluated by SAORank counting the in-out degree of centrality. On another hand, the KnowRoot system is designed based on the extended P & S ontology model to characterize the structure of abstract document into four sub-space of knowledge: know-what (problem), know-why (context), know-how (solution) and know-with (result), where a pre-trained language representation model K-BERT is used to classify the KnowScion candidates into the designed KnowRoot system with a fine-tuning classification task. In the knowledge grafting process, the connection unit is constructed based on the extracted domain knowledge triples of the K-BERT model, where the head element of a triple is from the KnowScion candidate set KnowWords satisfying the threshold value, the tail element is from the domain MKG to be extended, and a connection factor is used to evaluate the relationship of union combination. To the goal of knowledge reuse, the path based reasoning rules are designed for KnowTree reutilization. Finally, take the latest engineering case abstract (ECA) in whitegoods domain as resources, a case study is conducted to validate the proposed HGS strategy.", "venue": "IEEE Access", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "kg", "nlp", "kg", "nlp", "kg", "nlp", "onto"], "mention_counts": {"nlp": 3, "sw": 1, "onto": 1, "kg": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 1, "onto": 1, "kg": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "0e182e9b05b687c5ddc2d162b45dea80923e025f", "url": "https://www.semanticscholar.org/paper/0e182e9b05b687c5ddc2d162b45dea80923e025f", "title": "Artificial Intelligence for the Early Design Phases of Space Missions", "abstract": "Recent introduction of data mining methods has led to a paradigm shift in the way we can analyze space data. This paper demonstrates that Artificial Intelligence (AI), and especially the field of Knowledge Representation and Reasoning (KRR), could also be successfully employed at the start of the space mission life cycle via an Expert System (ES) used as a Design Engineering Assistant (DEA). An ES is an AI-based agent used to solve complex problems in particular fields. There are many examples of ES being successfully implemented in the aeronautical, agricultural, legal or medical fields. Applied to space mission design, and in particular, in the context of concurrent engineering sessions, an ES could serve as a knowledge engine and support the generation of the initial design inputs, provide easy and quick access to previous design decisions or push to explore new design options. Integrated to the User design environment, the DEA could become an active assistant following the design iterations and flagging model inconsistencies. Today, for space missions design, experts apply methods of concurrent engineering and Model-Based System Engineering, relying both on their implicit knowledge (i.e., past experiences, network) and on available explicit knowledge (i.e., past reports, publications, data sheets). The former knowledge type represents still the most significant amount of data, mostly unstructured, non-digital or digital data of various legacy formats. Searching for information through this data is highly time-consuming. A solution is to convert this data into structured data to be stored into a Knowledge Graph (KG) that can be traversed by an inference engine to provide reasoning and deductions on its nodes. Knowledge is extracted from the KG via a User Interface (UI) and a query engine providing reliable and relevant knowledge summaries to the Human experts. The DEA project aims to enhance the productivity of experts by providing them with new insights into a large amount of data accumulated in the field of space mission design. Natural Language Processing (NLP), Machine Learning (ML), Knowledge Management (KM) and Human-Machine Interaction (HMI) methods are leveraged to develop the DEA. Building the knowledge base manually is subjective, time-consuming, laborious and error bound. This is why the knowledge base generation and population rely on Ontology Learning (OL) methods. This OL approach follows a modified model of the Ontology Layer Cake. This paper describes the approach and the parameters used for the qualitative trade-off for the selection of the software to be adopted in the architecture of the ES. The study also displays the first results of the multi-word extraction and highlights the importance of Word Sense Disambiguation for the identification of synonyms in the context. This paper includes the detailed software architecture of both front and back-ends, as well as the tool requirements. Both architectures and requirements were refined after a set of interviews with experts from the European Space Agency. The paper finally presents the preliminary strategy to quantify and mitigate uncertainties within the ES.", "venue": "IEEE Aerospace Conference", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "wsd", "kg", "onto", "kg", "onto", "nlp", "nlp"], "mention_counts": {"nlp": 2, "kg": 3, "wsd": 1, "onto": 2}, "nlp_mention_counts": {"nlp": 2, "wsd": 1}, "ld_mention_counts": {"kg": 3, "onto": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "e1ae1c82a47c35f1c6f4bc34081fea6f4eb84337", "url": "https://www.semanticscholar.org/paper/e1ae1c82a47c35f1c6f4bc34081fea6f4eb84337", "title": "Mitigating linked data quality issues in knowledge-intense information extraction methods", "abstract": "Advances in research areas such as named entity linking and sentiment analysis have triggered the emergence of knowledge-intensive information extraction methods that combine classical information extraction with background knowledge from the Web. Despite data quality concerns, linked data sources such as DBpedia, GeoNames and Wikidata which encode facts in a standardized structured format are particularly attractive for such applications. This paper addresses the problem of data quality by introducing a framework that elaborates on linked data quality issues relevant to different stages of the background knowledge acquisition process, their impact on information extraction performance and applicable mitigation strategies. Applying this framework to named entity linking and data enrichment demonstrates the potential of the introduced mitigation strategies to lessen the impact of different kinds of data quality problems. An industrial use case that aims at the automatic generation of image metadata from image descriptions illustrates the successful deployment of knowledge-intensive information extraction in real-world applications and constraints introduced by data quality concerns.", "venue": "WIMS", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "ie", "ie", "ld", "ie", "ie", "ld", "ie"], "mention_counts": {"ld": 3, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"ld": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "8b8b695c57daeda6aa2eca81621e40b33bb73279", "url": "https://www.semanticscholar.org/paper/8b8b695c57daeda6aa2eca81621e40b33bb73279", "title": "NLP techniques associated with the OpenGALEN ontology for semi-automatic textual extraction of medical knowledge: abstracting and mapping equivalent linguistic and logical constructs", "abstract": "This research project presents methodological and theoretical issues related to the inter-relationship between linguistic and conceptual semantics, analysing the results obtained by the application of a NLP parser to a set of radiology reports. Our objective is to define a technique for associating linguistic methods with domain specific ontologies for semi-automatic extraction of intermediate representation (IR) information formats and medical ontological knowledge from clinical texts. We have applied the Edinburgh LTG natural language parser to 2810 clinical narratives describing radiology procedures. In a second step, we have used medical expertise and ontology formalism for identification of semantic structures and abstraction of IR schemas related to the processed texts. These IR schemas are an association of linguistic and conceptual knowledge, based on their semantic contents. This methodology aims to contribute to the elaboration of models relating linguistic and logical constructs based on empirical data analysis. Advance in this field might lead to the development of computational techniques for automatic enrichment of medical ontologies from real clinical environments, using descriptive knowledge implicit in large text corpora sources.", "venue": "AMIA", "citationCount": 20, "fieldsOfStudy": ["Medicine", "Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "nlp", "tp", "nlp", "onto"], "mention_counts": {"nlp": 2, "onto": 5, "tp": 1}, "nlp_mention_counts": {"nlp": 2, "tp": 1}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "994862df927cdb2331c29c5e581f2100dff6147b", "url": "https://www.semanticscholar.org/paper/994862df927cdb2331c29c5e581f2100dff6147b", "title": "Mining and Leveraging Background Knowledge for Improving Named Entity Linking", "abstract": "Knowledge-rich Information Extraction (IE) methods aspire towards combining classical IE with background knowledge obtained from third-party resources. Linked Open Data repositories that encode billions of machine readable facts from sources such as Wikipedia play a pivotal role in this development. The recent growth of Linked Data adoption for Information Extraction tasks has shed light on many data quality issues in these data sources that seriously challenge their usefulness such as completeness, timeliness and semantic correctness. Information Extraction methods are, therefore, faced with problems such as name variance and type confusability. If multiple linked data sources are used in parallel, additional concerns regarding link stability and entity mappings emerge. This paper develops methods for integrating Linked Data into Named Entity Linking methods and addresses challenges in regard to mining knowledge from Linked Data, mitigating data quality issues, and adapting algorithms to leverage this knowledge. Finally, we apply these methods to Recognyze, a graph-based Named Entity Linking (NEL) system, and provide a comprehensive evaluation which compares its performance to other well-known NEL systems, demonstrating the impact of the suggested methods on its own entity linking performance.", "venue": "Web Intelligence, Mining and Semantics", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ld", "ld", "lod", "ld", "ie", "ld", "ie"], "mention_counts": {"ld": 4, "lod": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"ld": 4, "lod": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "f4c3c5ed768195d6dfbb71d9c5dc222ac92744b4", "url": "https://www.semanticscholar.org/paper/f4c3c5ed768195d6dfbb71d9c5dc222ac92744b4", "title": "Linked Open Data Perspectives: Incorporating Linked Open Data into Information Extraction on the Web", "abstract": "Abstract Currently, the World Wide Web can be divided into two separate fields. The traditional Web of Documents consisting of hyperlinked web documents and the emerging Web of Data consisting of linked open data. We present ontology-based information extraction as core technology for bridging the gap between both fields. Based on this, we list three basic applications that integrate web data to web documents. Our SCOOBIE system can extract information of a linked open dataset mentioned as textual phrases in web documents. SCOOBIE returns machine interpretable metadata summarizing the content of a web document from the perspective of a linked open dataset. Based on SCOOBIE we present EPIPHANY, a system that returns extracted metadata back to the originating web document in form of semantic annotations. This allows users to request the Web of Data for more information about annotated subjects inside the web document. STERNTALER is a system that analyses extracted metadata from search results of a search engine. It generates semantic filters filled with facets of things that were extracted from web documents inside search results. This allows users filtering those web documents that contain information about specific subjects and facets. Zusammenfassung Das aktuelle \u201cWorld Wide Web\u201d l\u00e4sst sich in zwei Welten untergliedern. Einerseits das traditionelle Netz der Dokumente, bestehend aus verkn\u00fcpften Webseiten, andererseits das Netz der Daten, bestehend aus offenen und miteinander verkn\u00fcpften Datens\u00e4tzen (engl. \u201cLinked Open Data\u201d). Wir stellen ontologiebasierte Informationsextraktion als Basistechnologie vor, um beide Welten miteinander zu vereinen. Drei Anwendungen zeigen hierbei, wie sich das Netz der Dokumente mit dem Netz der Daten anreichern l\u00e4sst. Beim Analysieren von Webseiten erkennt das SCOOBIE System, ob einzelne Textfragmente als Entit\u00e4ten eines \u201cLinked Open Data\u201d-Datensatzes weitergehend beschrieben werden. Das Resultat von SCOOBIE sind maschinenverst\u00e4ndliche Metadaten, die den Inhalt der Webseite aus der Perspektive des jeweilig verwendeten Datensatzes heraus zusammenfassen. Basierend auf den Resultaten von SCOOBIE pr\u00e4sentieren wir das System EPIPHANY. EPIPHANY reichert das Quelldokument mit von SCOOBIE extrahierten Metadaten an, indem semantische Annotationen \u00fcber die von SCOOBIE ber\u00fccksichtigten Textfragmente erstellt werden. Dies erlaubt es Benutzern, weitere Informationen aus dem Netz der Daten \u00fcber annotierte Textpassagen anzufragen. Das System STERNTALER erweitert eine Suchmaschine, in dem es automatisch Metadaten aus Dokumenten der Suchresultate extrahiert. STERNTALER generiert auf Basis der extrahierten Metadaten semantische Filter, die mit Eigenschaften der im Dokument gefundenen Dingen gef\u00fcllt werden. Benutzern wird es hierdurch erm\u00f6glicht, solche Dokumente heraus zu filtern, die die gesuchten Informationen zu gew\u00fcnschten Dingen mit bestimmten Eigenschaften enthalten.", "venue": "it Inf. Technol.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "lod", "lod", "lod", "lod", "ie"], "mention_counts": {"lod": 4, "onto": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"lod": 4, "onto": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "98878502e7c8db6cb0a49a1d4c72fa868656118d", "url": "https://www.semanticscholar.org/paper/98878502e7c8db6cb0a49a1d4c72fa868656118d", "title": "Clinical named entity recognition: Challenges and opportunities", "abstract": "Information Extraction (IE), one of the important tasks in text analysis and Natural Language Processing (NLP), involves extracting meaningful pieces of knowledge from unstructured information sources, as unstructured data is computationally opaque. The intent of IE is to produce a knowledge base i.e. organize the information in a way that it is useful to people and arrange the information in a semantic way so that algorithms can make certain useful inferences from it. Named Entity Recognition (NER) is a sub-task of IE which finds and classifies the names/entities. Once these Named Entities (NE) are extracted, they can then be indexed and made searchable, relations can be derived, questions can be answered and many more. NER techniques are different for different domains, because of the uniqueness that exists in each of the domains, although the process depends on a number of fundamental Natural Language Processing (NLP) steps such as tokenization, part-of-speech tagging, parsing and model building. As an example, NER in the medical domain involves handling of a number of vital tasks such as identification of medical terms, attributes such as negation, severity, identification of relationships between entities and mapping terms in the document to concepts in domain specific ontologies. There is also a heavy dependence on domain specific resources such as medical dictionaries and ontologies such as the Unified Medical Language System (UMLS)[34]. In this paper, we focus on NER in the clinical domain. In particular, we will focus on the NER challenges and the qualitative analysis of clinical reports on the approaches we took for the named entities: anatomies, findings, location qualifier, and procedures.", "venue": "2016 IEEE International Conference on Big Data (Big Data)", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "nlp", "nlp", "nlp", "nlp", "kg"], "mention_counts": {"nlp": 4, "onto": 2, "kg": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 4, "ie": 1}, "ld_mention_counts": {"kg": 1, "onto": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "46b7f5c48f88d7472dda1cadd9b4ee321191428c", "url": "https://www.semanticscholar.org/paper/46b7f5c48f88d7472dda1cadd9b4ee321191428c", "title": "Query refinement for ontology information extraction", "abstract": "Ontology information extraction has gain popularity due to the increasing amount of ontologies developed over the years. World Wide Web Consortium (W3C) has introduced SPARQL query language to extract information. However SPARQL query language follow a specific pattern in order to find the triple through subgraph matching. The keywords used in the SPARQL query have to be exactly same as the existing keywords in the RDF data in-order to extract the required information. The paper introduced a method to ease the job of the user by generating SPARQL query from the query entered by the user. The method uses the object property list generated from the ontology and word's synonym to aid in SPARQL query generation. The result has shown that the proposed method clearly eases the need of the user to learn SPARQL syntax.", "venue": "2016 Third International Conference on Information Retrieval and Knowledge Management (CAMP)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "rdf", "onto", "onto", "ie", "ie", "onto", "ie"], "mention_counts": {"onto": 4, "rdf": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 4, "rdf": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "c3b2795a965643462a065eabc0fd617f23bb1854", "url": "https://www.semanticscholar.org/paper/c3b2795a965643462a065eabc0fd617f23bb1854", "title": "Building Information Extraction System Based on Computing Domain Ontology", "abstract": "In this paper, we present an Information Extraction (IE) system, which is built from unstructured text based on Computing domain ontology. The IE system comprises four sequential processing steps: preprocessing, topic identifier, building domain specific ontology and extracting information from text corpus. The first two steps perform generic Natural Language Processing (NLP) and Machine Learning tasks, while the last two phases are application-specific and require a thorough understanding of the application domain. Furthermore, the paper focuses on evaluating the IE IEsystem by selected methods. One of these methods that we introduced here, is comparative. Comparative evaluation performed in this study use of Key Exchange Algorithm with the same corpus to contrast results. Results generated by such experiments show that this IE system outperforms Key Exchange Algorithm, respectably.", "venue": "International Conference on Information Integration and Web-based Applications & Services", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "nlp", "ie", "ie", "ie"], "mention_counts": {"nlp": 2, "onto": 3, "ie": 3}, "nlp_mention_counts": {"nlp": 2, "ie": 3}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "53bca484a7aba3253d884a1cf67288ee875eedac", "url": "https://www.semanticscholar.org/paper/53bca484a7aba3253d884a1cf67288ee875eedac", "title": "Odaies: ontology-driven adaptive Web information extraction system", "abstract": "This paper proposes an ontology-driven self-adapting approach in the semi-structured Web information extraction field, where ontology provides semantic support and plays a central role during the extraction process. It outperforms traditional wrapper systems in adaptiveness and maintenance. Firstly, we build a domain-dependant ontology. Then we design three templates generating algorithms, which have self-adaptiveness and self-maintenance based on the ontology, to perform Web page information extraction. Experiment results show that our prototype system can achieve 100% recall and 97.64% precision.", "venue": "IEEE/WIC International Conference on Intelligent Agent Technology, 2003. IAT 2003.", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "ie", "onto", "onto", "ie"], "mention_counts": {"onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "a1e404af8343b0924798e23972668e9af958ff11", "url": "https://www.semanticscholar.org/paper/a1e404af8343b0924798e23972668e9af958ff11", "title": "Ontology-based Information Extraction from Technical Documents", "abstract": "This paper presents a novel system for extracting user relevant tabular information from documents. The presented system is generic and can be applied to any documents irrespective of their domain and the information they contain. In addition to the generic nature of the presented approach, it is robust and can deal with different document layouts followed while creating those documents. The presented system has two main modules; table detection and ontological information extraction. The table detection module extracts all tables from a given technical document while, the ontological information extraction module extracts only relevant tables from all of the detected tables. The generalization in this system is achieved by using ontologies, thus enabling the system to adapt itself, to a new set of documents from any other domain, according to any provided ontology. Furthermore, the presented system also provides a confidence score and explanation of the score for each of the extracted tables in terms of its relevancy. The system was evaluated on 80 real technical documents of hardware parts containing 2033 tables from 20 different brands of Industrial Boilers domain. The evaluation results show that the presented system extracted all of the relevant tables and achieves an overall precision, recall, and F-measure of 0.88, 1 and 0.93 respectively.", "venue": "ICAART", "citationCount": 12, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "onto", "ie", "ie", "onto"], "mention_counts": {"onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "5fd01ab3f5c39511f4a9311b3b7abb06a13d2be6", "url": "https://www.semanticscholar.org/paper/5fd01ab3f5c39511f4a9311b3b7abb06a13d2be6", "title": "XONTO: An Ontology-Based System for Semantic Information Extraction from PDF Documents", "abstract": "Information extraction is of paramount importance in several real world applications in the areas of business intelligence, competitive and military intelligence. Although several sophisticated and indeed complex approaches were proposed, they are still limited in many aspects. In this paper the novel ontology-based system named XONTO, that allows the semantic extraction of information from PDF unstructured documents, is presented. The XONTO system is founded on the idea of self-describing ontologies in which objects and classes can be equipped by a set of rules named descriptors. These rules represent patterns that allow to automatically recognize and extract ontology objects contained in PDF documents also when information is arranged in tabular form. This way a self-describing ontology expresses the semantic of the information to extract and the rules that, in turn, populate itself. In the paper XONTO system behaviors and structure are sketched by means of a running example.", "venue": "IEEE International Conference on Tools with Artificial Intelligence", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "ie", "ie", "onto", "onto"], "mention_counts": {"onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "42792ce88c1e98577a6e99c9ae318f71fde2ede2", "url": "https://www.semanticscholar.org/paper/42792ce88c1e98577a6e99c9ae318f71fde2ede2", "title": "Ontology-Based Information Extraction from Handwritten Documents", "abstract": "In this paper we introduce a new layer for the task of handwriting recognition. We add semantic information by means of ontologies. The task of our recognizer therefore is not only to recognize the ASCII transcription of the handwritten document, but also to identify the semantic concepts which appear in the text. This task is called ontology-based information extraction (OBIE), which has been applied to electronic documents recently. OBIE methods first segment the text into tokens, then identify their values and their corresponding instances of the ontology, and finally try to generate new facts based on the text. To the authors\u2019 knowledge, in this paper OBIE is proposed for the first time in handwriting literature. In our experiments we have evaluated the process up to the instantiation. We have found that using not only the top alternative, but also the k-best alternatives increases the performance of information extraction. Furthermore, the use of an ontology-based lexicon results in another performance increase.", "venue": "International Conference on Frontiers in Handwriting Recognition", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "ie", "onto", "onto"], "mention_counts": {"onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "bff822f93ca2f6c5d90ce5933b99465c10870426", "url": "https://www.semanticscholar.org/paper/bff822f93ca2f6c5d90ce5933b99465c10870426", "title": "Towards a Semantic Information Extraction Approach from Unstructured Documents", "abstract": "Recognizing and extracting meaningful information from semiand unstructured documents, taking into account their semantics, and storing them into database is an important problem in the context of information access and retrieval. This paper describes a novel logic-based approach to information extraction from both semiand unstructured documents. The approach, implemented in the HiLeX system, is founded on a new two-dimensional representation of documents constituting a unified abstract representation of both HTML pages and flat text documents. The semantics of information to be extracted is encoded by means of ontology expressed in DLP, an extension of disjunctive logic programming for ontology representation and reasoning, which has been recently implemented on top of the DLV system. Unlike previous systems, which are mainly syntactic, HiLeX combines both semantic and syntactic knowledge for a powerful information extraction. Each extraction pattern belongs to an ontology class and is expressed using regular expressions and/or an ad hoc two-dimensional language exploiting the document two-dimensional representation. The execution of DLP reasoning modules, encoding the HiLeX language expressions in term of logic rules, yields the actual extraction of information from the input document. HiLeX allows the semantic information extraction from both HTML pages and flat text documents by using synthetic and very expressive extraction patterns.", "venue": "SEBD", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "ie", "ie", "ie", "ie"], "mention_counts": {"onto": 3, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "312e6b42a75b2dc510aa30924da4ea7018697deb", "url": "https://www.semanticscholar.org/paper/312e6b42a75b2dc510aa30924da4ea7018697deb", "title": "Structural semantic interconnections: a knowledge-based approach to word sense disambiguation", "abstract": "Word sense disambiguation (WSD) is traditionally considered an AI-hard problem. A break-through in this field would have a significant impact on many relevant Web-based applications, such as Web information retrieval, improved access to Web services, information extraction, etc. Early approaches to WSD, based on knowledge representation techniques, have been replaced in the past few years by more robust machine learning and statistical techniques. The results of recent comparative evaluations of WSD systems, however, show that these methods have inherent limitations. On the other hand, the increasing availability of large-scale, rich lexical knowledge resources seems to provide new challenges to knowledge-based approaches. In this paper, we present a method, called structural semantic interconnections (SSI), which creates structural specifications of the possible senses for each word in a context and selects the best hypothesis according to a grammar G, describing relations between sense specifications. Sense specifications are created from several available lexical resources that we integrated in part manually, in part with the help of automatic procedures. The SSI algorithm has been applied to different semantic disambiguation problems, like automatic ontology population, disambiguation of sentences in generic texts, disambiguation of words in glossary definitions. Evaluation experiments have been performed on specific knowledge domains (e.g., tourism, computer networks, enterprise interoperability), as well as on standard disambiguation test sets.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citationCount": 382, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "kg", "kg", "wsd", "wsd", "wsd", "wsd", "ie"], "mention_counts": {"kg": 2, "wsd": 4, "onto": 1, "ie": 1}, "nlp_mention_counts": {"wsd": 4, "ie": 1}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "f5f679ef556b38f4361d24364965a968d9ebeea7", "url": "https://www.semanticscholar.org/paper/f5f679ef556b38f4361d24364965a968d9ebeea7", "title": "Query based information retrieval and knowledge extraction using Hadith datasets", "abstract": "In Natural language processing, one of the fundamental tasks is Named Entity Recognition (NER) that include identifying names of peoples, locations and other entities. Applications of NER include catboats, speech recognition, machine translation, knowledge extraction and intelligent search systems. NER is an active research domain for the last 10 years. In this paper, we propose a knowledge extraction framework to extract Named entities from Sahih AlBukhari Urdu translation which is a world known Hadith book. The proposed framework is based on finite state transducer system to extract entities and process the Hadith content using Part of Speech (POS) tagging. Conditional Random Field, an ensemble based algorithm, processes the extracted nouns for NER and classification. In the future, we aim to implement the proposed framework to rank the hadith content and apply the Vector Space Model.", "venue": "International Conference on Emerging Technologies", "citationCount": 20, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ke", "mt", "nlp"], "mention_counts": {"nlp": 1, "ke": 3, "mt": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 3, "mt": 1}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "d65f7de6bd94cd810ee4475e141531bcec332be1", "url": "https://www.semanticscholar.org/paper/d65f7de6bd94cd810ee4475e141531bcec332be1", "title": "Mining information from sentences through Semantic Web data and Information Extraction tasks", "abstract": "The Semantic Web provides guidelines for the representation of information about real-world objects (entities) and their relations (properties). This is helpful for the dissemination and consumption of information by people and applications. However, the information is mainly contained within natural language sentences, which do not have a structure or linguistic descriptions ready to be directly processed by computers. Thus, the challenge is to identify and extract the elements of information that can be represented. Hence, this article presents a strategy to extract information from sentences and its representation with Semantic Web standards. Our strategy involves Information Extraction tasks and a hybrid semantic similarity measure to get entities and relations that are later associated with individuals and properties from a Knowledge Base to create RDF triples (Subject\u2013Predicate\u2013Object structures). The experiments demonstrate the feasibility of our method and that it outperforms the accuracy provided by a pattern-based method from the literature.", "venue": "J. Inf. Sci.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "sw", "sw", "ie", "kg", "sw", "rdf", "ie"], "mention_counts": {"sw": 3, "kg": 1, "rdf": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"sw": 3, "kg": 1, "rdf": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "15c444758e9ce29f724fcf243de8a9f8c4a75f8a", "url": "https://www.semanticscholar.org/paper/15c444758e9ce29f724fcf243de8a9f8c4a75f8a", "title": "Intelligent Chatbot-LDA Recommender System", "abstract": "With the proliferation of distance platforms, in particular that of an open access such as Massive Online Open Courses (MOOC), the learner finds himself overwhelmed with data which are not all efficient for his interest. Besides, the MOOC has tools that allow learners to seek information, express their ideas, and participate in discussions in an online forum. This tool is a huge repository of rich data, which continues to evolve, however its exploitation is fiddly in the search for information relevant to the learner. Similarly, the task of the tutor seems to be difficult in management of a large number of learners. To this end, the development of a Chatbot able to meet the requests of learners in a natural language is necessary to the deroulement a course in the MOOC. The ChatBot plays the role of assistant and guide for the learners and for the tutors. However, ChatBot responses come from a knowledge base, which must be relevant. Knowledge extraction to answer questions is a difficult task due to the number of MOOC participants. Learners' interactions with the MOOC platform gen-erate massive information, particularly in discussion forums by seeking answers to their questions. Identifying and extracting knowledge from online forums requires collaborative interactions between learners. In this article we propose a new approach to answer learners' questions in a relevant and instantaneous way in a ChatBot in natural language. Our model is based on the LDA Bayesian statistical method, applied to threads posted in the forum and classifies them to provide the learner with a rich semantic response. These threads taken from the discussion forum in the form of knowledge will enrich the ChatBot knowledge database. In parallel, we will map the extracted knowledge to ontology, to provide the learner with pedagogical resources that will serve as learning support.", "venue": "International Journal of Emerging Technologies in Learning (iJET)", "citationCount": 9, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "onto", "ke", "ke"], "mention_counts": {"ke": 3, "onto": 1, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 1, "kg": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "e96a74d3cb4375a55dbf5b4c23b0fff1a8acf8bb", "url": "https://www.semanticscholar.org/paper/e96a74d3cb4375a55dbf5b4c23b0fff1a8acf8bb", "title": "NLP and ontology based clustering \u2014 An integrated approach for optimal information extraction from social web", "abstract": "Social Web is a heterogeneous collection of both structured and unstructured dataprimarily composed of contents from various social networking sites, blogs, online shopping sites and much more. The knowledge extracted from such data can be valuable for accuracy of search results in light of present explosion of information exchange over social web. The extraction of information patterns from unstructured data sets available at social networking sites viz. facebook, twitter, linkedin is a challenging task as it cannot be understood by machine to robotically process the data. Also, the major data source in the form of naive users triggers the significance of filtration of the relevant results. For effectual analysis of social web contents, this paper proposes integrated NLP and ontology based clustering TVC algorithm that generates semantically meaningful concepts from the social web content. The algorithm promises to optimize the web search results and to provide accuracy in searching the well treated unstructured social web contents.", "venue": "International Conference on Computing for Sustainable Global Development", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "nlp", "onto", "nlp", "ke", "ie", "onto"], "mention_counts": {"nlp": 2, "onto": 2, "ke": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "1daff1fdc6aa9427eb2e6af4c283effd0b67a111", "url": "https://www.semanticscholar.org/paper/1daff1fdc6aa9427eb2e6af4c283effd0b67a111", "title": "Multilingual Epidemic Event Extraction : From Simple Classification Methods to Open Information Extraction (OIE) and Ontology", "abstract": "There is an incredible amount of information available in the form of textual documents due to the growth of information sources. In order to get the information into an actionable way, it is common to use information extraction and more specifically the event extraction, it became crucial in various domains even in public health. In this paper, we address the problem of the epidemic event extraction in potentially any language, so that we tested different corpuses on an existed multilingual system for tele-epidemiology: the Data Analysis for Information Extraction in any Language(DANIEL) system. We focused on the influence of the number of documents on the performance of the system, on average results show that it is able to achieve a precision and recall around 82%, but when we resorted to the evaluation by event by checking whether it has been really detected or not, the results are not satisfactory according to this paper\u2019s evaluation. Our idea is to propose a system that uses an ontology which includes information in different languages and covers specific epidemiological concepts, it is also based on the multilingual open information extraction for the relation extraction step to reduce the expert intervention and to restrict the content for each text. We describe a methodology of five main stages: Pre-processing, relation extraction, named entity recognition (NER), event recognition and the matching between the information extracted and the ontology.", "venue": "RANLP", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "ie", "onto", "ie", "ie", "onto", "ie"], "mention_counts": {"onto": 3, "ie": 5}, "nlp_mention_counts": {"ie": 5}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "7f00241198142680e9f6db15101c6a229869066f", "url": "https://www.semanticscholar.org/paper/7f00241198142680e9f6db15101c6a229869066f", "title": "An Ontology-Based Information Extraction System for Residential Land Use Suitability Analysis", "abstract": "We propose an Ontology-Based Information Extraction (OBIE) system to automate the extraction of the criteria and values applied in Land Use Suitability Analysis (LUSA) from bylaw and regulation documents related to the geographic area of interest. The results obtained by our proposed LUSA OBIE system (land use suitability criteria and their values) are presented as an ontology populated with instances of the extracted criteria and property values. This latter output ontology is incorporated into a Multi-Criteria Decision Making (MCDM) model applied for constructing suitability maps for different kinds of land uses. The resulting maps may be the final desired product or can be incorporated into the cellular automata urban modeling and simulation for predicting future urban growth. A case study has been conducted where the output from LUSA OBIE is applied to help produce a suitability map for the City of Regina, Saskatchewan, to assist in the identification of suitable areas for residential development. A set of Saskatchewan bylaw and regulation documents were downloaded and input to the LUSA OBIE system. We accessed the extracted information using both the populated LUSA ontology and the set of annotated documents. In this regard, the LUSA OBIE system was effective in producing a final suitability map.", "venue": "Int. J. Softw. Eng. Knowl. Eng.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "ie", "onto", "onto", "onto", "ie"], "mention_counts": {"onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "d8bfa6df6dcc7f2b66bdf77a6b009ab438bdefd9", "url": "https://www.semanticscholar.org/paper/d8bfa6df6dcc7f2b66bdf77a6b009ab438bdefd9", "title": "Automatic rules extraction from medical texts", "abstract": "The majority of existing knowledge is encoded in unstructured texts and is not linked to formalized knowledge, like ontologies and rules. The potential solution to this problem is to acquire this knowledge through natural language processing (NLP) tools and text mining techniques. Prior work has focused on the automatic extraction of ontologies from texts, but the acquired knowledge is generally limited to simple hierarchies of terms. This paper presents a polyvalent framework for acquiring more complex relationships from texts and codes them in the form of rules. Our approach starts with existing domain knowledge represented as OWL ontology and SWRL \"Semantic Web Rule Language\" rules by applying NLP tools and text matching techniques to deduce different atoms as classes, properties etc. This is to capture the deductive knowledge in the form of new rules. We evaluate our approach thereafter by applying it on medical field more precisely Gynecology specialty, showing that this approach can generate automatically and accurately SWRL rules for the representation of more formal knowledge necessary for reasoning.", "venue": "2014 International Workshop on Advanced Information Systems for Enterprises", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "sw", "nlp", "onto", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 3, "sw": 1, "onto": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.6605978084834118}, {"paperId": "d8a1706b59818a31cd687581475678fcf513a0e1", "url": "https://www.semanticscholar.org/paper/d8a1706b59818a31cd687581475678fcf513a0e1", "title": "Construction of Semantic Web-based Knowledge Using Text Processing", "abstract": "Looking through the application examples of a semantic Web-based service, we explain how text processing can efficiently help to construct the ontology instance which is obligatorily required in establishing the service. Text processing is applied to class instantiation in the manner of identity resolution, to metadata expansion through topic and category assignment, and to object property add-up by using citation relation extraction. Consequently, 8,543 author URIs were constructed. For assigning multiple topics and categories to each literature, index terms were matched with thesaurus terms of which category names are mapped in advance. We also acquired total 135 citation networks using the citation relation automatically extracted from references in 7,237 papers. It expects that the above-mentioned methods which are put through this research will be multilaterally used in the semantic Web applications", "venue": "Fourth International Conference on Information Technology (ITNG'07)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "onto", "sw", "kg", "tp", "tp", "tp"], "mention_counts": {"sw": 3, "tp": 3, "onto": 1, "kg": 1}, "nlp_mention_counts": {"tp": 3}, "ld_mention_counts": {"sw": 3, "onto": 1, "kg": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "e5b95c50756ed9b259523b4b436313d36681b660", "url": "https://www.semanticscholar.org/paper/e5b95c50756ed9b259523b4b436313d36681b660", "title": "Knowledge Extraction and Inference from Text: Shallow, Deep, and Everything in Between", "abstract": "Systems for structured knowledge extraction and inference have made giant strides in the last decade. Starting from shallow linguistic tagging and coarse-grained recognition of named entities at the resolution of people, places, organizations, and times, modern systems link billions of pages of unstructured text with knowledge graphs having hundreds of millions of entities belonging to tens of thousands of types, and related by tens of thousands of relations. Via deep learning, systems build continuous representations of words, entities, types, and relations, and use these to continually discover new facts to add to the knowledge graph, and support search systems that go far beyond page-level \"ten blue links''. We will present a comprehensive catalog of the best practices in traditional and deep knowledge extraction, inference and search. We will trace the development of diverse families of techniques, explore their interrelationships, and point out various loose ends.", "venue": "SIGIR", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "kg", "ke", "ke"], "mention_counts": {"kg": 2, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 2, "ke": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "a16af0a0f67350ec15e970fee8fdfac18d4294ed", "url": "https://www.semanticscholar.org/paper/a16af0a0f67350ec15e970fee8fdfac18d4294ed", "title": "Knowledge Extraction and Applications utilizing Context Data in Knowledge Graphs", "abstract": "Context is widely considered for NLP and knowledge discovery since it highly influences the exact meaning of natural language. The scientific challenge is not only to extract such context data, but also to store this data for further NLP approaches. Here, we propose a multiple step knowledge graphbased approach to utilize context data for NLP and knowledge expression and extraction. We introduce the graph-theoretic foundation for a general context concept within semantic networks and show a proof-of-concept-based on biomedical literature and text mining. We discuss the impact of this novel approach on text analysis, various forms of text recognition and knowledge extraction and retrieval.", "venue": "Conference on Computer Science and Information Systems", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "nlp", "nlp", "ke", "ke", "nlp"], "mention_counts": {"nlp": 3, "kg": 1, "ke": 2}, "nlp_mention_counts": {"nlp": 3, "ke": 2}, "ld_mention_counts": {"kg": 1, "ke": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "765ac6eaa0731a31de66e2b81c1837b6522d1219", "url": "https://www.semanticscholar.org/paper/765ac6eaa0731a31de66e2b81c1837b6522d1219", "title": "Knowledge Extraction from Rise-Time Auto-Correlated Patterns", "abstract": "In this paper, the author presents an approach for automated knowledge extraction from rise time auto-correlated patterns by using self-organizing maps and k-means clustering. The extracted knowledge in terms of rules will be used as knowledge base for an expert system. Rise-time auto-correlated data patterns are used as a learning data set. The produced knowledge based was verified by using a conventional expert system.", "venue": "Int. J. Inf. Acquis.", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "ke", "ke", "ke"], "mention_counts": {"kg": 2, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 2, "ke": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "0c49994dd561b2d987e4ea0c164c4aca3e7992c3", "url": "https://www.semanticscholar.org/paper/0c49994dd561b2d987e4ea0c164c4aca3e7992c3", "title": "Knowledge Extraction from Source Code Based on Hidden Markov Model: Application to EPICAM", "abstract": "Large software systems evolve rapidly and these evolutions are usually integrated directly into source code without updating the conceptual model. As a consequence, implementation platforms evolve faster than business logic. Thus, when extracting knowledge to enrich or build an ontology, business logic is not always a complete data source. To solve this problem, some authors have suggested to adopt an ontology learning approach in order to extract knowledge from the source code. In this paper, we show how to realize this task using Hidden Markov Models. experiments on EPICAM, a tuberculosis surveillance system shows the relevance of this approach.", "venue": "2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA)", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "ke", "ke", "onto"], "mention_counts": {"ke": 3, "onto": 2}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "64bf8608cd216593e297868f0ad4dc653cd792da", "url": "https://www.semanticscholar.org/paper/64bf8608cd216593e297868f0ad4dc653cd792da", "title": "KBCT: a knowledge extraction and representation tool for fuzzy logic based systems", "abstract": "This paper presents a user-friendly portable tool designed and developed in order to make easier knowledge extraction and representation for fuzzy logic based systems. KBCT is an open source software that could be executed under Linux or Windows operating systems. Main goal of KBCT is the generation or refinement of fuzzy knowledge bases with a particular interest of obtaining interpretable partitions and rules. The use of fuzzy logic simplifies the knowledge extraction process and increase interpretability of rules because of the fuzzy rule expression is closed to expert natural language. KBCT lets the user define expert variables and rules, but also provide induction capabilities for partitions and rules. Both types of knowledge, expert and induced, are integrated under the expert control. In addition to this, the user can check consistency and quality of rule base at any moment. A simplify option is implemented in order to allow the user to reduce the size of rule base. The main objective consists of ensuring interpretability, non-redundancy and consistency of the knowledge base along the whole process.", "venue": "2004 IEEE International Conference on Fuzzy Systems (IEEE Cat. No.04CH37542)", "citationCount": 27, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 2}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "d4c679b725250dd59513fb00229a8b3e0fdb9c83", "url": "https://www.semanticscholar.org/paper/d4c679b725250dd59513fb00229a8b3e0fdb9c83", "title": "Automatic textual Knowledge Extraction based on Paragraph Constitutive Relations", "abstract": "People can understand the paragraph according to the paragraph constitutive relations, solve coreference resolution, dependency syntax, but the machine cannot. To let the machine better understand the paragraph, the paper proposes the automatic textual knowledge extraction considering paragraph constitutive relations. At first, the paragraph constitutive relations are defined, and the detailed analysis is given, including the grammar, semantics, structures of words, sentences and paragraphs. Then, the automatic textual knowledge extraction framework is proposed, where 3 additional databases are added and the adjust strategies are given for extracting entities and relations. Next, the method which extracting the knowledge graph of a paragraph through constitutive relations is given, especially the fusion extractions combining entity with relation are proposed besides entity extraction and relation extraction; the Bi-directional Long Short-Term Memory (Bi-LSTM) + conditional random field (CRF) model on the existing CoNLL dataset as assistance to spacy is applied in entity recognition, the dependency parsing is well processed in relation extraction. Finally, we developed the small B/S application to extract the entities and relationships and visualize the corresponding knowledge graph. The test experiment has achieved good results and applies this method to ICDM competition, the achievements made in the twelfth. More experiments showed our model is available and feasible, performs well, and it is able to build the corresponding knowledge according to the short paragraph.", "venue": "International Conference on Systems and Informatics", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 2}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "dafa062747bf7fbe0a913dbb044996a1c650937c", "url": "https://www.semanticscholar.org/paper/dafa062747bf7fbe0a913dbb044996a1c650937c", "title": "Muf: tool for knowledge extraction and knowledge base building", "abstract": "Muf is a tool for knowledge base (KB) building by extract-ing knowledge from texts. It is intended for cases when all documents have to be processed manually in order to ensure correctness of KB. Muf provides visual tools and some degree of mechanization to facilitate manual knowledge extraction and KB building. We also believe that manual processing of documents implies small number of docu-ments and it further implies that KB use case is well de-fined. Well defined use case allows us to decide which knowledge is worth of extraction and which not. Reducing amount of extracted knowledge also leads to less complex structure of KB. This all makes extraction and KB building tasks even easier, KB is easier to understand and deploy. If some parts of KB happen to be incorrect, Muf is able to trace the corresponding knowledge down to the text and allow user to fix it. The work was done on Czech drug la-bels, but we believe that Muf can be used also for different languages as well as different kinds of documents.", "venue": "K-CAP '07", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 2}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "66f5b5a674e90688af139fa477b0fd21c98f010c", "url": "https://www.semanticscholar.org/paper/66f5b5a674e90688af139fa477b0fd21c98f010c", "title": "Device Fabrication Knowledge Extraction from Materials Science Literature", "abstract": "Devices like solar cells, batteries etc. often comprise of a host of material types including organic, inorganic and hybrid materials. The fabrication procedures for these devices involve screening or designing the right set of materials and then subjecting them to a sequence of operations under very specific conditions. The performance characteristics of a device critically depend on the materials used in its fabrication, the specific operations carried out, their operating conditions and the specific sequence in which they are carried out. The space of potential materials, operations and operating conditions is vast, and selecting the right combination thereof to achieve the desired characteristics is a knowledge intensive activity. A large amount of such device fabrication knowledge is available in the form of publications, patents, company reports and so on. In this paper, we present a system that systematically extracts this knowledge from materials science literature. The extracted knowledge is represented as knowledge graphs conforming to an ontology that can be queried to make informed decisions in device fabrication procedures. The system first identifies the set of relevant paragraphs that contain fabrication knowledge. It then employs state of the art entity and relation extraction models to identify instances of operations, methods, materials, etc. and relations between them. The system then applies an unsupervised algorithm to identify sequences of operations representing fabrication procedures. We applied our system on solar cell fabrication knowledge extraction and achieved good performance. We believe our results provide much needed impetus for further work in this area.", "venue": "AAAI", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "kg", "ke", "onto"], "mention_counts": {"ke": 3, "onto": 1, "kg": 1}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 1, "kg": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "720c5c948c8bbddd4fcec312afeb25dd9d2f115d", "url": "https://www.semanticscholar.org/paper/720c5c948c8bbddd4fcec312afeb25dd9d2f115d", "title": "Utilizing Knowledge Graphs for Neural Machine Translation Augmentation", "abstract": "While neural networks have led to substantial progress in machine translation, their success depends heavily on large amounts of training data. However, parallel training corpora are not always readily available. Moreover, out-of-vocabulary words---mostly entities and terminological expressions---pose a difficult challenge to Neural Machine Translation systems. Recent efforts have tried to alleviate the data sparsity problem by augmenting the training data using different strategies, such as external knowledge injection. In this paper, we hypothesize that knowledge graphs enhance the semantic feature extraction of neural models, thus optimizing the translation of entities and terminological expressions in texts and consequently leading to better translation quality. We investigate two different strategies for incorporating knowledge graphs into neural models without modifying the neural network architectures. Additionally, we examine the effectiveness of our augmented models on domain-specific texts and ontologies. Our knowledge-graph-augmented neural translation model, dubbed KG-NMT, achieves significant and consistent improvements of +3 BLEU, METEOR and chrF3 on average on the newstest datasets between 2015 and 2018 for the WMT English-German translation task.", "venue": "K-CAP", "citationCount": 17, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "mt", "kg", "mt", "kg", "onto", "mt"], "mention_counts": {"kg": 4, "onto": 1, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"kg": 4, "onto": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "03c95e7a1ca6c24d94292658cde8316189e33b6c", "url": "https://www.semanticscholar.org/paper/03c95e7a1ca6c24d94292658cde8316189e33b6c", "title": "Using natural language to integrate, evaluate, and optimize extracted knowledge bases", "abstract": "Web Information Extraction (WIE) systems extract billions of unique facts, but integrating the assertions into a coherent knowledge base and evaluating across different WIE techniques remains a challenge. We propose a framework that utilizes natural language to integrate and evaluate extracted knowledge bases (KBs). In the framework, KBs are integrated by exchanging probability distributions over natural language, and evaluated by how well the output distributions predict held-out text. We describe the advantages of the approach, and detail remaining research challenges.", "venue": "Conference on Automated Knowledge Base Construction", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "kg", "kg", "ke", "kg", "ke"], "mention_counts": {"kg": 3, "ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"kg": 3, "ke": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "1c27cb8364a7655b2e4e8aa799970a08f90dea61", "url": "https://www.semanticscholar.org/paper/1c27cb8364a7655b2e4e8aa799970a08f90dea61", "title": "Building a Large-Scale Knowledge Base for Machine Translation", "abstract": "Knowledge-based machine translation (KBMT) systems have achieved excellent results in constrained domains, but have not yet scaled up to newspaper text. The reason is that knowledge resources (lexicons, grammar rules, world models) must be painstakingly handcrafted from scratch. One of the hypotheses being tested in the PAN-GLOSS machine translation project is whether or not these resources can be semi-automatically acquired on a very large scale. \n \nThis paper focuses on the construction of a large ontology (or knowledge base, or world model) for supporting KBMT. It contains representations for some 70,000 commonly encountered objects, processes, qualities, and relations. The ontology was constructed by merging various online dictionaries, semantic networks, and bilingual resources, through semi-automatic methods. Some of these methods (e.g., conceptual matching of semantic taxonomies) are broadly applicable to problems of importing/exporting knowledge from one KB to another. Other methods (e.g., bilingual matching) allow a knowledge engineer to build up an index to a KB in a second language, such as Spanish or Japanese.", "venue": "AAAI Conference on Artificial Intelligence", "citationCount": 380, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "onto", "kg", "mt", "mt", "onto", "mt"], "mention_counts": {"kg": 3, "onto": 2, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"kg": 3, "onto": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "78a63938f856c6161fb7420c8c57cdd4ff87ee2c", "url": "https://www.semanticscholar.org/paper/78a63938f856c6161fb7420c8c57cdd4ff87ee2c", "title": "Ten ways of leveraging ontologies for natural language processing and its enterprise applications", "abstract": "In the last years, Artificial Intelligence and Deep Learning have matured from a facinating research area to real-word applications across multiple domains. Enterprises adopt data-driven approaches for various use cases. With the increased adoption, such issues as governance of the models, deployment, scalability, reusablity and maintenance are widely addressed on the engineering side, but not so much on the knowledge side. In this paper, we demonstrate 10 ways of leveraging ontology for Natural Language Processing. Specifically, we explore the usage of ontologies and related standards for labeling schema, configuration, providing lexical data, powering rule engine and automated generation of rules, as well as providing a standard output format. Additionally, we discuss three NLP-based applications: semantic search, question answering and natural language querying and show how they can benefit from ontology usage. The paper summarizes our experience of using ontology in a number of projects for medical, enterprise, financial, legal and security domains.", "venue": "SBD@SIGMOD", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 3, "onto": 5}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "f830a3e60d24801cc8c8685fdc6b1bb97e74055f", "url": "https://www.semanticscholar.org/paper/f830a3e60d24801cc8c8685fdc6b1bb97e74055f", "title": "Towards Natural Language Question Answering Over Earth Observation Linked Data Using Attention-Based Neural Machine Translation", "abstract": "With an increase in Geospatial Linked Open Data being adopted and published over the web, there is a need to develop intuitive interfaces and systems for seamless and efficient exploratory analysis of such rich heterogeneous multi-modal datasets. This work is geared towards improving the exploration process of Earth Observation (EO) Linked Data by developing a natural language interface to facilitate querying. Questions asked over Earth Observation Linked Data have an inherent spatio-temporal dimension and can be represented using GeoSPArql. This paper seeks to study and analyze the use of RNN-based neural machine translation with attention for transforming natural language questions into GeoSPArql queries. Specifically, it aims to assess the feasibility of a neural approach for identifying and mapping spatial predicates in natural language to GeoSPARQL's topology vocabulary extension including - Egenhofer and RCC8 relations. The queries can then be executed over a triple store to yield answers for the natural language questions. A dataset consisting of mappings from natural language questions to GeoSPArql queries over the Corine Land Cover(CLC) Linked Data has been created to train and validate the deep neural network. From our experiments, it is evident that neural machine translation with attention is a promising approach for the task of translating spatial predicates in natural language questions to GeoSPArql queries.", "venue": "IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "lod", "mt", "ld", "ld", "mt", "mt", "ld"], "mention_counts": {"ld": 4, "lod": 1, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"ld": 4, "lod": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "bd573fa8258324f6db09817240e7287e455af996", "url": "https://www.semanticscholar.org/paper/bd573fa8258324f6db09817240e7287e455af996", "title": "RTM at SemEval-2016 Task 1: Predicting Semantic Similarity with Referential Translation Machines and Related Statistics", "abstract": "We use referential translation machines (RTMs) for predicting the semantic similarity of text in both STS Core and Cross-lingual STS. RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource. RTMs become 14th out of 26 submissions in Cross-lingual STS. We also present rankings of various prediction tasks using the performance of RTM in terms of MRAER, a normalized relative absolute error metric. 1 Semantic Agreement We participated in Semantic Textual Similarity task at SemEval-2016 (Bethard et al., 2016) with RTMs. RTMs identify translation acts between any two data sets with respect to interpretants, data close to the task instances, effectively judging monolingual and bilingual similarity. We use RTMs for predicting the semantic similarity of text. Interpretants are used to derive features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of the acts of translation, which may ubiquitously be observed in communication. Semantic Web\u2019s dream is to allow machines to share, exploit, and understand knowledge on the web. As more and more shared conceptualizations of domains emerge, we get closer to this goal. Semantic textual similarity (STS) task (Agirre et al., 2016) at SemEval-2016 (Bethard et al., 2016) is about quantifying the degree of similarity between two given sentences S1 and S2 in the same language (English) in STS Core (STS English) or in different languages (English or Spanish) in Cross-lingual STS (STS Spanish), with a real number in [0, 5]. S1 and S2 may be constructed using different models and with different conceptualizations of the world or different ontologies and different vocabulary. Even if two instances are categorized as same, they may have different implications for commonsense reasoning (both albatros and penguin are a bird) (Bi\u00e7ici, 2002). The existence of a single ontology that can cover all the required conceptual information for reaching semantic understanding is questionable because it would presume an agreement among all ontology experts. Yet, semantic agreement using heterogeneous ontologies may not be possible as well since in the most extreme case, they would not use the same tokens. Therefore, semantic textual similarity is harder than the Chinese room thought experiment (Internet Encyclopedia of Philosophy, 2016) since we are not given any instructions about how to answer queries. Our goal is to quantify the level of semantic agreement between S1 and S2 and RTMs use interpretants, data close to the task instances for building prediction models for semantic similarity. 2 Referential Translation Machine Each RTM model is a data translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. RTMs are powerful enough to be applicable in different domains and tasks while achieving top performance in both", "venue": "*SEMEVAL", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "onto", "mt", "onto", "sw", "onto", "onto", "mt"], "mention_counts": {"sw": 1, "onto": 4, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"sw": 1, "onto": 4}, "relevance_score": 0.6605978084834118}, {"paperId": "7e134f5955b261b864e74c91eccae9732fc0a789", "url": "https://www.semanticscholar.org/paper/7e134f5955b261b864e74c91eccae9732fc0a789", "title": "Wikification and Beyond: The Challenges of Entity and Concept Grounding", "abstract": "Contextual disambiguation and grounding of concepts and entities in natural language are essential to progress in many natural language understanding tasks and fundamental to many applications. Wikification aims at automatically identifying concept mentions in text and linking them to referents in a knowledge base (KB) (e.g., Wikipedia). Consider the sentence, \"The Times report on Blumenthal (D) has the potential to fundamentally reshape the contest in the Nutmeg State.\". A Wikifier should identify the key entities and concepts and map them to an encyclopedic resource (e.g., \u201cD\u201d refers to Democratic Party, and \u201cthe Nutmeg State\u201d refers to Connecticut. Wikification benefits end-users and Natural Language Processing (NLP) systems. Readers can better comprehend Wikified documents as information about related topics is readily accessible. For systems, a Wikified document elucidates concepts and entities by grounding them in an encyclopedic resource or an ontology. Wikification output has improved NLP down-stream tasks, including coreference resolution, user interest discovery , recommendation and search. This task has received increased attention in recent years from the NLP and Data Mining communities, partly fostered by the U.S. NIST Text Analysis Conference Knowledge Base Population (KBP) track, and several versions of it has been studied. These include Wikifying all concept mentions in a single text document; Wikifying a cluster of co-referential named entity mentions that appear across documents (Entity Linking), and Wikifying a whole document to a single concept. Other works relate this task to coreference resolution within and across documents and in the context of multiple text genres. 2 Content Overview", "venue": "ACL", "citationCount": 26, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "nlp", "onto", "nlp", "kg", "nlp", "nlu"], "mention_counts": {"nlp": 4, "onto": 1, "kg": 2, "nlu": 1}, "nlp_mention_counts": {"nlp": 4, "nlu": 1}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.6605978084834118}, {"paperId": "a0bedb248e49e8a1235bc83f13a4a28c05125dea", "url": "https://www.semanticscholar.org/paper/a0bedb248e49e8a1235bc83f13a4a28c05125dea", "title": "Creating a novel semantic video search engine through enrichment textual and temporal features of subtitled YouTube media fragments", "abstract": "Semantic video Annotation is an active research zone within the field of multimedia content understanding. With the stable increase of videos published on the famous video sharing platforms such as YouTube, more efforts are spent to automatically annotate these videos. In this paper, we propose a novel framework to annotating subtitled YouTube videos using both textual features such as all of portions extracted from web natural language processors in relation to subtitles, and temporal features such as the duration of the media fragments where particular entities are spotted. We implement SY-VSE (Subtitled YouTube Video Search Engine) as an efficient framework to cruising on the subtitled YouTube videos resident in the Linked Open Data (LOD) cloud. For realizing this purpose, we propose Unifier Module of Natural Language Processing (NLP) Tools Results (UM-NLPTR) for extracting main portions of the 10 NLP web tools from subtitles associated to YouTube videos in order to generate media fragments annotated with resources from the LOD cloud. Then, we propose Unifier Module of Popular API's Results (UM-PAR) containing the seven favorite web APIs to boost results of Named Entities (NE) obtained from UM-NLPTR. We will use dotNetRDF as a powerful and flexible API for working with Resource Description Framework (RDF) and SPARQL in .Net environments.", "venue": "International Conference on Computer and Knowledge Engineering", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "lod", "lod", "nlp", "nlp", "lod", "rdf", "rdf"], "mention_counts": {"nlp": 3, "lod": 3, "rdf": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"lod": 3, "rdf": 2}, "relevance_score": 0.6605978084834118}, {"paperId": "037e37645ecbc87e1214a847e78b861b45541612", "url": "https://www.semanticscholar.org/paper/037e37645ecbc87e1214a847e78b861b45541612", "title": "Comparing the Template-Based Approach to GF: the case of Afrikaans", "abstract": "Ontologies are usually represented in OWL that is not easy to grasp by domain experts. A solution to bridge this gap is to use a controlled natural language or natural language generation (NLG), which allows the knowledge in the ontology to be rendered automatically into a natural language. Several approaches exist to realise this. We used both templates and the Grammatical Framework (GF) and examined the feasibility of each by developing NLG modules for a language that had none: Afrikaans. The template system requires manual translation of the ontology\u2019s vocabulary into Afrikaans, if not already done so, while the GF system can translate the terms automatically. Yet, the template system is found to produce more grammatically correct sentences and verbalises the ontology slightly faster than the GF system. The template-based approach seems easier to extend for future development.", "venue": "WebNLG", "citationCount": 10, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlg", "nlg", "nlg", "onto", "onto", "onto"], "mention_counts": {"nlg": 3, "onto": 5}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"onto": 5}, "relevance_score": 0.6605978084834118}, {"paperId": "4390c695666f0c951c8c0c6ff60ac261c60ff922", "url": "https://www.semanticscholar.org/paper/4390c695666f0c951c8c0c6ff60ac261c60ff922", "title": "Survey on Defining Practices in Ontologies: Report Summary", "abstract": "Ontologies built using OBO Foundry principles are advised to include both formal (logical) definitions and natural language definitions. Depending on the effort, one or the other can be underrepresented. Possible explanations to this bottleneck are the high cost of producing wellwritten definitions; an insufficient understanding of the nature of natural language definitions or of logic; the lack of an operational theory of definitions; the lack of studies that evaluate usability and effectiveness of definitions in ontologies; a paucity of tools to help with definition authoring and checking.", "venue": "VDOS+DO@ICBO", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "nlp", "onto", "onto", "nlg", "nll", "onto", "nle"], "mention_counts": {"onto": 3, "nll": 1, "nlu": 1, "nlp": 1, "nlg": 1, "nle": 1}, "nlp_mention_counts": {"nll": 1, "nlu": 1, "nlp": 1, "nlg": 1, "nle": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "d9fe7500910d22d5dd98b4490f1bfb97a6a3d8a0", "url": "https://www.semanticscholar.org/paper/d9fe7500910d22d5dd98b4490f1bfb97a6a3d8a0", "title": "Generation of Multilingual Personalized Environmental Bulletins from an OWL-based Ontology", "abstract": "Natural Language Generation (NLG) from knowledge bases (KBs) has repeatedly been subject of research. However, most proposals tend to have in common that they start from KBs of limited size that either already contain linguistically-oriented knowledge structures or to whose structures different ways of realization are explicitly assigned. To avoid these limitations, we propose a three layer OWL-based ontology framework in which domain, domain communication and linguistic knowledge structures are clearly separated and show how a large scale instantiation of this framework in the environmental domain serves multilingual NLG.", "venue": "EnviroInfo", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Engineering"], "mentions": ["onto", "nlg", "onto", "nlg", "onto", "onto", "kg", "nlg"], "mention_counts": {"kg": 1, "nlg": 3, "onto": 4}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"kg": 1, "onto": 4}, "relevance_score": 0.6605978084834118}, {"paperId": "5096d7ac485371c980f06b4832642e1947696d35", "url": "https://www.semanticscholar.org/paper/5096d7ac485371c980f06b4832642e1947696d35", "title": "A cognitive approach to qualities for NLP", "abstract": "Unlike most current NLP knowledge bases, where the lexicalist approach prevails, FunGramKB is ontology-oriented, since the ontology plays a pivotal role between the lexical and the cognitive levels. The objective of this paper is to describe the semantic types assigned to qualities in FunGramKB ontology and how the cognitive approach adopted facilitates the structuring of the knowledge base as well as the reasoning in NLP systems.", "venue": "Proces. del Leng. Natural", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "onto", "kg", "nlp", "kg", "nlp"], "mention_counts": {"nlp": 3, "onto": 3, "kg": 2}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 2, "onto": 3}, "relevance_score": 0.6605978084834118}, {"paperId": "489cff534cf26da6112aec9f84e603abc8ecf46d", "url": "https://www.semanticscholar.org/paper/489cff534cf26da6112aec9f84e603abc8ecf46d", "title": "Analysis of natural language understanding technology based on Semantic Web ontology", "abstract": "The key technology of the semantic web are include: ontology, metadata including logic and reasoning and intelligent agents display. Ontology is a formal, explicit specification of conceptualization of the domain knowledge. This paper analyses problems of natural language understanding including: the expression of the target representation complexity, mapping type diversity, the difference of every element in the source of interaction between the degrees of it. This paper presents analysis of natural language understanding technology based on Semantic Web ontology. The experiment result demonstrates that treatment effect of the semantic ontology can improve the natural language understanding.", "venue": "ICM 2015", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "onto", "nlu", "nlu", "onto", "onto", "nlu", "onto", "nlu", "onto", "sw", "sw"], "mention_counts": {"sw": 3, "onto": 5, "nlu": 4}, "nlp_mention_counts": {"nlu": 4}, "ld_mention_counts": {"sw": 3, "onto": 5}, "relevance_score": 0.6546758600252723}, {"paperId": "08d9978e79b684e4e379ac03f2dfccefc37d0923", "url": "https://www.semanticscholar.org/paper/08d9978e79b684e4e379ac03f2dfccefc37d0923", "title": "Adapting Web information extraction knowledge via mining site-invariant and site-dependent features", "abstract": "We develop a novel framework that aims at automatically adapting previously learned information extraction knowledge from a source Web site to a new unseen target site in the same domain. Two kinds of features related to the text fragments from the Web documents are investigated. The first type of feature is called, a site-invariant feature. These features likely remain unchanged in Web pages from different sites in the same domain. The second type of feature is called a site-dependent feature. These features are different in the Web pages collected from different Web sites, while they are similar in the Web pages originating from the same site. In our framework, we derive the site-invariant features from previously learned extraction knowledge and the items previously collected or extracted from the source Web site. The derived site-invariant features will be exploited to automatically seek a new set of training examples in the new unseen target site. Both the site-dependent features and the site-invariant features of these automatically discovered training examples will be considered in the learning of new information extraction knowledge for the target site. We conducted extensive experiments on a set of real-world Web sites collected from three different domains to demonstrate the performance of our framework. For example, by just providing training examples from one online book catalog Web site, our approach can automatically extract information from ten different book catalog sites achieving an average precision and recall of 71.9% and 84.0% respectively without any further manual intervention.", "venue": "TOIT", "citationCount": 25, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "ie", "ie", "ie", "ie", "ke", "ke"], "mention_counts": {"ke": 4, "ie": 4}, "nlp_mention_counts": {"ke": 4, "ie": 4}, "ld_mention_counts": {"ke": 4}, "relevance_score": 0.6546758600252723}, {"paperId": "1d3356d7434d346cf849c9ef5e9761101be37ba9", "url": "https://www.semanticscholar.org/paper/1d3356d7434d346cf849c9ef5e9761101be37ba9", "title": "The LATO Knowledge Model for Automated Knowledge Extraction and Enrichment from Court Decisions Corpora", "abstract": "Knowledge extraction systems are strongly demanded in the legal domain, to provide legal actors like judges or lawyers with useful and relevant information to enforce a knowledge-based evaluation and judgement of new cases. In this paper, we present LATO-KM, a three-layer legal knowledge model where terms featuring legal knowledge, both law and case-law, are properly formalized as entities and relationships and they are implemented in the LATO ontology using SKOS. The LATO ontology constitutes the core component of CRIKE (CRIme Knowledge Extraction), a data-science approach and related tool environment conceived to support legal knowledge extraction and enrichment from a corpus of Court Decision documents.", "venue": "COUrT@CAiSE", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "skos", "ke", "onto", "onto", "ke", "kg"], "mention_counts": {"ke": 4, "skos": 1, "onto": 2, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "skos": 1, "onto": 2, "kg": 1}, "relevance_score": 0.6546758600252723}, {"paperId": "7279aa16c05b84abc0e2e47ff21c2e734351b257", "url": "https://www.semanticscholar.org/paper/7279aa16c05b84abc0e2e47ff21c2e734351b257", "title": "Using Semantics and NLP in Experimental Protocols", "abstract": "In this paper we present \u201cSMART Protocols\u201d, a semantic and NLP-based infrastructure for processing and enacting experimental protocols. Our contribution is twofold; on the one hand, SMART Protocols delivers a semantic layer that represents the knowledge encoded in experimental protocols. On the other hand, it builds the groundwork for making use of such semantics within an NLP framework. We emphasize on the semantic and NLP components, namely the SMART Protocols (SP) Ontology, the Sample Instrument Reagent Objective (SIRO) model and the text mining integrative architecture GATE. The SMART Protocols (SP) Ontology results from the analysis of over 300 experimental protocols in various domains \u2013molecular biology, cell and developmental biology and others. The gathered terminology is then evaluated, rules are improved accordingly and then a new iteration starts. The SIRO model defines an extended layer of metadata for experimental protocols; SIRO is also a Minimal Information (MI) model conceived in the same realm as the Patient Intervention Comparison Outcome (PICO) model that supports search, retrieval and classification purposes. The SIRO ontology development process includes NLP as well as domain expertise in the extraction of the vocabulary; domain experts extract an initial seed of terminology, then the process is automated by using gazetteers and extraction rules in JAPE. Both SIRO and the SP ontology are then used by our NLP engine, GATE. By combining comprehensive vocabularies with NLP rules and gazetteers we identify meaningful parts of speech in experimental protocols. Moreover, in cases for which SIRO is not available, our NLP automatically extracts it; also, searching for queries such as: \u201dWhat bacteria have been used in protocols for persister cells isolation\u201d", "venue": "SWAT4LS", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "nlp", "nlp", "nlp", "onto", "nlp", "nlp", "onto", "onto", "nlp"], "mention_counts": {"nlp": 8, "onto": 4}, "nlp_mention_counts": {"nlp": 8}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.6546758600252723}, {"paperId": "9f93af842016e9e08f5814b90bfac5c8fcd704b1", "url": "https://www.semanticscholar.org/paper/9f93af842016e9e08f5814b90bfac5c8fcd704b1", "title": "Healthcare-Event driven semantic knowledge extraction with hybrid data repository", "abstract": "In this paper, we introduce a Healthcare-Event (H-event) based knowledge extraction approach on a hybrid data repository. The repository collects (with individual user's permission) dynamic and large volume healthcare related data from various resources such as wearable sensors, social media Web APIs and our application itself. The proposed extraction approach relies on two data processing processes. One is the data integration process to dynamically retrieving the large data using public data service APIs. The first process also generates a set of big knowledge bases and stored in NoSQL storage. This paper will focus on the second extraction process that is the H-Event based ontological knowledge extraction for detecting and monitoring user's healthcare related situations, such as medical symptoms, treatments, conditions and daily activities from the NoSQL knowledge bases. The second process can be seen as post-processing step to detect more explicit healthcare knowledge about personalised health conditions and represent the knowledge using RDF formats in a semantic triple repository to enhance further data analytics.", "venue": "InTech", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "kg", "ke", "tp", "kg", "kg", "ke", "ke", "rdf"], "mention_counts": {"onto": 1, "ke": 3, "kg": 3, "tp": 1, "rdf": 1}, "nlp_mention_counts": {"ke": 3, "tp": 1}, "ld_mention_counts": {"kg": 3, "onto": 1, "ke": 3, "rdf": 1}, "relevance_score": 0.6546758600252723}, {"paperId": "37660114739e25002eb503b6630e53abe7d05c2b", "url": "https://www.semanticscholar.org/paper/37660114739e25002eb503b6630e53abe7d05c2b", "title": "Ceres: Harvesting Knowledge from the Semi-structured Web", "abstract": "Knowledge graphs have been used to support a wide range of applications and enhance search and QA for Google, Bing, Amazon Alexa, etc. However, we often miss long-tail knowledge, including unpopular entities, unpopular relations, and unpopular verticals. In this talk we describe our efforts in harvesting knowledge from semi-structured websites, which are often populated according to some templates using vast volume of data stored in underlying databases. We describe our Ceres system, which extracts knowledge from semi-structured web. AutoCeres is a ClosedIE system that extracts knowledge according to existing ontology. It improves the accuracy of fully automatic knowledge extraction from 60%+ of state-of-the-art to 90%+ on semi-structured data. OpenCeres is the first-ever OpenIE system on semi-structured data, that is able to identify new relations not readily included in existing ontologies. ZeroShotCeres goes further and enables extracting knowledge for completely new domains, where there is no seed knowledge for bootstrapping the extraction. Finally, we describe our other efforts in ontology alignment, entity linkage, graph mining, and QA, that allow us to best leverage the knowledge we extract for search and QA.", "venue": "International Conference on Information and Knowledge Management", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "onto", "ke", "ke", "onto", "onto", "kg"], "mention_counts": {"ke": 4, "onto": 3, "kg": 1}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "onto": 3, "kg": 1}, "relevance_score": 0.6546758600252723}, {"paperId": "7299951eed1626904b3cfeeb4b279c7d4bd680a9", "url": "https://www.semanticscholar.org/paper/7299951eed1626904b3cfeeb4b279c7d4bd680a9", "title": "Automated knowledge derivation: Domain\u2010independent techniques for domain\u2010restricted text sources", "abstract": "This article provides a description of the major components of a system that builds and updates a knowledge base by extracting the knowledge from natural language text. the knowledge extraction is done in a domain\u2010independent manner and does not rely on particular vocabulary or grammar constructions. the only restriction is that the input text must be technical text from some specific problem domain. an important capability of the system is that it can bootstrap itself. That is, beginning with only a description of the types of object and relationships to be stored in the knowledge base, the system can start with an empty knowledge base and build the knowledge base as it processes the text. the knowledge extraction system's success in extracting knowledge from various input texts was evaluated using scoring metrics reported by Lehnert and Sundheim [AI Mag., 12(3), 81\u201394 (1991)]. the initial results indicate that the knowledge extraction mechanism is both effective and independent of a particular author's writing style or a particular domain. \u00a9 1995 John Wiley & Sons, Inc.", "venue": "Int. J. Intell. Syst.", "citationCount": 7, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "kg", "ke", "ke", "kg", "ke", "kg"], "mention_counts": {"kg": 4, "ke": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"kg": 4, "ke": 4}, "relevance_score": 0.6546758600252723}, {"paperId": "4b8522529868a832e3a07e5a647e0b9cfe559d81", "url": "https://www.semanticscholar.org/paper/4b8522529868a832e3a07e5a647e0b9cfe559d81", "title": "Knowledge Extraction Based on Forgetting and Subontology Generation (DL Invited Talk Abstract)", "abstract": "This presentation will give an overview of our ongoing work in developing knowledge extraction methods for description logic based ontologies. Because the knowledge is not only given by the axioms stated in an ontology but also by the knowledge that can be inferred from these axioms, knowledge extraction is a challenging problem. Forgetting creates a compact and faithful representation of the stored knowledge over a user-specified signature by performing inferences on the symbols outside this signature. After an introduction of the idea of forgetting, an overview of our forgetting tools and some applications we have explored, I will discuss recent collaborative work with SNOMED International to create bespoke knowledge extraction software for the medical ontology SNOMED CT. The software creates a self-contained subontology containing definitions of a specified set of focus concepts which minimises the number of supporting symbols used and satisfies SNOMED modelling guidelines. Such subontologies make it easier to reuse and share content, assist with ontology analysis, and querying and classification is faster. The talk will give an overview of this research spanning several years, focussing on key ideas, findings, practical challenges encountered and current applications.", "venue": "Description Logics", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "ke", "onto", "onto", "onto", "ke", "onto", "ke"], "mention_counts": {"ke": 4, "onto": 4}, "nlp_mention_counts": {"ke": 4}, "ld_mention_counts": {"ke": 4, "onto": 4}, "relevance_score": 0.6546758600252723}, {"paperId": "b049c2c2d3f492db34514a0e20c7849befcaef38", "url": "https://www.semanticscholar.org/paper/b049c2c2d3f492db34514a0e20c7849befcaef38", "title": "A Natural Language Processing Pipeline to Extract Phenotypic Data from Formal Taxonomic Descriptions with a Focus on Flagellate Plants", "abstract": "Assembling large-scale phenotypic datasets for evolutionary and biodiversity studies of plants can be extremely difficult and time consuming. New semi-automated Natural Language Processing (NLP) pipelines can extract phenotypic data from taxonomic descriptions, and their performance can be enhanced by incorporating information from ontologies, like the Plant Ontology (PO) and the Plant Trait Ontology (TO). These ontologies are powerful tools for comparing phenotypes across taxa for large-scale evolutionary and ecological analyses, but they are largely focused on terms associated with flowering plants. We describe a bottom-up approach to identify terms from flagellate plants (including bryophytes, lycophytes, ferns, and gymnosperms) that can be added to existing plant ontologies. We first parsed a large corpus of electronic taxonomic descriptions using the Explorer of Taxon Concepts tool (http://taxonconceptexplorer.org/) and identified flagellate plant specific terms that were missing from the existing ontologies. We extracted new structure and trait terms, and we are currently incorporating the missing structure terms to the PO and modifying the definitions of existing terms to expand their coverage to flagellate plants. We will incorporate trait terms to the TO in the near future. Keywords\u2014Natural Language Processing; Plant Ontology; Plant Trait Ontology; taxonomic descriptions; flagellate plants; phenotypic traits; matrices; phylogeny", "venue": "ICBO", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "nlp", "nlp", "nlp", "onto", "onto", "nlp", "onto", "onto"], "mention_counts": {"nlp": 4, "onto": 8}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"onto": 8}, "relevance_score": 0.6546758600252723}, {"paperId": "786dfe6c2ec095b0dca581ed1fd79b7836d6d07c", "url": "https://www.semanticscholar.org/paper/786dfe6c2ec095b0dca581ed1fd79b7836d6d07c", "title": "Automatic Ontology-Based Knowledge Extraction from Web Documents", "abstract": "To bring the Semantic Web to life and provide advanced knowledge services, we need efficient ways to access and extract knowledge from Web documents. Although Web page annotations could facilitate such knowledge gathering, annotations are rare and will probably never be rich or detailed enough to cover all the knowledge these documents contain. Manual annotation is impractical and unscalable, and automatic annotation tools remain largely undeveloped. Specialized knowledge services therefore require tools that can search and extract specific knowledge directly from unstructured text on the Web, guided by an ontology that details what type of knowledge to harvest. An ontology uses concepts and relations to classify domain knowledge. Other researchers have used ontologies to support knowledge extraction, but few have explored their full potential in this domain. The paper considers the Artequakt project which links a knowledge extraction tool with an ontology to achieve continuous knowledge support and guide information extraction. The extraction tool searches online documents and extracts knowledge that matches the given classification structure. It provides this knowledge in a machine-readable format that will be automatically maintained in a knowledge base (KB). Knowledge extraction is further enhanced using a lexicon-based term expansion mechanism that provides extended ontology terminology.", "venue": "IEEE Intelligent Systems", "citationCount": 498, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ke", "ie", "ke", "kg", "ke", "ke", "onto", "ke", "ke", "onto", "kg", "onto", "sw"], "mention_counts": {"onto": 6, "ke": 6, "sw": 1, "kg": 2, "ie": 1}, "nlp_mention_counts": {"ke": 6, "ie": 1}, "ld_mention_counts": {"ke": 6, "sw": 1, "onto": 6, "kg": 2}, "relevance_score": 0.6362851125425543}, {"paperId": "8f03235731160c8418671dec9865b0f10a5b20a9", "url": "https://www.semanticscholar.org/paper/8f03235731160c8418671dec9865b0f10a5b20a9", "title": "OntoNERdIE - Mapping and Linking Ontologies to Named Entity Recognition and Information Extraction Resources", "abstract": "Semantic Web and NLP We describe an implemented offline procedure that maps OWL/RDF-encoded ontologies with large, dynamically maintained instance data to named entity recognition (NER) and information extraction (IE) engine resources, preserving hierarchical concept information and links back to the ontology concepts and instances. The main motivations are (i) improving NER/IE precision and recall in closed domains, (ii) exploiting linguistic knowledge (context, inflection, anaphora) for identifying ontology instances in texts more robustly, (iii) giving full access to ontology instances and concepts in natural language processing results, e.g. for subsequent ontology queries, navigation or inference, (iv) avoiding duplication of work in development and maintenance of similar resources in independent places, namely lingware and ontologies. We show an application in hybrid deep-shallow natural language processing that is e.g. used for question analysis in closed domains. Further applications could be automatic hyperlinking or other innovative semantic-web related applications.", "venue": "LREC", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "nlp", "sw", "onto", "onto", "onto", "ie", "onto", "sw", "rdf", "onto", "nlp", "onto", "nlp"], "mention_counts": {"onto": 8, "nlp": 3, "sw": 2, "rdf": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 3, "ie": 2}, "ld_mention_counts": {"sw": 2, "onto": 8, "rdf": 1}, "relevance_score": 0.6234546105271034}, {"paperId": "b3f2d5f9482fcb617e21599acca6e03aa5e968d1", "url": "https://www.semanticscholar.org/paper/b3f2d5f9482fcb617e21599acca6e03aa5e968d1", "title": "Flexible Ontology Population from Text: The OwlExporter", "abstract": "Ontology population from text is becoming increasingly important for NLP applications. Ontologies in OWL format provide for a standardized means of modeling, querying, and reasoning over large knowledge bases. Populated from natural language texts, they offer significant advantages over traditional export formats, such as plain XML. The development of text analysis systems has been greatly facilitated by modern NLP frameworks, such as the General Architecture for Text Engineering (GATE). However, ontology population is not currently supported by a standard component. We developed a GATE resource called the OwlExporter that allows to easily map existing NLP analysis pipelines to OWL ontologies, thereby allowing language engineers to create ontology population systems without requiring extensive knowledge of ontology APIs. A particular feature of our approach is the concurrent population and linking of a domainand NLP-ontology, including NLP-specific features such as safe reasoning over coreference chains.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 60, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "onto", "onto", "onto", "kg", "nlp", "onto", "nlp", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 5, "onto": 10, "kg": 1}, "nlp_mention_counts": {"nlp": 5}, "ld_mention_counts": {"kg": 1, "onto": 10}, "relevance_score": 0.6234546105271034}, {"paperId": "00634c422cf163a9d8959d78fe2943ff5073e4ce", "url": "https://www.semanticscholar.org/paper/00634c422cf163a9d8959d78fe2943ff5073e4ce", "title": "Extracting Knowledge from Parliamentary Debates for Studying Political Culture and Language", "abstract": "This paper presents knowledge extraction and natural language processing methods used to enrich the knowledge graph of the plenary debates (textual transcripts of speeches) of the Parliament of Finland. This knowledge graph includes some 960 000 speeches (1907\u20132021) interlinked with a prosopographical knowledge graph about the politicians. A recent subset of the speeches was used to extract named entities and topical keywords for semantic searching and browsing the data and for data analysis. The process is based on linguistic analysis, named entity linking, and automatic subject indexing. The results were included into the ParliamentSampo knowledge graph in a SPARQL endpoint. This data can be used for studying parliamentary language and culture in Digital Humanities research and for developing applications, such as the ParliamentSampo portal.", "venue": "TEXT2KG/MK@ESWC", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "nlp", "kg", "kg", "kg", "ke"], "mention_counts": {"nlp": 1, "kg": 4, "ke": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"kg": 4, "ke": 2}, "relevance_score": 0.6160945466525044}, {"paperId": "2f8c0fa4312babeda1891f05245b00d217eb7b8e", "url": "https://www.semanticscholar.org/paper/2f8c0fa4312babeda1891f05245b00d217eb7b8e", "title": "Using Text Analytics for Health to Get Meaningful Insights from a Corpus of COVID Scientific Papers", "abstract": "Since the beginning of COVID pandemic, there have been around 700000 scientific papers published on the subject. A human researcher cannot possibly get acquainted with such a huge text corpus \u2014 and therefore developing AI-based tools to help navigating this corpus and deriving some useful insights from it is highly needed. In this paper, we will use Text Analytics for Health pre-trained service together with some cloud tools to extract some knowledge from scientific papers, gain insights, and build a tool to help researcher navigate the paper collection in a meaningful way. Code. The code for experiments described in this paper is available from http://github.com/CloudAdvocacyAzurePaperExplorationWorkshop. Introduction: Automatic Paper Analysis Automatic scientific paper analysis is fast growing area of studies, and due to recent improvements in NLP techniques is has been greatly improved in the recent years. In this paper, we will show you how to derive specific insights from COVID papers, such as changes in medical treatment over time, or joint treatment strategies using several medications. The main approach we will describe in this paper is to extract as much semi-structured information from text as possible, and then store it into some NoSQL database for further processing. Storing information in the database would allow us to make some very specific queries to answer some of the questions, as well as to provide visual exploration tool for medical expert for structured search and insight generation. Processing large volume of papers is done by running parallel sweep job on Azure Machie Learning cluster. The overall architecture of the proposed system is shown in Fig. 1: Fig.1: Architecture of a system to extract knowledge insights from a corpus of scientific papers. Note that this architecture is built on top of the platform components of Microsoft Azure, which allows us to delegate many complex issues (such as scalability) to the cloud provider. COVID Scientific Papers and CORD Dataset The idea to apply NLP methods to scientific literature seems quite natural and has been proposed in many different works [1,2,3]. First of all, scientific texts are already well-structured, they contain things like keywords, abstract, as well as well-defined terms. Thus, at the very beginning of COVID pandemic, a research challenge has been launched on Kaggle to analyze scientific papers on the subject. The dataset behind this competition is called CORD [4], and it contains constantly updated corpus of everything that is published on topics related to COVID. This dataset consists of the following parts: \u2022 Metadata file Metadata.csv contains most important information for all publications in one place. Each paper in this table has unique identifier cord_uid (which in fact does not happen to be completely unique, once you start working with the dataset). The information includes: Title of publication, Journal, Authors, Abstract, Date of publication, doi \u2022 Full-text papers in document_parses directory that contain structured text in JSON format, which greatly simplifies the analysis. \u2022 Pre-built Document Embeddings that maps cord_uids to float vectors that reflect some overall semantics of the paper. In this paper, we will focus on paper abstracts, because they contain the most important information from the paper. However, for full analysis of the dataset, it makes sense to use the same approach on full texts as well. Natural Language Processing Tasks In the recent years, there has been a huge progress in the field of Natural Language Processing, and very powerful neural network language models have been trained. In the area of NLP, the following tasks are typically considered: \u2022 Text classification / intent recognition \u2014 In this task, we need to classify a piece of text into a number of categories. This is a typical classification task. \u2022 Sentiment Analysis \u2014 We need to return a number that shows how positive or negative the text is. This is a typical regression task. \u2022 Named Entity Recognition (NER) \u2014 In NER, we need to extract named entities from text, and determine their type. For example, we may be looking for names of medicines, or diagnoses. Another task similar to NER is keyword extraction. \u2022 Text summarization \u2014 Here we want to be able to produce a short version of the original text, or to select the most important pieces of text. \u2022 Question Answering \u2014 In this task, we are given a piece of text and a question, and our goal is to find the exact answer to this question from text. \u2022 Open-Domain Question Answering (ODQA) \u2014 The main difference from previous task is that we are given a large corpus of text, and we need to find the answer to our question somewhere in the whole corpus. In [5] have described how we can use ODQA approach to automatically find answers to specific COVID questions. However, this approach does not provide insights into the text corpus. To make some insights from text, NER seems to be the most prominent technique to use. If we can find specific entities that are present in text, we could then perform semantically rich search in text that answers specific questions, as well as obtain data on co-occurrence of different entities, figuring out specific scenarios that interest us. To train NER model, as well as any other neural language model, we need a reasonably large dataset that is properly marked up. Finding those datasets is often not an easy task, and producing them for new problem domain often requires initial human effort to mark up the data. Pre-Trained Language Models and Text Analytics for Health Cognitive Service Luckily, modern transformer language models can be trained in semi-supervised manner using transfer learning. First, the base language model (for example, BERT [6]) is trained on a large corpus of text first, and then can be specialized to a specific task such as classification or NER on a smaller dataset. This transfer learning process can also contain additional step \u2014 further training of generic pretrained model on a domain-specific dataset. For example, in the area of medical science Microsoft Research has pre-trained a model called PubMedBERT [7], using texts from PubMed repository. This model can then be further adopted to different specific tasks, provided we have some specialized datasets available. However, training a model requires a lot of skills and computational power, in addition to a dataset. Microsoft (as well as some other large cloud vendors) also make some pre-trained models available through the REST API. Those services are called Cognitive Services, and one of those services for working with text is called Text Analytics [8]. It can do the following: \u2022 Keyword extraction and NER for some common entity types, such as people, organizations, dates/times, etc. \u2022 Sentiment analysis \u2022 Language Detection \u2022 Entity Linking, by automatically adding internet links to some most common entities. This also performs disambiguation, for example Mars can refer to both the planet or a chocolate bar, and correct link would be used depending on the context. For example, here is the result of analyzing one medical paper abstract by Text Analytics: As you can see, some specific entities (for example, HCQ, which is short for hydroxychloroquine) are not recognized at all. Recently, a special version of the service, called Text Analytics for Health [9] was released, which exposes pre-trained PubMedBERT model with some additional capabilities. Here is the result of extracting entities from the same piece of text using Text Analytics for Health: Text Analytics is a REST service, which can be called by using Text Analytics Python SDK in the following manner: poller = text_analytics_client.begin_analyze_healthcare_entities([txt]) res = list(poller.result()) print(res) In addition to just the list of entities, we also get the following: \u2022 Enity Mapping of entities to standard medical ontologies, such as UMLS [10]. \u2022 Relations between entities inside the text, such as TimeOfCondition, etc. \u2022 Negation, which indicated that an entity was used in negative context, for example COVID-19 diagnosis did not occur. Fig. 2: Results of entity extraction, linking and ontology mapping returned by Text Analytics for Health In addition to using Python SDK, we can also call Text Analytics using REST API directly. This is useful if you are using a programming language that does not have a corresponding SDK, or if you prefer to receive Text Analytics result in the JSON format for further storage or processing. In Python, this can be easily done using requests library: uri = f\"{endpoint}/text/analytics/v3.1/entities/ health/jobs?model-version=v3.1\" headers = { \"Ocp-Apim-Subscription-Key\" : key } resp = requests.post(uri,headers=headers,data=doc) res = resp.json() if res['status'] == 'succeeded': result = t['results'] else: result = None Resulting JSON file will look like this: {\"id\": \"jk62qn0z\", \"entities\": [ {\"offset\": 24, \"length\": 28, \"text\": \"coronavirus disease pandemic\", \"category\": \"Diagnosis\", \"confidenceScore\": 0.98, \"isNegated\": false}, {\"offset\": 54, \"length\": 8, \"text\": \"COVID-19\", \"category\": \"Diagnosis\", \"confidenceScore\": 1.0, \"isNegated\": false, \"links\": [ {\"dataSource\": \"UMLS\", \"id\": \"C5203670\"}, {\"dataSource\": \"ICD10CM\", \"id\": \"U07.1\"}, ... ]}, \"relations\": [ {\"relationType\": \"Abbreviation\", \"bidirectional\": true, \"source\": \"#/results/documents/2/entities/6\", \"target\": \"#/results/documents/2/entities/7\"}, ...], } In production code, one may want to incorporate a mechanism that will retry the operation when an error is returned by the service. Parallel Paper Processing using Azure Machine Learning Cluster Since the dataset currently contains ~700K paper abstracts, processing them sequentially through Text Analytics would be quite time-consuming. To run this code in parallel, we can use technologies such as Azure Batch or Azure Machine Learning [11]. Both allow you to create a cluster of ide", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "nlp", "onto", "ke", "nlp", "nlp"], "mention_counts": {"nlp": 5, "onto": 2, "ke": 1}, "nlp_mention_counts": {"nlp": 5, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 2}, "relevance_score": 0.6160945466525044}, {"paperId": "1b6798695de27880009346c6c2023665139b0014", "url": "https://www.semanticscholar.org/paper/1b6798695de27880009346c6c2023665139b0014", "title": "Ontology-Based Question Answering over Corporate Structured Data", "abstract": "Ontology-based approach to the Natural Language Understanding (NLU) processing allows to improve questions answering quality in dialogue systems. We describe our NLU engine architecture and evaluate its implementation. The engine transforms user's input into the SPARQL SELECT, ASK or INSERT query to the knowledge graph provided by the ontology-based data virtualization platform. The transformation is based on the lexical level of the knowledge graph built according to the Ontolex ontology. The described approach can be applied for graph data population tasks and to the question answering systems implementation, including chat bots. We describe the dialogue engine for a chat bot which can keep the conversation context and ask clarifying questions, simulating some aspects of the human logical thinking. Our approach uses graph-based algorithms to avoid gathering datasets, required in the neural nets-based approaches, and provide better explainability of our models. Using question answering engine in conjunction with data virtualization layer over the corporate data sources allows extracting facts from the structured data to be used in conversation.", "venue": "2021 International Symposium on Knowledge, Ontology, and Theory (KNOTH)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "nlu", "onto", "onto", "nlu", "onto", "kg", "kg", "onto"], "mention_counts": {"kg": 2, "onto": 4, "nlu": 3}, "nlp_mention_counts": {"nlu": 3}, "ld_mention_counts": {"kg": 2, "onto": 4}, "relevance_score": 0.6160945466525044}, {"paperId": "5bc1bb36cb73796ed68535b1ee4e4ba50d4bf0f3", "url": "https://www.semanticscholar.org/paper/5bc1bb36cb73796ed68535b1ee4e4ba50d4bf0f3", "title": "Bridging the Gap Between Ontology and Lexicon via Class-Specific Association Rules Mined from a Loosely-Parallel Text-Data Corpus", "abstract": "There is a well-known lexical gap between content expressed in the form of natural language (NL) texts and content stored in an RDF knowledge base (KB). For tasks such as Information Extraction (IE), this gap needs to be bridged from NL to KB, so that facts extracted from text can be represented in RDF and can then be added to an RDF KB. For tasks such as Natural Language Generation, this gap needs to be bridged from KB to NL, so that facts stored in an RDF KB can be verbalized and read by humans. In this paper we propose LexExMachina, a new methodology that induces correspondences between lexical elements and KB elements by mining class-specific association rules. As an example of such an association rule, consider the rule that predicts that if the text about a person contains the token \"Greek\", then this person has the relation nationality to the entity Greece. Another rule predicts that if the text about a settlement contains the token \"Greek\", then this settlement has the relation country to the entity Greece. Such a rule can help in question answering, as it maps an adjective to the relevant KB terms, and it can help in information extraction from text. We propose and empirically investigate a set of 20 types of class-specific association rules together with different interestingness measures to rank them. We apply our method on a loosely-parallel text-data corpus that consists of data from DBpedia and texts from Wikipedia, and evaluate and provide empirical evidence for the utility of the rules for Question Answering.", "venue": "International Conference on Language, Data, and Knowledge", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "rdf", "nlg", "rdf", "rdf", "rdf", "onto", "kg"], "mention_counts": {"onto": 1, "nlg": 1, "kg": 1, "rdf": 4, "ie": 2}, "nlp_mention_counts": {"nlg": 1, "ie": 2}, "ld_mention_counts": {"kg": 1, "onto": 1, "rdf": 4}, "relevance_score": 0.6160945466525044}, {"paperId": "9f007722471e2855b2d87ef03be7072aeff3bf50", "url": "https://www.semanticscholar.org/paper/9f007722471e2855b2d87ef03be7072aeff3bf50", "title": "Populating a Domain Ontology from a Web Biographical Dictionary of Music - An Unsupervised Rule-based Method to Handle Brazilian Portuguese Texts", "abstract": "An increasing amount of information is available on the web and usually is expressed as text, representing unstructured or semi-structured data. Semantic information is implicit in these texts, since they are mainly intended for human consumption and interpretation. Since unstructured information is not easily handled automatically, an information extraction process has to be used to identify concepts and establish relations among them. Information extraction outcome can be represented as a domain ontology. Ontologies are an appropriate way to represent structured knowledge bases, enabling sharing, reuse and inference. In this paper, an information extraction process is used for populating a domain ontology. It targets Brazilian Portuguese texts from a biographical dictionary of music, which requires specific tools due to some language unique aspects. An unsupervised rule-based method is proposed. Through this process, latent concepts and relations expressed in natural language can be extracted and represented as an ontology, allowing new uses and visualizations of the content, such as semantically browsing and inferring new", "venue": "WEBIST", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "onto", "ie", "onto", "ie", "kg"], "mention_counts": {"kg": 1, "onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"kg": 1, "onto": 5}, "relevance_score": 0.6160945466525044}, {"paperId": "3eb09dfdb66aec5b4276f4368973b42b6821fc2e", "url": "https://www.semanticscholar.org/paper/3eb09dfdb66aec5b4276f4368973b42b6821fc2e", "title": "NERD meets NIF: Lifting NLP Extraction Results to the Linked Data Cloud", "abstract": "We have often heard that data is the new oil. In particular, extracting information from semi-structured textual documents on the Web is key to realize the Linked Data vision. Several attempts have been proposed to extract knowledge from textual documents, extracting named entities, classifying them according to pre-dened taxonomies and disambiguating them through URIs identifying real world entities. As a step towards interconnecting the Web of documents via those entities, dierent extractors have been proposed. Although they share the same main purpose (extracting named entity), they dier from numerous aspects such as their underlying dictionary or ability to disambiguate entities. We have developed NERD, an API and a front-end user interface powered by an ontology to unify various named entity extractors. The unied result output is serialized in RDF according to the NIF specication and published back on the Linked Data cloud. We evaluated NERD with a dataset composed of ve TED talk transcripts, a dataset composed of 1000 New York Times articles and a dataset composed of the 217 abstracts of the papers published at WWW 2011.", "venue": "LDOW", "citationCount": 80, "fieldsOfStudy": ["Computer Science"], "mentions": ["ld", "ld", "ke", "nlp", "ie", "ld", "onto", "rdf"], "mention_counts": {"onto": 1, "ld": 3, "ke": 1, "nlp": 1, "rdf": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 1}, "ld_mention_counts": {"ld": 3, "ke": 1, "onto": 1, "rdf": 1}, "relevance_score": 0.6160945466525044}, {"paperId": "29b8164adfae1cdd5fd4af4ab43d0bed1764e9db", "url": "https://www.semanticscholar.org/paper/29b8164adfae1cdd5fd4af4ab43d0bed1764e9db", "title": "Unsupervised wrapper induction using linked data", "abstract": "This work explores the usage of Linked Data for Web scale Information Extraction and shows encouraging results on the task of Wrapper Induction. We propose a simple knowledge based method which is (i) highly flexible with respect to different domains and (ii) does not require any training material, but exploits Linked Data as background knowledge source to build essential learning resources. The major contribution of this work is a study of how Linked Data - an imprecise, redundant and large-scale knowledge resource - can be used to support Web scale Information Extraction in an effective and efficient way and identify the challenges involved. We show that, for domains that are covered, Linked Data serve as a powerful knowledge resource for Information Extraction. Experiments on a publicly available dataset demonstrate that, under certain conditions, this simple unsupervised approach can achieve competitive results against some complex state of the art that always depends on training data.", "venue": "International Conference on Knowledge Capture", "citationCount": 34, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ld", "ld", "ld", "ld", "ie", "ie", "kg", "ld"], "mention_counts": {"ld": 5, "kg": 1, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"ld": 5, "kg": 1}, "relevance_score": 0.6160945466525044}, {"paperId": "6f9896545e7c1735ec291d163d3aa9eab8827546", "url": "https://www.semanticscholar.org/paper/6f9896545e7c1735ec291d163d3aa9eab8827546", "title": "ENRICHMENT AND POPULATION OF A GEOSPATIAL ONTOLOGY FOR SEMANTIC INFORMATION EXTRACTION", "abstract": "Abstract. The massive amount of user-generated content available today presents a new challenge for the geospatial domain and a great opportunity to delve into linguistic, semantic, and cognitive aspects of geographic information. Ontology-based information extraction is a new, prominent field in which a domain ontology guides the extraction process and the identification of pre-defined concepts, properties, and instances from natural language texts. The paper describes an approach for enriching and populating a geospatial ontology using both a top-down and a bottom-up approach in order to enable semantic information extraction. The top-down approach is applied in order to incorporate knowledge from existing ontologies. The bottom-up approach is applied in order to enrich and populate the geospatial ontology with semantic information (concepts, relations, and instances) extracted from domain-specific web content.\n", "venue": "The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "onto", "ie", "onto", "ie", "onto"], "mention_counts": {"onto": 6, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.6160945466525044}, {"paperId": "3e420b7b052fe1bc6d1c69585a5ad3f38c7f432d", "url": "https://www.semanticscholar.org/paper/3e420b7b052fe1bc6d1c69585a5ad3f38c7f432d", "title": "Hierarchical, perceptron-like learning for ontology-based information extraction", "abstract": "Recent work on ontology-based Information Extraction (IE) has tried to make use of knowledge from the target ontology in order to improve semantic annotation results. However, very few approaches exploit the ontology structure itself, and those that do so, have some limitations. This paper introduces a hierarchical learning approach for IE, which uses the target ontology as an essential part of the extraction process, by taking into account the relations between concepts. The approach is evaluated on the largest available semantically annotated corpus. The results demonstrate clearly the benefits of using knowledge from the ontology as input to the information extraction process. We also demonstrate the advantages of our approach over other state-of-the-art learning systems on a commonly used benchmark dataset.", "venue": "The Web Conference", "citationCount": 75, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "ie", "onto", "onto", "onto", "ie", "onto"], "mention_counts": {"onto": 6, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.6160945466525044}, {"paperId": "be04b5820056ea1216face453bb84c69af6deac0", "url": "https://www.semanticscholar.org/paper/be04b5820056ea1216face453bb84c69af6deac0", "title": "Exploiting ASP for Semantic Information Extraction", "abstract": "The paper describes HiLeX, a new ASP-based system for the extraction of information from unstructured documents. Unlike previous systems, which are mainly syntactic, HiLeX combines both semantic and syntactic knowledge for a powerful information extraction. In particular, the exploitation of background knowledge, stored in a domain ontology, allows to empower significantly the information extraction mechanisms. HiLeX is founded on a new two-dimensional representation of documents, and heavily exploits DLP\u2013 an extension of disjunctive logic programming for ontology representation and reasoning which has been recently implemented on top of DLV . The domain ontology is represented in DLP, and the extraction patterns are encoded by DLP reasoning modules, whose execution yields the actual extraction of information from the input document. HiLeX allows to extract information from both HTML and flat text documents.", "venue": "ASp", "citationCount": 34, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ie", "ie", "ie", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 3, "ie": 6}, "nlp_mention_counts": {"ie": 6}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "49bf3c3215f536882b10b15940795eecaff6d96f", "url": "https://www.semanticscholar.org/paper/49bf3c3215f536882b10b15940795eecaff6d96f", "title": "Ontology-based approach to enhance medical web information extraction", "abstract": "\nPurpose\nThe purpose of this study is to propose a framework for extracting medical information from the Web using domain ontologies. Patient\u2013Doctor conversations have become prevalent on the Web. For instance, solutions like HealthTap or AskTheDoctors allow patients to ask doctors health-related questions. However, most online health-care consumers still struggle to express their questions efficiently due mainly to the expert/layman language and knowledge discrepancy. Extracting information from these layman descriptions, which typically lack expert terminology, is challenging. This hinders the efficiency of the underlying applications such as information retrieval. Herein, an ontology-driven approach is proposed, which aims at extracting information from such sparse descriptions using a meta-model.\n\n\nDesign/methodology/approach\nA meta-model is designed to bridge the gap between the vocabulary of the medical experts and the consumers of the health services. The meta-model is mapped with SNOMED-CT to access the comprehensive medical vocabulary, as well as with WordNet to improve the coverage of layman terms during information extraction. To assess the potential of the approach, an information extraction prototype based on syntactical patterns is implemented.\n\n\nFindings\nThe evaluation of the approach on the gold standard corpus defined in Task1 of ShARe CLEF 2013 showed promising results, an F-score of 0.79 for recognizing medical concepts in real-life medical documents.\n\n\nOriginality/value\nThe originality of the proposed approach lies in the way information is extracted. The context defined through a meta-model proved to be efficient for the task of information extraction, especially from layman descriptions.\n", "venue": "Int. J. Web Inf. Syst.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "ie", "ie", "ie", "onto", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 3, "ie": 6}, "nlp_mention_counts": {"ie": 6}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "4a1aa01243360319a6accd1b187bd741d6ccf66c", "url": "https://www.semanticscholar.org/paper/4a1aa01243360319a6accd1b187bd741d6ccf66c", "title": "Autonomous Decentralized Kernel Cache Architecture for Multi Ontology Based Information Extraction on Microsoft Windows", "abstract": "Ontology Based Information Extraction (OBIE) is being adopted in various domains in order to improve the system's precision and recall. Though use of multiple ontologies in different semantic based Information Extraction systems helps to improve the system extraction accuracy but the performance of system degrades significantly. This paper proposes autonomous decentralized kernel cache architecture to improve the query time for OBIE systems that utilize multiple ontologies. In order to minimize performance bottlenecks we propose caching most frequently used relations of the ontologies in the autonomous and decentralized kernel cache in order to reduce the query time against different semantic queries. Results show performance improvement for different queries made on YAGO2s and DBpedia ontologies using the proposed kernel based Onto-Cache.", "venue": "2017 IEEE 13th International Symposium on Autonomous Decentralized System (ISADS)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "onto", "onto", "onto", "onto", "onto", "ie", "ie"], "mention_counts": {"onto": 6, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.6160945466525044}, {"paperId": "a9428763a64ed8b88942370ee180d10943493f29", "url": "https://www.semanticscholar.org/paper/a9428763a64ed8b88942370ee180d10943493f29", "title": "Biological ontology enhancement with fuzzy relations: a text-mining framework", "abstract": "Domain ontology can help in information retrieval from documents. But ontology is a pre-defined structure with crisp concept descriptions and inter-concept relations. However, due to the dynamic nature of the document repository, ontology should be upgradeable with information extracted through text mining of documents in the domain. This also necessitates that concepts, their descriptions and inter-concept relations should be associated with a degree of fuzziness that will indicate the support for the extracted knowledge according to the currently available resources. Supports may be revised with more knowledge coming in future. This approach preserves the basic structured knowledge format for storing domain knowledge, but at the same time allows for update of information. In this paper, we have proposed a mechanism which initiates text mining with a set of ontological concepts, and thereafter extracts fuzzy relations through text mining. Membership values of relations are functions of frequency of co-occurrence of concepts and relations. We have worked on the GENIA corpus and shown how fuzzy relations can be further used for guided information extraction from MEDLINE documents.", "venue": "International Conference on Wirtschaftsinformatik", "citationCount": 23, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "onto", "onto", "onto", "ke", "ie", "onto"], "mention_counts": {"ke": 1, "onto": 5, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 5}, "relevance_score": 0.6160945466525044}, {"paperId": "72beb9f1a95275bbf974c8f2ff05b1248169a263", "url": "https://www.semanticscholar.org/paper/72beb9f1a95275bbf974c8f2ff05b1248169a263", "title": "Event Extraction and Representation: A Case Study for the Portuguese Language", "abstract": "Text information extraction is an important natural language processing (NLP) task, which aims to automatically identify, extract, and represent information from text. In this context, event extraction plays a relevant role, allowing actions, agents, objects, places, and time periods to be identified and represented. The extracted information can be represented by specialized ontologies, supporting knowledge-based reasoning and inference processes. In this work, we will describe, in detail, our proposal for event extraction from Portuguese documents. The proposed approach is based on a pipeline of specialized natural language processing tools; namely, a part-of-speech tagger, a named entities recognizer, a dependency parser, semantic role labeling, and a knowledge extraction module. The architecture is language-independent, but its modules are language-dependent and can be built using adequate AI (i.e., rule-based or machine learning) methodologies. The developed system was evaluated with a corpus of Portuguese texts and the obtained results are presented and analysed. The current limitations and future work are discussed in detail.", "venue": "Inf.", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ke", "nlp", "nlp", "nlp", "ie", "kg"], "mention_counts": {"onto": 1, "nlp": 3, "ke": 1, "kg": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 3, "ke": 1, "ie": 2}, "ld_mention_counts": {"ke": 1, "onto": 1, "kg": 1}, "relevance_score": 0.6160945466525044}, {"paperId": "7aec6f54b6f2949f263306e24200b0b43ffcdb83", "url": "https://www.semanticscholar.org/paper/7aec6f54b6f2949f263306e24200b0b43ffcdb83", "title": "The Cancerology ontology: Designed to support the search of evidence-based oncology from biomedical literatures", "abstract": "This work proposes a new ontology, called the Cancerology, where it faces a problem of unclear analysis in a biomedical text processing because existing ontologies such National Cancer Institute's Thesaurus and Ontology do not offer some information relating to domain specific variations in terms that can be provided by the domain expert. This ontology is experimented through a method of text classification with retrieving the relevant cervix cancer abstracts relating to clinical trials from PubMed. The experimental results show more effectiveness for increasing the accuracy. This demonstrates that the Cancerology may be also effective for other areas of text processing and analysis, especially in the particular domain of oncology literature such as intelligent search service, text mining, and knowledge extraction.", "venue": "2011 24th International Symposium on Computer-Based Medical Systems (CBMS)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "tp", "onto", "ke", "onto", "tp", "onto", "onto"], "mention_counts": {"ke": 1, "onto": 5, "tp": 2}, "nlp_mention_counts": {"ke": 1, "tp": 2}, "ld_mention_counts": {"ke": 1, "onto": 5}, "relevance_score": 0.6160945466525044}, {"paperId": "3fc27edf7ed52cddf56118586d5bf418cdcf1855", "url": "https://www.semanticscholar.org/paper/3fc27edf7ed52cddf56118586d5bf418cdcf1855", "title": "A Framework for Institutional Risk Identification using Knowledge Graphs and Automated News Profiling", "abstract": "Organizations around the world face an array of risks impacting their operations globally. It is imperative to have a robust risk identification process to detect and evaluate the impact of potential risks before they materialize. Given the nature of the task and the current requirements of deep subject matter expertise, most organizations utilize a heavily manual process. In our work, we develop an automated system that (a) continuously monitors global news, (b) is able to autonomously identify and characterize risks, (c) is able to determine the proximity of reaching triggers to determine the distance from the manifestation of the risk impact and (d) identifies organization\u2019s operational areas that may be most impacted by the risk. Other contributions also include: (a) a knowledge graph representation of risks and (b) relevant news matching to risks identified by the organization utilizing a neural embedding model to match the textual description of a given risk with multi-lingual news. Introduction & Related Work Global institutions are exposed to various types of risks, ranging from market risks related to the institution\u2019s core functions (CFI 2020) (BOE 2020) to operational, compliance, cyber-security, geopolitical and reputational risks (Matellio 2020). Risks in these areas are inherently hard to identify and quantify. Risk mitigation is also extremely challenging, which is why the runway provided by its identification and quantification is crucial. Unfortunately, the lack of proper risk assessment has led to the demise of several organizations once the risk manifested (De Haas and Van Horen 2012). In our work, we present a system for risk identification utilizing knowledge graphs for representing risk areas and a neural embedding model (Reimers and Gurevych 2019) for multi-lingual news matching tailored towards financial institutions. The formal definition of a risk and the study of methods for risk assessment and mitigation has a long history of academic research (Henley and Kumamoto 1981), (Covello and Mumpower 1985), (Rechard 1999), (Bedford, Cooke et al. 2001), (Thompson, Deisler Jr, and Schwing 2005) and (Zio 2009). Similarly, the use of natural language processing and Copyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. knowledge graphs for news recommendation has been studied extensively and is used widely in practice. The majority of the work has focused on developing news recommendation systems tailored towards users preferences. (Wang et al. 2020) describes a news recommendation system employed by a major financial ratings agency utilizing a neural embedding model developed by (Peters et al. 2018) for news contextual embeddings representation. Other approaches to news recommendations include (IJntema et al. 2010) which utilize externally developed ontologies to find news, collaborative filtering (Lu et al. 2015) and graph embeddings (Ren, Long, and Xu 2019). System Architecture The system proposed (Figure 1) consists of four main components starting with a given set of risks identified by domain experts and producing a list of relevant news for each risk. The input to the system is a repository of risk descriptions provided by the domain experts. The first component extracts a set of relevant entities from the textual description of the risk. The second component uses these entities to construct a knowledge graph. The third component searches for a set of keywords related to the risk and parses them from multiple news sources. The fourth component is a neural network model used to rank news events using contextual embeddings generated for headlines as well as the risk descriptors. In order to demonstrate the end-to-end workflow of our system, we created a set of artificial risks shown in table 1 which a financial institution would face inspired from the risk types defined in (Matellio 2020). Risk Information Extraction Given a textual description of the risk, the text is decomposed into (1) trigger (the root cause of the risk), (2) outcome (the impact of the given risk) and (3) exposure vessel (the entity/vessel the risk impacts). Several approaches were tested to decompose the text into the three categories above. One of these is based on a deep bi-LSTM neural network sequence prediction model developed by (Stanovsky et al. 2018) for supervised open information extraction. The model breaks a given sentence (in our case the risk text) into the relationships they express. In particular, the model extracts a list of propositions, each ar X iv :2 10 9. 09 10 3v 1 [ cs .A I] 1 9 Se p 20 21 (1) Cyber-attacks targeting the retail banking business causing a loss of customer data (2) US China trade war escalation affecting the corporate and investment banking business causing a decrease in revenues (3) Employee misconduct in the investment banking business causing a reputational damage (4) Technology infrastructure failure in the corporate and investment banking business causing a reputational damage and/or monetary loss Table 1: Examples of Institutional Risks", "venue": "ArXiv", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "onto", "kg", "ie", "kg", "nlp", "kg", "ie"], "mention_counts": {"nlp": 1, "kg": 5, "onto": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 1, "ie": 2}, "ld_mention_counts": {"kg": 5, "onto": 1}, "relevance_score": 0.6160945466525044}, {"paperId": "31163b803bfeb79a3e7e5f5a7fdebdae3cc880e8", "url": "https://www.semanticscholar.org/paper/31163b803bfeb79a3e7e5f5a7fdebdae3cc880e8", "title": "Natural Language Processing of Textual Requirements", "abstract": "Natural language processing (NLP) is the application of automated parsing and machine learning techniques to ana lyze standard text. Applications of NLP to requirements engineering include extraction of ontologies from a requirements speci fication, and use of NLP to verify the consistency and/or completion of a requirements specification. This work-in-progress paper describes a new approach to the interpretation, organizati on and management of textual requirements through the use of application-specific ontologies and natural language proc essing. We also design and exercise a prototype software tool that implements the new framework on a simplified model of an aircraft. Keywords-Systems Engineering; Ontologies; Natural Language Processing; Requirements; Rule Checking.", "venue": "International Conference on Systems", "citationCount": 26, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "onto", "nlp", "nlp", "nlp", "nlp", "onto", "onto"], "mention_counts": {"nlp": 6, "onto": 3}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "4d4d82e3b95a23a5d7ba711b20e79229eaf43c0d", "url": "https://www.semanticscholar.org/paper/4d4d82e3b95a23a5d7ba711b20e79229eaf43c0d", "title": "Bio-SODA: Enabling Natural Language Question Answering over Knowledge Graphs without Training Data", "abstract": "The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20% and by an even higher factor on more complex bioinformatics datasets.", "venue": "International Conference on Statistical and Scientific Database Management", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "kg", "ld", "kg", "kg", "nlp", "kg", "nlp", "nlp"], "mention_counts": {"ld": 1, "sw": 1, "nlp": 3, "kg": 4}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"ld": 1, "sw": 1, "kg": 4}, "relevance_score": 0.6160945466525044}, {"paperId": "25fd8d3757b342534a1f569633fd62001bb16421", "url": "https://www.semanticscholar.org/paper/25fd8d3757b342534a1f569633fd62001bb16421", "title": "Exploiters-Based Knowledge Extraction in Object-Oriented Knowledge Representation", "abstract": "This paper contains the consideration of knowledge extraction mechanisms of such object-oriented knowledge representation models as frames, object-oriented programming and object-oriented dynamic networks. In addition, conception of universal exploiters within object-oriented dynamic networks is also discussed. The main result of the paper is introduction of new exploiters-based knowledge extraction approach, which provides generation of a finite set of new classes of objects, based on the basic set of classes. The methods for calculation of quantity of new classes, which can be obtained using proposed approach, and of quantity of types, which each of them describes, are proposed. Proof that basic set of classes, extended according to proposed approach, together with union exploiter create upper semilattice is given. The approach always allows generating of finitely defined set of new classes of objects for any object-oriented dynamic network. A quantity of these classes can be precisely calculated before the generation. It allows saving of only basic set of classes in the knowledge base.", "venue": "CS&P", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "kg", "ke", "ke", "kg"], "mention_counts": {"ke": 3, "kg": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "c6948c903421753b5ae5b4a14fffddb66890152c", "url": "https://www.semanticscholar.org/paper/c6948c903421753b5ae5b4a14fffddb66890152c", "title": "A Multi-intelligent Agent Architecture for Knowledge Extraction: Novel Approaches for Automatic Production Rules Extraction", "abstract": "In this paper, multi-intelligent agent architecture has been proposed for automatic knowledge extraction from its resources (domain experts and text documents). The extracted knowledge should be stored in a knowledge base to be used later by knowledge-based systems. This article aims to produce an effective knowledge base by cooperation between expert mining and text mining techniques. Firstly, we are constructing an Expert Mining Intelligent Agent (EMIA) able to interview with domain experts for mining problem solving knowledge as production rules in a specific diagnosis domain. It is also responsible for extracting the patterns or linguistic expressions and save it in a conceptual database. Secondly, we are constructing a Text Mining Intelligent Agent (TMIA) capable of extracting production rules from a text document corpus. The achievement of that extraction can be performed by a text document categorization based on a traditional term weighting scheme (TF-IDF) and using the Stanford parser to analyze and produce a parsing tree for each sentence in that document. Then, the TMIA looks for all causal words and takes them as separation words to generate patterns and sub-patterns based on the conceptual database. Finally, the TMIA stores those patterns and sub-patterns in a pre-formatted template and displays it to a domain expert for a modification process to construct accurate production rule.", "venue": "MUE 2014", "citationCount": 15, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "kg", "ke", "ke", "ke"], "mention_counts": {"kg": 3, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 3, "ke": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "bfff31c2a5d6ddf2740244ed4ee41e1757cf32a2", "url": "https://www.semanticscholar.org/paper/bfff31c2a5d6ddf2740244ed4ee41e1757cf32a2", "title": "Extracting Descriptions of Location Relations from Implicit Textual Networks", "abstract": "For the retrieval of concise entity relation information from large collections or streams of documents, existing approaches can be grouped into the categories of (multi-document) summarization and knowledge extraction. The former tend to fall short for this task due to the involved amount of information that cannot be easily condensed, while knowledge extraction approaches are often pattern-based and too discriminative for exploratory purposes. For location relations in particular, this translates to a set of very short relationship descriptors that predominantly encode hierarchical or containment relations such as located in or capital of. As a result, available knowledge bases that are typically populated through knowledge extraction are limited to these discrete and typed relations. In contrast, the representation of document collections as implicit networks of entities, terms, and sentences has emerged as a way to encode a much wider range of entity relations and occurrences, which can be leveraged for filtering the relevant information and enabling subsequent interactive explorations. In this paper, we discuss the extraction of descriptive sentences for sets of entities from such implicit networks to support an interactive exploration, and apply them to the extraction of complex location relations that are not hierarchical or containment-based. We introduce and compare efficient ranking methods for sentence extraction that address this entity-centric search task by leveraging entity and term relations in implicit network representations of large document collections. Based on Wikipedia articles and Wikidata as a knowledge base, we demonstrate the extraction of novel location relations that are not contained in the knowledge base.", "venue": "GIR", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "kg", "kg", "ke", "ke"], "mention_counts": {"kg": 3, "ke": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"kg": 3, "ke": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "1f37f644793050108dc8367ed6960abe2874ea0d", "url": "https://www.semanticscholar.org/paper/1f37f644793050108dc8367ed6960abe2874ea0d", "title": "LYAM++ results for OAEI 2015", "abstract": "The paper presents a novel technique for aligning cross-lingual ontologies that does not rely on machine translation, but uses the large multilingual semantic network BabelNet as a source of background knowledge. In addition, our approach applies a novel orchestration of the components of the matching workflow. We present our results on the evaluation challenge Multifarm. 1 Presentation of the System In spite of the considerable advance that has been made in the field of on-tology matching recently, many questions remain open [1]. The current work addresses the challenge of using background knowledge with a focus on aligning cross-lingual ontologies, i.e., ontologies defined in different natural languages [2]. Indeed, considering multilingual and cross-lingual information is becoming more and more important, in view particularly of the growing number of web content-creating non-English users and the clear demand of cross-language in-teroperability. In the context of the web of data, it is important to propose procedures for linking vocabularies across natural languages, in order to foster the creation of a veritable global information network. The use of different natural languages in the concepts and relations labeling process is becoming an important source of ontology heterogeneity. The methods that have been proposed to deal with it most commonly rely on automatic translation of labels to a single target language [3] or apply machine learning techniques [2]. However, machine translation tolerates low precision levels and machine learning methods require large training corpus that is rarely available in an ontology matching scenario. An inherent problem of translation is that there is often a lack of exact one-to-one correspondence between the terms in different natural languages. 1.1 State, Purpose, General Statement We present LYAM++ (Yet Another Matcher-Light)[4], a fully automatic cross-lingual ontology matching system that does not rely on machine translation. Instead, we make use of the openly available general-purpose multilingual semantic network BabelNet 1 in order to recreate the missing semantic context 1 http://babelnet.org/", "venue": "Organizational Memories", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "onto", "onto", "onto", "mt", "mt", "onto", "onto"], "mention_counts": {"onto": 6, "mt": 3}, "nlp_mention_counts": {"mt": 3}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.6160945466525044}, {"paperId": "f3ec7b83f58965faec4972b7678a4a6f930f1ab8", "url": "https://www.semanticscholar.org/paper/f3ec7b83f58965faec4972b7678a4a6f930f1ab8", "title": "Teanga: A Linked Data based platform for Natural Language Processing", "abstract": "In this paper, we describe Teanga, a linked data based platform for natural language processing (NLP). Teanga enables the use of many NLP services from a single interface, whether the need was to use a single service or multiple services in a pipeline. Teanga focuses on the problem of NLP services interoperability by using linked data to define the types of services input and output. Teanga\u2019s strengths include being easy to install and run, easy to use, able to run multiple NLP tasks from one interface and helping users to build a pipeline of tasks through a graphical user interface.", "venue": "International Conference on Language Resources and Evaluation", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "ld", "ld", "nlp", "nlp", "ld", "nlp", "nlp", "nlp"], "mention_counts": {"ld": 3, "nlp": 6}, "nlp_mention_counts": {"nlp": 6}, "ld_mention_counts": {"ld": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "8a6a9910e9a1a89eea1ed271bd178b337733eb30", "url": "https://www.semanticscholar.org/paper/8a6a9910e9a1a89eea1ed271bd178b337733eb30", "title": "Practical Approach to Knowledge-based Question Answering with Natural Language Understanding and Advanced Reasoning", "abstract": "The complexity of natural language and the open-domain nature of the World Wide Web have caused modem-day question answering systems to rely only on information retrieval techniques and shallow natural language processing tasks. This approach has brought about serious drawbacks namely restriction on the nature of question and response. This restriction constitutes the first problem addressed by this \nresearch. Through recent academic works, many researchers have begun to acknowledge the problem and agreed that the solution comes in the form of a new approach based on natural language understanding and reasoning in a knowledge-based environment. Due to the infancy stage of this new approach and practical consideration, the current practices vary greatly and are mostly based on only low-level natural language understanding, minimalist representation formalism and conventional reasoning approach without advanced features. As a result, not only were these systems found to be inadequate to solve the first problem but have also created the second problem, that is the limitation to scale across domains and to real-life natural language text. This research hypothesized that a practical approach in the form of a solution framework which combines full-discourse natural language understanding, powerful representation formalism capable of exploiting ontological information and reasoning approach with advanced features, will solve both the first and second problem without compromising practicality factors.The solution framework is implemented as a system called \"Natural Language Understanding and \nReasoning for Intelligence\" (NaLURI). More importantly, two evaluations and their results are presented to demonstrate that the inclusion of more demanding features into a question answering system will not only allow for a wider range of questions and better response quality, but does not affect the response time, hence approving the hypothesis of this research.", "venue": "ArXiv", "citationCount": 19, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlu", "nlp", "nlu", "kg", "nlu", "nlu", "nlu", "kg"], "mention_counts": {"nlp": 1, "onto": 1, "kg": 2, "nlu": 5}, "nlp_mention_counts": {"nlp": 1, "nlu": 5}, "ld_mention_counts": {"kg": 2, "onto": 1}, "relevance_score": 0.6160945466525044}, {"paperId": "6c94c0968b2ef08f8ac5cae64874e0a8d1adc138", "url": "https://www.semanticscholar.org/paper/6c94c0968b2ef08f8ac5cae64874e0a8d1adc138", "title": "How to Prepare an API for Programming in Natural Language", "abstract": "Natural language interfaces are becoming more and more common but are extremely difficult to build, to maintain, and to port to new domains. NLCI, the Natural Language Command Interpreter, is an architecture for building and porting such interfaces quickly.\nNLCI accepts commands as plain English texts and translates the input sentences into sequences of API calls that implement the intended actions. At its core is an ontology that models the API that is to be used. Then a natural language understanding pipeline analyzes the English input and generates source code. The analyses are independent of a particular API; switching\nto a different API only requires provision of a new ontology.\nIn this demonstration we show how a developer can provide a natural language interface for his or her API by preparing an API ontology. We also show how NLCI analyzes the input text, how we evaluated its results, and how well it performs. As an example we use an API that steers a Lego EV3 robot.", "venue": "SEMANTiCS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlu", "onto", "nlg", "nll", "onto", "nle", "onto", "nlu", "nlp"], "mention_counts": {"onto": 3, "nll": 1, "nlu": 2, "nlp": 1, "nlg": 1, "nle": 1}, "nlp_mention_counts": {"nll": 1, "nlu": 2, "nlp": 1, "nlg": 1, "nle": 1}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.6160945466525044}, {"paperId": "6f42de94eb1d897e031db991c135a7e385d1740e", "url": "https://www.semanticscholar.org/paper/6f42de94eb1d897e031db991c135a7e385d1740e", "title": "Review: Ontological Semantics by Sergei Nirenburg and Victor Raskin", "abstract": "The book under review presents one particular approach to natural language understanding with emphasis on automatic semantic analysis. \"Ontological Semantics\" is in fact a name for the \"ideal form\" of the system the authors propose together with the related theories and assumptions. The monograph is divided into two major parts: a theoretical Part I and a practical Part II. Being more a computational linguist than a formal semanticist I would like to focus my attention mainly on the second part describing the layout of a complete natural language processing system. Part I may be viewed as a (polemic) introduction to computational linguistics, semantics, formal ontology, partly even to the theory of science. The practical motivation of the authors is apparent in their argumention against the efforts to base the entire natural language understanding system on a single formalism which, being suitable for one task, makes it unnecessarily hard or impossible to deal with another. Instead, they argue for a concept of \"microtheories\" ? separable and replaceable modules corresponding to the respective tasks of natural language analysis (morphological and syntactic analysis, coreference, temporality, word sense desambiguation etc.) and communicating with each other in a well-defined manner. Part II actually describes such a system. First, the central notion of \"text meaning representation\" (TMR) is introduced. TMR is basically a set of propositions connected through text-level discourse relations. Additionally, there is an arbitrary number of modalities and references together with a single TMR time specification (contains the time specifications for the respective propositions) and a single style specification. These basic categories contain graph structures of complex values and pointers. In spite of the complexity of the concrete attributes this approach is in general very intuitive ? the relevant information is captured as a graph centered around the respective propositions augmenting any information necessary with attribute values and binary relations. The system relies heavily on the use of external knowledge, which is divided into four components: the ontology, the fact repository, the lexicon, and the onomasticon. The ontology represents a concept (type) hierarchy. The actual instances of a type (e.g. London of type City) are present in the fact repository. The language expressions (e.g. the word \"London\") representing a proper name are listed in the onomasticon and linked to the entities of the fact repository. The lexicon contains all other language material and is linked to the ontology. A separate chapter describes ways of \u2026", "venue": "Prague Bull. Math. Linguistics", "citationCount": 2, "fieldsOfStudy": ["Philosophy", "Computer Science"], "mentions": ["nlu", "nlp", "nlu", "onto", "onto", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 6, "nlu": 2}, "nlp_mention_counts": {"nlp": 1, "nlu": 2}, "ld_mention_counts": {"onto": 6}, "relevance_score": 0.6160945466525044}, {"paperId": "347926b02188fb6d9e6839ab027cc4a34394c73d", "url": "https://www.semanticscholar.org/paper/347926b02188fb6d9e6839ab027cc4a34394c73d", "title": "Linguistic Linked Open Data (LLOD) \u2013 Building the cloud", "abstract": "The last decades have seen an immense maturation of Natural Language Processing (NLP) and an increased interest to apply NLP techniques and resources to real-world applications in business and academia. This process has certainly been facilitated by the increased availability of language data in the internet age, and the subsequent paradigm shift to statistical approaches, but also it coincided with an increasing acceptance of empirical approaches in linguistics and related academic fields, including empirical approaches to typology (Greenberg, 1963), corpus linguistics (Francis and Kucera, 1979, Brown Corpus), and (computational) lexicography (Kucera, 1969), as well as the dawn of Digital Humanities (Busa, 1974). Given the complexity of language and the analysis of linguistic data on different levels, its investigation involves a broad band-width of formalisms and resources used to analyze, process and generate natural language. With the transition to empirical, data-driven research, the primary challenge in the field is thus to store, connect and exploit the wealth of language data available in all its heterogeneity. Interoperability of language resources has hence been an important issue addressed by the community since the late 1980s (Text Encoding Initiative, 1990), but still remains a problem that is solved only partially, i.e., on the level of specific sub-types of linguistic resources, such as lexical resources (Francopoulo et al., 2006) or annotated corpora (Ide and Suderman, 2007), respectively. A closely related challenge is information integration, i.e., how information from different sources can be retrieved and combined in an efficient way. Recently, both challenges have been addressed by means of Linked Data principles (Chiarcos et al., 2013a,b), eventually leading to the formation of a Linguistic Linked Open Data (LLOD) cloud (Chiarcos et al., 2012b). The talk describes its current state of development, it presents selected examples for main types of linguistic resources in the LLOD cloud, and objectives leading to the adaptation of Linked Data principles for any of these. Further, the talk elaborates on history and goals behind this effort, its relation to established standardization initiatives in the field, and on-going community activities conducted under the umbrella of the Open Linguistics Working Group (OWLG) of the Open Knowledge Foundation (Chiarcos et al., 2012a), an initiative of experts from various fields concerned with linguistic data, which works towards", "venue": "SWAIE@RANLP", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "nlp", "llod", "nlp", "llod", "lod", "ld", "llod", "ld", "llod", "nlp", "llod", "lod"], "mention_counts": {"ld": 2, "nlp": 3, "nlg": 1, "lod": 2, "llod": 5}, "nlp_mention_counts": {"nlp": 3, "nlg": 1}, "ld_mention_counts": {"ld": 2, "llod": 5, "lod": 2}, "relevance_score": 0.608623419919635}, {"paperId": "09a7c04ad3b8a664db4242892683e24d2c3b3a3d", "url": "https://www.semanticscholar.org/paper/09a7c04ad3b8a664db4242892683e24d2c3b3a3d", "title": "From Research Articles to Knowledge Graphs", "abstract": "Understanding and extracting knowledge contained in text and encoding it as linked data for the WEB is a highly complex task that poses several challenges, requiring expertise from different fields such as conceptual modeling, natural language processing and web technologies including web mining, linked data generation and publishing, etc. When it comes to the scholarly domain, the transformation of human readable research articles into machine comprehensible knowledge bases is considered of high importance and necessity today due to the explosion of scientific publications in every major discipline, that makes it increasingly difficult for experts to maintain an overview of their domain or relate ideas from different domains. This situation could be significantly alleviated by knowledge bases capable of supporting queries such as: find all papers that address a given problem; how was the problem solved; which methods are employed by whom in addressing particular tasks; etc. that currently cannot be addressed by commonly used search engines such as Google Scholar or Semantic Scholar. This tutorial addresses the above challenge by introducing the participants to methods required in order to model knowledge regarding a given domain, extract information from available texts using advanced machine learning techniques, associate it with other information mined from the web in order to infer new knowledge and republish everything as linked open data on the Web. To this end, we will use a specific use case \u2013 that of the scholarly domain, and will show how to model research processes, extract them from research articles, associate them with contextual information from article metadata and other linked repositories and create knowledge bases available as linked data. Our aim is to show how methodologies from different computer science fields, namely natural language processing, machine learning and conceptual modeling, can be combined with Web technologies in a single meaningful workflow.", "venue": "WWW", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "kg", "kg", "kg", "kg", "ld", "nlp", "ld", "ie", "lod", "ld", "ke"], "mention_counts": {"ld": 3, "nlp": 2, "lod": 1, "ke": 1, "kg": 4, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1, "ie": 1}, "ld_mention_counts": {"ld": 3, "kg": 4, "lod": 1, "ke": 1}, "relevance_score": 0.608623419919635}, {"paperId": "23a94bd104f4da5c7f536cf4c92c685230a1fae4", "url": "https://www.semanticscholar.org/paper/23a94bd104f4da5c7f536cf4c92c685230a1fae4", "title": "ROXXI: Reviving witness dOcuments to eXplore eXtracted Information", "abstract": "In recent years, there has been considerable research on information extraction and constructing RDF knowledge bases. In general, the goal is to extract all relevant information from a corpus of documents, store it into an ontology, and answer future queries based only on the created knowledge base. Thus, the original documents become dispensable. On the one hand, an ontology is a convenient and non-redundant structured source of information, based on which specific queries can be answered efficiently. On the other hand, many users doubt the correctness of facts and ontology subgraphs presented to them as query results without proof. Instead, users often wish to verify the obtained facts or subgraphs by reading about them in context, i.e., in a document relating the facts and providing background information. In this demo, we present ROXXI, a system operating on top of an existing knowledge base and reviving the abandoned witness documents. In doing so, it goes the opposite way of information extraction approaches -- starting with ontological facts and tracing their way back to the documents they were extracted from. ROXXI offers interfaces for expert users (SPARQL) as well as for non-experts (ontology browser) and provides a ranked list of documents each associated with a content snippet highlighting the queried facts in context. At the demonstration site, we will show the advantages of this novel approach towards document retrieval and illustrate the benefits of reviving the documents that information extraction approaches neglect.", "venue": "Proceedings of the VLDB Endowment", "citationCount": 16, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "rdf", "ie", "ie", "kg", "kg", "onto", "onto", "onto", "ie", "kg", "ie", "onto"], "mention_counts": {"kg": 3, "onto": 5, "rdf": 1, "ie": 4}, "nlp_mention_counts": {"ie": 4}, "ld_mention_counts": {"kg": 3, "onto": 5, "rdf": 1}, "relevance_score": 0.608623419919635}, {"paperId": "90265c0a6c44aea84ddb0a4bdf0cba0ec640cb60", "url": "https://www.semanticscholar.org/paper/90265c0a6c44aea84ddb0a4bdf0cba0ec640cb60", "title": "Using Semantic Web Technology to Support NLG. Case Study: OWL finds RAGS", "abstract": "The semantic web is a general vision for supporting knowledge-based processing across the WWW and its successors. As such, semantic web technology has potential to support the exchange and processing of complex NLG data. This paper discusses one particular approach to data sharing and exchange that was developed for NLG - the RAGS framework. This was developed independently of the semantic web. RAGS was relatively complex and involved a number of idiosyncratic features. However, we present a rational reconstruction of RAGS in terms of semantic web concepts, which yields a relatively simple approach that can exploit semantic web technology directly. Given that RAGS was motivated by the concerns of the NLG community, it is perhaps remarkable that its aspirations seem to fit so well with semantic web technology.", "venue": "INLG", "citationCount": 8, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlg", "sw", "sw", "nlg", "sw", "kg", "sw", "sw", "sw", "sw", "nlg", "nlg", "onto"], "mention_counts": {"sw": 7, "nlg": 4, "onto": 1, "kg": 1}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"sw": 7, "onto": 1, "kg": 1}, "relevance_score": 0.608623419919635}, {"paperId": "a789dad4ea1ea14e44db93d1e8d181ba92a8f78c", "url": "https://www.semanticscholar.org/paper/a789dad4ea1ea14e44db93d1e8d181ba92a8f78c", "title": "Towards an OWL-based framework for extracting information from clinical texts", "abstract": "In this paper, we present our ongoing work towards an OWL-based framework for extracting a variety of information (including patient history) from clinical texts. Our framework integrates a well-known natural language processing (NLP) system by converting its ontology and output logical form interpretation into the Web Ontology Language (OWL). The OWL-based Semantic Query-Enhanced Web Rule Language (SQWRL) is then used as a platform for authoring Semantic Web-aware rules for extracting information of interest from the OWL knowledge based created from parsing a clinical report. We also describe our ongoing work on using this system for extracting a timeline-based patient medical record from the history of present illness section of clinical texts.", "venue": "ACM International Conference on Bioinformatics, Computational Biology and Biomedicine", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "onto", "ie", "onto", "kg", "sw", "onto", "nlp", "onto", "nlp", "onto"], "mention_counts": {"onto": 7, "nlp": 2, "sw": 1, "kg": 1, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"kg": 1, "sw": 1, "onto": 7}, "relevance_score": 0.608623419919635}, {"paperId": "dccc0fc798935c891aa5d8ea1337e73c346acd8f", "url": "https://www.semanticscholar.org/paper/dccc0fc798935c891aa5d8ea1337e73c346acd8f", "title": "Modeling UIMA type system using web ontology language: towards interoperability among UIMA-based NLP tools", "abstract": "With the recent development and adoption of NLP framework architectures, NLP modules/tools developed independently in the research community can be adopted as integrated applications. Development of wrappers and interfaces required to adopt NLP modules/tools, however, still requires huge amount of efforts. In this paper, we focus on one NLP framework architecture, UIMA (Unstructured Information Management Architecture), which defines annotations as types described in a type system and can achieve direct interoperability if a common type system is used. We explore the use of ontology to model UIMA types and argue existing ontology development or reasoning tools can be utilized to understand types (we use types and annotations interchangeably) from existing NLP systems developed under UIMA, define equivalent annotations in different NLP systems, and apply the practice in the ontology community to draw agreements on the definition of common NLP types, thereby achieving better interoperability among NLP modules/tools.", "venue": "MIXHS '12", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "nlp", "nlp", "nlp", "nlp", "nlp", "nlp", "nlp", "nlp", "onto", "nlp"], "mention_counts": {"nlp": 9, "onto": 4}, "nlp_mention_counts": {"nlp": 9}, "ld_mention_counts": {"onto": 4}, "relevance_score": 0.608623419919635}, {"paperId": "9ebac0d205739c3d0cfe4ff273f7f54dc99e89b2", "url": "https://www.semanticscholar.org/paper/9ebac0d205739c3d0cfe4ff273f7f54dc99e89b2", "title": "A REVIEW OF WORD SENSE DISAMBIGUATION METHOD", "abstract": "Background: Word Sense Disambiguation (WSD) is known to have a detrimental effect on the precision of information retrieval systems, where WSD is the ability to identify the meanings of words in context. There is a challenge in inference-correct-sensing on ambiguous words. Through many years of research, there have been various solutions to WSD that have been proposed; they have been divided into supervised and knowledge-based unsupervised. Objective: The first objective of this study was to explore the state-of-art of the WSD method with a hybrid method using ontology concepts. Then, with the findings, we may understand which tools are available to build WSD components. The second objective was to determine which method would be the best in giving good performance results of WSD, by analysing how the methods were used to answer specific WSD questions, their production, and how their performance was analysed. Methods: A review of the literature was conducted relating to the performance of WSD research, which used a comparison method of information retrieval analysis. The study compared the types of methods used in case, and examined methods for tools production, tools training, and analysis of performance. Results: In total 12 papers were found that satisfied all 3 inclusion criteria, and there was an anchor paper assigned to be referred. We chose the knowledge-based unsupervised approach because it has fewer word sets constraints than the supervised approaches which require training data. Concept-based ontology will help WSD in finding the semantic words concept with respect to another concept around it. Conclusion: Many methods was explored and compared to determine the most suitable way to build a WSD model based on semantics between words in query texts that can be related to the knowledge concept by using ontological knowledge presentation.", "venue": "Journal of Information Systems and Technology Management", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["wsd", "wsd", "onto", "wsd", "wsd", "wsd", "wsd", "onto", "wsd", "kg", "kg", "wsd", "wsd", "wsd", "wsd", "wsd", "onto"], "mention_counts": {"kg": 2, "wsd": 12, "onto": 3}, "nlp_mention_counts": {"wsd": 12}, "ld_mention_counts": {"kg": 2, "onto": 3}, "relevance_score": 0.587352245731331}, {"paperId": "1e629c7d5933581e8aa71a6dc54b758dbee211a6", "url": "https://www.semanticscholar.org/paper/1e629c7d5933581e8aa71a6dc54b758dbee211a6", "title": "Standards & best practice for multilingual computational lexicons: ISLE MILE and more\u201d", "abstract": "ISLE (International Standards for Language Engineering) is a transatlantic standards oriented initiative under the Human Language Technology (HLT) programme within the EU-US International Research Co-operation. It is a continuation of the European EAGLES (Expert Advisory Group for Language Engineering Standards) initiative, carried out through a number of subsequent projects funded by the European Commission (EC) since 1993. Within the multilingual computational lexicons Working Group, ISLE aims at: extending EAGLES work on lexical semantics, necessary to establish inter-language links; designing and proposing standards for multilingual lexicons; developing a prototype tool to implement lexicon guidelines and standards; creating exemplary EAGLESconformant sample lexicons and tagging exemplary corpora for validation purposes; and developing standardised evaluation procedures for lexicons. After a short introduction on the ISLE proposal for standards, the MILE (Multilingual ISLE Lexical Entry), we wil l focus the discussion on short and medium term requirements with respect to standards for multilingual lexicons and content encoding, in particular industrial requirements. We will stress the importance of reaching consensus on (li nguistic and non-linguistic) \u201ccontent\u201d, in addition to agreement on formats and encoding issues, and wil l define further steps necessary to converge on common priorities. Semantic Web standards and the needs of content processing technologies will be also addressed. 1. Goals of the Panel ISLE International Standards for Language Engineering) is a transatlantic standards oriented initiative under the Human Language Technology (HLT) programme within the EU-US International Research Cooperation. It is a continuation of the long standing European EAGLES (Expert Advisory Group for Language Engineering Standards) initiative, carried out through a number of subsequent projects funded by the European Commission (EC) since 1993. Within the multilingual computational lexicons Working Group (CLWG), ISLE aims at: extending EAGLES work on lexical semantics, necessary to establi sh inter-language links; designing and proposing standards for multilingual lexicons; developing a prototype tool to implement lexicon guidelines and standards; creating exemplary EAGLES-conformant sample lexicons and tagging exemplary corpora for validation purposes; and developing standardised evaluation procedures for lexicons. The CLWG is committed to the consensual definition of a standardized infrastructure to develop multilingual resources for HLT applications, with particular attention to the needs of Machine Translation and Crosslingual Information Retrieval systems. The Panel wil l include, in addition to ISLE members, developers and users of multilingual systems and of content management systems, and researchers interested in multilingual and content encoding standards. After a short introduction on the ISLE proposal for the MILE (Multilingual ISLE Lexical Entry) a general http://www.ilc.pi.cnr.it/EAGLES96/isle/ISLE_Home_Page.htm. schema for the encoding of multilingual lexical information to be intended as a meta-entry, acting as a common representational layer for multilingual lexical resources -, we will focus the discussion on short and medium term requirements with respect to standards for multilingual lexicons and content encoding, in particular industrial requirements. We will stress the importance of reaching consensus on (linguistic and non-linguistic) \u201ccontent\u201d, in addition to agreement on formats and encoding issues, and wil l try to define further steps necessary to converge on common priorities. Semantic Web standards and the needs of content processing technologies will be also addressed. 2. A few Issues for the Panel If we break the global problem of multilingual content technologies into small more manageable pieces, Linguistic Resources (LR) are certainly one of these pieces. Which is the relevance and impact of the availabilit y of (good, deep, knowledge intensive) resources (lexicons, ontologies, corpora) for high-quality cross-lingual/multilingual systems? It is obvious that different technologies/applications \u2013 and different approaches within the same application need different information types: e.g. the needs of CLIR or content access systems are quite different from MT systems. Do we have examples of reall y \u2018good\u2019 bil ingual/multilingual lexicons, at least for some applications? Which are the priority information types for different multilingual content management systems? Are we able to establi sh clear lexical/linguistic/knowledge requirements for different application types, or even component technologies? And to define steps to gradually reach consensus? Which is the respective role of e.g. annotated corpora, monolingual lexicons (with different information types), bimultilingual lexicons, ontologies, knowledge bases, etc? Can we aim at basic, general purpose bil ingual/multilingual lexicons, to be tuned, adapted to different applications? A key strategic question also for the funding agencies is: for which type of resources to invest? With respect to short vs. medium term results? Is there the need for robust systems, able to acquire/tune lexical/linguistic knowledge, to accompany static basic resources? in particular, systems able to acquire multilingual lexical/linguistic information? Do we have good sources of bi-/multil ingual information (machine readable dictionaries, corpora, ...)? And reliable methods for acquisition? Do we have to rely on parallel corpora? Or it is more advisable to aim at the use of \u2018comparable corpora\u2019, accompanied by robust technologies for annotation (at different levels: morphosyntactic, syntactic/functional, semantic, ...), and by a shared set of text annotation schemata? What is the relation between lexical standards and text annotation standards? In particular when we speak about \u201ccontent\u201d interoperability, is the field \u2018mature\u2019 enough to converge around agreed standards? Or is the market compelli ng us toward operational standards? Is the field of multilingual lexical resources ready to tackle the challenges set by the Semantic Web development? Knowledge management is critical. Is it an achievable goal to arrive at some commonly agreed text annotation protocol also for the semantic/conceptual level (in order to be able to automatically establi sh links among different languages)? A last but criti cal question: if we had real-size lexicons plus conceptual systems with very fine-grained semantic/conceptual information, would there be systems (non ad-hoc toy systems) able to use them? It seems sometimes that there is a loop, or a vicious circle, between i) lack of suitable, large-size and knowledge intensive, resources (lexicons, ontologies, corpora, with many different types of syntactic, semantic, conceptual information encoded), and ii) systems\u2019 abil ity to use them effectively. Should we define a strategy of research and development within which the two paths are pursued in parallel, closely interact with each other, and be gradually integrated?", "venue": "LREC", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "mt", "hlt", "onto", "sw", "sw", "hlt", "onto", "onto", "sw"], "mention_counts": {"onto": 3, "sw": 3, "mt": 1, "kg": 1, "hlt": 2}, "nlp_mention_counts": {"hlt": 2, "mt": 1}, "ld_mention_counts": {"kg": 1, "sw": 3, "onto": 3}, "relevance_score": 0.5715444760934602}, {"paperId": "3ec17263a68d673854b67cd99abc110cb6e483f4", "url": "https://www.semanticscholar.org/paper/3ec17263a68d673854b67cd99abc110cb6e483f4", "title": "An Interoperable Platform for Multi-Grain Text Annotation", "abstract": "In this paper, we describe an interoperable platform for creating annotated corpora in different languages and domains. It focuses on two most widely used for practical information processing tasks levels of linguistic annotations, - morphological and conceptual, that can be performed separately or combined. The platform consists of two main modules, a program shell and a knowledge base. The program shell is universal and features flexible settings that ensure its adaptation to multilingual corpora of various domains and different levels of annotation. It is provided with several interfaces for knowledge acquisition and annotation control. The annotation platform knowledge base includes language-independent and language-dependent linguistic information. The language-independent information is presented by multilingual domain ontology, while the core of the language-dependent component of the platform knowledge base includes unilingual onto-lexicons. The annotation process consists in the practical realization of ontological analysis. In performing the annotation task, the NLP techniques are used to automatically support, rather than completely replace human judgment. The platform is multifunctional, and in addition to corpora annotation, it can directly be used for different types of theoretical linguistic research, e.g., terminology analysis, cross-linguistic comparative studies, etc. The paper covers both, the platform design and its application in the frame of a real project on the conceptual annotation of the \"Terrorism\" domain corpora in the Russian, English and French languages. which both the rule-based NLP technique and/or quantitative measures can be applied. The paper covers the platform general design and its application for the conceptual annotation of the \"Terrorism\" domain corpora in English Russian and French. The potential of the developed interoperable platform as a research tool to define quantitative metrics for tag disambiguation is also demonstrated on the example of the conceptual-level annotation. The suggested quantitative metrics account for a) the frequency of the concept usage in unilingual corpora annotations and b) the variety of the unilingual lexical units mapped into a multilingual ontological concept. The specificity of the approach is that a) the unit of the ontological analysis is taken to be a multicomponent phrase rather than a single word and b) tag disambiguation can supported by the rule-based NLP technology through the fully functional platform tagger interpreter and/or by quantitative measures.", "venue": "Intelligent Memory Systems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "kg", "onto", "kg", "nlp", "onto", "onto", "kg", "nlp"], "mention_counts": {"nlp": 3, "onto": 4, "kg": 3}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"kg": 3, "onto": 4}, "relevance_score": 0.5715444760934602}, {"paperId": "43668495fa67a6d0f5a2b3c7914264d5ac6eb333", "url": "https://www.semanticscholar.org/paper/43668495fa67a6d0f5a2b3c7914264d5ac6eb333", "title": "Ontologies, Knowledge Representation, and Machine Learning for Translational Research: Recent Contributions", "abstract": "Summary Objectives : To select, present, and summarize the most relevant papers published in 2018 and 2019 in the field of Ontologies and Knowledge Representation, with a particular focus on the intersection between Ontologies and Machine Learning. Methods : A comprehensive review of the medical informatics literature was performed to select the most interesting papers published in 2018 and 2019 and that document the utility of ontologies for computational analysis, including machine learning. Results : Fifteen articles were selected for inclusion in this survey paper. The chosen articles belong to three major themes: (i) the identification of phenotypic abnormalities in electronic health record (EHR) data using the Human Phenotype Ontology ; (ii) word and node embedding algorithms to supplement natural language processing (NLP) of EHRs and other medical texts; and (iii) hybrid ontology and NLP-based approaches to extracting structured and unstructured components of EHRs. Conclusion : Unprecedented amounts of clinically relevant data are now available for clinical and research use. Machine learning is increasingly being applied to these data sources for predictive analytics, precision medicine, and differential diagnosis. Ontologies have become an essential component of software pipelines designed to extract, code, and analyze clinical information by machine learning algorithms. The intersection of machine learning and semantics is proving to be an innovative space in clinical research.", "venue": "Yearbook of medical informatics", "citationCount": 6, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["onto", "nlp", "onto", "onto", "onto", "onto", "onto", "nlp", "nlp", "onto"], "mention_counts": {"nlp": 3, "onto": 7}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.5715444760934602}, {"paperId": "9520f78f645893b688ee42f6773ed649f7ed5dac", "url": "https://www.semanticscholar.org/paper/9520f78f645893b688ee42f6773ed649f7ed5dac", "title": "Learning to Refine an Automatically Extracted Knowledge Base Using Markov Logic", "abstract": "A number of text mining and information extraction projects such as Text Runner and NELL seek to automatically build knowledge bases from the rapidly growing amount of information on the web. In order to scale to the size of the web, these projects often employ ad hoc heuristics to reason about uncertain and contradictory information rather than reasoning jointly about all candidate facts. In this paper, we present a Markov logic-based system for cleaning an extracted knowledge base. This allows a scalable system such as NELL to take advantage of joint probabilistic inference, or, conversely, allows Markov logic to be applied to a web scale problem. Our system uses only the ontological constraints and confidence values of the original system, along with human-labeled data if available. The labeled data can be used to calibrate the confidence scores from the original system or learn the effectiveness of individual extraction patterns. To achieve scalability, we introduce a neighborhood grounding method that only instantiates the part of the network most relevant to the given query. This allows us to partition the knowledge cleaning task into tractable pieces that can be solved individually. In experiments on NELL's knowledge base, we evaluate several variants of our approach and find that they improve both F1 and area under the precision-recall curve.", "venue": "2012 IEEE 12th International Conference on Data Mining", "citationCount": 62, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "kg", "ke", "kg", "kg", "kg", "ke"], "mention_counts": {"kg": 4, "onto": 1, "ke": 2, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"kg": 4, "onto": 1, "ke": 2}, "relevance_score": 0.5715444760934602}, {"paperId": "f0be0064681941cf2185c1f3043d36e44f96a4b3", "url": "https://www.semanticscholar.org/paper/f0be0064681941cf2185c1f3043d36e44f96a4b3", "title": "Extraction of temporal facts and events from Wikipedia", "abstract": "Recently, large-scale knowledge bases have been constructed by automatically extracting relational facts from text. Unfortunately, most of the current knowledge bases focus on static facts and ignore the temporal dimension. However, the vast majority of facts are evolving with time or are valid only during a particular time period. Thus, time is a significant dimension that should be included in knowledge bases.\n In this paper, we introduce a complete information extraction framework that harvests temporal facts and events from semi-structured data and free text of Wikipedia articles to create a temporal ontology. First, we extend a temporal data representation model by making it aware of events. Second, we develop an information extraction method which harvests temporal facts and events from Wikipedia infoboxes, categories, lists, and article titles in order to build a temporal knowledge base. Third, we show how the system can use its extracted knowledge for further growing the knowledge base.\n We demonstrate the effectiveness of our proposed methods through several experiments. We extracted more than one million temporal facts with precision over 90% for extraction from semi-structured data and almost 70% for extraction from text.", "venue": "TempWeb '12", "citationCount": 51, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "kg", "ke", "kg", "ie", "onto", "kg", "kg", "kg"], "mention_counts": {"kg": 5, "onto": 1, "ke": 1, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"kg": 5, "onto": 1, "ke": 1}, "relevance_score": 0.5715444760934602}, {"paperId": "fd800ff14570c08ef07a8a1de70a4542df9bde59", "url": "https://www.semanticscholar.org/paper/fd800ff14570c08ef07a8a1de70a4542df9bde59", "title": "An assessment on domain ontology-based information extraction techniques", "abstract": "Domain ontology is used in information retrieval to retrieve more relevant information from a collection of unstructured information source. Information retrieval is becoming an important research area in the field of computer science. Information retrieval (IR) is generally concerned with the searching and retrieving of knowledge-based information from database. In this paper, we represent the various models and techniques for information retrieval. Domain ontology is a description of domain concepts with relation and properties to be used in knowledge engineering as a knowledge base. In this paper, various domain ontology-based information retrieval methods have been reviewed. A comparative analysis is made on all the available methods, which will allow the analyst to choose the suitable domain ontology-based information extraction method. There are various methods developed to make the information extraction more efficient. The methods have been classified as Boolean, vector space, semantic-based techniques and probabilistic. Semantic-based information retrieval can still be classified as semantic association, semantic similarity and semantic annotation. This assessment allows the developer to choose the best fit model for their requirement in an efficient way.", "venue": "Int. J. Serv. Technol. Manag.", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ie", "kg", "ie", "onto", "onto", "kg", "ie"], "mention_counts": {"kg": 2, "onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"kg": 2, "onto": 5}, "relevance_score": 0.5715444760934602}, {"paperId": "57d9e792392f4384a20f69f59a03c1418f7bed06", "url": "https://www.semanticscholar.org/paper/57d9e792392f4384a20f69f59a03c1418f7bed06", "title": "Towards Semi Automatic Construction of a Lexical Ontology for Persian", "abstract": "Lexical ontologies and semantic lexicons are important resources in natural language processing. They are used in various tasks and applications, especially where semantic processing is evolved such as question answering, machine translation, text understanding, information retrieval and extraction, content management, text summarization, knowledge acquisition and semantic search engines. Although there are a number of semantic lexicons for English and some other languages, Persian lacks such a complete resource to be used in NLP works. In this paper we introduce an ongoing project on developing a lexical ontology for Persian called FarsNet. We exploited a hybrid semi-automatic approach to acquire lexical and conceptual knowledge from resources such as WordNet, bilingual dictionaries, mono-lingual corpora and morpho-syntactic and semantic templates. FarsNet is an ontology whose elements are lexicalized in Persian. It provides links between various types of words (cross POS relations) and also between words and their corresponding concepts in other ontologies (cross ontologies relations). FarsNet aggregates the power of WordNet on nouns, the power of FrameNet on verbs and the wide range of conceptual relations from ontology community.", "venue": "LREC", "citationCount": 14, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "mt", "nlp", "onto", "onto", "nlp", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 7, "mt": 1}, "nlp_mention_counts": {"nlp": 2, "mt": 1}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.5715444760934602}, {"paperId": "a4f1f350823a2b2b023fac5b8ec446544c0bc19c", "url": "https://www.semanticscholar.org/paper/a4f1f350823a2b2b023fac5b8ec446544c0bc19c", "title": "Development of an Ontology for Aerospace Engine Components Degradation in Service", "abstract": "This paper presents the development of an ontology for component service degradation. In this paper, degradation mechanisms in gas turbine metallic components are used for a case study to explain how a taxonomy within an ontology can be validated. The validation method used in this paper uses an iterative process and sanity checks. Data extracted from on-demand textual information are filtered and grouped into classes of degradation mechanisms. Various concepts are systematically and hierarchically arranged for use in the service maintenance ontology. The allocation of the mechanisms to the AS-IS ontology presents a robust data collection hub. Data integrity is guaranteed when the TO-BE ontology is introduced to analyse processes relative to various failure events. The initial evaluation reveals improvement in the performance of the TO-BE domain ontology based on iterations and updates with recognised mechanisms. The information extracted and collected is required to improve service knowledge and performance feedback which are important for service engineers. Existing research areas such as natural language processing, knowledge management, and information extraction were also examined.", "venue": "International Conference on Knowledge Engineering and Ontology Development", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "nlp", "onto", "onto", "ie", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 1, "onto": 7, "ie": 2}, "nlp_mention_counts": {"nlp": 1, "ie": 2}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.5715444760934602}, {"paperId": "6f569660807624f94b185aef1622759c9d42e02d", "url": "https://www.semanticscholar.org/paper/6f569660807624f94b185aef1622759c9d42e02d", "title": "Semantic association rule mining in text using domain ontology", "abstract": "Online news websites are now valuable archives for both current and old news regarding various issues, particularly those that relate to the political and historical contexts of a country. These news platforms have become an important medium for all forms of political activities such as branding, campaigns, and communication. Online newspapers make large volume of textual data available, which are rich in political and historical inferences that can be leveraged for national development. In this paper we report a procedure for ontology-based association rule mining for knowledge extraction from text. Ordinarily, association rule mining algorithms have the limitations of generating many non-interesting rules, huge number of discovered rules, and low algorithm performance. This research demonstrates a procedure for improving the performance of association rule mining in text mining by using domain ontology. To do this, a study context of Nigerian politics based on information extracted from a Nigerian online newspaper was selected, and a methodology that combined natural language processing methods, ontology-based keywords extraction, and the modified Generating Association Rules based on Weighting scheme (GARW) was applied. The result obtained from the study revealed that compared to non-ontology based association rule mining approaches, our procedure provides significant rule reduction in the number of generated rules, and produced rules which are more semantically related to the problem context. The study validates the capability of domain ontology to improve the performance of association rule mining algorithms, particularly when dealing with unstructured textual data.", "venue": "Int. J. Metadata Semant. Ontologies", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "onto", "ie", "onto", "onto", "onto", "onto", "nlp"], "mention_counts": {"nlp": 1, "ke": 1, "onto": 6, "ie": 1}, "nlp_mention_counts": {"nlp": 1, "ke": 1, "ie": 1}, "ld_mention_counts": {"ke": 1, "onto": 6}, "relevance_score": 0.5715444760934602}, {"paperId": "ebda79a0277823de82d12ef577c3e0e654579d40", "url": "https://www.semanticscholar.org/paper/ebda79a0277823de82d12ef577c3e0e654579d40", "title": "Extraction, Merging, and Monitoring of Company Data from Heterogeneous Sources", "abstract": "We describe the implementation of an enterprise monitoring system that builds on an ontology-based information extraction (OBIE) component applied to heterogeneous data sources. The OBIE component consists of several IE modules - each extracting on a regular temporal basis a specific fraction of company data from a given data source - and a merging tool, which is used to aggregate all the extracted information about a company. The full set of information about companies, which is to be extracted and merged by the OBIE component, is given in the schema of a domain ontology, which is guiding the information extraction process. The monitoring system, in case it detects changes in the extracted and merged information on a company with respect to the actual state of the knowledge base of the underlying ontology, ensures the update of the population of the ontology. As we are using an ontology extended with temporal information, the system is able to assign time intervals to any of the object instances. Additionally, detected changes can be communicated to end-users, who can validate and possibly correct the resulting updates in the knowledge base.", "venue": "LREC", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "onto", "onto", "kg", "ie", "ie", "ie", "onto", "onto"], "mention_counts": {"kg": 2, "onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"kg": 2, "onto": 5}, "relevance_score": 0.5715444760934602}, {"paperId": "75bccfd7aa7e32b7d0717de21892802342f31ae7", "url": "https://www.semanticscholar.org/paper/75bccfd7aa7e32b7d0717de21892802342f31ae7", "title": "Building Domain Ontologies from Text Analysis: An Application for Question Answering", "abstract": "In the field of information extraction and automatic question answering access to a domain ontology may be of great help. But the main problem is building such an ontology, a difficult and time consuming task. We propose an approach in which the domain ontology is learned from the linguistic analysis of a number of texts which represent the domain itself. NLP analysis is done with GETARUNS system. GETARUNS can build a Discourse Model and is able to assign a relevance score to each entity. From Discourse Model we extract best candidates to become concepts in the domain ontology. To arrange concepts in the correct hierarchy we use WordNet taxonomy. Once the domain ontology is built we reconsider the texts to extract information. In this phase the entities recognized at discourse level are used to create instances of the concepts. The predicate-argument structure of the verb is used to construct instance slots for concepts. Eventually, the question answering task is performed by translating the natural language question in a suitable form and use that to query the Discourse Model enriched by the ontology.", "venue": "NLUCS", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ie", "onto", "onto", "onto", "nlp", "onto", "ie"], "mention_counts": {"nlp": 1, "onto": 7, "ie": 2}, "nlp_mention_counts": {"nlp": 1, "ie": 2}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.5715444760934602}, {"paperId": "8eabda075e8966770d869f0738189609fa5087f9", "url": "https://www.semanticscholar.org/paper/8eabda075e8966770d869f0738189609fa5087f9", "title": "Ontology-based intelligent information extraction system on the semantic Web", "abstract": "This paper presents an intelligent information extraction system based on ontology on the semantic Web. It describes the kernel architecture of the system, and introduces the formal representation of ontology in detail. It also describes the ontology mapping and ontology evolution process. Lastly, the paper shows the structure of the intelligent information extraction system.", "venue": "Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "ie", "sw", "onto", "onto", "onto", "ie", "ie", "sw"], "mention_counts": {"sw": 2, "onto": 5, "ie": 3}, "nlp_mention_counts": {"ie": 3}, "ld_mention_counts": {"sw": 2, "onto": 5}, "relevance_score": 0.5715444760934602}, {"paperId": "15f8656f3d217530a36e15fee256647c7ce43ae8", "url": "https://www.semanticscholar.org/paper/15f8656f3d217530a36e15fee256647c7ce43ae8", "title": "Using Markov Logic to Refine an Automatically Extracted Knowledge Base", "abstract": "A number of information extraction (IE) projects such as NELL and TextRunner seek to build a usable knowledge base from the rapidly growing amount of information on the web. However, these solutions use heuristic approaches to reasoning rather than sound probabilistic inference. In this paper, we present a method based on Markov logic for cleaning an automatically extracted knowledge base using only the confidence values and ontological constraints of the original system. Our approach works by reasoning jointly over all candidate facts. To achieve scalability, we introduce a neighborhood grounding method that only instantiates the part of the network most relevant to the given query. This allows us to partition the knowledge cleaning task into tractable pieces that can be solved individually. In experiments on NELL\u2019s knowledge base, our method improves both F1 and AUC.", "venue": "StarAI@UAI", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "onto", "ke", "kg", "kg", "kg", "ke", "kg"], "mention_counts": {"ke": 2, "onto": 1, "kg": 4, "ie": 1}, "nlp_mention_counts": {"ke": 2, "ie": 1}, "ld_mention_counts": {"ke": 2, "onto": 1, "kg": 4}, "relevance_score": 0.5715444760934602}, {"paperId": "3ee438c9013da33c81bedb5400b832ab3a5bd9a1", "url": "https://www.semanticscholar.org/paper/3ee438c9013da33c81bedb5400b832ab3a5bd9a1", "title": "A System Based on Ontology and Case-Based Reasoning to Support Distributed Teams", "abstract": "The intrinsic nature of distributed software development (DSD) brings new challenges, such as communication issues and sharing information efficiently. Software companies have a tendency to face these challenges using individual and isolated approaches, making difficult to spread good practices for the DSD community. In other contexts, concepts and techniques from Artificial Intelligence (AI) are frequently used in order to improve the functioning of systems and process. This work is based on the following AI concepts: ontologies, case-based reasoning (CBR) and natural language processing (NLP). We propose a system, based on ontology and case-based reasoning, that operates as follows: i) we use a tool for ontology storage, access and processing; and ii) an ontology-based CBR tool which aims to aid software companies by recommending techniques and best practices for minimizing or solving potential challenges that may be faced by DSD processes. The main results from this research are: i) a specific ontology for distributed software development teams; ii) a tool to facilitate the access and manipulation of the proposed ontology; and iii) a case based reasoning system that utilizes natural language processing. Initial results of the performed experiments indicate a success rate of 91.7% in the recommendation of solutions for potential problems coming from DSD processes.", "venue": "2015 12th International Conference on Information Technology - New Generations", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "nlp", "onto", "nlp", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 3, "onto": 7}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"onto": 7}, "relevance_score": 0.5715444760934602}, {"paperId": "8c5413e0ede944a0c4f72f93289cede4b5d9f052", "url": "https://www.semanticscholar.org/paper/8c5413e0ede944a0c4f72f93289cede4b5d9f052", "title": "Ontology Engineering and Knowledge Extraction for Cross-Lingual Retrieval", "abstract": "In this paper, we show that by integrating existing NLP techniques and Semantic Web tools in a novel way, we can provide a valuable contribution to the solution of the knowledge acquisition bottleneck problem. NLP techniques to create a domain ontology on the basis of an open domain corpus have been combined with Semantic Web tools. More specifically, Watson and Prompt have been employed to enhance the kick-o ontology while Cornetto, a lexical database for Dutch, has been adopted to establish a link between the concepts and their Dutch lexicalization. The lexicalized ontology constitutes the basis for the cross-language retrieval of learning objects within the LT4eL eLearning project.", "venue": "RANLP", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "onto", "ke", "onto", "onto", "sw", "nlp", "sw"], "mention_counts": {"nlp": 2, "sw": 2, "onto": 4, "ke": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"ke": 1, "sw": 2, "onto": 4}, "relevance_score": 0.5715444760934602}, {"paperId": "dfdc5cd1cc1ab42a6032761a852df2290b2be870", "url": "https://www.semanticscholar.org/paper/dfdc5cd1cc1ab42a6032761a852df2290b2be870", "title": "Mining Scholarly Data for Fine-Grained Knowledge Graph Construction", "abstract": "Knowledge graphs (KG) are large network of entities and relationships, tipically expressed as RDF triples, relevant to a specific domain or an organization. Scientific Knowledge Graphs (SKGs) focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. The next big challenge in this field regards the generation of SKGs that also contain a explicit representation of the knowledge presented in research publications. In this paper, we present a preliminary approach that uses a set of NLP and Deep Learning methods for extracting entities and relationships from research publications and then integrates them in a KG. More specifically, we i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, ii) describe an approach for integrating entities and relationships generated by these tools, iii) analyse an automatically generated Knowledge Graph including 10, 425 entities and 25, 655 relationships derived from 12, 007 publications in the field of Semantic Web, and iv) discuss how Deep Learning methods can be applied to overcome some limitations of the current techniques.", "venue": "DL4KG@ESWC", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "nlp", "rdf", "kg", "nlp", "sw", "ke", "kg"], "mention_counts": {"nlp": 2, "sw": 1, "ke": 1, "kg": 4, "rdf": 1}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"kg": 4, "sw": 1, "ke": 1, "rdf": 1}, "relevance_score": 0.5715444760934602}, {"paperId": "0ebe28f05195c532d8e39260b4c5034e6d2da3d9", "url": "https://www.semanticscholar.org/paper/0ebe28f05195c532d8e39260b4c5034e6d2da3d9", "title": "Pangloss: A Knowledge-based Machine Assisted Translation Research Project - Site", "abstract": "are developing a Translator's Workstation to assist a user in the translation of newspaper articles in the area of finance (mergers and acquisitions) in one language (Spanish initially) into a second language (English). At its core is a multilingual, knowledge-based, interlingual, interactive , machine-assisted translation system consisting of a source language analysis component, an interactive aug-mentor, and a target language generation component. In the initial phase, the CRL's objectives are to develop tools for constructing lexical items and ontological entries automatically from on-line resources, to develop the initial Spanish analysis component, and, jointly with CMT and ISI, to establish the infrastructure for the three site project, develop the formats and initial content of the interlingua, the ontology, and the knowledge base, and to prepare design documents for the second phase versions of the analysis and generation components, the augmentor, and the translator's workstation. With respect to developing tools for extracting information from on-line resources, during the first year we are focusing on providing general classificatory information for use in constructing the lexicons of the Spanish analysis and English generation components. Here we are building on work on automatically constructing interlin-gual word sense specifications from Longman's which we are extending and adapting to language particular lexical entries. With respect to the Spanish analysis component , the objective is to modify and extend the Spanish component of the CRL's multilingual machine translation system both in terms of coverage and robustness. At this point, the three sites have established the central infrastructure for the Pangloss project. For the first year system, the formats of the interlingua, the ontology, and the knowledge base have been set and initial design of the interlingua for the second phase version is underway. As a preliminary to the work on extracting information from on-line resources, we continue to gather resources in the form of monolingual and bilingual dictionaries and monolingual and bilingual corpora. We have obtained Collins English Dictionary, Collins Bilingual Spanish-English Dictionary and are looking into a Spanish mono-lingual dictionary and Japanese monolingual and bilingual dictionaries. We have identified a source for Span-ish texts in the financial domain and are seeking further sources as well as sources of Japanese texts. The central resource to date, however, is the Longman Dictionary of Contemporary English and the information being extracted relates to word formation, syntactic category, syntactic subcategorization, and semantic selection restrictions. Procedures are currently being developed to provide \u2026", "venue": "HLT", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "ie", "kg", "onto", "kg", "kg", "kg", "mt", "onto", "ie"], "mention_counts": {"kg": 4, "onto": 3, "mt": 1, "ie": 2}, "nlp_mention_counts": {"mt": 1, "ie": 2}, "ld_mention_counts": {"kg": 4, "onto": 3}, "relevance_score": 0.5715444760934602}, {"paperId": "31b74274e6020fc132d64c98c1f458dc19bc9695", "url": "https://www.semanticscholar.org/paper/31b74274e6020fc132d64c98c1f458dc19bc9695", "title": "Biological Nomenclatures: A Source of Lexical Knowledge and Ambiguity", "abstract": "There has been increased work in developing automated systems that involve natural language processing (NLP) to recognize and extract genomic information from the literature. Recognition and identification of biological entities is a critical step in this process. NLP systems generally rely on nomenclatures and ontological specifications as resources for determining the names of the entities, assigning semantic categories that are consistent with the corresponding ontology, and assignment of identifiers that map to well-defined entities within a particular nomenclature. Although nomenclatures and ontologies are valuable for text processing systems, they were developed to aid researchers and are heterogeneous in structure and semantics. A uniform resource that is automatically generated from diverse resources, and that is designed for NLP purposes would be a useful tool for the field, and would further database interoperability. This paper presents work towards this goal. We have automatically created lexical resources from four model organism nomenclature systems (mouse, fly, worm, and yeast), and have studied performance of the resources within an existing NLP system, GENIES. Using nomenclatures is not straightforward because issues concerning ambiguity, synonymy, and name variations are quite challenging. In this paper we focus mainly on ambiguity. We determined that the number of ambiguous gene names within the individual nomenclatures, across the four nomenclatures, and with general English ranged from 0%-10.18%, 1.187%-20.30%, and 0%-2.49% respectively. When actually processing text, we found the rate of ambiguous occurrences (not counting ambiguities stemming from English words) to range from 2.4%-32.9% depending on the organisms considered.", "venue": "Pacific Symposium on Biocomputing", "citationCount": 99, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["tp", "onto", "onto", "nlp", "nlp", "nlp", "onto", "nlp", "tp", "nlp"], "mention_counts": {"nlp": 5, "onto": 3, "tp": 2}, "nlp_mention_counts": {"nlp": 5, "tp": 2}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.5715444760934602}, {"paperId": "784b8506e1848f661651f865a07ec88051da216b", "url": "https://www.semanticscholar.org/paper/784b8506e1848f661651f865a07ec88051da216b", "title": "Content selection as semantic-based ontology exploration", "abstract": "Natural Language (NL) based access to information contained in Knowledge Bases (KBs) has been tackled by approaches following different paradigms. One strand of research deals with the task of ontology-based data access and data exploration (Franconi et al., 2010; Franconi et al., 2011). This type of approach relies on two pillar components. The first one is an ontology describing the underlying domain with a set of reasoning based query construction operations. This component guides the lay user in the formulation of a KB query by proposing alternatives for query expansion. The second is a Natural Language Generation (NLG) system to hide the details of the formal query language to the user. Our ultimate goal is the automatic creation of a corpus of KB queries for development and evaluation of NLG systems. The task we address is the following. Given an ontology K, automatically select from K descriptions q which yield sensible user queries. The difficulty lies in the fact that ontologies often omit important disjointness axioms and adequate domain or range restrictions (Rector et al., 2004; Poveda-Villal\u00f3n et al., 2012). For instance, the toy ontology shown in Figure 1 licences the meaningless query in (1). This happens because there is no disjointness axiom between the Song and Rectangular concepts and/or because the domain of the marriedTo relation is not restricted to persons.", "venue": "WebNLG", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "kg", "nlg", "onto", "onto", "onto", "nlg", "onto", "nlg", "onto"], "mention_counts": {"kg": 1, "nlg": 3, "onto": 6}, "nlp_mention_counts": {"nlg": 3}, "ld_mention_counts": {"kg": 1, "onto": 6}, "relevance_score": 0.5715444760934602}, {"paperId": "36ddc7f293de2b705cdc716505586e4b12d14a2b", "url": "https://www.semanticscholar.org/paper/36ddc7f293de2b705cdc716505586e4b12d14a2b", "title": "Mining ontological knowledge using Nyaya framework", "abstract": "Ontology has become the buzzword of the knowledge and semantics community. The process of automatically constructing an ontology with completeness and reduced time has become the need of the hour. This paper presents the method for automatically constructing an ontology for any domain based on the Indian philosophical system, the Nyaya Sastra. Nyaya defines the whole world from atom to universe. This categorisation provided by Nyaya acts as the framework for extracting ontological relations from documents obtained from the web. With Nyaya and Natural Language Processing (NLP) techniques, an improved and enriched knowledge can be obtained from web documents. This paper explains the way of constructing an ontology (i.e., extracting taxonomical and nontaxonomical relations) using Nyaya in detail. The extracted knowledge includes concepts, relations and qualities pertaining to a concept. A Semi-Supervised Learning (SSL) technique for learning Nyaya categories is also explained.", "venue": "Int. J. Netw. Virtual Organisations", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "ke", "nlp", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "ke": 1, "onto": 6}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"ke": 1, "onto": 6}, "relevance_score": 0.5715444760934602}, {"paperId": "ed6df6df195c7d426fad4a5d15f82776ad59fd03", "url": "https://www.semanticscholar.org/paper/ed6df6df195c7d426fad4a5d15f82776ad59fd03", "title": "Using Natural Language Processing Techniques to Provide Personalized Educational Materials for Chronic Disease Patients in China: Development and Assessment of a Knowledge-Based Health Recommender System", "abstract": "Background Health education emerged as an important intervention for improving the awareness and self-management abilities of chronic disease patients. The development of information technologies has changed the form of patient educational materials from traditional paper materials to electronic materials. To date, the amount of patient educational materials on the internet is tremendous, with variable quality, which makes it hard to identify the most valuable materials by individuals lacking medical backgrounds. Objective The aim of this study was to develop a health recommender system to provide appropriate educational materials for chronic disease patients in China and evaluate the effect of this system. Methods A knowledge-based recommender system was implemented using ontology and several natural language processing (NLP) techniques. The development process was divided into 3 stages. In stage 1, an ontology was constructed to describe patient characteristics contained in the data. In stage 2, an algorithm was designed and implemented to generate recommendations based on the ontology. Patient data and educational materials were mapped to the ontology and converted into vectors of the same length, and then recommendations were generated according to similarity between these vectors. In stage 3, the ontology and algorithm were incorporated into an mHealth system for practical use. Keyword extraction algorithms and pretrained word embeddings were used to preprocess educational materials. Three strategies were proposed to improve the performance of keyword extraction. System evaluation was based on a manually assembled test collection for 50 patients and 100 educational documents. Recommendation performance was assessed using the macro precision of top-ranked documents and the overall mean average precision (MAP). Results The constructed ontology contained 40 classes, 31 object properties, 67 data properties, and 32 individuals. A total of 80 SWRL rules were defined to implement the semantic logic of mapping patient original data to the ontology vector space. The recommender system was implemented as a separate Web service connected with patients' smartphones. According to the evaluation results, our system can achieve a macro precision up to 0.970 for the top 1 recommendation and an overall MAP score up to 0.628. Conclusions This study demonstrated that a knowledge-based health recommender system has the potential to accurately recommend educational materials to chronic disease patients. Traditional NLP techniques combined with improvement strategies for specific language and domain proved to be effective for improving system performance. One direction for future work is to explore the effect of such systems from the perspective of patients in a practical setting.", "venue": "JMIR Medical Informatics", "citationCount": 13, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["kg", "onto", "kg", "onto", "onto", "nlp", "nlp", "kg", "onto", "onto", "nlp", "onto", "nlp", "onto"], "mention_counts": {"nlp": 4, "kg": 3, "onto": 7}, "nlp_mention_counts": {"nlp": 4}, "ld_mention_counts": {"kg": 3, "onto": 7}, "relevance_score": 0.5676040851861229}, {"paperId": "494550ad73407effb29db14022066e69dc711920", "url": "https://www.semanticscholar.org/paper/494550ad73407effb29db14022066e69dc711920", "title": "A Flexible Conversational Dialog System for MP3 Player", "abstract": "In recent years, an increasing number of new devices have found their way into the cars we drive. Speech-operated devices in particular provide a great service to drivers by minimizing distraction, so that they can keep their hands on the wheel and their eyes on the road. This presentation will demonstrate our latest development of an in-car dialog system for an MP3 player designed under a joint research effort from Bosch RTC, VW ERL, Stanford CSLI, and SRI STAR Lab funded by NIST ATP [Weng et al 2004] with this goal in mind. This project has developed a number of new technologies, some of which are already incorporated in the system. These include: end-pointing with prosodic cues, error identification and recovering strategies, flexible multi-threaded, multi-device dialog management, and content optimization and organization strategies. A number of important language phenomena are also covered in the system's functionality. For instance, one may use words relying on context, such as 'this,' 'that,' 'it,' and 'them,' to reference items mentioned in particular use contexts. Different types of verbal revision are also permitted by the system, providing a great convenience to its users. The system supports multi-threaded dialogs so that users can diverge to a different topic before the current one is finished and still come back to the first after the second topic is done. To lower the cognitive load on the drivers, the content optimization component organizes any information given to users based on ontological structures, and may also refine users' queries via various strategies. Domain knowledge is represented using OWL, a web ontology language recommended by W3C, which should greatly facilitate its portability to new domains.The spoken dialog system consists of a number of components (see Fig. 1 for details). Instead of the hub architecture employed by Communicator projects [Senef et al, 1998], it is developed in Java and uses a flexible event-based, message-oriented middleware. This allows for dynamic registration of new components. Among the component modules in Figure 1, we use the Nuance speech recognition engine with class-based ngrams and dynamic grammars, and the Nuance Vocalizer as the TTS engine. The Speech Enhancer removes noises and echo. The Prosody module will provide additional features to the Natural Language Understanding (NLU) and Dialogue Manager (DM) modules to improve their performance.The NLU module takes a sequence of recognized words and tags, performs a deep linguistic analysis with probabilistic models, and produces an XML-based semantic feature structure representation. Parallel to the deep analysis, a topic classifier assigns top n topics to the utterance, which are used in the cases where the dialog manager cannot make any sense of the parsed structure. The NLU module also supports dynamic updates of the knowledge base.The CSLI DM module mediates and manages interaction. It uses the dialogue-move approach to maintain dialogue context, which is then used to interpret incoming utterances (including fragments and revisions), resolve NPs, construct salient responses, track issues, etc. Dialogue states can also be used to bias SR expectation and improve SR performance, as has been performed in previous applications of the DM. Detailed descriptions of the DM can be found in [Lemon et al 2002; Mirkovic & Cavedon 2005].The Knowledge Manager (KM) controls access to knowledge base sources (such as domain knowledge and device information) and their updates. Domain knowledge is structured according to domain-dependent ontologies. The current KM makes use of OWL, a W3C standard, to represent the ontological relationships between domain entities. Protege (http://protege.stanford.edu), a domain-independent ontology tool, is used to maintain the ontology offline. In a typical interaction, the DM converts a user's query into a semantic frame (i.e. a set of semantic constraints) and sends this to the KM via the content optimizer.The Content Optimization module acts as an intermediary between the dialogue management module and the knowledge management module during the query process. It receives semantic frames from the DM, resolves possible ambiguities, and queries the KM. Depending on the items in the query result as well as the configurable properties, the module selects and performs an appropriate optimization strategy.Early evaluation shows that the system has a task completion rate of 80% on 11 tasks of MP3 player domain, ranging from playing requests to music database queries. Porting to a restaurant selection domain is currently under way.", "venue": "HLT", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "kg", "onto", "onto", "onto", "onto", "nlu", "nlu", "nlu", "onto", "nlu", "onto", "onto", "onto"], "mention_counts": {"kg": 2, "onto": 8, "nlu": 4}, "nlp_mention_counts": {"nlu": 4}, "ld_mention_counts": {"kg": 2, "onto": 8}, "relevance_score": 0.5676040851861229}, {"paperId": "6e818c3ccacb749214066c497ffe40ab948f1460", "url": "https://www.semanticscholar.org/paper/6e818c3ccacb749214066c497ffe40ab948f1460", "title": "Frequent Itemset Mining and Multi-Layer Network-Based Analysis of RDF Databases", "abstract": "Triplestores or resource description framework (RDF) stores are purpose-built databases used to organise, store and share data with context. Knowledge extraction from a large amount of interconnected data requires effective tools and methods to address the complexity and the underlying structure of semantic information. We propose a method that generates an interpretable multilayered network from an RDF database. The method utilises frequent itemset mining (FIM) of the subjects, predicates and the objects of the RDF data, and automatically extracts informative subsets of the database for the analysis. The results are used to form layers in an analysable multidimensional network. The methodology enables a consistent, transparent, multi-aspect-oriented knowledge extraction from the linked dataset. To demonstrate the usability and effectiveness of the methodology, we analyse how the science of sustainability and climate change are structured using the Microsoft Academic Knowledge Graph. In the case study, the FIM forms networks of disciplines to reveal the significant interdisciplinary science communities in sustainability and climate change. The constructed multilayer network then enables an analysis of the significant disciplines and interdisciplinary scientific areas. To demonstrate the proposed knowledge extraction process, we search for interdisciplinary science communities and then measure and rank their multidisciplinary effects. The analysis identifies discipline similarities, pinpointing the similarity between atmospheric science and meteorology as well as between geomorphology and oceanography. The results confirm that frequent itemset mining provides an informative sampled subsets of RDF databases which can be simultaneously analysed as layers of a multilayer network.", "venue": "Mathematics", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "ke", "rdf", "rdf", "rdf", "rdf", "rdf", "ie", "ke", "rdf", "ke"], "mention_counts": {"kg": 1, "ke": 3, "rdf": 6, "ie": 1}, "nlp_mention_counts": {"ke": 3, "ie": 1}, "ld_mention_counts": {"kg": 1, "ke": 3, "rdf": 6}, "relevance_score": 0.5676040851861229}, {"paperId": "7acf4700a44f34fae7dea9aa9bc1dc6bf5b0f02e", "url": "https://www.semanticscholar.org/paper/7acf4700a44f34fae7dea9aa9bc1dc6bf5b0f02e", "title": "Information Extraction as an Ontology Population Task and Its Application to Genic Interactions", "abstract": "Ontologies are a well-motivated formal representation to model knowledge needed to extract and encode data from text. Yet, their tight integration with Information Extraction (IE) systems is still a research issue, a fortiori with complex ones that go beyond hierarchies. In this paper, we introduce an original architecture where IE is specified by designing an ontology, and the extraction process is seen as an Ontology Population (OP) task. Concepts and relations of the ontology define a normalized text representation. As their abstraction level is irrelevant for text extraction, we introduced a Lexical Layer (LL) along with the ontology, i.e. relations and classes at an intermediate level of normalization between raw text and concepts. On the contrary to previous IE systems, the extraction process only involves normalizing the outputs of Natural Language Processing (NLP) modules with instances of the ontology and the LL. All the remaining reasoning is left to a query module, which uses the inference rules of the ontology to derive new instances by deduction. In this context, these inference rules subsume classical extraction rules or patterns by providing access to appropriate abstraction level and domain knowledge. To acquire those rules, we adopt an Ontology Learning (OL) perspective, and automatically acquire the inference rules with relational Machine Learning (ML). Our approach is validated on a genic interaction extraction task from a Bacillus subtilis bacterium text corpus. We reach a global recall of 89.3% and a precision of 89.6%, with high scores for the ten conceptual relations in the ontology.", "venue": "IEEE International Conference on Tools with Artificial Intelligence", "citationCount": 16, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "ie", "ie", "onto", "onto", "onto", "onto", "nlp", "onto", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 10, "ie": 2}, "nlp_mention_counts": {"nlp": 2, "ie": 2}, "ld_mention_counts": {"onto": 10}, "relevance_score": 0.5676040851861229}, {"paperId": "36af51f13b2c1c24f13c6b468a7113054f8c4327", "url": "https://www.semanticscholar.org/paper/36af51f13b2c1c24f13c6b468a7113054f8c4327", "title": "Ontology-Based Information Extraction for Knowledge Enrichment and Validation", "abstract": "Ontology is widely used as a mean to represent and share common concepts and knowledge from a particular domain or specialisation. As a knowledge representation, the knowledge within an ontology must be able to evolve along with the recent changes and updates within the community practice. In this paper, we propose a new Ontology-based Information Extraction (OBIE) system that extends existing systems in order to enrich and validate an ontology. Our model enables the ontology to find related recent knowledge in the domain from communities, by exploiting their underlying knowledge as keywords. The knowledge extraction process uses ontology-based and pattern-based information extraction technique. Not only the extracted knowledge enriches the ontology, it also validates contradictory instance-related statements within the ontology that is no longer relevant to recent practices. We determine a confidence value during the enrichment and validation process to ensure the stability of the enriched ontology. We implement the model and present a case study in herbal medicine domain. The result of the enrichment and validation process shows promising results. Moreover, we analyse how our proposed model contributes to the achievement of a richer and stable ontology.", "venue": "International Conference on Advanced Information Networking and Applications", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "onto", "onto", "onto", "onto", "onto", "onto", "ke", "onto", "onto", "onto", "ie", "ie", "ie", "onto"], "mention_counts": {"ke": 2, "onto": 11, "ie": 3}, "nlp_mention_counts": {"ke": 2, "ie": 3}, "ld_mention_counts": {"ke": 2, "onto": 11}, "relevance_score": 0.5550494160031108}, {"paperId": "0e5f4ce4c3ce2a208a5353825b1fd7223b89a50e", "url": "https://www.semanticscholar.org/paper/0e5f4ce4c3ce2a208a5353825b1fd7223b89a50e", "title": "ConTrOn: Continuously Trained Ontology based on Technical Data Sheets and Wikidata", "abstract": "In engineering projects involving various parts from global suppliers, one common task is to determine which parts are best suited for the project requirements. Information about specific parts' characteristics is published in so called data sheets. However, these data sheets are oftentimes only published in textual form, e.g., as a PDF. Hence, they have to be transformed into a machine-interpretable format. This transformation process still requires a lot of manual intervention and is prone to errors. Automated approaches make use of ontologies to capture the given domain and thus improve automated information extraction from the data sheets. However, ontologies rely solely on experiences and perspectives of their creators at the time of creation and cannot accumulate knowledge over time on their own. This paper presents ConTrOn -- Continuously Trained Ontology -- a system that automatically augments ontologies. ConTrOn tackles terminology problems by combining the knowledge extracted from data sheets with an ontology created by domain experts and external knowledge bases such as WordNet and Wikidata. To demonstrate how the enriched ontology can improve the information extraction process, we selected data sheets from spacecraft development as a use case. The evaluation results show that the amount of information extracted from data sheets based on ontologies is significantly increased after the ontology enrichment.", "venue": "ArXiv", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "onto", "onto", "ie", "onto", "ie", "ke", "onto", "ie", "onto", "kg", "onto", "onto"], "mention_counts": {"ke": 1, "onto": 9, "kg": 1, "ie": 3}, "nlp_mention_counts": {"ke": 1, "ie": 3}, "ld_mention_counts": {"ke": 1, "onto": 9, "kg": 1}, "relevance_score": 0.5311625932181889}, {"paperId": "1dcd743a7f370585d0b1faa6c790cd6a205b0e95", "url": "https://www.semanticscholar.org/paper/1dcd743a7f370585d0b1faa6c790cd6a205b0e95", "title": "NaturalOWL : Generating Texts from OWL Ontologies in Protege and in Second Life", "abstract": "NaturalOWL is an open-source natural language generation engine written in Java. It produces descriptions of individuals (e.g., items for sale, museum exhibits) and classes (e.g., types of exhibits) in English and Greek from OWL DL ontologies. The ontologies must have been annotated in RDF with linguistic and user modeling resources. We demonstrate a plug-in for Prot\u00e9g\u00e9 that can be used to produce these resources and to generate texts by invoking NaturalOWL. We also demonstrate how NaturalOWL can be used by robotic avatars in Second Life to describe the exhibits of virtual museums. NaturalOWL demonstrates the benefits of Natural Language Generation (NLG) on the Semantic Web. Organizations that need to publish information about objects, such as exhibits or products, can publish OWL ontologies instead of texts. NLG engines, embedded in browsers or Web servers, can then render the ontologies in multiple natural languages, whereas computer programs may access the ontologies directly.", "venue": "European Conference on Artificial Intelligence", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "sw", "onto", "onto", "nlg", "onto", "nlg", "onto", "onto", "nlg", "onto", "nlg", "rdf", "onto", "onto"], "mention_counts": {"sw": 1, "nlg": 4, "onto": 9, "rdf": 1}, "nlp_mention_counts": {"nlg": 4}, "ld_mention_counts": {"sw": 1, "onto": 9, "rdf": 1}, "relevance_score": 0.5311625932181889}, {"paperId": "3248121914962cf63116ed91cacede86dbb7cc94", "url": "https://www.semanticscholar.org/paper/3248121914962cf63116ed91cacede86dbb7cc94", "title": "Neural language models for the multilingual, transcultural, and multimodal Semantic Web", "abstract": "A vision of a truly multilingual Semantic Web has found strong support with the Linguistic Linked Open Data community. Standards, such as OntoLex-Lemon, highlight the importance of explicit linguistic modeling in relation to ontologies and knowledge graphs. Nevertheless, there is room for improvement in terms of automation, usability, and interoperability. Neural Language Models have achieved several breakthroughs and successes considerably beyond Natural Language Processing (NLP) tasks and recently also in terms of multimodal representations. Several paths naturally open up to port these successes to the Semantic Web, from automatically translating linguistic information associated with structured knowledge resources to multimodal question-answering with machine translation. Language is also an important vehicle for culture, an aspect that deserves considerably more attention. Building on existing approaches, this article envisions joint forces between Neural Language Models and Semantic Web technologies for multilingual, transcultural, and multimodal information access and presents open challenges and opportunities in this direction.", "venue": "Semantic Web", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "lod", "sw", "llod", "sw", "nlp", "sw", "mt", "sw", "nlp"], "mention_counts": {"onto": 1, "sw": 4, "lod": 1, "llod": 1, "nlp": 2, "mt": 1, "kg": 1}, "nlp_mention_counts": {"nlp": 2, "mt": 1}, "ld_mention_counts": {"onto": 1, "sw": 4, "lod": 1, "llod": 1, "kg": 1}, "relevance_score": 0.5294660559538056}, {"paperId": "875df92d3cd292eacb8e851cba945a2ead0846b3", "url": "https://www.semanticscholar.org/paper/875df92d3cd292eacb8e851cba945a2ead0846b3", "title": "Intelligent Learning for Knowledge Graph towards Geological Data", "abstract": "Knowledge graph (KG) as a popular semantic network has been widely used. It provides an effective way to describe semantic entities and their relationships by extending ontology in the entity level. This article focuses on the application of KG in the traditional geological field and proposes a novel method to construct KG. On the basis of natural language processing (NLP) and data mining (DM) algorithms, we analyze those key technologies for designing a KG towards geological data, including geological knowledge extraction and semantic association. Through this typical geological ontology extracting on a large number of geological documents and open linked data, the semantic interconnection is achieved, KG framework for geological data is designed, application system of KG towards geological data is constructed, and dynamic updating of the geological information is completed accordingly. Specifically, unsupervised intelligent learning method using linked open data is incorporated into the geological document preprocessing, which generates a geological domain vocabulary ultimately. Furthermore, some application cases in the KG system are provided to show the effectiveness and efficiency of our proposed intelligent learning approach for KG.", "venue": "Scientific Programming", "citationCount": 22, "fieldsOfStudy": ["Computer Science"], "mentions": ["kg", "onto", "lod", "nlp", "ke", "kg", "nlp", "onto", "ld", "lod"], "mention_counts": {"onto": 2, "ld": 1, "nlp": 2, "lod": 2, "ke": 1, "kg": 2}, "nlp_mention_counts": {"nlp": 2, "ke": 1}, "ld_mention_counts": {"onto": 2, "ld": 1, "ke": 1, "lod": 2, "kg": 2}, "relevance_score": 0.5294660559538056}, {"paperId": "c823521300431fc55649c2f290259d91b303afa1", "url": "https://www.semanticscholar.org/paper/c823521300431fc55649c2f290259d91b303afa1", "title": "Arabic-English Automatic Ontology Mapping Based on Machine Readable Dictionary", "abstract": "Ontologies are the backbone of the semantic web and allow software agents to interoperate effectively. An ontology is able to represent and to clarify concepts and inter-concept relationships and can be used as a framework to represent underlying domain concepts expressed in many different languages. One way to do this is by mapping Ontologies in different languages using an inter-lingual index. In this paper we present a new methodology for ontology mapping in different script human languages (Arabic/English). We identify the steps of extracting concepts on both ontologies and automatically mapping them based on Machine Readable Dictionary (MRD) and Word Sense Disambiguation (WSD) tools. The paper also discusses a unique tool that automatically extracts unmapped concepts and uses MRD and WSD to match them and create semantic bridges between the ontologies.", "venue": "2009 Mexican International Conference on Computer Science", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "onto", "onto", "wsd", "wsd", "onto", "wsd", "onto", "onto", "onto", "onto"], "mention_counts": {"sw": 1, "wsd": 3, "onto": 7}, "nlp_mention_counts": {"wsd": 3}, "ld_mention_counts": {"sw": 1, "onto": 7}, "relevance_score": 0.5294660559538056}, {"paperId": "03b144c391ab21049f9bcd7900e192cfe582ff29", "url": "https://www.semanticscholar.org/paper/03b144c391ab21049f9bcd7900e192cfe582ff29", "title": "Ontological Approach Based on Multi-Agent System for Indexing and Filtering Arabic Documents", "abstract": "In recent years, Automatic Natural Language Processing (ANLP) for Arabic language has received a great amount of attention for the development of several applications such as question answering, information retrieval and translation, etc. However, there are a few automated applications using Semantic Web technologies for retrieving Arabic-language documents despite the high demand and need for this content. In addition, the Arabic language presents serious challenges to researchers and developers of NLP applications. These challenges are due to the complexity of the morphological, syntactic and semantic characteristics specific to the Arabic text, which requires the use of semantic resources such as ontology. In our work, we propose a new approach based on ontology and multi-agent systems to index and filter Arabic documents. Our proposal is composed of five layers, each layer contains several agents: (1) Lexical Layer; (2) Syntactic Layer; (3) Semantic Layer; (4) Indexing Layer; and GUI/Interface Layer. Our Arabic ontology is manually constructed on the basis of schemes and their semantics meanings. We use also combination of Arabic WordNet contents and Arabic VerbNet in the process of constructing the ontology. We use the semantic similarity to find the relevant documents according to the user\u2019s queries. The aim of this paper is to study the effect of patterns in solving the problem of the semantic indexing system (SIS). The main objective is to improve the quality of the indexing process to ensure the accuracy of the information search of relevant documents based on us ers\u2019 multiword queries, and also to reduce indexing and search time. Indeed, our experiments are conducted on the basis of the combination of two Arab corpus: OSAC and SemEval. We compared our results Ontological Approach Based on Multi-Agent System for Indexing and Filtering Arabic Documents with Lucene index for the same data and, we found that our approach achieves much better results than the other. Subject Categories and Descriptors: [H.3.1 Content Analysis and Indexing]; [H.3.3 Information Search and Retrieval] ; [I.2.11 Distributed Artificial Intelligence]; Multiagent systems General Terms: Information Retrieval, Natural Language Processing, Arabic Ontology", "venue": "J. Digit. Inf. Manag.", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "onto", "nlp", "onto", "onto", "onto", "onto", "nlp", "sw", "onto"], "mention_counts": {"nlp": 3, "sw": 1, "onto": 7}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"sw": 1, "onto": 7}, "relevance_score": 0.5294660559538056}, {"paperId": "9e7a8b246757b9032e173f082ad616dfd64b655a", "url": "https://www.semanticscholar.org/paper/9e7a8b246757b9032e173f082ad616dfd64b655a", "title": "Revitalizing the Global Public Health Intelligence Network (GPHIN)", "abstract": "Objective:\u00a0 To rebuild the software that underpins the Global Public Health Intelligence Network using modern natural language processing techniques to support recent and future improvements in situational awareness capability. Introduction:\u00a0 The Global Public Health Intelligence Network is a non-traditional all-hazards multilingual surveillance system introduced in 1997 by the Government of Canada in collaboration with the World Health Organization. 1 \u00a0GPHIN software collects news articles, media releases, and incident reports and analyzes them for information about communicable diseases, natural disasters, product recalls, radiological events and other public health crises. Since 2016, the Public Health Agency of Canada (PHAC) and National Research Council Canada (NRC) have collaborated to replace GPHIN with a modular platform that incorporates modern natural language processing techniques to support more ambitious situational awareness goals. Methods:\u00a0 The updated GPHIN platform assembles several natural language processing tools to annotate incoming data in order to support situational awareness; broadly, GPHIN aims to extract knowledge from data. Data are collected in 10 languages and are machine translated to English. Several of the machine translation models use high performance neural networks. Language models are updated regularly and support external dictionaries for handling emerging domain-specific terms that might not yet exist in the parallel corpora used to train the models. All incoming documents are assigned a relevance score. Machine learning models estimate a score based on similarity to sets of known high-relevance and known low-relevance documents. PHAC analysts provide feedback on the scoring from time to time in the course of their work, and this feedback is used to periodically retrain scoring models. Documents are assigned keywords using two ontologies: an all-hazards multilingual taxonomy hand-compiled at PHAC, and the U.S. National Library of Medicine\u2019s Unified Medical Language System (UMLS). Categories are assigned probabilistically to incoming articles (e.g., human infectious diseases, animal infectious diseases, substance abuse, environmental hazards), largely using affinity scores that correspond to keywords. Dates and times are resolved to canonical forms, so that mentions like\u00a0 last Tuesday \u00a0get resolved to specific dates; this makes it possible to sequence data about a single event that are released at varying frequencies and with varying timeliness. Cities, states/provinces, and countries are identified in the documents, and gaps in the hierarchical geographic relationships are filled in. Locations are disambiguated based on collocations; the system distinguishes between and correctly resolves Ottawa, KS vs. Ottawa, ON, Canada, for example. Countries are displayed with their socio-economic population statistics (Gini coefficient, human development index, median age, and so on). The system attempts to detect and reconcile near-duplicate articles in order to handle instances where one article is released on a newswire and subsequently gets lightly edited and syndicated in dozens or hundreds of local papers; this improves the signal-to-noise ratio of the data in GPHIN for better productivity. Template-based reports (where the same document may get re-issued with a new number of cases but no other changes, for example) are still a challenge, but whitelisting tools reduce the false positive rate. The system provides tools for constructing arbitrarily detailed searches, tied to colour-coded maps and graphs that update on-the-fly, and offers short extractive summaries of each search result for easy filtering. GPHIN also generates topical knowledge graphs about sets of articles that seek to reveal surprising correlations in the data; for example, graphically reconciling and highlighting cases where several neighbouring countries all have reports of a mysterious disease and where a particular mosquito is mentioned. Next steps in the ongoing rejuvenation involve collating discrete articles and documents into narrative timelines that track an ongoing event: collecting all data about the spread of an infectious disease outbreak or perhaps the aftermath of an earthquake in the developing world. Our research is focussing on how to build line lists from such a stream of news articles about an event and how to detect important change points in the ongoing narrative. Results:\u00a0 The new GPHIN platform was launched in August 2016 in order to support syndromic surveillance activities for the Rio 2016 Olympics, and has been updated incrementally since then to offer further capabilities to professional users in 30 countries. Its modular construction supports current situational awareness activities as well as further research into advanced natural language processing techniques. Conclusions:\u00a0 We improved (and continue to improve) GPHIN with modern natural language processing techniques, including better translations, relevance scoring, categorization, near-duplicate detection, and improved data visualization tools, all towards the goal of more productive and more trustworthy situational awareness.", "venue": "Online Journal of Public Health Informatics", "citationCount": 11, "fieldsOfStudy": ["Computer Science"], "mentions": ["mt", "mt", "nlp", "nlp", "nlp", "nlp", "kg", "onto", "ke", "nlp"], "mention_counts": {"onto": 1, "ke": 1, "nlp": 5, "mt": 2, "kg": 1}, "nlp_mention_counts": {"nlp": 5, "ke": 1, "mt": 2}, "ld_mention_counts": {"kg": 1, "onto": 1, "ke": 1}, "relevance_score": 0.5294660559538056}, {"paperId": "f65fb80bd2e193a07f19e36567c103ff9d61d356", "url": "https://www.semanticscholar.org/paper/f65fb80bd2e193a07f19e36567c103ff9d61d356", "title": "Natural Language Processing as a Foundation of the Semantic Web", "abstract": "The main argument of this paper is that Natural Language Processing (NLP) does, and will continue to, underlie the Semantic Web (SW), including its initial construction from unstructured sources like the World Wide Web (WWW), whether its advocates realise this or not. Chiefly, we argue, such NLP activity is the only way up to a defensible notion of meaning at conceptual levels (in the original SW diagram) based on lower level empirical computations over usage. Our aim is definitely not to claim logic-bad, NLP-good in any simple-minded way, but to argue that the SW will be a fascinating interaction of these two methodologies, again like the WWW (which has been basically a field for statistical NLP research) but with deeper content. Only NLP technologies (and chiefly information extraction) will be able to provide the requisite RDF knowledge stores for the SW from existing unstructured text databases in the WWW, and in the vast quantities needed. There is no alternative at this point, since a wholly or mostly hand-crafted SW is also unthinkable, as is a SW built from scratch and without reference to the WWW. We also assume that, whatever the limitations on current SW representational power we have drawn attention to here, the SW will continue to grow in a distributed manner so as to serve the needs of scientists, even if it is not perfect. The WWW has already shown how an imperfect artefact can become indispensable.", "venue": "Foundations and Trends\u00ae in Web Science", "citationCount": 43, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "nlp", "nlp", "nlp", "sw", "sw", "nlp", "nlp", "nlp", "ie", "rdf"], "mention_counts": {"nlp": 7, "sw": 2, "rdf": 1, "ie": 1}, "nlp_mention_counts": {"nlp": 7, "ie": 1}, "ld_mention_counts": {"sw": 2, "rdf": 1}, "relevance_score": 0.5294660559538056}, {"paperId": "262efeac080867e71a4ba9d9fcddfd1bdd365988", "url": "https://www.semanticscholar.org/paper/262efeac080867e71a4ba9d9fcddfd1bdd365988", "title": "Data mining for building knowledge bases: techniques, architectures and applications", "abstract": "Abstract Data mining techniques for extracting knowledge from text have been applied extensively to applications including question answering, document summarisation, event extraction and trend monitoring. However, current methods have mainly been tested on small-scale customised data sets for specific purposes. The availability of large volumes of data and high-velocity data streams (such as social media feeds) motivates the need to automatically extract knowledge from such data sources and to generalise existing approaches to more practical applications. Recently, several architectures have been proposed for what we call knowledge mining: integrating data mining for knowledge extraction from unstructured text (possibly making use of a knowledge base), and at the same time, consistently incorporating this new information into the knowledge base. After describing a number of existing knowledge mining systems, we review the state-of-the-art literature on both current text mining methods (emphasising stream mining) and techniques for the construction and maintenance of knowledge bases. In particular, we focus on mining entities and relations from unstructured text data sources, entity disambiguation, entity linking and question answering. We conclude by highlighting general trends in knowledge mining research and identifying problems that require further research to enable more extensive use of knowledge bases.", "venue": "Knowledge engineering review (Print)", "citationCount": 13, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "kg", "kg", "kg", "ke", "kg", "kg", "ke"], "mention_counts": {"ke": 3, "kg": 5}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "kg": 5}, "relevance_score": 0.5294660559538056}, {"paperId": "ee59fd54380a309e31cfe595d0205398305e25a5", "url": "https://www.semanticscholar.org/paper/ee59fd54380a309e31cfe595d0205398305e25a5", "title": "Combining RDF Graph Data and Embedding Models for an Augmented Knowledge Graph", "abstract": "Vector embedding models have recently become popular for encoding both structured and unstructured data. In the context of knowledge graphs such models often serve as additional evidence supporting various tasks related to the knowledge base population: e.g., information extraction or link prediction to expand the original dataset. However, the embedding models themselves are often not used directly alongside structured data: they merely serve as additional evidence for structured knowledge extraction. In the metaphactory knowledge graph management platform, we use federated hybrid SPARQL queries for combining explicit information stated in the graph, implicit information from the associated embedding models, and information extracted using vector embeddings in a transparent way for the end user. In this paper we show how we integrated RDF data with vector space models to construct an augmented knowledge graph to be used in customer applications.", "venue": "The Web Conference", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["ie", "kg", "rdf", "kg", "ie", "rdf", "kg", "kg", "kg", "ke"], "mention_counts": {"kg": 5, "ke": 1, "rdf": 2, "ie": 2}, "nlp_mention_counts": {"ke": 1, "ie": 2}, "ld_mention_counts": {"kg": 5, "ke": 1, "rdf": 2}, "relevance_score": 0.5294660559538056}, {"paperId": "bd478853a2bf7a7cd51264a70c8bccfd4b22b8bc", "url": "https://www.semanticscholar.org/paper/bd478853a2bf7a7cd51264a70c8bccfd4b22b8bc", "title": "Partnering enhanced-NLP with semantic analysis in support of information extraction", "abstract": "Information extraction using Natural Language Processing (NLP) tools focuses on extracting explicitly stated information from textual material. This includes Named Entity Recognition (NER), which produces entities and some of the relationships that may exist among them. Intelligent analysis requires examining the entities in the context of the entire document. While some of the relationships among the recognized entities may be preserved during extraction, the overall context of a document may not be preserved. In order to perform intelligent analysis on the extracted information, we provide an ontology, which describes the domain of the extracted information, in addition to rules that govern the classification and interpretation of added elements. The ontology is at the core of an interactive system that assists analysts with the collection, extraction, organization, analysis and retrieval of information, with the topic of \"terrorism financing\" as a case study. User interaction provides valuable assistance in assigning meaning to extracted information. The system is designed as a set of tools to provide the user with the flexibility and power to ensure accurate inference. This case study demonstrates the information extraction features as well as the inference power that is supported by the ontology.", "venue": "ODiSE'10", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "ie", "onto", "nlp", "ie", "ie", "ie", "onto", "ie", "nlp"], "mention_counts": {"nlp": 3, "onto": 3, "ie": 5}, "nlp_mention_counts": {"nlp": 3, "ie": 5}, "ld_mention_counts": {"onto": 3}, "relevance_score": 0.5294660559538056}, {"paperId": "e63c2574d3f76312117b2be5e01cda4c25ab2ce3", "url": "https://www.semanticscholar.org/paper/e63c2574d3f76312117b2be5e01cda4c25ab2ce3", "title": "Extracting Knowledge from the Bible: A Comparison between the Old and the New Testament", "abstract": "The objective of this work is to present a comparison between the Old Testament and the New Testament in terms of knowledge extraction and ontology learning. It is a knows fact that these two books have many differences in term of size, practice of worship, prophecy and there is also a difference in the time period when they were written. By applying the ontology learning and knowledge extraction methods we were interested do see if these differences are revealed and what are the similarities among them. Ontology learning can be applied for the semantic analysis of text, in order to extract concepts, relations, which can be further used for automated summaries or critical comparison. Such activities are important in education as they can allow dynamic creation of content or analyses that can be further used in the educational process. Since ontology-learning methods require large corpus of unstructured data, we have chosen the Bible as source for the text. In this way, the new developed methods are validated, and they can be used successfully in other educational domains. The Bible is the religious text of Christians and Jews. The Bible contains a collection of scriptures that was written by many authors, at different time and locations. Computationally, the Bible contains semi-structured information due to its organized structure of scriptures and numbered chapters. We have used Text2Onto as the main tool in order to obtain the most relevant concepts from the New Testament and then from The Old Testament. After that we analyze the most relevant concepts and the range of similarity for each domain identified in the New Testament and in The Old Testament. We can mention that there are no studies reported in the literature using ontology extraction for this religious domain. Those methods can be employed for automatic generation of content that can be further used in the educational process.", "venue": "2019 International Conference on Automation, Computational and Technology Management (ICACTM)", "citationCount": 1, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "ke", "onto", "onto", "onto", "ke", "onto"], "mention_counts": {"ke": 3, "onto": 5}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 5}, "relevance_score": 0.5294660559538056}, {"paperId": "7210ae1cc8b14cc78c7665c47cb27ab933d36279", "url": "https://www.semanticscholar.org/paper/7210ae1cc8b14cc78c7665c47cb27ab933d36279", "title": "Knowledge Extraction of Cohort Characteristics in Research Publications", "abstract": "When healthcare providers review the results of a clinical trial study to understand its applicability to their practice, they typically analyze how well the characteristics of the study cohort correspond to those of the patients they see. We have previously created a study cohort ontology to standardize this information and make it accessible for knowledge-based decision support. The extraction of this information from research publications is challenging, however, given the wide variance in reporting cohort characteristics in a tabular representation. To address this issue, we have developed an ontology-enabled knowledge extraction pipeline for automatically constructing knowledge graphs from the cohort characteristics found in PDF-formatted research papers. We evaluated our approach using a training and test set of 41 research publications and found an overall accuracy of 83.3% in correctly assembling the knowledge graphs. Our research provides a promising approach for extracting knowledge more broadly from tabular information in research publications.", "venue": "AMIA", "citationCount": 0, "fieldsOfStudy": ["Computer Science", "Medicine"], "mentions": ["ke", "kg", "onto", "ke", "kg", "ke", "kg", "onto"], "mention_counts": {"ke": 3, "onto": 2, "kg": 3}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 2, "kg": 3}, "relevance_score": 0.5294660559538056}, {"paperId": "80fe5612a705bf7a65012ba30fe9e52fbc364a91", "url": "https://www.semanticscholar.org/paper/80fe5612a705bf7a65012ba30fe9e52fbc364a91", "title": "Knowledge Extraction Guided by Ontologies - Database Marketing Application", "abstract": "The knowledge extraction in large databases has being known as a long term and interactive project. In spite of such complexity and different options for the knowledge achievement, there is a research opportunity that could be explored, throughout the ontologies support. Then this support may be used for knowledge sharing and reuse. This paper describes a research of an ontological approach for leveraging semantic content of ontologies to improve knowledge extraction in a oil company marketing databases. We attain to analyze how ontologies and knowledge discovery process may interoperate and present our efforts to propose a possible framework for a formal integration.", "venue": "International Conference on Enterprise Information Systems", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "onto", "onto", "onto", "onto", "ke", "ke", "onto"], "mention_counts": {"ke": 3, "onto": 5}, "nlp_mention_counts": {"ke": 3}, "ld_mention_counts": {"ke": 3, "onto": 5}, "relevance_score": 0.5294660559538056}, {"paperId": "0e650e3df99fc8e8c129b81bfe51f58a98c31a6a", "url": "https://www.semanticscholar.org/paper/0e650e3df99fc8e8c129b81bfe51f58a98c31a6a", "title": "RDF Knowledge Graph Visualization From a Knowledge Extraction System", "abstract": "In this paper, we present a system to visualize RDF knowledge graphs. These graphs are obtained from a knowledge extraction system designed by GEOLSemantics. This extraction is performed using natural language processing and trigger detection. The user can visualize subgraphs by selecting some ontology features like concepts or individuals. The system is also multilingual, with the use of the annotated ontology in English, French, Arabic and Chinese.", "venue": "SumPre-HSWI@ESWC", "citationCount": 5, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "kg", "kg", "rdf", "nlp", "ke", "rdf", "onto", "ke"], "mention_counts": {"onto": 2, "nlp": 1, "ke": 2, "kg": 2, "rdf": 2}, "nlp_mention_counts": {"nlp": 1, "ke": 2}, "ld_mention_counts": {"kg": 2, "onto": 2, "ke": 2, "rdf": 2}, "relevance_score": 0.5294660559538056}, {"paperId": "2a3f4add59abbd1d640cbd9cefd03f6c835c2746", "url": "https://www.semanticscholar.org/paper/2a3f4add59abbd1d640cbd9cefd03f6c835c2746", "title": "Natural Language Generation in the context of the Semantic Web", "abstract": "Natural Language Generation (NLG) is concerned with transforming given content input into a natural language out- put, given some communicative goal. Although this input can take various forms and representations, it is the semantic/conceptual representations that have always been considered as the \"natural\" starting ground for NLG. Therefore, it is natural that the Se- mantic Web (SW), with its machine-processable representation of information with explicitly defined semantics, has attracted the interest of NLG practitioners from early on. We attempt to provide an overview of the main paradigms of NLG from SW data, emphasizing how the Semantic Web provides opportunities for the NLG community to improve their state-of-the-art ap- proaches whilst bringing about challenges that need to be addressed before we can speak of a real symbiosis between NLG and the Semantic Web.", "venue": "Semantic Web", "citationCount": 69, "fieldsOfStudy": ["Computer Science"], "mentions": ["sw", "sw", "nlg", "sw", "nlg", "nlg", "nlg", "nlg", "nlg", "nlg", "nlg"], "mention_counts": {"sw": 3, "nlg": 8}, "nlp_mention_counts": {"nlg": 8}, "ld_mention_counts": {"sw": 3}, "relevance_score": 0.5294660559538056}, {"paperId": "e010061f99bcdc008b93f1feaab4690c4cf0bc66", "url": "https://www.semanticscholar.org/paper/e010061f99bcdc008b93f1feaab4690c4cf0bc66", "title": "Extracting Structured Knowledge for Semantic Web by Mining Wikipedia", "abstract": "Since Wikipedia has become a huge scale database storing wide-range of human knowledge, it is a promising corpus for knowledge extraction. A considerable number of researches on Wikipedia mining have been conducted and the fact that Wikipedia is an invaluable corpus has been confirmed. Wikipedia's impressive characteristics are not limited to the scale, but also include the dense link structure, URI for word sense disambiguation, well structured Infoboxes, and the category tree. One of the popular approaches in Wikipedia Mining is to use Wikipedia's category tree as an ontology and a number of researchers proved that Wikipedia's categories are promising resources for ontology construction by showing significant results. In this work, we try to prove the capability of Wikipedia as a corpus for knowledge extraction and how it works in the Semantic Web environment. We show two achievements; Wikipedia Thesaurus, a huge scale association thesaurus by mining the Wikipedia's link structure, and Wikipedia Ontology, a Web ontology extracted by mining Wikipedia articles.", "venue": "International Workshop on the Semantic Web", "citationCount": 6, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "sw", "onto", "onto", "onto", "wsd", "ke", "ke", "sw"], "mention_counts": {"sw": 2, "wsd": 1, "onto": 4, "ke": 2}, "nlp_mention_counts": {"ke": 2, "wsd": 1}, "ld_mention_counts": {"sw": 2, "onto": 4, "ke": 2}, "relevance_score": 0.5294660559538056}, {"paperId": "19cb6142ba2e2cc252c6440928c39a7057beb79b", "url": "https://www.semanticscholar.org/paper/19cb6142ba2e2cc252c6440928c39a7057beb79b", "title": "Causal knowledge extraction by natural language processing in material science: a case study in chemical vapor deposition", "abstract": "Scientific publications written in natural language still play a central role as our knowledge source. However, due to the flood of publications, the literature survey process has become a highly time-consuming and tangled process, especially for novices of the discipline. Therefore, tools supporting the literature-survey process may help the individual scientist to explore new useful domains. Natural language processing (NLP) is expected as one of the promising techniques to retrieve, abstract, and extract knowledge. In this contribution, NLP is firstly applied to the literature of chemical vapor deposition (CVD), which is a sub-discipline of materials science and is a complex and interdisciplinary field of research involving chemists, physicists, engineers, and materials scientists. Causal knowledge extraction from the literature is demonstrated using NLP.", "venue": "Data Sci. J.", "citationCount": 4, "fieldsOfStudy": ["Computer Science"], "mentions": ["ke", "nlp", "nlp", "nlp", "ke", "ke", "nlp", "nlp"], "mention_counts": {"nlp": 5, "ke": 3}, "nlp_mention_counts": {"nlp": 5, "ke": 3}, "ld_mention_counts": {"ke": 3}, "relevance_score": 0.5294660559538056}, {"paperId": "7e5c9df1cfb8730295c80cbd8c94cba64baa7930", "url": "https://www.semanticscholar.org/paper/7e5c9df1cfb8730295c80cbd8c94cba64baa7930", "title": "Constituent vs Dependency Parsing-Based RDF Model Generation from Dengue Patients' Case Sheets", "abstract": "Electronic Health Record (EHR) systems in healthcare organisations are primarily maintained in isolation from each other that makes interoperability of unstructured(text) data stored in these EHR systems challenging in the healthcare domain. Similar information may be described using different terminologies by different applications that can be evaded by transforming the content into the Resource Description Framework (RDF) model that is interoperable amongst organisations. RDF requires a document\u2019s contents to be translated into a repository of triplets (subject, predicate, object) known as RDF statements. Natural Language Processing (NLP) techniques can help get actionable insights from these text data and create triplets for RDF model generation. This paper discusses two NLP-based approaches to generate the RDF models from unstructured patients\u2019 documents, namely dependency structure-based and constituent(phrase) structure-based parser. Models generated by both approaches are evaluated in two aspects: exhaustiveness of the represented knowledge and the model generation time. The precision measure is used to compute the models\u2019 exhaustiveness in terms of the number of facts that are transformed into RDF representations.", "venue": "J. Inf. Knowl. Manag.", "citationCount": 0, "fieldsOfStudy": ["Computer Science"], "mentions": ["rdf", "rdf", "nlp", "rdf", "rdf", "rdf", "nlp", "rdf", "nlp", "rdf", "rdf"], "mention_counts": {"nlp": 3, "rdf": 8}, "nlp_mention_counts": {"nlp": 3}, "ld_mention_counts": {"rdf": 8}, "relevance_score": 0.5294660559538056}, {"paperId": "7294b5fa7f85a565f51ea675e5cc55be3e395bf4", "url": "https://www.semanticscholar.org/paper/7294b5fa7f85a565f51ea675e5cc55be3e395bf4", "title": "Using a Natural Language Understanding System to Generate Semantic Web Content", "abstract": "We describe our research on automatically generating rich semantic annotations of text and making it available on the Semantic Web. In particular, we discuss the challenges involved in adapting the OntoSem natural language processing system for this purpose. OntoSem, an implementation of the theory of ontological semantics under continuous development for over fifteen years, uses a specia lly constructed NLP-oriented ontology and an ontologicalsemantic lexicon to translate English text into a custom ontology-motivated knowledge representation language, the language of text meaning representations (TMRs). OntoSem concentrates on a variety of ambiguity resolution tasks as well as processing unexpected input and reference. To adapt OntoSem\u2019s representation to the Semantic Web, we developed a translation system, OntoSem2OWL, between the TMR language into the Semantic Web language OWL. We next used OntoSem and OntoSem2OWL to support SemNews, an e xperimental web service that monitors RSS news sources, processes the summaries of the news stories and publishes a structured representation of the meaning of the text in the news story.", "venue": "International Journal on Semantic Web and Information Systems (IJSWIS)", "citationCount": 37, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "onto", "nlp", "sw", "nlu", "nlp", "sw", "sw", "sw", "onto", "onto"], "mention_counts": {"nlp": 2, "sw": 4, "onto": 4, "nlu": 1}, "nlp_mention_counts": {"nlp": 2, "nlu": 1}, "ld_mention_counts": {"sw": 4, "onto": 4}, "relevance_score": 0.5294660559538056}, {"paperId": "8cf13b1f64b6f97af9466fe078390e543480bb10", "url": "https://www.semanticscholar.org/paper/8cf13b1f64b6f97af9466fe078390e543480bb10", "title": "Practical World Modeling for NLP Applications", "abstract": "Practical NLP applications requiring semantic and pragmatic analysis of texts necessitate the construction of a world model, an ontology, to support interpretation of text elements. Constraints on world model elements serve as heuristics on the cooccurrence of lexical and other meanings in the text, facilitating both natural language understanding and generation. Propositional meanings (defined in the lexicon in terms of links to the world model) trickle down to the text meaning representation as instances of world model entities. Our primary objective in world modeling is to support multilingual applications, so constructing a language-independent ontology is crucial. The word sense view of ontology building leads to proliferation of concepts whenever words in different languages do not \"line-up\" (see EDR 1990), while using a core set of \"primitives\" is limited for large-scale applications, if shades of meaning are to be captured. In our environment, concept acquisition is guided by examining cross-linguistic evidence and representational trade-offs. In other large-scale ontology projects, the separation of lexical from conceptual knowledge is not always clear, as in the Cyc project at MCC, a knowledge base containing millions of facts about the world (Lenat and Guha, 1990), or the KT system (Dahlgren, 1988), which classifies commonsense knowledge for English words. In the DIONYSUS project at CMU, the world model, the lexicon and the text meaning representation are closely interconnected, in terms of their content and format. World modeling is supported by the ONTOS system, which consists of a) a constraint language, b) an ontology, or set of general concepts, c) a set of domain models and d) an intelligent knowledge acquisition interface. The basic features of the ONTOS constraint language are as follows (see Carlson & Nirenburg, 1990, for details). A world model is a collection of frames. A frame is a named set of slots, interpreted as an ontological concept (voluntary-olfactory-event, geopolitical-entity). A slot represents an ontological property (temperature, caused-by) and consists of a named set of facets. A facet is a named set of fillers. Facets refer to the status of property values, e.g.: value actual values of property (e.g., for concept instances) default typical value of a property sem set of\"legal\" values; akin to selectional restrictions", "venue": "ANLP", "citationCount": 3, "fieldsOfStudy": ["Computer Science"], "mentions": ["nlp", "onto", "nlp", "kg", "onto", "onto", "onto", "onto", "onto", "onto", "nlu"], "mention_counts": {"nlp": 2, "onto": 7, "kg": 1, "nlu": 1}, "nlp_mention_counts": {"nlp": 2, "nlu": 1}, "ld_mention_counts": {"kg": 1, "onto": 7}, "relevance_score": 0.5294660559538056}, {"paperId": "79d23fab9c14796a0364f7e6eba451dc4e294ae6", "url": "https://www.semanticscholar.org/paper/79d23fab9c14796a0364f7e6eba451dc4e294ae6", "title": "OntoSem: an Ontology Semantic Representation Methodology for Biomedical Domain", "abstract": "Ontologies are essential description tools for biomedical concepts and entities, supporting biomedical fundamental research such as semantic similarity analysis, protein-protein interaction prediction and so on. An increasing amount of ontology-like domain knowledge is published in scientific publications, meanwhile, advanced natural language processing (NLP) techniques have been widespread to extract information from text resources automatically, both of which facilitate the exploration of the semantic representation of biomedical ontologies. We propose a novel distributional semantic representation methodology based on the combination of two pre-trained and domain-specific word embedding tools, the non-contextualized Word2Vec and the context-dependent NCBI-blueBERT, to enhance the encoding ability for biomedical ontologies. Furthermore, we utilize a randomly initialized bidirectional LSTM to project the obtained word vector sequence to a fixed-length sentence vector, facilitating a flexible and uniform way for the computation of downstream tasks. We evaluate our method in two categories of tasks: the similarity access of ontology terms, and the ontology annotationbased protein-protein interaction classification. Experimental results demonstrate that our method provides encouraging results compared to the baselines in all tests. Our approach offers promising opportunities for representing ontologies semantics and in turn characterizing entities including proteins in biomedical research.", "venue": "2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)", "citationCount": 2, "fieldsOfStudy": ["Computer Science"], "mentions": ["onto", "nlp", "ie", "onto", "onto", "onto", "onto", "nlp", "onto", "onto", "onto"], "mention_counts": {"nlp": 2, "onto": 8, "ie": 1}, "nlp_mention_counts": {"nlp": 2, "ie": 1}, "ld_mention_counts": {"onto": 8}, "relevance_score": 0.5294660559538056}]